{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "from torchvision.models import vgg16, vgg19\n",
    "from torchvision import transforms\n",
    "import glob\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "from collections import namedtuple\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import figure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Image Transformation Network\n",
    "This is based on Justin C. Johnson's paper, [Perceptual Losses for Real-Time Style Transfer and Super-Resolution](https://cs.stanford.edu/people/jcjohns/papers/eccv16/JohnsonECCV16.pdf).\n",
    "Here, we are going to make the proposed network for generating the end image (as in the convolutional neural network the will learn weights in order to efficiently calculate how to create an image based on a style)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Architecture\n",
    "![Architecture](./images/transformation-network-table.PNG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        \n",
    "        # For the transformation network, the authors only used 3x3 convolutions\n",
    "        self.conv = nn.Conv2d(in_channels = self.in_channels,\n",
    "                               out_channels = self.out_channels,\n",
    "                               kernel_size = 3)\n",
    "        self.batch_norm = nn.InstanceNorm2d(self.out_channels, affine=True)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # First convolution\n",
    "        orig_x = x.clone()\n",
    "        x = self.conv(x)\n",
    "        x = self.batch_norm(x)\n",
    "        x = self.relu(x)\n",
    "        \n",
    "        # Second convolution\n",
    "        x = self.conv(x)\n",
    "        x = self.batch_norm(x)\n",
    "        \n",
    "        # Now add the original to the new one (and use center cropping)\n",
    "        # Calulate the different between the size of each feature (in terms \n",
    "        # of height/width) to get the center of the original feature\n",
    "        height_diff = orig_x.size()[2] - x.size()[2]\n",
    "        width_diff = orig_x.size()[3] - x.size()[3]\n",
    "        \n",
    "        # Add the original to the new (complete the residual block)\n",
    "        x = x + orig_x[:, :,\n",
    "                                 height_diff//2:(orig_x.size()[2] - height_diff//2), \n",
    "                                 width_diff//2:(orig_x.size()[3] - width_diff//2)]\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageTransformationNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ImageTransformationNetwork, self).__init__()\n",
    "        # Use reflection padding to keep the end shape\n",
    "        self.ref_pad = nn.ReflectionPad2d(40)\n",
    "        \n",
    "        # Initial convolutions\n",
    "        self.conv1 = nn.Conv2d(in_channels = 3,\n",
    "                               out_channels = 32,\n",
    "                               kernel_size = 9,\n",
    "                               padding = 6,\n",
    "                               padding_mode = 'reflect')\n",
    "        \n",
    "        self.conv2 = nn.Conv2d(in_channels = 32,\n",
    "                               out_channels = 64,\n",
    "                               kernel_size = 3,\n",
    "                               stride = 2)\n",
    "        \n",
    "        self.conv3 = nn.Conv2d(in_channels = 64,\n",
    "                               out_channels = 128,\n",
    "                               kernel_size = 3,\n",
    "                               stride = 2)\n",
    "        \n",
    "        # Residual Blocks\n",
    "        self.resblock1 = ResidualBlock(in_channels = 128,\n",
    "                                       out_channels = 128)\n",
    "        \n",
    "        self.resblock2 = ResidualBlock(in_channels = 128,\n",
    "                                       out_channels = 128)\n",
    "        \n",
    "        self.resblock3 = ResidualBlock(in_channels = 128,\n",
    "                                       out_channels = 128)\n",
    "        \n",
    "        self.resblock4 = ResidualBlock(in_channels = 128,\n",
    "                                       out_channels = 128)\n",
    "        \n",
    "        self.resblock5 = ResidualBlock(in_channels = 128,\n",
    "                                       out_channels = 128)\n",
    "        \n",
    "        # Transpose convoltutions\n",
    "        self.trans_conv1 = nn.ConvTranspose2d(in_channels=128,\n",
    "                                             out_channels=64,\n",
    "                                             kernel_size=2,\n",
    "                                             stride=2)\n",
    "        \n",
    "        self.trans_conv2 = nn.ConvTranspose2d(in_channels=64,\n",
    "                                              out_channels=32,\n",
    "                                              kernel_size=2,\n",
    "                                              stride=2)\n",
    "        \n",
    "        # End with one last convolution\n",
    "        self.conv4 = nn.Conv2d(in_channels = 32,\n",
    "                               out_channels = 3,\n",
    "                               kernel_size = 9,\n",
    "                               padding = 4,\n",
    "                               padding_mode = 'reflect')\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Apply reflection padding\n",
    "        x = self.ref_pad(x)\n",
    "        \n",
    "        # Apply the initial convolutions\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.conv3(x)\n",
    "        \n",
    "        # Apply the residual blocks\n",
    "        x = self.resblock1(x)\n",
    "        x = self.resblock2(x)\n",
    "        x = self.resblock3(x)\n",
    "        x = self.resblock4(x)\n",
    "        x = self.resblock5(x)        \n",
    "        \n",
    "        #  Apply the transpose convolutions\n",
    "        x = self.trans_conv1(x)\n",
    "        x = self.trans_conv2(x)\n",
    "        \n",
    "        # Apply the final convolution\n",
    "        x = self.conv4(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 128, 80, 80])\n"
     ]
    }
   ],
   "source": [
    "# Test to confirm the residual network works\n",
    "resblock = ResidualBlock(128, 128)\n",
    "test = torch.randn(2, 128, 84, 84)\n",
    "out = resblock(test)\n",
    "print(out.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3, 256, 256])\n"
     ]
    }
   ],
   "source": [
    "# Test to confirm the transormational network works\n",
    "transformation_net = ImageTransformationNetwork()\n",
    "test = torch.randn(2, 3, 256, 256)\n",
    "out = transformation_net(test)\n",
    "print(out.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cost Functions\n",
    "Now we must implement the different cost functions\n",
    "\n",
    "Style Relu Indices: 3, 8, 15, 22  \n",
    "Content Relu Index: 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ForwardVGG19(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ForwardVGG19, self).__init__()\n",
    "        features = list(vgg19(pretrained=True).features)\n",
    "        self.features = nn.ModuleList(features).eval()\n",
    "        \n",
    "    def forward(self, x, style):\n",
    "        results = []\n",
    "        for i, model in enumerate(self.features):\n",
    "            x = model(x)\n",
    "            if style:\n",
    "                if i in {3, 8, 15, 22}:\n",
    "                    results.append(x)\n",
    "            \n",
    "            else:\n",
    "                if i == 15:\n",
    "                    results.append(x)\n",
    "        \n",
    "        return results\n",
    "\n",
    "forward_vgg = ForwardVGG19()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_batch(batch):\n",
    "    \"\"\"\n",
    "    Before we send an image into the VGG16, we have to normalize it\n",
    "    \"\"\"\n",
    "    vgg_means = [123.68, 116.779, 103.94]\n",
    "    ret = torch.zeros(*batch.size())\n",
    "    ret[:, 0, :, :] = batch[:, 0, :, :] - vgg_means[0]\n",
    "    ret[:, 1, :, :] = batch[:, 1, :, :] - vgg_means[1]\n",
    "    ret[:, 2, :, :] = batch[:, 2, :, :] - vgg_means[2]\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def denormalize_batch(batch):\n",
    "    vgg_means = [123.68, 116.779, 103.94]\n",
    "    ret = torch.zeros(*batch.size())\n",
    "    ret[:, 0, :, :] = batch[:, 0, :, :] + vgg_means[0]\n",
    "    ret[:, 1, :, :] = batch[:, 1, :, :] + vgg_means[1]\n",
    "    ret[:, 2, :, :] = batch[:, 2, :, :] + vgg_means[2]\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_noise(batch):\n",
    "    \"\"\"\n",
    "    For the input image, we have to add noise so that the loss between the content image and \n",
    "    input image is not 0\n",
    "    \"\"\"\n",
    "    mean = 0.0\n",
    "    std = 10.0\n",
    "    ret = batch + np.random.normal(mean, std, batch.shape)\n",
    "    ret = np.clip(batch, 0, 255)\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gram(matrix):\n",
    "    \"\"\"\n",
    "    Computes the gram matrix\n",
    "    \"\"\"\n",
    "    batches, channels, height, width = matrix.size()\n",
    "    return (torch.matmul(matrix.view(batches, channels, -1),\n",
    "            torch.transpose(matrix.view(batches, channels, -1), 1, 2)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_fro_norm(matrix):\n",
    "    \"\"\"\n",
    "    Computes the frobenius norm\n",
    "    \"\"\"\n",
    "    batches, height, width = matrix.size()\n",
    "    return torch.norm(matrix.view(batches, 1, -1), 'fro', dim=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def content_cost(input, target):\n",
    "    # First normalize both the input and target (preprocess for VGG16)\n",
    "    input_norm = normalize_batch(input)\n",
    "    target_norm = normalize_batch(target)\n",
    "\n",
    "    input_layers = forward_vgg(input_norm, False)\n",
    "    target_layers = forward_vgg(target_norm, False)\n",
    "\n",
    "    accumulated_loss = 0\n",
    "    for layer in range(len(input_layers)):\n",
    "        batches, channels, height, width = input_layers[layer].size()\n",
    "        accumulated_loss = accumulated_loss + (1/(channels * height * width)) * \\\n",
    "                            (torch.sum(torch.square(input_layers[layer] - target_layers[layer])))\n",
    "    \n",
    "    return accumulated_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def style_cost(input, target):\n",
    "    # First normalize both the input and target (preprocess for VGG16)\n",
    "    input_norm = normalize_batch(input)\n",
    "    target_norm = normalize_batch(target)\n",
    "\n",
    "    input_layers = forward_vgg(input_norm, True)\n",
    "    target_layers = forward_vgg(target_norm, True)\n",
    "    \n",
    "    accumulated_loss = 0\n",
    "    for layer in range(len(input_layers)):\n",
    "        batches, channels, height, width = input_layers[layer].size()\n",
    "        accumulated_loss = accumulated_loss + 0.25 * ((1/(4 * channels**2 * (height * width) ** 2)) * \\\n",
    "                            torch.sum(torch.square(compute_gram(input_layers[layer]) -\n",
    "                                                    compute_gram(target_layers[layer]))))\n",
    "    \n",
    "    return accumulated_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def total_variation_cost(input):\n",
    "    tvloss = (\n",
    "        torch.sum(torch.abs(input[:, :, :, :-1] - input[:, :, :, 1:])) + \n",
    "        torch.sum(torch.abs(input[:, :, :-1, :] - input[:, :, 1:, :]))\n",
    "    )\n",
    "    return tvloss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def total_cost(input, targets):\n",
    "    REG_TV = 1e-4\n",
    "    REG_STYLE = 1e-4\n",
    "    REG_CONTENT = 1e-6\n",
    "    content, style = targets\n",
    "    closs = content_cost(input, content)\n",
    "    sloss = style_cost(input, style)\n",
    "    tvloss = total_variation_cost(input)\n",
    "    \n",
    "    return REG_CONTENT * closs + REG_STYLE * sloss + REG_TV * tvloss\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "IMG_DIMENSIONS = (256, 256)\n",
    "DATA = list(glob.iglob('data/content_images/*'))\n",
    "STYLE_IMAGE = np.asarray(Image.open('data/style_image.png').resize(IMG_DIMENSIONS)).transpose(2, 0, 1)[0:3]\n",
    "\n",
    "# Make the style image a batch and convert\n",
    "STYLE_IMAGE = STYLE_IMAGE.reshape(1, 3, 256, 256)\n",
    "MAX_TRAIN = 80000\n",
    "MAX_VAL = 81000\n",
    "TOTAL_DATA = len(DATA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 4 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def show_sample_images():\n",
    "    f, axarr = plt.subplots(2,2)\n",
    "    \n",
    "    # Show colored images\n",
    "    axarr[0,0].imshow(np.asarray(Image.open(DATA[0]).resize(IMG_DIMENSIONS)))\n",
    "    axarr[0,1].imshow(np.asarray(Image.open(DATA[4]).resize(IMG_DIMENSIONS)))\n",
    "    axarr[1,0].imshow(np.asarray(Image.open(DATA[8]).resize(IMG_DIMENSIONS)))\n",
    "    \n",
    "    # Grayscale example\n",
    "    grayscale = np.asarray(Image.open(DATA[13]).resize(IMG_DIMENSIONS))\n",
    "    grayscale = np.stack((grayscale, grayscale, grayscale)).transpose(1, 2, 0)\n",
    "    axarr[1,1].imshow(grayscale)\n",
    "    \n",
    "show_sample_images()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1fc284af908>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Show the chosen style image\n",
    "plt.imshow(STYLE_IMAGE[0].transpose(1, 2, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_training_batch(current_batch, batch_size, set_type):\n",
    "    \"\"\"\n",
    "    Load different batches of data (essentially a custom data loader for training, validation, and testing)\n",
    "    \"\"\"\n",
    "    # The initial position is where we want to start getting the batch\n",
    "    # So it is the starting index of the batch\n",
    "    initial_pos = current_batch * batch_size\n",
    "    \n",
    "    # List to store the images\n",
    "    images = []\n",
    "    \n",
    "    # Make sure the batch is within the [0, MAX_TRAIN]\n",
    "    if set_type == 'train':\n",
    "        if initial_pos + batch_size > MAX_TRAIN:\n",
    "            batch_size = MAX_TRAIN - initial_pos\n",
    "    \n",
    "    # Make sure the batch is within the [MAX_TRAIN, MAX_VAL]\n",
    "    elif set_type == 'val':\n",
    "        initial_pos = MAX_TRAIN + initial_pos\n",
    "        if initial_pos + batch_size > MAX_VAL:\n",
    "            batch_size = MAX_VAL - initial_pos\n",
    "    \n",
    "    # Make sure the batch is within the [MAX_VAL, TOTAL_DATA]\n",
    "    elif set_type == 'test':\n",
    "        initial_pos = MAX_VAL + initial_pos\n",
    "        if initial_pos + batch_size > TOTAL_DATA:\n",
    "            batch_size = TOTAL_DATA - initial_pos\n",
    "\n",
    "    for f in DATA[initial_pos:initial_pos + batch_size]:\n",
    "        # Resize the image to 256 x 256\n",
    "        image = np.asarray(Image.open(f).resize(IMG_DIMENSIONS))\n",
    "        \n",
    "        # If the image is grayscale, stack the image 3 times to get 3 channels\n",
    "        if image.shape == IMG_DIMENSIONS:\n",
    "            image = np.stack((image, image, image))\n",
    "            images.append(image)\n",
    "            continue\n",
    "            \n",
    "        # Transpose the image to have channels first\n",
    "        image = image.transpose(2, 0, 1)\n",
    "        images.append(image)\n",
    "    \n",
    "    return np.array(images)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 4\n",
    "STYLE_IMAGE_TENSOR = torch.from_numpy(np.copy(STYLE_IMAGE)).float()\n",
    "transformation_net = ImageTransformationNetwork()\n",
    "opt = optim.Adam(transformation_net.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transformation_net.load_state_dict(torch.load('pause.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Batch: 14854 Loss: 3463.666992\n",
      "Training Batch: 14855 Loss: 3617.374512\n",
      "Training Batch: 14856 Loss: 3451.955078\n",
      "Training Batch: 14857 Loss: 3394.084473\n",
      "Training Batch: 14858 Loss: 3656.362793\n",
      "Training Batch: 14859 Loss: 3521.206299\n",
      "Training Batch: 14860 Loss: 3906.015869\n",
      "Training Batch: 14861 Loss: 3944.203613\n",
      "Training Batch: 14862 Loss: 3484.304932\n",
      "Training Batch: 14863 Loss: 4305.142578\n",
      "Training Batch: 14864 Loss: 4087.800293\n",
      "Training Batch: 14865 Loss: 3826.578125\n",
      "Training Batch: 14866 Loss: 3954.672852\n",
      "Training Batch: 14867 Loss: 3599.393066\n",
      "Training Batch: 14868 Loss: 3580.975098\n",
      "Training Batch: 14869 Loss: 3855.296875\n",
      "Training Batch: 14870 Loss: 3457.821777\n",
      "Training Batch: 14871 Loss: 3538.825684\n",
      "Training Batch: 14872 Loss: 3459.415039\n",
      "Training Batch: 14873 Loss: 3488.235596\n",
      "Training Batch: 14874 Loss: 3641.508057\n",
      "Training Batch: 14875 Loss: 3456.867676\n",
      "Training Batch: 14876 Loss: 3437.383789\n",
      "Training Batch: 14877 Loss: 3452.706299\n",
      "Training Batch: 14878 Loss: 3553.733398\n",
      "Training Batch: 14879 Loss: 3428.774902\n",
      "Training Batch: 14880 Loss: 3472.968750\n",
      "Training Batch: 14881 Loss: 3485.439453\n",
      "Training Batch: 14882 Loss: 3469.503906\n",
      "Training Batch: 14883 Loss: 3480.684814\n",
      "Training Batch: 14884 Loss: 3483.493164\n",
      "Training Batch: 14885 Loss: 3529.742432\n",
      "Training Batch: 14886 Loss: 3581.752441\n",
      "Training Batch: 14887 Loss: 3567.061523\n",
      "Training Batch: 14888 Loss: 3480.064453\n",
      "Training Batch: 14889 Loss: 3447.529297\n",
      "Training Batch: 14890 Loss: 3595.577148\n",
      "Training Batch: 14891 Loss: 3568.428711\n",
      "Training Batch: 14892 Loss: 3403.949707\n",
      "Training Batch: 14893 Loss: 3473.777832\n",
      "Training Batch: 14894 Loss: 3564.751465\n",
      "Training Batch: 14895 Loss: 3604.485352\n",
      "Training Batch: 14896 Loss: 3495.820801\n",
      "Training Batch: 14897 Loss: 3456.945557\n",
      "Training Batch: 14898 Loss: 3503.854248\n",
      "Training Batch: 14899 Loss: 3449.120850\n",
      "Training Batch: 14900 Loss: 3588.727783\n",
      "Training Batch: 14901 Loss: 3452.287598\n",
      "Training Batch: 14902 Loss: 3460.045654\n",
      "Training Batch: 14903 Loss: 3549.857422\n",
      "Training Batch: 14904 Loss: 3444.538818\n",
      "Training Batch: 14905 Loss: 3513.292480\n",
      "Training Batch: 14906 Loss: 3560.057617\n",
      "Training Batch: 14907 Loss: 3598.003906\n",
      "Training Batch: 14908 Loss: 3614.794922\n",
      "Training Batch: 14909 Loss: 3492.947510\n",
      "Training Batch: 14910 Loss: 3346.897705\n",
      "Training Batch: 14911 Loss: 3652.616943\n",
      "Training Batch: 14912 Loss: 3502.816895\n",
      "Training Batch: 14913 Loss: 3423.667969\n",
      "Training Batch: 14914 Loss: 3460.804932\n",
      "Training Batch: 14915 Loss: 3469.986816\n",
      "Training Batch: 14916 Loss: 3469.037598\n",
      "Training Batch: 14917 Loss: 3426.473633\n",
      "Training Batch: 14918 Loss: 3648.835205\n",
      "Training Batch: 14919 Loss: 3701.566895\n",
      "Training Batch: 14920 Loss: 3493.588379\n",
      "Training Batch: 14921 Loss: 3510.125977\n",
      "Training Batch: 14922 Loss: 3474.224854\n",
      "Training Batch: 14923 Loss: 3335.021973\n",
      "Training Batch: 14924 Loss: 3372.461914\n",
      "Training Batch: 14925 Loss: 3458.771729\n",
      "Training Batch: 14926 Loss: 3544.971680\n",
      "Training Batch: 14927 Loss: 3830.979492\n",
      "Training Batch: 14928 Loss: 3569.824951\n",
      "Training Batch: 14929 Loss: 3912.328857\n",
      "Training Batch: 14930 Loss: 3589.079590\n",
      "Training Batch: 14931 Loss: 3607.388428\n",
      "Training Batch: 14932 Loss: 3515.514160\n",
      "Training Batch: 14933 Loss: 3518.481445\n",
      "Training Batch: 14934 Loss: 3511.391113\n",
      "Training Batch: 14935 Loss: 3626.031738\n",
      "Training Batch: 14936 Loss: 3513.239014\n",
      "Training Batch: 14937 Loss: 3540.583008\n",
      "Training Batch: 14938 Loss: 3715.049072\n",
      "Training Batch: 14939 Loss: 3616.114258\n",
      "Training Batch: 14940 Loss: 3692.109863\n",
      "Training Batch: 14941 Loss: 3829.800293\n",
      "Training Batch: 14942 Loss: 3580.021729\n",
      "Training Batch: 14943 Loss: 3528.909180\n",
      "Training Batch: 14944 Loss: 3631.954102\n",
      "Training Batch: 14945 Loss: 3378.887207\n",
      "Training Batch: 14946 Loss: 3571.329102\n",
      "Training Batch: 14947 Loss: 3502.726562\n",
      "Training Batch: 14948 Loss: 3449.305664\n",
      "Training Batch: 14949 Loss: 3487.979004\n",
      "Training Batch: 14950 Loss: 3392.536133\n",
      "Training Batch: 14951 Loss: 3951.816895\n",
      "Training Batch: 14952 Loss: 4184.659180\n",
      "Training Batch: 14953 Loss: 3523.408447\n",
      "Training Batch: 14954 Loss: 3459.098145\n",
      "Training Batch: 14955 Loss: 3572.073486\n",
      "Training Batch: 14956 Loss: 3657.166504\n",
      "Training Batch: 14957 Loss: 3635.380615\n",
      "Training Batch: 14958 Loss: 3612.774414\n",
      "Training Batch: 14959 Loss: 3558.779785\n",
      "Training Batch: 14960 Loss: 3798.879883\n",
      "Training Batch: 14961 Loss: 4003.092773\n",
      "Training Batch: 14962 Loss: 3793.319336\n",
      "Training Batch: 14963 Loss: 3593.394287\n",
      "Training Batch: 14964 Loss: 3383.674805\n",
      "Training Batch: 14965 Loss: 3571.680176\n",
      "Training Batch: 14966 Loss: 3462.672363\n",
      "Training Batch: 14967 Loss: 3552.307129\n",
      "Training Batch: 14968 Loss: 3477.131348\n",
      "Training Batch: 14969 Loss: 3635.837891\n",
      "Training Batch: 14970 Loss: 3524.388184\n",
      "Training Batch: 14971 Loss: 3487.547852\n",
      "Training Batch: 14972 Loss: 3517.896973\n",
      "Training Batch: 14973 Loss: 3378.431885\n",
      "Training Batch: 14974 Loss: 3524.279541\n",
      "Training Batch: 14975 Loss: 3491.395508\n",
      "Training Batch: 14976 Loss: 3609.738770\n",
      "Training Batch: 14977 Loss: 3737.018066\n",
      "Training Batch: 14978 Loss: 3554.314209\n",
      "Training Batch: 14979 Loss: 3433.914551\n",
      "Training Batch: 14980 Loss: 3520.962402\n",
      "Training Batch: 14981 Loss: 3457.549805\n",
      "Training Batch: 14982 Loss: 3545.993652\n",
      "Training Batch: 14983 Loss: 3491.360107\n",
      "Training Batch: 14984 Loss: 3359.230713\n",
      "Training Batch: 14985 Loss: 3932.791992\n",
      "Training Batch: 14986 Loss: 3467.939697\n",
      "Training Batch: 14987 Loss: 3453.332031\n",
      "Training Batch: 14988 Loss: 3447.947754\n",
      "Training Batch: 14989 Loss: 4120.567871\n",
      "Training Batch: 14990 Loss: 3630.185059\n",
      "Training Batch: 14991 Loss: 3501.647461\n",
      "Training Batch: 14992 Loss: 3483.863770\n",
      "Training Batch: 14993 Loss: 3430.126465\n",
      "Training Batch: 14994 Loss: 3448.147461\n",
      "Training Batch: 14995 Loss: 3558.099609\n",
      "Training Batch: 14996 Loss: 3538.548828\n",
      "Training Batch: 14997 Loss: 3465.108887\n",
      "Training Batch: 14998 Loss: 3564.113037\n",
      "Training Batch: 14999 Loss: 3560.594238\n",
      "Training Batch: 15000 Loss: 3857.756836\n",
      "Training Batch: 15001 Loss: 3541.032227\n",
      "Training Batch: 15002 Loss: 3558.563477\n",
      "Training Batch: 15003 Loss: 3394.854980\n",
      "Training Batch: 15004 Loss: 3525.609375\n",
      "Training Batch: 15005 Loss: 3540.204102\n",
      "Training Batch: 15006 Loss: 3454.291992\n",
      "Training Batch: 15007 Loss: 3471.020752\n",
      "Training Batch: 15008 Loss: 3462.331543\n",
      "Training Batch: 15009 Loss: 3426.488281\n",
      "Training Batch: 15010 Loss: 3459.999512\n",
      "Training Batch: 15011 Loss: 3515.205566\n",
      "Training Batch: 15012 Loss: 3415.043457\n",
      "Training Batch: 15013 Loss: 3507.375000\n",
      "Training Batch: 15014 Loss: 3474.873535\n",
      "Training Batch: 15015 Loss: 3491.285156\n",
      "Training Batch: 15016 Loss: 3560.174316\n",
      "Training Batch: 15017 Loss: 3526.874512\n",
      "Training Batch: 15018 Loss: 3546.381836\n",
      "Training Batch: 15019 Loss: 3419.594238\n",
      "Training Batch: 15020 Loss: 3599.703857\n",
      "Training Batch: 15021 Loss: 3436.075928\n",
      "Training Batch: 15022 Loss: 3759.820801\n",
      "Training Batch: 15023 Loss: 3492.112793\n",
      "Training Batch: 15024 Loss: 3659.188721\n",
      "Training Batch: 15025 Loss: 3424.120605\n",
      "Training Batch: 15026 Loss: 3592.128906\n",
      "Training Batch: 15027 Loss: 3442.675781\n",
      "Training Batch: 15028 Loss: 3851.846191\n",
      "Training Batch: 15029 Loss: 4004.822266\n",
      "Training Batch: 15030 Loss: 3429.965820\n",
      "Training Batch: 15031 Loss: 3665.228760\n",
      "Training Batch: 15032 Loss: 3606.904297\n",
      "Training Batch: 15033 Loss: 3718.522461\n",
      "Training Batch: 15034 Loss: 3508.769287\n",
      "Training Batch: 15035 Loss: 3576.492920\n",
      "Training Batch: 15036 Loss: 3471.364990\n",
      "Training Batch: 15037 Loss: 3527.787598\n",
      "Training Batch: 15038 Loss: 3461.461670\n",
      "Training Batch: 15039 Loss: 3412.860596\n",
      "Training Batch: 15040 Loss: 3517.179199\n",
      "Training Batch: 15041 Loss: 3429.570557\n",
      "Training Batch: 15042 Loss: 3520.652832\n",
      "Training Batch: 15043 Loss: 3488.739746\n",
      "Training Batch: 15044 Loss: 3395.912109\n",
      "Training Batch: 15045 Loss: 3498.574707\n",
      "Training Batch: 15046 Loss: 3447.390625\n",
      "Training Batch: 15047 Loss: 3405.682861\n",
      "Training Batch: 15048 Loss: 3485.028809\n",
      "Training Batch: 15049 Loss: 3591.710938\n",
      "Training Batch: 15050 Loss: 3450.803467\n",
      "Training Batch: 15051 Loss: 3507.453125\n",
      "Training Batch: 15052 Loss: 3477.873047\n",
      "Training Batch: 15053 Loss: 3567.704590\n",
      "Training Batch: 15054 Loss: 3588.891113\n",
      "Training Batch: 15055 Loss: 3479.374023\n",
      "Training Batch: 15056 Loss: 3467.940430\n",
      "Training Batch: 15057 Loss: 3477.384521\n",
      "Training Batch: 15058 Loss: 3358.023926\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Batch: 15059 Loss: 3708.360352\n",
      "Training Batch: 15060 Loss: 3603.982422\n",
      "Training Batch: 15061 Loss: 3520.444824\n",
      "Training Batch: 15062 Loss: 3593.112793\n",
      "Training Batch: 15063 Loss: 3636.582764\n",
      "Training Batch: 15064 Loss: 3522.096680\n",
      "Training Batch: 15065 Loss: 3480.205322\n",
      "Training Batch: 15066 Loss: 3328.469238\n",
      "Training Batch: 15067 Loss: 3404.954102\n",
      "Training Batch: 15068 Loss: 3754.780762\n",
      "Training Batch: 15069 Loss: 3546.344727\n",
      "Training Batch: 15070 Loss: 3632.434326\n",
      "Training Batch: 15071 Loss: 3350.316895\n",
      "Training Batch: 15072 Loss: 3459.780762\n",
      "Training Batch: 15073 Loss: 3415.013428\n",
      "Training Batch: 15074 Loss: 3411.379395\n",
      "Training Batch: 15075 Loss: 3484.295410\n",
      "Training Batch: 15076 Loss: 3602.218750\n",
      "Training Batch: 15077 Loss: 3438.988281\n",
      "Training Batch: 15078 Loss: 3420.512207\n",
      "Training Batch: 15079 Loss: 3335.569336\n",
      "Training Batch: 15080 Loss: 3465.136475\n",
      "Training Batch: 15081 Loss: 3661.194824\n",
      "Training Batch: 15082 Loss: 3487.320068\n",
      "Training Batch: 15083 Loss: 3529.174072\n",
      "Training Batch: 15084 Loss: 3580.964355\n",
      "Training Batch: 15085 Loss: 3477.860352\n",
      "Training Batch: 15086 Loss: 3504.240479\n",
      "Training Batch: 15087 Loss: 3481.605957\n",
      "Training Batch: 15088 Loss: 3567.316895\n",
      "Training Batch: 15089 Loss: 3466.625000\n",
      "Training Batch: 15090 Loss: 3707.407715\n",
      "Training Batch: 15091 Loss: 3471.826172\n",
      "Training Batch: 15092 Loss: 3512.642090\n",
      "Training Batch: 15093 Loss: 3451.675049\n",
      "Training Batch: 15094 Loss: 3442.561035\n",
      "Training Batch: 15095 Loss: 3443.884277\n",
      "Training Batch: 15096 Loss: 3558.362549\n",
      "Training Batch: 15097 Loss: 3421.960449\n",
      "Training Batch: 15098 Loss: 3488.548340\n",
      "Training Batch: 15099 Loss: 3597.598389\n",
      "Training Batch: 15100 Loss: 3390.275635\n",
      "Training Batch: 15101 Loss: 3518.860352\n",
      "Training Batch: 15102 Loss: 3509.167480\n",
      "Training Batch: 15103 Loss: 3489.521484\n",
      "Training Batch: 15104 Loss: 3564.458984\n",
      "Training Batch: 15105 Loss: 3540.507324\n",
      "Training Batch: 15106 Loss: 3603.685303\n",
      "Training Batch: 15107 Loss: 3434.488281\n",
      "Training Batch: 15108 Loss: 3447.549805\n",
      "Training Batch: 15109 Loss: 3470.009277\n",
      "Training Batch: 15110 Loss: 3471.522949\n",
      "Training Batch: 15111 Loss: 3568.837402\n",
      "Training Batch: 15112 Loss: 3554.279297\n",
      "Training Batch: 15113 Loss: 3500.866943\n",
      "Training Batch: 15114 Loss: 3362.560059\n",
      "Training Batch: 15115 Loss: 3488.812988\n",
      "Training Batch: 15116 Loss: 3508.131836\n",
      "Training Batch: 15117 Loss: 3427.269775\n",
      "Training Batch: 15118 Loss: 3509.137451\n",
      "Training Batch: 15119 Loss: 3391.695068\n",
      "Training Batch: 15120 Loss: 3537.248047\n",
      "Training Batch: 15121 Loss: 3484.436279\n",
      "Training Batch: 15122 Loss: 3710.015381\n",
      "Training Batch: 15123 Loss: 3553.474609\n",
      "Training Batch: 15124 Loss: 3424.394043\n",
      "Training Batch: 15125 Loss: 3528.235352\n",
      "Training Batch: 15126 Loss: 3617.180176\n",
      "Training Batch: 15127 Loss: 3609.485596\n",
      "Training Batch: 15128 Loss: 3509.405762\n",
      "Training Batch: 15129 Loss: 3405.436035\n",
      "Training Batch: 15130 Loss: 3522.556641\n",
      "Training Batch: 15131 Loss: 3435.451172\n",
      "Training Batch: 15132 Loss: 3601.048340\n",
      "Training Batch: 15133 Loss: 3578.039062\n",
      "Training Batch: 15134 Loss: 3506.366211\n",
      "Training Batch: 15135 Loss: 3594.160645\n",
      "Training Batch: 15136 Loss: 3523.805176\n",
      "Training Batch: 15137 Loss: 3744.486328\n",
      "Training Batch: 15138 Loss: 3562.386475\n",
      "Training Batch: 15139 Loss: 3374.033691\n",
      "Training Batch: 15140 Loss: 3533.726074\n",
      "Training Batch: 15141 Loss: 3530.193604\n",
      "Training Batch: 15142 Loss: 3657.516602\n",
      "Training Batch: 15143 Loss: 3502.069336\n",
      "Training Batch: 15144 Loss: 3490.829346\n",
      "Training Batch: 15145 Loss: 3581.826172\n",
      "Training Batch: 15146 Loss: 3516.653320\n",
      "Training Batch: 15147 Loss: 3412.961914\n",
      "Training Batch: 15148 Loss: 3470.252441\n",
      "Training Batch: 15149 Loss: 3588.810059\n",
      "Training Batch: 15150 Loss: 3590.180664\n",
      "Training Batch: 15151 Loss: 3398.383301\n",
      "Training Batch: 15152 Loss: 3525.129395\n",
      "Training Batch: 15153 Loss: 3483.651123\n",
      "Training Batch: 15154 Loss: 3535.977539\n",
      "Training Batch: 15155 Loss: 3582.546875\n",
      "Training Batch: 15156 Loss: 3524.035645\n",
      "Training Batch: 15157 Loss: 3422.170898\n",
      "Training Batch: 15158 Loss: 3492.852539\n",
      "Training Batch: 15159 Loss: 3540.185059\n",
      "Training Batch: 15160 Loss: 3656.541504\n",
      "Training Batch: 15161 Loss: 3577.366211\n",
      "Training Batch: 15162 Loss: 3835.719238\n",
      "Training Batch: 15163 Loss: 3489.946777\n",
      "Training Batch: 15164 Loss: 3685.720459\n",
      "Training Batch: 15165 Loss: 3484.857666\n",
      "Training Batch: 15166 Loss: 3576.809814\n",
      "Training Batch: 15167 Loss: 3501.387451\n",
      "Training Batch: 15168 Loss: 3506.518555\n",
      "Training Batch: 15169 Loss: 3455.086914\n",
      "Training Batch: 15170 Loss: 3388.064453\n",
      "Training Batch: 15171 Loss: 3338.787109\n",
      "Training Batch: 15172 Loss: 3377.595215\n",
      "Training Batch: 15173 Loss: 3418.724609\n",
      "Training Batch: 15174 Loss: 3449.944824\n",
      "Training Batch: 15175 Loss: 3337.266113\n",
      "Training Batch: 15176 Loss: 3662.093262\n",
      "Training Batch: 15177 Loss: 3366.890625\n",
      "Training Batch: 15178 Loss: 3325.613281\n",
      "Training Batch: 15179 Loss: 3402.624512\n",
      "Training Batch: 15180 Loss: 3406.097168\n",
      "Training Batch: 15181 Loss: 3518.524658\n",
      "Training Batch: 15182 Loss: 3737.570557\n",
      "Training Batch: 15183 Loss: 3633.372070\n",
      "Training Batch: 15184 Loss: 3456.645020\n",
      "Training Batch: 15185 Loss: 3500.850586\n",
      "Training Batch: 15186 Loss: 3481.105469\n",
      "Training Batch: 15187 Loss: 3493.196289\n",
      "Training Batch: 15188 Loss: 3500.954590\n",
      "Training Batch: 15189 Loss: 3605.869629\n",
      "Training Batch: 15190 Loss: 3556.236816\n",
      "Training Batch: 15191 Loss: 3591.489746\n",
      "Training Batch: 15192 Loss: 3572.036621\n",
      "Training Batch: 15193 Loss: 3872.339844\n",
      "Training Batch: 15194 Loss: 3485.547363\n",
      "Training Batch: 15195 Loss: 3441.835449\n",
      "Training Batch: 15196 Loss: 3499.147949\n",
      "Training Batch: 15197 Loss: 3406.226562\n",
      "Training Batch: 15198 Loss: 3454.778320\n",
      "Training Batch: 15199 Loss: 3544.291992\n",
      "Training Batch: 15200 Loss: 3456.072266\n",
      "Training Batch: 15201 Loss: 3625.120117\n",
      "Training Batch: 15202 Loss: 3484.955811\n",
      "Training Batch: 15203 Loss: 3503.222656\n",
      "Training Batch: 15204 Loss: 3431.643066\n",
      "Training Batch: 15205 Loss: 3507.169434\n",
      "Training Batch: 15206 Loss: 3491.960449\n",
      "Training Batch: 15207 Loss: 3703.329102\n",
      "Training Batch: 15208 Loss: 3454.871338\n",
      "Training Batch: 15209 Loss: 3394.699463\n",
      "Training Batch: 15210 Loss: 3442.181396\n",
      "Training Batch: 15211 Loss: 3423.487793\n",
      "Training Batch: 15212 Loss: 3390.458496\n",
      "Training Batch: 15213 Loss: 3437.786133\n",
      "Training Batch: 15214 Loss: 3398.040527\n",
      "Training Batch: 15215 Loss: 3492.907959\n",
      "Training Batch: 15216 Loss: 3676.673340\n",
      "Training Batch: 15217 Loss: 3616.968262\n",
      "Training Batch: 15218 Loss: 3470.973145\n",
      "Training Batch: 15219 Loss: 3459.858887\n",
      "Training Batch: 15220 Loss: 3427.094727\n",
      "Training Batch: 15221 Loss: 3392.099609\n",
      "Training Batch: 15222 Loss: 3441.595703\n",
      "Training Batch: 15223 Loss: 3565.634277\n",
      "Training Batch: 15224 Loss: 3728.026123\n",
      "Training Batch: 15225 Loss: 3534.736572\n",
      "Training Batch: 15226 Loss: 3655.266113\n",
      "Training Batch: 15227 Loss: 3387.567383\n",
      "Training Batch: 15228 Loss: 3322.755127\n",
      "Training Batch: 15229 Loss: 3430.244629\n",
      "Training Batch: 15230 Loss: 3530.722656\n",
      "Training Batch: 15231 Loss: 3531.598145\n",
      "Training Batch: 15232 Loss: 3439.292480\n",
      "Training Batch: 15233 Loss: 3518.526367\n",
      "Training Batch: 15234 Loss: 3524.626709\n",
      "Training Batch: 15235 Loss: 3583.541992\n",
      "Training Batch: 15236 Loss: 3498.217773\n",
      "Training Batch: 15237 Loss: 3423.675293\n",
      "Training Batch: 15238 Loss: 3440.711914\n",
      "Training Batch: 15239 Loss: 3475.364258\n",
      "Training Batch: 15240 Loss: 3677.407715\n",
      "Training Batch: 15241 Loss: 3758.665527\n",
      "Training Batch: 15242 Loss: 3532.156982\n",
      "Training Batch: 15243 Loss: 3545.403564\n",
      "Training Batch: 15244 Loss: 3693.362793\n",
      "Training Batch: 15245 Loss: 3556.957764\n",
      "Training Batch: 15246 Loss: 3525.890137\n",
      "Training Batch: 15247 Loss: 3324.807373\n",
      "Training Batch: 15248 Loss: 3470.390625\n",
      "Training Batch: 15249 Loss: 3412.363770\n",
      "Training Batch: 15250 Loss: 3379.716309\n",
      "Training Batch: 15251 Loss: 3373.291504\n",
      "Training Batch: 15252 Loss: 3440.489014\n",
      "Training Batch: 15253 Loss: 3360.690918\n",
      "Training Batch: 15254 Loss: 3508.016113\n",
      "Training Batch: 15255 Loss: 3617.276367\n",
      "Training Batch: 15256 Loss: 3432.407959\n",
      "Training Batch: 15257 Loss: 3533.519531\n",
      "Training Batch: 15258 Loss: 3457.745117\n",
      "Training Batch: 15259 Loss: 3441.263184\n",
      "Training Batch: 15260 Loss: 3424.273193\n",
      "Training Batch: 15261 Loss: 3518.374512\n",
      "Training Batch: 15262 Loss: 3491.846924\n",
      "Training Batch: 15263 Loss: 3611.161621\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Batch: 15264 Loss: 3600.899170\n",
      "Training Batch: 15265 Loss: 3725.699707\n",
      "Training Batch: 15266 Loss: 3440.582031\n",
      "Training Batch: 15267 Loss: 3614.620605\n",
      "Training Batch: 15268 Loss: 3574.229980\n",
      "Training Batch: 15269 Loss: 3423.930176\n",
      "Training Batch: 15270 Loss: 3442.180176\n",
      "Training Batch: 15271 Loss: 3392.932129\n",
      "Training Batch: 15272 Loss: 3497.485107\n",
      "Training Batch: 15273 Loss: 3613.631104\n",
      "Training Batch: 15274 Loss: 3546.166748\n",
      "Training Batch: 15275 Loss: 3446.356689\n",
      "Training Batch: 15276 Loss: 3374.628906\n",
      "Training Batch: 15277 Loss: 3316.112793\n",
      "Training Batch: 15278 Loss: 3351.366699\n",
      "Training Batch: 15279 Loss: 3446.942139\n",
      "Training Batch: 15280 Loss: 3499.337891\n",
      "Training Batch: 15281 Loss: 3391.912842\n",
      "Training Batch: 15282 Loss: 3553.728516\n",
      "Training Batch: 15283 Loss: 3655.886230\n",
      "Training Batch: 15284 Loss: 3544.130371\n",
      "Training Batch: 15285 Loss: 3418.866211\n",
      "Training Batch: 15286 Loss: 3553.561523\n",
      "Training Batch: 15287 Loss: 3525.250000\n",
      "Training Batch: 15288 Loss: 3680.492188\n",
      "Training Batch: 15289 Loss: 3529.736328\n",
      "Training Batch: 15290 Loss: 3529.420410\n",
      "Training Batch: 15291 Loss: 3457.638672\n",
      "Training Batch: 15292 Loss: 3565.198486\n",
      "Training Batch: 15293 Loss: 3547.455078\n",
      "Training Batch: 15294 Loss: 3496.497070\n",
      "Training Batch: 15295 Loss: 3643.460938\n",
      "Training Batch: 15296 Loss: 3451.012207\n",
      "Training Batch: 15297 Loss: 3467.894043\n",
      "Training Batch: 15298 Loss: 3544.760254\n",
      "Training Batch: 15299 Loss: 3583.947998\n",
      "Training Batch: 15300 Loss: 3375.659180\n",
      "Training Batch: 15301 Loss: 3361.442627\n",
      "Training Batch: 15302 Loss: 3651.573242\n",
      "Training Batch: 15303 Loss: 3482.507812\n",
      "Training Batch: 15304 Loss: 3486.296875\n",
      "Training Batch: 15305 Loss: 3697.127686\n",
      "Training Batch: 15306 Loss: 3518.793701\n",
      "Training Batch: 15307 Loss: 3471.260254\n",
      "Training Batch: 15308 Loss: 3376.516357\n",
      "Training Batch: 15309 Loss: 3405.692383\n",
      "Training Batch: 15310 Loss: 3483.423096\n",
      "Training Batch: 15311 Loss: 3364.514648\n",
      "Training Batch: 15312 Loss: 3412.250488\n",
      "Training Batch: 15313 Loss: 3365.017090\n",
      "Training Batch: 15314 Loss: 3495.412109\n",
      "Training Batch: 15315 Loss: 3757.462402\n",
      "Training Batch: 15316 Loss: 3643.161133\n",
      "Training Batch: 15317 Loss: 3484.662598\n",
      "Training Batch: 15318 Loss: 3667.847656\n",
      "Training Batch: 15319 Loss: 3680.074707\n",
      "Training Batch: 15320 Loss: 3586.548584\n",
      "Training Batch: 15321 Loss: 3624.450684\n",
      "Training Batch: 15322 Loss: 3453.814453\n",
      "Training Batch: 15323 Loss: 3552.536133\n",
      "Training Batch: 15324 Loss: 3490.152344\n",
      "Training Batch: 15325 Loss: 3469.853516\n",
      "Training Batch: 15326 Loss: 3659.312988\n",
      "Training Batch: 15327 Loss: 3577.700684\n",
      "Training Batch: 15328 Loss: 3568.482666\n",
      "Training Batch: 15329 Loss: 3423.037842\n",
      "Training Batch: 15330 Loss: 3452.713379\n",
      "Training Batch: 15331 Loss: 3366.045410\n",
      "Training Batch: 15332 Loss: 3538.772461\n",
      "Training Batch: 15333 Loss: 3520.843994\n",
      "Training Batch: 15334 Loss: 3556.893555\n",
      "Training Batch: 15335 Loss: 3563.647461\n",
      "Training Batch: 15336 Loss: 3579.798828\n",
      "Training Batch: 15337 Loss: 3544.918213\n",
      "Training Batch: 15338 Loss: 3504.722168\n",
      "Training Batch: 15339 Loss: 3605.836914\n",
      "Training Batch: 15340 Loss: 3513.018066\n",
      "Training Batch: 15341 Loss: 3588.678223\n",
      "Training Batch: 15342 Loss: 3399.645996\n",
      "Training Batch: 15343 Loss: 3432.407471\n",
      "Training Batch: 15344 Loss: 3704.028320\n",
      "Training Batch: 15345 Loss: 3469.001709\n",
      "Training Batch: 15346 Loss: 3347.524170\n",
      "Training Batch: 15347 Loss: 3356.399902\n",
      "Training Batch: 15348 Loss: 3436.398682\n",
      "Training Batch: 15349 Loss: 3434.829590\n",
      "Training Batch: 15350 Loss: 3459.052246\n",
      "Training Batch: 15351 Loss: 3397.482422\n",
      "Training Batch: 15352 Loss: 3366.937988\n",
      "Training Batch: 15353 Loss: 3648.489258\n",
      "Training Batch: 15354 Loss: 3390.774902\n",
      "Training Batch: 15355 Loss: 3429.235352\n",
      "Training Batch: 15356 Loss: 3567.177246\n",
      "Training Batch: 15357 Loss: 3560.851074\n",
      "Training Batch: 15358 Loss: 3527.778320\n",
      "Training Batch: 15359 Loss: 3603.992188\n",
      "Training Batch: 15360 Loss: 3653.455566\n",
      "Training Batch: 15361 Loss: 3554.269775\n",
      "Training Batch: 15362 Loss: 3611.166504\n",
      "Training Batch: 15363 Loss: 3556.003418\n",
      "Training Batch: 15364 Loss: 3476.631836\n",
      "Training Batch: 15365 Loss: 4145.908203\n",
      "Training Batch: 15366 Loss: 3988.516602\n",
      "Training Batch: 15367 Loss: 3453.576172\n",
      "Training Batch: 15368 Loss: 3818.894287\n",
      "Training Batch: 15369 Loss: 3628.960693\n",
      "Training Batch: 15370 Loss: 3726.285645\n",
      "Training Batch: 15371 Loss: 3810.594971\n",
      "Training Batch: 15372 Loss: 3543.935059\n",
      "Training Batch: 15373 Loss: 3482.389648\n",
      "Training Batch: 15374 Loss: 3419.651123\n",
      "Training Batch: 15375 Loss: 3535.801758\n",
      "Training Batch: 15376 Loss: 3804.877930\n",
      "Training Batch: 15377 Loss: 3600.678223\n",
      "Training Batch: 15378 Loss: 4207.262207\n",
      "Training Batch: 15379 Loss: 3818.827637\n",
      "Training Batch: 15380 Loss: 3390.754639\n",
      "Training Batch: 15381 Loss: 3378.749512\n",
      "Training Batch: 15382 Loss: 3418.899658\n",
      "Training Batch: 15383 Loss: 3566.623291\n",
      "Training Batch: 15384 Loss: 3468.034424\n",
      "Training Batch: 15385 Loss: 3659.380371\n",
      "Training Batch: 15386 Loss: 3958.935547\n",
      "Training Batch: 15387 Loss: 3647.544189\n",
      "Training Batch: 15388 Loss: 3567.130371\n",
      "Training Batch: 15389 Loss: 3449.420410\n",
      "Training Batch: 15390 Loss: 3665.947021\n",
      "Training Batch: 15391 Loss: 3375.671631\n",
      "Training Batch: 15392 Loss: 3510.152344\n",
      "Training Batch: 15393 Loss: 3468.546875\n",
      "Training Batch: 15394 Loss: 3448.257812\n",
      "Training Batch: 15395 Loss: 3487.328613\n",
      "Training Batch: 15396 Loss: 3488.577637\n",
      "Training Batch: 15397 Loss: 3503.182617\n",
      "Training Batch: 15398 Loss: 3407.387207\n",
      "Training Batch: 15399 Loss: 3445.544434\n",
      "Training Batch: 15400 Loss: 3535.473145\n",
      "Training Batch: 15401 Loss: 3392.969727\n",
      "Training Batch: 15402 Loss: 3787.104004\n",
      "Training Batch: 15403 Loss: 3505.219727\n",
      "Training Batch: 15404 Loss: 3432.128906\n",
      "Training Batch: 15405 Loss: 3479.471680\n",
      "Training Batch: 15406 Loss: 3501.591064\n",
      "Training Batch: 15407 Loss: 3364.479980\n",
      "Training Batch: 15408 Loss: 3489.357910\n",
      "Training Batch: 15409 Loss: 3437.479492\n",
      "Training Batch: 15410 Loss: 3492.761230\n",
      "Training Batch: 15411 Loss: 3662.836426\n",
      "Training Batch: 15412 Loss: 3573.412354\n",
      "Training Batch: 15413 Loss: 3501.691895\n",
      "Training Batch: 15414 Loss: 3312.989990\n",
      "Training Batch: 15415 Loss: 3369.331543\n",
      "Training Batch: 15416 Loss: 3368.275391\n",
      "Training Batch: 15417 Loss: 3403.514648\n",
      "Training Batch: 15418 Loss: 3516.553223\n",
      "Training Batch: 15419 Loss: 3489.906006\n",
      "Training Batch: 15420 Loss: 3487.294434\n",
      "Training Batch: 15421 Loss: 3419.097168\n",
      "Training Batch: 15422 Loss: 3471.513428\n",
      "Training Batch: 15423 Loss: 3545.469727\n",
      "Training Batch: 15424 Loss: 3304.746094\n",
      "Training Batch: 15425 Loss: 3546.961670\n",
      "Training Batch: 15426 Loss: 3561.092285\n",
      "Training Batch: 15427 Loss: 3513.232910\n",
      "Training Batch: 15428 Loss: 3448.180664\n",
      "Training Batch: 15429 Loss: 4684.949219\n",
      "Training Batch: 15430 Loss: 4122.937012\n",
      "Training Batch: 15431 Loss: 4073.342773\n",
      "Training Batch: 15432 Loss: 3627.663574\n",
      "Training Batch: 15433 Loss: 3589.816406\n",
      "Training Batch: 15434 Loss: 3734.754883\n",
      "Training Batch: 15435 Loss: 3704.420898\n",
      "Training Batch: 15436 Loss: 3533.288574\n",
      "Training Batch: 15437 Loss: 3445.173096\n",
      "Training Batch: 15438 Loss: 3760.287354\n",
      "Training Batch: 15439 Loss: 3587.326416\n",
      "Training Batch: 15440 Loss: 3768.822510\n",
      "Training Batch: 15441 Loss: 3836.992676\n",
      "Training Batch: 15442 Loss: 3668.153809\n",
      "Training Batch: 15443 Loss: 3738.008789\n",
      "Training Batch: 15444 Loss: 3539.725098\n",
      "Training Batch: 15445 Loss: 3631.089355\n",
      "Training Batch: 15446 Loss: 3419.050293\n",
      "Training Batch: 15447 Loss: 3467.730469\n",
      "Training Batch: 15448 Loss: 3533.618896\n",
      "Training Batch: 15449 Loss: 3476.003906\n",
      "Training Batch: 15450 Loss: 3425.736816\n",
      "Training Batch: 15451 Loss: 3413.103516\n",
      "Training Batch: 15452 Loss: 3503.498291\n",
      "Training Batch: 15453 Loss: 3466.961914\n",
      "Training Batch: 15454 Loss: 3758.900391\n",
      "Training Batch: 15455 Loss: 3553.204102\n",
      "Training Batch: 15456 Loss: 3570.826904\n",
      "Training Batch: 15457 Loss: 3523.078613\n",
      "Training Batch: 15458 Loss: 3637.820801\n",
      "Training Batch: 15459 Loss: 3881.125977\n",
      "Training Batch: 15460 Loss: 3485.873047\n",
      "Training Batch: 15461 Loss: 3442.779053\n",
      "Training Batch: 15462 Loss: 3451.158203\n",
      "Training Batch: 15463 Loss: 3465.899658\n",
      "Training Batch: 15464 Loss: 3554.078125\n",
      "Training Batch: 15465 Loss: 3508.466309\n",
      "Training Batch: 15466 Loss: 3601.138916\n",
      "Training Batch: 15467 Loss: 3505.459961\n",
      "Training Batch: 15468 Loss: 3403.457275\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Batch: 15469 Loss: 3426.362793\n",
      "Training Batch: 15470 Loss: 3480.149658\n",
      "Training Batch: 15471 Loss: 3382.454102\n",
      "Training Batch: 15472 Loss: 3687.525879\n",
      "Training Batch: 15473 Loss: 3514.833252\n",
      "Training Batch: 15474 Loss: 3570.382080\n",
      "Training Batch: 15475 Loss: 3563.812744\n",
      "Training Batch: 15476 Loss: 3447.577637\n",
      "Training Batch: 15477 Loss: 3482.241699\n",
      "Training Batch: 15478 Loss: 3428.121094\n",
      "Training Batch: 15479 Loss: 3514.169189\n",
      "Training Batch: 15480 Loss: 3411.591309\n",
      "Training Batch: 15481 Loss: 3445.154053\n",
      "Training Batch: 15482 Loss: 3468.594482\n",
      "Training Batch: 15483 Loss: 3478.599121\n",
      "Training Batch: 15484 Loss: 3435.951904\n",
      "Training Batch: 15485 Loss: 3541.572266\n",
      "Training Batch: 15486 Loss: 3505.610840\n",
      "Training Batch: 15487 Loss: 3548.204590\n",
      "Training Batch: 15488 Loss: 3549.093018\n",
      "Training Batch: 15489 Loss: 3673.756836\n",
      "Training Batch: 15490 Loss: 3448.268066\n",
      "Training Batch: 15491 Loss: 3473.695068\n",
      "Training Batch: 15492 Loss: 4095.543213\n",
      "Training Batch: 15493 Loss: 4207.717285\n",
      "Training Batch: 15494 Loss: 3844.877930\n",
      "Training Batch: 15495 Loss: 3533.142578\n",
      "Training Batch: 15496 Loss: 3583.350098\n",
      "Training Batch: 15497 Loss: 3409.174561\n",
      "Training Batch: 15498 Loss: 3585.249756\n",
      "Training Batch: 15499 Loss: 3561.398926\n",
      "Training Batch: 15500 Loss: 3447.436768\n",
      "Training Batch: 15501 Loss: 3440.965820\n",
      "Training Batch: 15502 Loss: 3439.431152\n",
      "Training Batch: 15503 Loss: 3542.080566\n",
      "Training Batch: 15504 Loss: 3432.114258\n",
      "Training Batch: 15505 Loss: 3530.349121\n",
      "Training Batch: 15506 Loss: 3682.826660\n",
      "Training Batch: 15507 Loss: 3431.080078\n",
      "Training Batch: 15508 Loss: 3418.494873\n",
      "Training Batch: 15509 Loss: 3440.454834\n",
      "Training Batch: 15510 Loss: 3398.073730\n",
      "Training Batch: 15511 Loss: 3534.491943\n",
      "Training Batch: 15512 Loss: 3619.884521\n",
      "Training Batch: 15513 Loss: 3616.885010\n",
      "Training Batch: 15514 Loss: 3528.648438\n",
      "Training Batch: 15515 Loss: 3482.773926\n",
      "Training Batch: 15516 Loss: 3399.849365\n",
      "Training Batch: 15517 Loss: 3425.486816\n",
      "Training Batch: 15518 Loss: 3622.686523\n",
      "Training Batch: 15519 Loss: 3617.785400\n",
      "Training Batch: 15520 Loss: 3640.989258\n",
      "Training Batch: 15521 Loss: 3663.916504\n",
      "Training Batch: 15522 Loss: 3568.140137\n",
      "Training Batch: 15523 Loss: 3846.927246\n",
      "Training Batch: 15524 Loss: 3662.731445\n",
      "Training Batch: 15525 Loss: 3570.164795\n",
      "Training Batch: 15526 Loss: 3430.229980\n",
      "Training Batch: 15527 Loss: 3421.780762\n",
      "Training Batch: 15528 Loss: 3500.000977\n",
      "Training Batch: 15529 Loss: 3376.916016\n",
      "Training Batch: 15530 Loss: 3477.730469\n",
      "Training Batch: 15531 Loss: 3448.653320\n",
      "Training Batch: 15532 Loss: 3530.099365\n",
      "Training Batch: 15533 Loss: 3563.227051\n",
      "Training Batch: 15534 Loss: 3522.156250\n",
      "Training Batch: 15535 Loss: 3517.958008\n",
      "Training Batch: 15536 Loss: 3433.698730\n",
      "Training Batch: 15537 Loss: 3884.623779\n",
      "Training Batch: 15538 Loss: 3758.645996\n",
      "Training Batch: 15539 Loss: 3526.997314\n",
      "Training Batch: 15540 Loss: 3675.699463\n",
      "Training Batch: 15541 Loss: 3507.125488\n",
      "Training Batch: 15542 Loss: 3409.105469\n",
      "Training Batch: 15543 Loss: 3680.446777\n",
      "Training Batch: 15544 Loss: 4125.520508\n",
      "Training Batch: 15545 Loss: 3992.094238\n",
      "Training Batch: 15546 Loss: 3754.445068\n",
      "Training Batch: 15547 Loss: 4109.960449\n",
      "Training Batch: 15548 Loss: 3862.240234\n",
      "Training Batch: 15549 Loss: 3517.734619\n",
      "Training Batch: 15550 Loss: 3495.606689\n",
      "Training Batch: 15551 Loss: 3586.612305\n",
      "Training Batch: 15552 Loss: 3544.327148\n",
      "Training Batch: 15553 Loss: 3678.052246\n",
      "Training Batch: 15554 Loss: 3604.243896\n",
      "Training Batch: 15555 Loss: 3586.461914\n",
      "Training Batch: 15556 Loss: 3857.313965\n",
      "Training Batch: 15557 Loss: 3765.032227\n",
      "Training Batch: 15558 Loss: 3393.450195\n",
      "Training Batch: 15559 Loss: 3517.876953\n",
      "Training Batch: 15560 Loss: 3728.671387\n",
      "Training Batch: 15561 Loss: 3491.747070\n",
      "Training Batch: 15562 Loss: 3468.380859\n",
      "Training Batch: 15563 Loss: 3568.954346\n",
      "Training Batch: 15564 Loss: 3617.040527\n",
      "Training Batch: 15565 Loss: 3458.034668\n",
      "Training Batch: 15566 Loss: 3571.198486\n",
      "Training Batch: 15567 Loss: 3540.836426\n",
      "Training Batch: 15568 Loss: 3487.138916\n",
      "Training Batch: 15569 Loss: 3522.424805\n",
      "Training Batch: 15570 Loss: 3573.102051\n",
      "Training Batch: 15571 Loss: 3456.944092\n",
      "Training Batch: 15572 Loss: 3386.087402\n",
      "Training Batch: 15573 Loss: 3427.548096\n",
      "Training Batch: 15574 Loss: 3513.941406\n",
      "Training Batch: 15575 Loss: 3400.460938\n",
      "Training Batch: 15576 Loss: 3575.748535\n",
      "Training Batch: 15577 Loss: 3580.264648\n",
      "Training Batch: 15578 Loss: 3658.033936\n",
      "Training Batch: 15579 Loss: 3565.702148\n",
      "Training Batch: 15580 Loss: 3480.756104\n",
      "Training Batch: 15581 Loss: 3492.831787\n",
      "Training Batch: 15582 Loss: 3915.372559\n",
      "Training Batch: 15583 Loss: 3887.357910\n",
      "Training Batch: 15584 Loss: 3416.713623\n",
      "Training Batch: 15585 Loss: 3396.965332\n",
      "Training Batch: 15586 Loss: 3520.919922\n",
      "Training Batch: 15587 Loss: 3433.063965\n",
      "Training Batch: 15588 Loss: 3512.527832\n",
      "Training Batch: 15589 Loss: 3491.787109\n",
      "Training Batch: 15590 Loss: 3405.189941\n",
      "Training Batch: 15591 Loss: 3448.409668\n",
      "Training Batch: 15592 Loss: 3344.464355\n",
      "Training Batch: 15593 Loss: 3532.910645\n",
      "Training Batch: 15594 Loss: 3509.779785\n",
      "Training Batch: 15595 Loss: 3522.776855\n",
      "Training Batch: 15596 Loss: 3656.746582\n",
      "Training Batch: 15597 Loss: 3546.432129\n",
      "Training Batch: 15598 Loss: 3761.767090\n",
      "Training Batch: 15599 Loss: 3611.281250\n",
      "Training Batch: 15600 Loss: 3651.494141\n",
      "Training Batch: 15601 Loss: 3532.077148\n",
      "Training Batch: 15602 Loss: 3342.857666\n",
      "Training Batch: 15603 Loss: 3649.567871\n",
      "Training Batch: 15604 Loss: 3587.272461\n",
      "Training Batch: 15605 Loss: 3450.332520\n",
      "Training Batch: 15606 Loss: 3402.403809\n",
      "Training Batch: 15607 Loss: 3525.521973\n",
      "Training Batch: 15608 Loss: 3527.937012\n",
      "Training Batch: 15609 Loss: 3475.672852\n",
      "Training Batch: 15610 Loss: 3568.631592\n",
      "Training Batch: 15611 Loss: 3632.808838\n",
      "Training Batch: 15612 Loss: 3367.339355\n",
      "Training Batch: 15613 Loss: 3485.329102\n",
      "Training Batch: 15614 Loss: 3448.228271\n",
      "Training Batch: 15615 Loss: 3417.032959\n",
      "Training Batch: 15616 Loss: 3642.348633\n",
      "Training Batch: 15617 Loss: 3471.131836\n",
      "Training Batch: 15618 Loss: 3590.715820\n",
      "Training Batch: 15619 Loss: 3452.673340\n",
      "Training Batch: 15620 Loss: 3454.036621\n",
      "Training Batch: 15621 Loss: 3615.353760\n",
      "Training Batch: 15622 Loss: 3520.598633\n",
      "Training Batch: 15623 Loss: 3502.303223\n",
      "Training Batch: 15624 Loss: 3471.373047\n",
      "Training Batch: 15625 Loss: 3381.520996\n",
      "Training Batch: 15626 Loss: 3538.038818\n",
      "Training Batch: 15627 Loss: 3432.876465\n",
      "Training Batch: 15628 Loss: 3464.864014\n",
      "Training Batch: 15629 Loss: 3476.247314\n",
      "Training Batch: 15630 Loss: 3496.829834\n",
      "Training Batch: 15631 Loss: 3548.970947\n",
      "Training Batch: 15632 Loss: 3485.803711\n",
      "Training Batch: 15633 Loss: 3530.301758\n",
      "Training Batch: 15634 Loss: 3425.015625\n",
      "Training Batch: 15635 Loss: 3654.343262\n",
      "Training Batch: 15636 Loss: 3593.401855\n",
      "Training Batch: 15637 Loss: 3495.617188\n",
      "Training Batch: 15638 Loss: 3514.081543\n",
      "Training Batch: 15639 Loss: 3506.268066\n",
      "Training Batch: 15640 Loss: 3522.873535\n",
      "Training Batch: 15641 Loss: 3609.525635\n",
      "Training Batch: 15642 Loss: 3390.305664\n",
      "Training Batch: 15643 Loss: 3452.343994\n",
      "Training Batch: 15644 Loss: 3475.517578\n",
      "Training Batch: 15645 Loss: 3422.453613\n",
      "Training Batch: 15646 Loss: 3426.828857\n",
      "Training Batch: 15647 Loss: 3539.197754\n",
      "Training Batch: 15648 Loss: 3335.934082\n",
      "Training Batch: 15649 Loss: 3516.290039\n",
      "Training Batch: 15650 Loss: 3470.614258\n",
      "Training Batch: 15651 Loss: 3472.593750\n",
      "Training Batch: 15652 Loss: 3475.375488\n",
      "Training Batch: 15653 Loss: 3469.771729\n",
      "Training Batch: 15654 Loss: 3520.987061\n",
      "Training Batch: 15655 Loss: 3603.804199\n",
      "Training Batch: 15656 Loss: 3658.358887\n",
      "Training Batch: 15657 Loss: 3514.195312\n",
      "Training Batch: 15658 Loss: 3401.085449\n",
      "Training Batch: 15659 Loss: 3345.234863\n",
      "Training Batch: 15660 Loss: 3629.842285\n",
      "Training Batch: 15661 Loss: 3589.165039\n",
      "Training Batch: 15662 Loss: 3612.825684\n",
      "Training Batch: 15663 Loss: 3528.825195\n",
      "Training Batch: 15664 Loss: 3696.302734\n",
      "Training Batch: 15665 Loss: 3504.178223\n",
      "Training Batch: 15666 Loss: 3699.918213\n",
      "Training Batch: 15667 Loss: 3419.798096\n",
      "Training Batch: 15668 Loss: 3952.468262\n",
      "Training Batch: 15669 Loss: 3392.534668\n",
      "Training Batch: 15670 Loss: 3477.997559\n",
      "Training Batch: 15671 Loss: 3558.800781\n",
      "Training Batch: 15672 Loss: 3791.686523\n",
      "Training Batch: 15673 Loss: 3650.172363\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Batch: 15674 Loss: 3403.729736\n",
      "Training Batch: 15675 Loss: 3557.141602\n",
      "Training Batch: 15676 Loss: 3628.946777\n",
      "Training Batch: 15677 Loss: 3378.560303\n",
      "Training Batch: 15678 Loss: 3494.625488\n",
      "Training Batch: 15679 Loss: 3661.205078\n",
      "Training Batch: 15680 Loss: 3599.336914\n",
      "Training Batch: 15681 Loss: 3534.149414\n",
      "Training Batch: 15682 Loss: 3412.091309\n",
      "Training Batch: 15683 Loss: 3442.745117\n",
      "Training Batch: 15684 Loss: 3560.966797\n",
      "Training Batch: 15685 Loss: 3532.943359\n",
      "Training Batch: 15686 Loss: 3520.050293\n",
      "Training Batch: 15687 Loss: 3501.326172\n",
      "Training Batch: 15688 Loss: 3578.429688\n",
      "Training Batch: 15689 Loss: 3384.544922\n",
      "Training Batch: 15690 Loss: 3506.231934\n",
      "Training Batch: 15691 Loss: 3447.416992\n",
      "Training Batch: 15692 Loss: 3458.308594\n",
      "Training Batch: 15693 Loss: 3483.972656\n",
      "Training Batch: 15694 Loss: 3581.065674\n",
      "Training Batch: 15695 Loss: 3625.553711\n",
      "Training Batch: 15696 Loss: 3527.812012\n",
      "Training Batch: 15697 Loss: 3453.985107\n",
      "Training Batch: 15698 Loss: 3464.754639\n",
      "Training Batch: 15699 Loss: 3434.238281\n",
      "Training Batch: 15700 Loss: 3596.239746\n",
      "Training Batch: 15701 Loss: 3360.291504\n",
      "Training Batch: 15702 Loss: 3637.821289\n",
      "Training Batch: 15703 Loss: 3509.964600\n",
      "Training Batch: 15704 Loss: 3522.775391\n",
      "Training Batch: 15705 Loss: 3483.023438\n",
      "Training Batch: 15706 Loss: 3628.187988\n",
      "Training Batch: 15707 Loss: 3455.156738\n",
      "Training Batch: 15708 Loss: 3527.406738\n",
      "Training Batch: 15709 Loss: 3454.353271\n",
      "Training Batch: 15710 Loss: 3442.113770\n",
      "Training Batch: 15711 Loss: 3509.000977\n",
      "Training Batch: 15712 Loss: 3729.722656\n",
      "Training Batch: 15713 Loss: 3996.862305\n",
      "Training Batch: 15714 Loss: 3546.426270\n",
      "Training Batch: 15715 Loss: 3606.535645\n",
      "Training Batch: 15716 Loss: 3599.944824\n",
      "Training Batch: 15717 Loss: 3637.563232\n",
      "Training Batch: 15718 Loss: 3564.912598\n",
      "Training Batch: 15719 Loss: 3315.551758\n",
      "Training Batch: 15720 Loss: 3492.200684\n",
      "Training Batch: 15721 Loss: 3700.184082\n",
      "Training Batch: 15722 Loss: 3501.142822\n",
      "Training Batch: 15723 Loss: 3334.512695\n",
      "Training Batch: 15724 Loss: 3876.240234\n",
      "Training Batch: 15725 Loss: 3513.940430\n",
      "Training Batch: 15726 Loss: 3433.513672\n",
      "Training Batch: 15727 Loss: 3456.309082\n",
      "Training Batch: 15728 Loss: 3391.033691\n",
      "Training Batch: 15729 Loss: 3439.203857\n",
      "Training Batch: 15730 Loss: 3591.390869\n",
      "Training Batch: 15731 Loss: 3511.971191\n",
      "Training Batch: 15732 Loss: 3523.406006\n",
      "Training Batch: 15733 Loss: 3598.026367\n",
      "Training Batch: 15734 Loss: 3488.717285\n",
      "Training Batch: 15735 Loss: 3641.624023\n",
      "Training Batch: 15736 Loss: 3552.527832\n",
      "Training Batch: 15737 Loss: 3695.072510\n",
      "Training Batch: 15738 Loss: 3688.735352\n",
      "Training Batch: 15739 Loss: 3360.294922\n",
      "Training Batch: 15740 Loss: 3419.626953\n",
      "Training Batch: 15741 Loss: 3376.258057\n",
      "Training Batch: 15742 Loss: 3690.717041\n",
      "Training Batch: 15743 Loss: 3596.861328\n",
      "Training Batch: 15744 Loss: 3556.857422\n",
      "Training Batch: 15745 Loss: 3588.448730\n",
      "Training Batch: 15746 Loss: 3444.726562\n",
      "Training Batch: 15747 Loss: 3444.612793\n",
      "Training Batch: 15748 Loss: 3486.577393\n",
      "Training Batch: 15749 Loss: 3554.012939\n",
      "Training Batch: 15750 Loss: 3506.622559\n",
      "Training Batch: 15751 Loss: 3459.018555\n",
      "Training Batch: 15752 Loss: 3392.673096\n",
      "Training Batch: 15753 Loss: 3491.936279\n",
      "Training Batch: 15754 Loss: 3546.340332\n",
      "Training Batch: 15755 Loss: 3430.607422\n",
      "Training Batch: 15756 Loss: 3474.804199\n",
      "Training Batch: 15757 Loss: 3701.310059\n",
      "Training Batch: 15758 Loss: 3409.976318\n",
      "Training Batch: 15759 Loss: 3621.099121\n",
      "Training Batch: 15760 Loss: 3777.821777\n",
      "Training Batch: 15761 Loss: 3415.409180\n",
      "Training Batch: 15762 Loss: 3504.763672\n",
      "Training Batch: 15763 Loss: 3429.642090\n",
      "Training Batch: 15764 Loss: 3409.167480\n",
      "Training Batch: 15765 Loss: 3472.045410\n",
      "Training Batch: 15766 Loss: 3350.392090\n",
      "Training Batch: 15767 Loss: 3576.527832\n",
      "Training Batch: 15768 Loss: 3485.067871\n",
      "Training Batch: 15769 Loss: 3478.025391\n",
      "Training Batch: 15770 Loss: 3677.238770\n",
      "Training Batch: 15771 Loss: 3463.217773\n",
      "Training Batch: 15772 Loss: 3607.212402\n",
      "Training Batch: 15773 Loss: 3486.603027\n",
      "Training Batch: 15774 Loss: 3374.200684\n",
      "Training Batch: 15775 Loss: 3636.213379\n",
      "Training Batch: 15776 Loss: 3468.305908\n",
      "Training Batch: 15777 Loss: 3480.441895\n",
      "Training Batch: 15778 Loss: 3524.385986\n",
      "Training Batch: 15779 Loss: 3636.201172\n",
      "Training Batch: 15780 Loss: 3462.763672\n",
      "Training Batch: 15781 Loss: 3490.820801\n",
      "Training Batch: 15782 Loss: 3596.587891\n",
      "Training Batch: 15783 Loss: 3513.556152\n",
      "Training Batch: 15784 Loss: 3478.666504\n",
      "Training Batch: 15785 Loss: 3435.343750\n",
      "Training Batch: 15786 Loss: 3595.499756\n",
      "Training Batch: 15787 Loss: 3415.839844\n",
      "Training Batch: 15788 Loss: 3512.624512\n",
      "Training Batch: 15789 Loss: 3491.072266\n",
      "Training Batch: 15790 Loss: 3349.063477\n",
      "Training Batch: 15791 Loss: 3597.166260\n",
      "Training Batch: 15792 Loss: 3618.427246\n",
      "Training Batch: 15793 Loss: 3434.189941\n",
      "Training Batch: 15794 Loss: 3632.173340\n",
      "Training Batch: 15795 Loss: 3445.422363\n",
      "Training Batch: 15796 Loss: 3392.592773\n",
      "Training Batch: 15797 Loss: 3427.208496\n",
      "Training Batch: 15798 Loss: 3400.049316\n",
      "Training Batch: 15799 Loss: 3534.771973\n",
      "Training Batch: 15800 Loss: 3425.379883\n",
      "Training Batch: 15801 Loss: 3653.023438\n",
      "Training Batch: 15802 Loss: 3594.764160\n",
      "Training Batch: 15803 Loss: 3506.264160\n",
      "Training Batch: 15804 Loss: 3452.482422\n",
      "Training Batch: 15805 Loss: 3434.771240\n",
      "Training Batch: 15806 Loss: 3515.919434\n",
      "Training Batch: 15807 Loss: 3554.637695\n",
      "Training Batch: 15808 Loss: 3666.943848\n",
      "Training Batch: 15809 Loss: 3529.529053\n",
      "Training Batch: 15810 Loss: 3439.987305\n",
      "Training Batch: 15811 Loss: 3467.395264\n",
      "Training Batch: 15812 Loss: 3591.733154\n",
      "Training Batch: 15813 Loss: 3569.323975\n",
      "Training Batch: 15814 Loss: 3494.556641\n",
      "Training Batch: 15815 Loss: 3336.981934\n",
      "Training Batch: 15816 Loss: 3344.349609\n",
      "Training Batch: 15817 Loss: 3387.033691\n",
      "Training Batch: 15818 Loss: 3490.905762\n",
      "Training Batch: 15819 Loss: 3489.831055\n",
      "Training Batch: 15820 Loss: 3455.980957\n",
      "Training Batch: 15821 Loss: 3406.420410\n",
      "Training Batch: 15822 Loss: 3482.971191\n",
      "Training Batch: 15823 Loss: 3662.454590\n",
      "Training Batch: 15824 Loss: 3783.637939\n",
      "Training Batch: 15825 Loss: 3422.733398\n",
      "Training Batch: 15826 Loss: 3478.951172\n",
      "Training Batch: 15827 Loss: 3581.576660\n",
      "Training Batch: 15828 Loss: 3857.229492\n",
      "Training Batch: 15829 Loss: 3649.981689\n",
      "Training Batch: 15830 Loss: 3726.283691\n",
      "Training Batch: 15831 Loss: 3514.410645\n",
      "Training Batch: 15832 Loss: 3546.411133\n",
      "Training Batch: 15833 Loss: 3604.186523\n",
      "Training Batch: 15834 Loss: 3875.493164\n",
      "Training Batch: 15835 Loss: 3592.401123\n",
      "Training Batch: 15836 Loss: 3513.586182\n",
      "Training Batch: 15837 Loss: 3394.580322\n",
      "Training Batch: 15838 Loss: 3546.442627\n",
      "Training Batch: 15839 Loss: 3587.471191\n",
      "Training Batch: 15840 Loss: 3515.625977\n",
      "Training Batch: 15841 Loss: 3442.735596\n",
      "Training Batch: 15842 Loss: 3454.653320\n",
      "Training Batch: 15843 Loss: 3434.525635\n",
      "Training Batch: 15844 Loss: 3488.800293\n",
      "Training Batch: 15845 Loss: 3670.187500\n",
      "Training Batch: 15846 Loss: 3611.744141\n",
      "Training Batch: 15847 Loss: 3557.915527\n",
      "Training Batch: 15848 Loss: 3602.054199\n",
      "Training Batch: 15849 Loss: 3419.349609\n",
      "Training Batch: 15850 Loss: 3600.282715\n",
      "Training Batch: 15851 Loss: 3482.944824\n",
      "Training Batch: 15852 Loss: 3503.617676\n",
      "Training Batch: 15853 Loss: 3539.220703\n",
      "Training Batch: 15854 Loss: 3364.431641\n",
      "Training Batch: 15855 Loss: 3440.816406\n",
      "Training Batch: 15856 Loss: 3509.056641\n",
      "Training Batch: 15857 Loss: 3350.302734\n",
      "Training Batch: 15858 Loss: 3439.415527\n",
      "Training Batch: 15859 Loss: 3487.815430\n",
      "Training Batch: 15860 Loss: 3554.686768\n",
      "Training Batch: 15861 Loss: 3604.068359\n",
      "Training Batch: 15862 Loss: 3511.952148\n",
      "Training Batch: 15863 Loss: 3566.281250\n",
      "Training Batch: 15864 Loss: 3686.610596\n",
      "Training Batch: 15865 Loss: 3634.630615\n",
      "Training Batch: 15866 Loss: 3446.437256\n",
      "Training Batch: 15867 Loss: 3538.316162\n",
      "Training Batch: 15868 Loss: 3418.472900\n",
      "Training Batch: 15869 Loss: 3531.103516\n",
      "Training Batch: 15870 Loss: 3546.652588\n",
      "Training Batch: 15871 Loss: 3521.657227\n",
      "Training Batch: 15872 Loss: 3390.697754\n",
      "Training Batch: 15873 Loss: 3555.966309\n",
      "Training Batch: 15874 Loss: 3720.410156\n",
      "Training Batch: 15875 Loss: 3541.269043\n",
      "Training Batch: 15876 Loss: 3457.883789\n",
      "Training Batch: 15877 Loss: 3596.193115\n",
      "Training Batch: 15878 Loss: 3662.549072\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Batch: 15879 Loss: 3473.810791\n",
      "Training Batch: 15880 Loss: 3684.531006\n",
      "Training Batch: 15881 Loss: 3398.681641\n",
      "Training Batch: 15882 Loss: 3449.372803\n",
      "Training Batch: 15883 Loss: 3662.896484\n",
      "Training Batch: 15884 Loss: 3521.891602\n",
      "Training Batch: 15885 Loss: 3575.155762\n",
      "Training Batch: 15886 Loss: 3564.913574\n",
      "Training Batch: 15887 Loss: 3513.193848\n",
      "Training Batch: 15888 Loss: 3582.697754\n",
      "Training Batch: 15889 Loss: 3482.108398\n",
      "Training Batch: 15890 Loss: 3479.759521\n",
      "Training Batch: 15891 Loss: 3484.514160\n",
      "Training Batch: 15892 Loss: 3481.095459\n",
      "Training Batch: 15893 Loss: 3459.616699\n",
      "Training Batch: 15894 Loss: 3390.903809\n",
      "Training Batch: 15895 Loss: 3512.281250\n",
      "Training Batch: 15896 Loss: 3544.477783\n",
      "Training Batch: 15897 Loss: 3579.814453\n",
      "Training Batch: 15898 Loss: 3613.153320\n",
      "Training Batch: 15899 Loss: 3489.606445\n",
      "Training Batch: 15900 Loss: 3455.276367\n",
      "Training Batch: 15901 Loss: 3557.801270\n",
      "Training Batch: 15902 Loss: 3514.243164\n",
      "Training Batch: 15903 Loss: 3451.454102\n",
      "Training Batch: 15904 Loss: 3368.130371\n",
      "Training Batch: 15905 Loss: 3533.610352\n",
      "Training Batch: 15906 Loss: 3376.781738\n",
      "Training Batch: 15907 Loss: 3431.825439\n",
      "Training Batch: 15908 Loss: 3357.335449\n",
      "Training Batch: 15909 Loss: 3459.418457\n",
      "Training Batch: 15910 Loss: 3792.926270\n",
      "Training Batch: 15911 Loss: 3571.255371\n",
      "Training Batch: 15912 Loss: 3443.958984\n",
      "Training Batch: 15913 Loss: 3503.370850\n",
      "Training Batch: 15914 Loss: 3553.984375\n",
      "Training Batch: 15915 Loss: 3492.040771\n",
      "Training Batch: 15916 Loss: 3392.509277\n",
      "Training Batch: 15917 Loss: 3453.056396\n",
      "Training Batch: 15918 Loss: 3366.195801\n",
      "Training Batch: 15919 Loss: 3364.353516\n",
      "Training Batch: 15920 Loss: 3354.237061\n",
      "Training Batch: 15921 Loss: 3550.617432\n",
      "Training Batch: 15922 Loss: 3470.530273\n",
      "Training Batch: 15923 Loss: 3664.787598\n",
      "Training Batch: 15924 Loss: 3416.642090\n",
      "Training Batch: 15925 Loss: 3536.540039\n",
      "Training Batch: 15926 Loss: 3516.543945\n",
      "Training Batch: 15927 Loss: 3470.611328\n",
      "Training Batch: 15928 Loss: 3559.926270\n",
      "Training Batch: 15929 Loss: 3395.954590\n",
      "Training Batch: 15930 Loss: 3638.708740\n",
      "Training Batch: 15931 Loss: 3590.623535\n",
      "Training Batch: 15932 Loss: 3460.274414\n",
      "Training Batch: 15933 Loss: 3403.780029\n",
      "Training Batch: 15934 Loss: 3679.478271\n",
      "Training Batch: 15935 Loss: 3443.485352\n",
      "Training Batch: 15936 Loss: 3432.418213\n",
      "Training Batch: 15937 Loss: 3440.292725\n",
      "Training Batch: 15938 Loss: 3385.104248\n",
      "Training Batch: 15939 Loss: 3511.452148\n",
      "Training Batch: 15940 Loss: 3523.109375\n",
      "Training Batch: 15941 Loss: 3404.088379\n",
      "Training Batch: 15942 Loss: 3502.988281\n",
      "Training Batch: 15943 Loss: 3497.775391\n",
      "Training Batch: 15944 Loss: 3510.941406\n",
      "Training Batch: 15945 Loss: 3503.929688\n",
      "Training Batch: 15946 Loss: 3648.171387\n",
      "Training Batch: 15947 Loss: 3647.088135\n",
      "Training Batch: 15948 Loss: 3475.922607\n",
      "Training Batch: 15949 Loss: 3675.134277\n",
      "Training Batch: 15950 Loss: 3413.499756\n",
      "Training Batch: 15951 Loss: 3447.487793\n",
      "Training Batch: 15952 Loss: 3474.905762\n",
      "Training Batch: 15953 Loss: 3386.003906\n",
      "Training Batch: 15954 Loss: 3584.181885\n",
      "Training Batch: 15955 Loss: 3430.950928\n",
      "Training Batch: 15956 Loss: 3550.391602\n",
      "Training Batch: 15957 Loss: 3440.994629\n",
      "Training Batch: 15958 Loss: 3517.853027\n",
      "Training Batch: 15959 Loss: 3430.417969\n",
      "Training Batch: 15960 Loss: 3339.844971\n",
      "Training Batch: 15961 Loss: 3558.716309\n",
      "Training Batch: 15962 Loss: 3463.708496\n",
      "Training Batch: 15963 Loss: 3466.146484\n",
      "Training Batch: 15964 Loss: 3573.129395\n",
      "Training Batch: 15965 Loss: 3484.764160\n",
      "Training Batch: 15966 Loss: 3513.281250\n",
      "Training Batch: 15967 Loss: 3400.214355\n",
      "Training Batch: 15968 Loss: 3686.974121\n",
      "Training Batch: 15969 Loss: 3658.756836\n",
      "Training Batch: 15970 Loss: 3462.192383\n",
      "Training Batch: 15971 Loss: 3415.757812\n",
      "Training Batch: 15972 Loss: 3520.361572\n",
      "Training Batch: 15973 Loss: 3478.662598\n",
      "Training Batch: 15974 Loss: 3407.402344\n",
      "Training Batch: 15975 Loss: 3312.545898\n",
      "Training Batch: 15976 Loss: 3414.999268\n",
      "Training Batch: 15977 Loss: 3714.959473\n",
      "Training Batch: 15978 Loss: 3488.563477\n",
      "Training Batch: 15979 Loss: 3615.620605\n",
      "Training Batch: 15980 Loss: 3524.133789\n",
      "Training Batch: 15981 Loss: 3573.996582\n",
      "Training Batch: 15982 Loss: 3420.125977\n",
      "Training Batch: 15983 Loss: 3332.649414\n",
      "Training Batch: 15984 Loss: 3542.210205\n",
      "Training Batch: 15985 Loss: 3523.213867\n",
      "Training Batch: 15986 Loss: 3489.890625\n",
      "Training Batch: 15987 Loss: 3433.844727\n",
      "Training Batch: 15988 Loss: 3732.286621\n",
      "Training Batch: 15989 Loss: 3494.140137\n",
      "Training Batch: 15990 Loss: 3364.729004\n",
      "Training Batch: 15991 Loss: 3498.520508\n",
      "Training Batch: 15992 Loss: 3509.367188\n",
      "Training Batch: 15993 Loss: 3524.753662\n",
      "Training Batch: 15994 Loss: 3421.402344\n",
      "Training Batch: 15995 Loss: 3505.436035\n",
      "Training Batch: 15996 Loss: 3547.730469\n",
      "Training Batch: 15997 Loss: 3782.563477\n",
      "Training Batch: 15998 Loss: 3457.899902\n",
      "Training Batch: 15999 Loss: 3585.870117\n",
      "Training Batch: 16000 Loss: 3566.640137\n",
      "Training Batch: 16001 Loss: 3515.970215\n",
      "Training Batch: 16002 Loss: 3380.837646\n",
      "Training Batch: 16003 Loss: 3676.817383\n",
      "Training Batch: 16004 Loss: 3421.182129\n",
      "Training Batch: 16005 Loss: 3493.800781\n",
      "Training Batch: 16006 Loss: 3523.696045\n",
      "Training Batch: 16007 Loss: 3499.214111\n",
      "Training Batch: 16008 Loss: 3566.125488\n",
      "Training Batch: 16009 Loss: 3340.046875\n",
      "Training Batch: 16010 Loss: 3406.424805\n",
      "Training Batch: 16011 Loss: 3338.157715\n",
      "Training Batch: 16012 Loss: 3511.642090\n",
      "Training Batch: 16013 Loss: 3447.952393\n",
      "Training Batch: 16014 Loss: 3543.421387\n",
      "Training Batch: 16015 Loss: 3453.283691\n",
      "Training Batch: 16016 Loss: 3502.206055\n",
      "Training Batch: 16017 Loss: 3489.777832\n",
      "Training Batch: 16018 Loss: 3372.121338\n",
      "Training Batch: 16019 Loss: 3534.997314\n",
      "Training Batch: 16020 Loss: 3445.001465\n",
      "Training Batch: 16021 Loss: 3454.631836\n",
      "Training Batch: 16022 Loss: 3343.318604\n",
      "Training Batch: 16023 Loss: 3508.316406\n",
      "Training Batch: 16024 Loss: 3397.604980\n",
      "Training Batch: 16025 Loss: 3637.852783\n",
      "Training Batch: 16026 Loss: 3753.987305\n",
      "Training Batch: 16027 Loss: 3548.650879\n",
      "Training Batch: 16028 Loss: 3447.026855\n",
      "Training Batch: 16029 Loss: 3348.014160\n",
      "Training Batch: 16030 Loss: 3522.676025\n",
      "Training Batch: 16031 Loss: 3385.802734\n",
      "Training Batch: 16032 Loss: 3357.036621\n",
      "Training Batch: 16033 Loss: 3548.075439\n",
      "Training Batch: 16034 Loss: 3537.828613\n",
      "Training Batch: 16035 Loss: 3473.257812\n",
      "Training Batch: 16036 Loss: 3447.486816\n",
      "Training Batch: 16037 Loss: 3513.005371\n",
      "Training Batch: 16038 Loss: 3457.407959\n",
      "Training Batch: 16039 Loss: 3508.103027\n",
      "Training Batch: 16040 Loss: 3387.484375\n",
      "Training Batch: 16041 Loss: 3356.066895\n",
      "Training Batch: 16042 Loss: 3580.002930\n",
      "Training Batch: 16043 Loss: 3443.717285\n",
      "Training Batch: 16044 Loss: 3511.685547\n",
      "Training Batch: 16045 Loss: 3584.499512\n",
      "Training Batch: 16046 Loss: 3463.443359\n",
      "Training Batch: 16047 Loss: 3519.011963\n",
      "Training Batch: 16048 Loss: 3425.704834\n",
      "Training Batch: 16049 Loss: 3402.173828\n",
      "Training Batch: 16050 Loss: 3472.280273\n",
      "Training Batch: 16051 Loss: 3443.890625\n",
      "Training Batch: 16052 Loss: 3361.059082\n",
      "Training Batch: 16053 Loss: 3456.623047\n",
      "Training Batch: 16054 Loss: 3553.245117\n",
      "Training Batch: 16055 Loss: 3532.420410\n",
      "Training Batch: 16056 Loss: 3661.903809\n",
      "Training Batch: 16057 Loss: 3421.859863\n",
      "Training Batch: 16058 Loss: 3469.399902\n",
      "Training Batch: 16059 Loss: 3415.165527\n",
      "Training Batch: 16060 Loss: 3399.855957\n",
      "Training Batch: 16061 Loss: 3374.633789\n",
      "Training Batch: 16062 Loss: 4046.270264\n",
      "Training Batch: 16063 Loss: 4171.889648\n",
      "Training Batch: 16064 Loss: 3499.749023\n",
      "Training Batch: 16065 Loss: 3411.094727\n",
      "Training Batch: 16066 Loss: 3337.964111\n",
      "Training Batch: 16067 Loss: 3513.218994\n",
      "Training Batch: 16068 Loss: 3413.266602\n",
      "Training Batch: 16069 Loss: 3559.439941\n",
      "Training Batch: 16070 Loss: 3660.986816\n",
      "Training Batch: 16071 Loss: 3494.152344\n",
      "Training Batch: 16072 Loss: 3567.516846\n",
      "Training Batch: 16073 Loss: 3366.901367\n",
      "Training Batch: 16074 Loss: 3454.391602\n",
      "Training Batch: 16075 Loss: 3544.198242\n",
      "Training Batch: 16076 Loss: 3447.105469\n",
      "Training Batch: 16077 Loss: 3536.808594\n",
      "Training Batch: 16078 Loss: 3652.387207\n",
      "Training Batch: 16079 Loss: 3629.403809\n",
      "Training Batch: 16080 Loss: 3477.723389\n",
      "Training Batch: 16081 Loss: 3699.453857\n",
      "Training Batch: 16082 Loss: 3548.665039\n",
      "Training Batch: 16083 Loss: 3749.905029\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Batch: 16084 Loss: 3921.185059\n",
      "Training Batch: 16085 Loss: 3504.561035\n",
      "Training Batch: 16086 Loss: 3776.526123\n",
      "Training Batch: 16087 Loss: 3393.768799\n",
      "Training Batch: 16088 Loss: 3797.013672\n",
      "Training Batch: 16089 Loss: 3910.176270\n",
      "Training Batch: 16090 Loss: 3453.849609\n",
      "Training Batch: 16091 Loss: 3810.251465\n",
      "Training Batch: 16092 Loss: 3739.615234\n",
      "Training Batch: 16093 Loss: 3407.416016\n",
      "Training Batch: 16094 Loss: 3454.620605\n",
      "Training Batch: 16095 Loss: 3466.442139\n",
      "Training Batch: 16096 Loss: 3555.979248\n",
      "Training Batch: 16097 Loss: 3517.336182\n",
      "Training Batch: 16098 Loss: 3477.365234\n",
      "Training Batch: 16099 Loss: 3602.801758\n",
      "Training Batch: 16100 Loss: 3535.171875\n",
      "Training Batch: 16101 Loss: 3628.136719\n",
      "Training Batch: 16102 Loss: 3363.160645\n",
      "Training Batch: 16103 Loss: 3383.366211\n",
      "Training Batch: 16104 Loss: 3383.970215\n",
      "Training Batch: 16105 Loss: 3573.753906\n",
      "Training Batch: 16106 Loss: 3479.975098\n",
      "Training Batch: 16107 Loss: 3464.547852\n",
      "Training Batch: 16108 Loss: 3527.627930\n",
      "Training Batch: 16109 Loss: 3415.400146\n",
      "Training Batch: 16110 Loss: 3734.643555\n",
      "Training Batch: 16111 Loss: 3538.330566\n",
      "Training Batch: 16112 Loss: 3530.786621\n",
      "Training Batch: 16113 Loss: 3532.305664\n",
      "Training Batch: 16114 Loss: 3400.458984\n",
      "Training Batch: 16115 Loss: 3574.470703\n",
      "Training Batch: 16116 Loss: 3577.268066\n",
      "Training Batch: 16117 Loss: 3571.073975\n",
      "Training Batch: 16118 Loss: 3690.196777\n",
      "Training Batch: 16119 Loss: 3441.311035\n",
      "Training Batch: 16120 Loss: 3557.206543\n",
      "Training Batch: 16121 Loss: 3496.680176\n",
      "Training Batch: 16122 Loss: 3569.242676\n",
      "Training Batch: 16123 Loss: 3482.253662\n",
      "Training Batch: 16124 Loss: 3536.592773\n",
      "Training Batch: 16125 Loss: 3371.237305\n",
      "Training Batch: 16126 Loss: 3450.389893\n",
      "Training Batch: 16127 Loss: 3441.469727\n",
      "Training Batch: 16128 Loss: 3417.064941\n",
      "Training Batch: 16129 Loss: 3384.620850\n",
      "Training Batch: 16130 Loss: 3327.079102\n",
      "Training Batch: 16131 Loss: 3482.780273\n",
      "Training Batch: 16132 Loss: 3430.679199\n",
      "Training Batch: 16133 Loss: 3584.311523\n",
      "Training Batch: 16134 Loss: 3671.355957\n",
      "Training Batch: 16135 Loss: 3362.344238\n",
      "Training Batch: 16136 Loss: 3457.413574\n",
      "Training Batch: 16137 Loss: 3492.243652\n",
      "Training Batch: 16138 Loss: 3335.123047\n",
      "Training Batch: 16139 Loss: 3794.708984\n",
      "Training Batch: 16140 Loss: 3635.754395\n",
      "Training Batch: 16141 Loss: 3687.229248\n",
      "Training Batch: 16142 Loss: 3597.873047\n",
      "Training Batch: 16143 Loss: 3535.608398\n",
      "Training Batch: 16144 Loss: 3403.577393\n",
      "Training Batch: 16145 Loss: 3527.266357\n",
      "Training Batch: 16146 Loss: 3362.076172\n",
      "Training Batch: 16147 Loss: 3487.334717\n",
      "Training Batch: 16148 Loss: 3552.690674\n",
      "Training Batch: 16149 Loss: 3508.211426\n",
      "Training Batch: 16150 Loss: 3616.825928\n",
      "Training Batch: 16151 Loss: 3575.292480\n",
      "Training Batch: 16152 Loss: 3478.915527\n",
      "Training Batch: 16153 Loss: 3405.820068\n",
      "Training Batch: 16154 Loss: 3458.844238\n",
      "Training Batch: 16155 Loss: 3390.735352\n",
      "Training Batch: 16156 Loss: 3546.318359\n",
      "Training Batch: 16157 Loss: 3775.036621\n",
      "Training Batch: 16158 Loss: 3628.524414\n",
      "Training Batch: 16159 Loss: 3426.823730\n",
      "Training Batch: 16160 Loss: 3501.686523\n",
      "Training Batch: 16161 Loss: 3509.271973\n",
      "Training Batch: 16162 Loss: 3474.654053\n",
      "Training Batch: 16163 Loss: 3383.197510\n",
      "Training Batch: 16164 Loss: 3484.837158\n",
      "Training Batch: 16165 Loss: 3533.766846\n",
      "Training Batch: 16166 Loss: 3554.068115\n",
      "Training Batch: 16167 Loss: 3515.785645\n",
      "Training Batch: 16168 Loss: 3626.435059\n",
      "Training Batch: 16169 Loss: 3456.804443\n",
      "Training Batch: 16170 Loss: 3449.474609\n",
      "Training Batch: 16171 Loss: 3516.064453\n",
      "Training Batch: 16172 Loss: 3462.661133\n",
      "Training Batch: 16173 Loss: 3439.123535\n",
      "Training Batch: 16174 Loss: 3450.385986\n",
      "Training Batch: 16175 Loss: 3549.914551\n",
      "Training Batch: 16176 Loss: 3572.161621\n",
      "Training Batch: 16177 Loss: 3578.956543\n",
      "Training Batch: 16178 Loss: 3532.575684\n",
      "Training Batch: 16179 Loss: 3491.091064\n",
      "Training Batch: 16180 Loss: 3469.116455\n",
      "Training Batch: 16181 Loss: 3459.225586\n",
      "Training Batch: 16182 Loss: 3651.609863\n",
      "Training Batch: 16183 Loss: 3471.317627\n",
      "Training Batch: 16184 Loss: 3448.880859\n",
      "Training Batch: 16185 Loss: 3540.234131\n",
      "Training Batch: 16186 Loss: 3736.297852\n",
      "Training Batch: 16187 Loss: 3544.621338\n",
      "Training Batch: 16188 Loss: 3487.836914\n",
      "Training Batch: 16189 Loss: 3543.528564\n",
      "Training Batch: 16190 Loss: 3367.051270\n",
      "Training Batch: 16191 Loss: 3481.279541\n",
      "Training Batch: 16192 Loss: 3412.507080\n",
      "Training Batch: 16193 Loss: 3365.661621\n",
      "Training Batch: 16194 Loss: 3530.236328\n",
      "Training Batch: 16195 Loss: 3456.145508\n",
      "Training Batch: 16196 Loss: 3416.444824\n",
      "Training Batch: 16197 Loss: 3406.670898\n",
      "Training Batch: 16198 Loss: 3521.076660\n",
      "Training Batch: 16199 Loss: 3403.546387\n",
      "Training Batch: 16200 Loss: 3462.773438\n",
      "Training Batch: 16201 Loss: 3441.992188\n",
      "Training Batch: 16202 Loss: 3520.875488\n",
      "Training Batch: 16203 Loss: 3453.907959\n",
      "Training Batch: 16204 Loss: 3319.995117\n",
      "Training Batch: 16205 Loss: 3471.212402\n",
      "Training Batch: 16206 Loss: 3373.053223\n",
      "Training Batch: 16207 Loss: 3422.929688\n",
      "Training Batch: 16208 Loss: 3673.690918\n",
      "Training Batch: 16209 Loss: 3410.439941\n",
      "Training Batch: 16210 Loss: 3886.606934\n",
      "Training Batch: 16211 Loss: 3574.538086\n",
      "Training Batch: 16212 Loss: 3580.747070\n",
      "Training Batch: 16213 Loss: 3406.354004\n",
      "Training Batch: 16214 Loss: 3622.396240\n",
      "Training Batch: 16215 Loss: 3507.485352\n",
      "Training Batch: 16216 Loss: 3449.783203\n",
      "Training Batch: 16217 Loss: 3491.790039\n",
      "Training Batch: 16218 Loss: 3399.837402\n",
      "Training Batch: 16219 Loss: 3780.924316\n",
      "Training Batch: 16220 Loss: 3741.317383\n",
      "Training Batch: 16221 Loss: 3796.507812\n",
      "Training Batch: 16222 Loss: 3583.386719\n",
      "Training Batch: 16223 Loss: 3396.833984\n",
      "Training Batch: 16224 Loss: 3345.240234\n",
      "Training Batch: 16225 Loss: 3384.627930\n",
      "Training Batch: 16226 Loss: 3703.488770\n",
      "Training Batch: 16227 Loss: 3770.781738\n",
      "Training Batch: 16228 Loss: 3481.314453\n",
      "Training Batch: 16229 Loss: 3447.001953\n",
      "Training Batch: 16230 Loss: 3464.061035\n",
      "Training Batch: 16231 Loss: 3353.660645\n",
      "Training Batch: 16232 Loss: 3620.023438\n",
      "Training Batch: 16233 Loss: 3465.793457\n",
      "Training Batch: 16234 Loss: 3623.604980\n",
      "Training Batch: 16235 Loss: 3420.885742\n",
      "Training Batch: 16236 Loss: 3501.324219\n",
      "Training Batch: 16237 Loss: 3484.657471\n",
      "Training Batch: 16238 Loss: 3616.354980\n",
      "Training Batch: 16239 Loss: 3676.314209\n",
      "Training Batch: 16240 Loss: 3515.713379\n",
      "Training Batch: 16241 Loss: 3518.824707\n",
      "Training Batch: 16242 Loss: 3583.575195\n",
      "Training Batch: 16243 Loss: 3322.908203\n",
      "Training Batch: 16244 Loss: 3436.366699\n",
      "Training Batch: 16245 Loss: 3327.463379\n",
      "Training Batch: 16246 Loss: 3301.468994\n",
      "Training Batch: 16247 Loss: 3356.270752\n",
      "Training Batch: 16248 Loss: 3463.087891\n",
      "Training Batch: 16249 Loss: 3475.181641\n",
      "Training Batch: 16250 Loss: 3397.937988\n",
      "Training Batch: 16251 Loss: 3383.284668\n",
      "Training Batch: 16252 Loss: 3314.325195\n",
      "Training Batch: 16253 Loss: 3362.410645\n",
      "Training Batch: 16254 Loss: 3329.515625\n",
      "Training Batch: 16255 Loss: 3404.579346\n",
      "Training Batch: 16256 Loss: 3363.798096\n",
      "Training Batch: 16257 Loss: 3481.269043\n",
      "Training Batch: 16258 Loss: 3411.660156\n",
      "Training Batch: 16259 Loss: 3441.097900\n",
      "Training Batch: 16260 Loss: 3409.552490\n",
      "Training Batch: 16261 Loss: 3334.898926\n",
      "Training Batch: 16262 Loss: 3432.316650\n",
      "Training Batch: 16263 Loss: 3409.031738\n",
      "Training Batch: 16264 Loss: 3509.645508\n",
      "Training Batch: 16265 Loss: 3787.171143\n",
      "Training Batch: 16266 Loss: 3508.802246\n",
      "Training Batch: 16267 Loss: 3601.224121\n",
      "Training Batch: 16268 Loss: 3497.263672\n",
      "Training Batch: 16269 Loss: 3379.703613\n",
      "Training Batch: 16270 Loss: 3476.791992\n",
      "Training Batch: 16271 Loss: 3444.082520\n",
      "Training Batch: 16272 Loss: 3695.865723\n",
      "Training Batch: 16273 Loss: 4122.357910\n",
      "Training Batch: 16274 Loss: 3678.704590\n",
      "Training Batch: 16275 Loss: 3489.882324\n",
      "Training Batch: 16276 Loss: 3652.997070\n",
      "Training Batch: 16277 Loss: 3830.903076\n",
      "Training Batch: 16278 Loss: 4052.197998\n",
      "Training Batch: 16279 Loss: 3785.341553\n",
      "Training Batch: 16280 Loss: 3586.388184\n",
      "Training Batch: 16281 Loss: 3477.308105\n",
      "Training Batch: 16282 Loss: 3539.638184\n",
      "Training Batch: 16283 Loss: 3571.346436\n",
      "Training Batch: 16284 Loss: 3580.686523\n",
      "Training Batch: 16285 Loss: 3472.812500\n",
      "Training Batch: 16286 Loss: 4050.844238\n",
      "Training Batch: 16287 Loss: 3778.060059\n",
      "Training Batch: 16288 Loss: 3914.325684\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Batch: 16289 Loss: 3715.295898\n",
      "Training Batch: 16290 Loss: 3588.570312\n",
      "Training Batch: 16291 Loss: 3470.469971\n",
      "Training Batch: 16292 Loss: 3684.784180\n",
      "Training Batch: 16293 Loss: 3667.593018\n",
      "Training Batch: 16294 Loss: 3645.129395\n",
      "Training Batch: 16295 Loss: 3569.447021\n",
      "Training Batch: 16296 Loss: 3428.823242\n",
      "Training Batch: 16297 Loss: 3390.850098\n",
      "Training Batch: 16298 Loss: 3421.958496\n",
      "Training Batch: 16299 Loss: 3440.245850\n",
      "Training Batch: 16300 Loss: 3369.691895\n",
      "Training Batch: 16301 Loss: 3529.676758\n",
      "Training Batch: 16302 Loss: 3538.756836\n",
      "Training Batch: 16303 Loss: 3497.437012\n",
      "Training Batch: 16304 Loss: 3774.104980\n",
      "Training Batch: 16305 Loss: 3568.503174\n",
      "Training Batch: 16306 Loss: 3872.318359\n",
      "Training Batch: 16307 Loss: 3806.568359\n",
      "Training Batch: 16308 Loss: 3406.442383\n",
      "Training Batch: 16309 Loss: 3367.435303\n",
      "Training Batch: 16310 Loss: 3502.722656\n",
      "Training Batch: 16311 Loss: 3964.540039\n",
      "Training Batch: 16312 Loss: 3550.359375\n",
      "Training Batch: 16313 Loss: 3527.686279\n",
      "Training Batch: 16314 Loss: 3466.095459\n",
      "Training Batch: 16315 Loss: 3667.238525\n",
      "Training Batch: 16316 Loss: 3434.544678\n",
      "Training Batch: 16317 Loss: 3438.513672\n",
      "Training Batch: 16318 Loss: 3394.248535\n",
      "Training Batch: 16319 Loss: 3410.586914\n",
      "Training Batch: 16320 Loss: 3405.618652\n",
      "Training Batch: 16321 Loss: 3432.983398\n",
      "Training Batch: 16322 Loss: 3485.665039\n",
      "Training Batch: 16323 Loss: 3444.708496\n",
      "Training Batch: 16324 Loss: 3566.987061\n",
      "Training Batch: 16325 Loss: 3577.859375\n",
      "Training Batch: 16326 Loss: 3427.016602\n",
      "Training Batch: 16327 Loss: 3393.321289\n",
      "Training Batch: 16328 Loss: 3431.497559\n",
      "Training Batch: 16329 Loss: 3539.505371\n",
      "Training Batch: 16330 Loss: 3502.106445\n",
      "Training Batch: 16331 Loss: 3393.276855\n",
      "Training Batch: 16332 Loss: 3496.592041\n",
      "Training Batch: 16333 Loss: 3850.565918\n",
      "Training Batch: 16334 Loss: 3547.422852\n",
      "Training Batch: 16335 Loss: 3731.727539\n",
      "Training Batch: 16336 Loss: 3545.107910\n",
      "Training Batch: 16337 Loss: 3423.486572\n",
      "Training Batch: 16338 Loss: 3367.045898\n",
      "Training Batch: 16339 Loss: 3368.100586\n",
      "Training Batch: 16340 Loss: 3578.695312\n",
      "Training Batch: 16341 Loss: 3425.467285\n",
      "Training Batch: 16342 Loss: 3445.437988\n",
      "Training Batch: 16343 Loss: 3463.914062\n",
      "Training Batch: 16344 Loss: 3353.253174\n",
      "Training Batch: 16345 Loss: 3443.847900\n",
      "Training Batch: 16346 Loss: 3348.764160\n",
      "Training Batch: 16347 Loss: 3541.118408\n",
      "Training Batch: 16348 Loss: 3475.910156\n",
      "Training Batch: 16349 Loss: 3635.638672\n",
      "Training Batch: 16350 Loss: 3378.312012\n",
      "Training Batch: 16351 Loss: 3392.389404\n",
      "Training Batch: 16352 Loss: 3512.308594\n",
      "Training Batch: 16353 Loss: 3459.462402\n",
      "Training Batch: 16354 Loss: 3381.152832\n",
      "Training Batch: 16355 Loss: 3386.526123\n",
      "Training Batch: 16356 Loss: 3525.511230\n",
      "Training Batch: 16357 Loss: 3488.773438\n",
      "Training Batch: 16358 Loss: 3389.367432\n",
      "Training Batch: 16359 Loss: 3513.271484\n",
      "Training Batch: 16360 Loss: 3448.181152\n",
      "Training Batch: 16361 Loss: 3437.686523\n",
      "Training Batch: 16362 Loss: 3392.900391\n",
      "Training Batch: 16363 Loss: 3557.221191\n",
      "Training Batch: 16364 Loss: 3446.137695\n",
      "Training Batch: 16365 Loss: 3574.838135\n",
      "Training Batch: 16366 Loss: 3497.321777\n",
      "Training Batch: 16367 Loss: 3524.114746\n",
      "Training Batch: 16368 Loss: 3508.167480\n",
      "Training Batch: 16369 Loss: 3578.837891\n",
      "Training Batch: 16370 Loss: 3759.895020\n",
      "Training Batch: 16371 Loss: 3388.775879\n",
      "Training Batch: 16372 Loss: 3451.152344\n",
      "Training Batch: 16373 Loss: 3349.077393\n",
      "Training Batch: 16374 Loss: 3499.764160\n",
      "Training Batch: 16375 Loss: 3419.110352\n",
      "Training Batch: 16376 Loss: 3517.229004\n",
      "Training Batch: 16377 Loss: 3492.907227\n",
      "Training Batch: 16378 Loss: 3704.602539\n",
      "Training Batch: 16379 Loss: 3569.750488\n",
      "Training Batch: 16380 Loss: 3613.286133\n",
      "Training Batch: 16381 Loss: 3444.375977\n",
      "Training Batch: 16382 Loss: 3548.372559\n",
      "Training Batch: 16383 Loss: 3462.958740\n",
      "Training Batch: 16384 Loss: 3438.953613\n",
      "Training Batch: 16385 Loss: 3566.503418\n",
      "Training Batch: 16386 Loss: 3432.171875\n",
      "Training Batch: 16387 Loss: 3571.062012\n",
      "Training Batch: 16388 Loss: 3540.653564\n",
      "Training Batch: 16389 Loss: 3502.513672\n",
      "Training Batch: 16390 Loss: 3418.261230\n",
      "Training Batch: 16391 Loss: 3719.046875\n",
      "Training Batch: 16392 Loss: 3423.699707\n",
      "Training Batch: 16393 Loss: 3604.034668\n",
      "Training Batch: 16394 Loss: 3475.327637\n",
      "Training Batch: 16395 Loss: 3471.793701\n",
      "Training Batch: 16396 Loss: 3595.350830\n",
      "Training Batch: 16397 Loss: 3468.688477\n",
      "Training Batch: 16398 Loss: 3395.163330\n",
      "Training Batch: 16399 Loss: 3458.717285\n",
      "Training Batch: 16400 Loss: 3588.354980\n",
      "Training Batch: 16401 Loss: 3540.586914\n",
      "Training Batch: 16402 Loss: 3819.921875\n",
      "Training Batch: 16403 Loss: 3775.454102\n",
      "Training Batch: 16404 Loss: 3405.066406\n",
      "Training Batch: 16405 Loss: 3640.104736\n",
      "Training Batch: 16406 Loss: 3332.235596\n",
      "Training Batch: 16407 Loss: 3603.698730\n",
      "Training Batch: 16408 Loss: 3565.569824\n",
      "Training Batch: 16409 Loss: 3381.466797\n",
      "Training Batch: 16410 Loss: 3488.867676\n",
      "Training Batch: 16411 Loss: 3442.888672\n",
      "Training Batch: 16412 Loss: 3392.576660\n",
      "Training Batch: 16413 Loss: 3457.389648\n",
      "Training Batch: 16414 Loss: 3484.633545\n",
      "Training Batch: 16415 Loss: 3436.946777\n",
      "Training Batch: 16416 Loss: 3335.673828\n",
      "Training Batch: 16417 Loss: 3407.547363\n",
      "Training Batch: 16418 Loss: 3416.089111\n",
      "Training Batch: 16419 Loss: 3493.654785\n",
      "Training Batch: 16420 Loss: 3590.524902\n",
      "Training Batch: 16421 Loss: 3347.328613\n",
      "Training Batch: 16422 Loss: 3484.404297\n",
      "Training Batch: 16423 Loss: 3491.951660\n",
      "Training Batch: 16424 Loss: 3445.894043\n",
      "Training Batch: 16425 Loss: 3419.503906\n",
      "Training Batch: 16426 Loss: 3333.524170\n",
      "Training Batch: 16427 Loss: 3312.360107\n",
      "Training Batch: 16428 Loss: 3407.104248\n",
      "Training Batch: 16429 Loss: 3432.008545\n",
      "Training Batch: 16430 Loss: 3377.493896\n",
      "Training Batch: 16431 Loss: 3428.605713\n",
      "Training Batch: 16432 Loss: 3551.973145\n",
      "Training Batch: 16433 Loss: 3337.429443\n",
      "Training Batch: 16434 Loss: 3560.664551\n",
      "Training Batch: 16435 Loss: 3592.124512\n",
      "Training Batch: 16436 Loss: 3499.598145\n",
      "Training Batch: 16437 Loss: 3687.499512\n",
      "Training Batch: 16438 Loss: 3630.479492\n",
      "Training Batch: 16439 Loss: 3415.497070\n",
      "Training Batch: 16440 Loss: 3512.123291\n",
      "Training Batch: 16441 Loss: 3461.049805\n",
      "Training Batch: 16442 Loss: 3523.739258\n",
      "Training Batch: 16443 Loss: 3460.499268\n",
      "Training Batch: 16444 Loss: 3531.291748\n",
      "Training Batch: 16445 Loss: 3457.458496\n",
      "Training Batch: 16446 Loss: 3498.927246\n",
      "Training Batch: 16447 Loss: 3488.982422\n",
      "Training Batch: 16448 Loss: 3452.417236\n",
      "Training Batch: 16449 Loss: 3532.602051\n",
      "Training Batch: 16450 Loss: 3375.338379\n",
      "Training Batch: 16451 Loss: 3570.696777\n",
      "Training Batch: 16452 Loss: 3444.277344\n",
      "Training Batch: 16453 Loss: 3476.436035\n",
      "Training Batch: 16454 Loss: 3419.510742\n",
      "Training Batch: 16455 Loss: 3453.939209\n",
      "Training Batch: 16456 Loss: 3618.205322\n",
      "Training Batch: 16457 Loss: 3445.126953\n",
      "Training Batch: 16458 Loss: 3487.561279\n",
      "Training Batch: 16459 Loss: 3450.705566\n",
      "Training Batch: 16460 Loss: 3489.627441\n",
      "Training Batch: 16461 Loss: 3408.557129\n",
      "Training Batch: 16462 Loss: 3471.046875\n",
      "Training Batch: 16463 Loss: 3533.362793\n",
      "Training Batch: 16464 Loss: 4132.049805\n",
      "Training Batch: 16465 Loss: 3525.086914\n",
      "Training Batch: 16466 Loss: 3433.098145\n",
      "Training Batch: 16467 Loss: 3377.421875\n",
      "Training Batch: 16468 Loss: 3559.716309\n",
      "Training Batch: 16469 Loss: 3571.927734\n",
      "Training Batch: 16470 Loss: 3679.731689\n",
      "Training Batch: 16471 Loss: 3529.774902\n",
      "Training Batch: 16472 Loss: 3871.338623\n",
      "Training Batch: 16473 Loss: 3504.903076\n",
      "Training Batch: 16474 Loss: 3520.091309\n",
      "Training Batch: 16475 Loss: 3410.198730\n",
      "Training Batch: 16476 Loss: 3465.728027\n",
      "Training Batch: 16477 Loss: 3458.032227\n",
      "Training Batch: 16478 Loss: 3443.549316\n",
      "Training Batch: 16479 Loss: 3445.040527\n",
      "Training Batch: 16480 Loss: 3419.819824\n",
      "Training Batch: 16481 Loss: 3583.973633\n",
      "Training Batch: 16482 Loss: 3330.745361\n",
      "Training Batch: 16483 Loss: 3508.905762\n",
      "Training Batch: 16484 Loss: 3486.759521\n",
      "Training Batch: 16485 Loss: 3571.545654\n",
      "Training Batch: 16486 Loss: 3369.410156\n",
      "Training Batch: 16487 Loss: 3476.814453\n",
      "Training Batch: 16488 Loss: 3442.509766\n",
      "Training Batch: 16489 Loss: 3491.336914\n",
      "Training Batch: 16490 Loss: 3504.507812\n",
      "Training Batch: 16491 Loss: 3466.154297\n",
      "Training Batch: 16492 Loss: 3596.895020\n",
      "Training Batch: 16493 Loss: 3449.376465\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Batch: 16494 Loss: 3535.029785\n",
      "Training Batch: 16495 Loss: 3754.060059\n",
      "Training Batch: 16496 Loss: 3594.929688\n",
      "Training Batch: 16497 Loss: 3414.911621\n",
      "Training Batch: 16498 Loss: 3384.808594\n",
      "Training Batch: 16499 Loss: 3500.228027\n",
      "Training Batch: 16500 Loss: 3614.639648\n",
      "Training Batch: 16501 Loss: 3401.201416\n",
      "Training Batch: 16502 Loss: 3625.053223\n",
      "Training Batch: 16503 Loss: 3398.285645\n",
      "Training Batch: 16504 Loss: 3475.706055\n",
      "Training Batch: 16505 Loss: 3418.649414\n",
      "Training Batch: 16506 Loss: 3743.076660\n",
      "Training Batch: 16507 Loss: 3560.678223\n",
      "Training Batch: 16508 Loss: 3560.482910\n",
      "Training Batch: 16509 Loss: 3469.293945\n",
      "Training Batch: 16510 Loss: 3606.167480\n",
      "Training Batch: 16511 Loss: 3487.520996\n",
      "Training Batch: 16512 Loss: 3411.655762\n",
      "Training Batch: 16513 Loss: 3394.416504\n",
      "Training Batch: 16514 Loss: 3484.793213\n",
      "Training Batch: 16515 Loss: 3443.046387\n",
      "Training Batch: 16516 Loss: 3412.534668\n",
      "Training Batch: 16517 Loss: 3561.300781\n",
      "Training Batch: 16518 Loss: 3447.909912\n",
      "Training Batch: 16519 Loss: 3472.400146\n",
      "Training Batch: 16520 Loss: 3422.138672\n",
      "Training Batch: 16521 Loss: 3409.412842\n",
      "Training Batch: 16522 Loss: 3446.755615\n",
      "Training Batch: 16523 Loss: 3423.134033\n",
      "Training Batch: 16524 Loss: 3300.614990\n",
      "Training Batch: 16525 Loss: 3442.175293\n",
      "Training Batch: 16526 Loss: 3436.546143\n",
      "Training Batch: 16527 Loss: 3298.525146\n",
      "Training Batch: 16528 Loss: 3432.821045\n",
      "Training Batch: 16529 Loss: 3418.628418\n",
      "Training Batch: 16530 Loss: 3483.637207\n",
      "Training Batch: 16531 Loss: 3374.454590\n",
      "Training Batch: 16532 Loss: 3508.335938\n",
      "Training Batch: 16533 Loss: 3412.879883\n",
      "Training Batch: 16534 Loss: 3352.302246\n",
      "Training Batch: 16535 Loss: 3402.155762\n",
      "Training Batch: 16536 Loss: 3380.300537\n",
      "Training Batch: 16537 Loss: 3413.137695\n",
      "Training Batch: 16538 Loss: 3395.583008\n",
      "Training Batch: 16539 Loss: 3344.677979\n",
      "Training Batch: 16540 Loss: 3507.441650\n",
      "Training Batch: 16541 Loss: 3400.032715\n",
      "Training Batch: 16542 Loss: 3455.273438\n",
      "Training Batch: 16543 Loss: 3588.577148\n",
      "Training Batch: 16544 Loss: 3394.688232\n",
      "Training Batch: 16545 Loss: 3389.459961\n",
      "Training Batch: 16546 Loss: 3525.626221\n",
      "Training Batch: 16547 Loss: 3339.972168\n",
      "Training Batch: 16548 Loss: 3519.983887\n",
      "Training Batch: 16549 Loss: 3505.043457\n",
      "Training Batch: 16550 Loss: 3465.750000\n",
      "Training Batch: 16551 Loss: 3436.649170\n",
      "Training Batch: 16552 Loss: 3547.133789\n",
      "Training Batch: 16553 Loss: 3479.719238\n",
      "Training Batch: 16554 Loss: 3565.520020\n",
      "Training Batch: 16555 Loss: 3491.573242\n",
      "Training Batch: 16556 Loss: 3401.596680\n",
      "Training Batch: 16557 Loss: 3494.691895\n",
      "Training Batch: 16558 Loss: 3431.027344\n",
      "Training Batch: 16559 Loss: 3431.483887\n",
      "Training Batch: 16560 Loss: 3385.996338\n",
      "Training Batch: 16561 Loss: 3353.210938\n",
      "Training Batch: 16562 Loss: 3421.476562\n",
      "Training Batch: 16563 Loss: 3455.216797\n",
      "Training Batch: 16564 Loss: 3393.981934\n",
      "Training Batch: 16565 Loss: 3358.036133\n",
      "Training Batch: 16566 Loss: 3472.732422\n",
      "Training Batch: 16567 Loss: 3648.748047\n",
      "Training Batch: 16568 Loss: 3592.884277\n",
      "Training Batch: 16569 Loss: 3525.856201\n",
      "Training Batch: 16570 Loss: 3397.134277\n",
      "Training Batch: 16571 Loss: 3532.857178\n",
      "Training Batch: 16572 Loss: 3514.324219\n",
      "Training Batch: 16573 Loss: 3589.882324\n",
      "Training Batch: 16574 Loss: 3418.950195\n",
      "Training Batch: 16575 Loss: 3441.742188\n",
      "Training Batch: 16576 Loss: 3563.265137\n",
      "Training Batch: 16577 Loss: 3431.877930\n",
      "Training Batch: 16578 Loss: 3841.766602\n",
      "Training Batch: 16579 Loss: 3800.840088\n",
      "Training Batch: 16580 Loss: 3596.311523\n",
      "Training Batch: 16581 Loss: 3505.886963\n",
      "Training Batch: 16582 Loss: 3430.094482\n",
      "Training Batch: 16583 Loss: 3413.126465\n",
      "Training Batch: 16584 Loss: 3776.770264\n",
      "Training Batch: 16585 Loss: 3519.486572\n",
      "Training Batch: 16586 Loss: 3583.096924\n",
      "Training Batch: 16587 Loss: 3753.758789\n",
      "Training Batch: 16588 Loss: 3418.520020\n",
      "Training Batch: 16589 Loss: 3526.018066\n",
      "Training Batch: 16590 Loss: 3572.261475\n",
      "Training Batch: 16591 Loss: 3505.489014\n",
      "Training Batch: 16592 Loss: 3400.676514\n",
      "Training Batch: 16593 Loss: 3521.012207\n",
      "Training Batch: 16594 Loss: 3368.000977\n",
      "Training Batch: 16595 Loss: 3467.230225\n",
      "Training Batch: 16596 Loss: 3427.479492\n",
      "Training Batch: 16597 Loss: 3401.503418\n",
      "Training Batch: 16598 Loss: 3543.328857\n",
      "Training Batch: 16599 Loss: 3336.277344\n",
      "Training Batch: 16600 Loss: 3355.470215\n",
      "Training Batch: 16601 Loss: 3309.584961\n",
      "Training Batch: 16602 Loss: 3400.083008\n",
      "Training Batch: 16603 Loss: 3711.798828\n",
      "Training Batch: 16604 Loss: 3618.352539\n",
      "Training Batch: 16605 Loss: 3556.788574\n",
      "Training Batch: 16606 Loss: 3593.257324\n",
      "Training Batch: 16607 Loss: 3577.760010\n",
      "Training Batch: 16608 Loss: 3649.822510\n",
      "Training Batch: 16609 Loss: 3563.966797\n",
      "Training Batch: 16610 Loss: 3410.654785\n",
      "Training Batch: 16611 Loss: 3388.674316\n",
      "Training Batch: 16612 Loss: 3353.794922\n",
      "Training Batch: 16613 Loss: 3424.710449\n",
      "Training Batch: 16614 Loss: 3351.920410\n",
      "Training Batch: 16615 Loss: 3424.825684\n",
      "Training Batch: 16616 Loss: 3548.547852\n",
      "Training Batch: 16617 Loss: 3407.191895\n",
      "Training Batch: 16618 Loss: 3628.202148\n",
      "Training Batch: 16619 Loss: 3526.757812\n",
      "Training Batch: 16620 Loss: 3395.858887\n",
      "Training Batch: 16621 Loss: 3461.643555\n",
      "Training Batch: 16622 Loss: 3398.939453\n",
      "Training Batch: 16623 Loss: 3471.201904\n",
      "Training Batch: 16624 Loss: 3380.778564\n",
      "Training Batch: 16625 Loss: 3527.693848\n",
      "Training Batch: 16626 Loss: 3567.524414\n",
      "Training Batch: 16627 Loss: 3434.712891\n",
      "Training Batch: 16628 Loss: 3515.367188\n",
      "Training Batch: 16629 Loss: 3537.565430\n",
      "Training Batch: 16630 Loss: 3506.395264\n",
      "Training Batch: 16631 Loss: 3410.471680\n",
      "Training Batch: 16632 Loss: 3422.159180\n",
      "Training Batch: 16633 Loss: 3332.624512\n",
      "Training Batch: 16634 Loss: 3454.812256\n",
      "Training Batch: 16635 Loss: 3447.754883\n",
      "Training Batch: 16636 Loss: 3358.797607\n",
      "Training Batch: 16637 Loss: 3395.632324\n",
      "Training Batch: 16638 Loss: 3391.153320\n",
      "Training Batch: 16639 Loss: 3381.937500\n",
      "Training Batch: 16640 Loss: 3321.167725\n",
      "Training Batch: 16641 Loss: 3417.986328\n",
      "Training Batch: 16642 Loss: 3402.562744\n",
      "Training Batch: 16643 Loss: 3452.036865\n",
      "Training Batch: 16644 Loss: 3505.362305\n",
      "Training Batch: 16645 Loss: 3418.910156\n",
      "Training Batch: 16646 Loss: 3457.258301\n",
      "Training Batch: 16647 Loss: 3544.914795\n",
      "Training Batch: 16648 Loss: 3369.013672\n",
      "Training Batch: 16649 Loss: 3435.736816\n",
      "Training Batch: 16650 Loss: 3450.258789\n",
      "Training Batch: 16651 Loss: 3659.187012\n",
      "Training Batch: 16652 Loss: 3913.142334\n",
      "Training Batch: 16653 Loss: 3722.690186\n",
      "Training Batch: 16654 Loss: 3880.974609\n",
      "Training Batch: 16655 Loss: 3843.932861\n",
      "Training Batch: 16656 Loss: 3457.450439\n",
      "Training Batch: 16657 Loss: 3773.530762\n",
      "Training Batch: 16658 Loss: 3563.170898\n",
      "Training Batch: 16659 Loss: 3485.500977\n",
      "Training Batch: 16660 Loss: 3454.371582\n",
      "Training Batch: 16661 Loss: 3432.164062\n",
      "Training Batch: 16662 Loss: 3333.580566\n",
      "Training Batch: 16663 Loss: 3476.966309\n",
      "Training Batch: 16664 Loss: 3452.350830\n",
      "Training Batch: 16665 Loss: 3490.729004\n",
      "Training Batch: 16666 Loss: 3661.732910\n",
      "Training Batch: 16667 Loss: 3387.769043\n",
      "Training Batch: 16668 Loss: 3412.108398\n",
      "Training Batch: 16669 Loss: 3908.777344\n",
      "Training Batch: 16670 Loss: 3562.291504\n",
      "Training Batch: 16671 Loss: 3423.842773\n",
      "Training Batch: 16672 Loss: 3503.291992\n",
      "Training Batch: 16673 Loss: 3480.324463\n",
      "Training Batch: 16674 Loss: 3391.604004\n",
      "Training Batch: 16675 Loss: 3585.350586\n",
      "Training Batch: 16676 Loss: 3422.395996\n",
      "Training Batch: 16677 Loss: 3794.203613\n",
      "Training Batch: 16678 Loss: 3575.672852\n",
      "Training Batch: 16679 Loss: 3480.635010\n",
      "Training Batch: 16680 Loss: 3498.389160\n",
      "Training Batch: 16681 Loss: 3551.645752\n",
      "Training Batch: 16682 Loss: 3682.337402\n",
      "Training Batch: 16683 Loss: 3501.339844\n",
      "Training Batch: 16684 Loss: 3488.602539\n",
      "Training Batch: 16685 Loss: 3460.655273\n",
      "Training Batch: 16686 Loss: 3299.044434\n",
      "Training Batch: 16687 Loss: 3433.525391\n",
      "Training Batch: 16688 Loss: 3563.298828\n",
      "Training Batch: 16689 Loss: 3532.555664\n",
      "Training Batch: 16690 Loss: 3772.413330\n",
      "Training Batch: 16691 Loss: 3408.308105\n",
      "Training Batch: 16692 Loss: 3419.178467\n",
      "Training Batch: 16693 Loss: 3597.561768\n",
      "Training Batch: 16694 Loss: 3442.567383\n",
      "Training Batch: 16695 Loss: 3379.939209\n",
      "Training Batch: 16696 Loss: 3501.567383\n",
      "Training Batch: 16697 Loss: 3517.727539\n",
      "Training Batch: 16698 Loss: 3524.973145\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Batch: 16699 Loss: 3389.610352\n",
      "Training Batch: 16700 Loss: 3572.940430\n",
      "Training Batch: 16701 Loss: 3640.641113\n",
      "Training Batch: 16702 Loss: 3575.309814\n",
      "Training Batch: 16703 Loss: 3528.176270\n",
      "Training Batch: 16704 Loss: 3489.719727\n",
      "Training Batch: 16705 Loss: 3513.729248\n",
      "Training Batch: 16706 Loss: 3373.494873\n",
      "Training Batch: 16707 Loss: 3336.418457\n",
      "Training Batch: 16708 Loss: 3424.888184\n",
      "Training Batch: 16709 Loss: 3434.429688\n",
      "Training Batch: 16710 Loss: 3354.798340\n",
      "Training Batch: 16711 Loss: 3530.660156\n",
      "Training Batch: 16712 Loss: 3514.463867\n",
      "Training Batch: 16713 Loss: 3466.899902\n",
      "Training Batch: 16714 Loss: 3420.257080\n",
      "Training Batch: 16715 Loss: 3448.228027\n",
      "Training Batch: 16716 Loss: 3492.145020\n",
      "Training Batch: 16717 Loss: 3770.704346\n",
      "Training Batch: 16718 Loss: 3352.236816\n",
      "Training Batch: 16719 Loss: 3529.508301\n",
      "Training Batch: 16720 Loss: 3382.243164\n",
      "Training Batch: 16721 Loss: 3308.679199\n",
      "Training Batch: 16722 Loss: 3602.913818\n",
      "Training Batch: 16723 Loss: 3657.729736\n",
      "Training Batch: 16724 Loss: 3637.854492\n",
      "Training Batch: 16725 Loss: 3360.842773\n",
      "Training Batch: 16726 Loss: 3803.044434\n",
      "Training Batch: 16727 Loss: 3439.751465\n",
      "Training Batch: 16728 Loss: 3424.964355\n",
      "Training Batch: 16729 Loss: 3458.333496\n",
      "Training Batch: 16730 Loss: 3502.653564\n",
      "Training Batch: 16731 Loss: 3669.452637\n",
      "Training Batch: 16732 Loss: 3369.937500\n",
      "Training Batch: 16733 Loss: 3497.052246\n",
      "Training Batch: 16734 Loss: 3466.029541\n",
      "Training Batch: 16735 Loss: 3400.798828\n",
      "Training Batch: 16736 Loss: 3389.350586\n",
      "Training Batch: 16737 Loss: 3455.997803\n",
      "Training Batch: 16738 Loss: 3645.833496\n",
      "Training Batch: 16739 Loss: 3335.885498\n",
      "Training Batch: 16740 Loss: 3418.644043\n",
      "Training Batch: 16741 Loss: 3575.330078\n",
      "Training Batch: 16742 Loss: 3416.723633\n",
      "Training Batch: 16743 Loss: 3377.973145\n",
      "Training Batch: 16744 Loss: 3445.871582\n",
      "Training Batch: 16745 Loss: 3412.721191\n",
      "Training Batch: 16746 Loss: 3380.578613\n",
      "Training Batch: 16747 Loss: 3475.406738\n",
      "Training Batch: 16748 Loss: 3373.979004\n",
      "Training Batch: 16749 Loss: 3441.274658\n",
      "Training Batch: 16750 Loss: 3351.034424\n",
      "Training Batch: 16751 Loss: 3470.104004\n",
      "Training Batch: 16752 Loss: 3375.316406\n",
      "Training Batch: 16753 Loss: 3531.894775\n",
      "Training Batch: 16754 Loss: 3520.344727\n",
      "Training Batch: 16755 Loss: 3620.567383\n",
      "Training Batch: 16756 Loss: 3469.757568\n",
      "Training Batch: 16757 Loss: 3589.262695\n",
      "Training Batch: 16758 Loss: 3593.218750\n",
      "Training Batch: 16759 Loss: 3498.406250\n",
      "Training Batch: 16760 Loss: 3700.892090\n",
      "Training Batch: 16761 Loss: 3445.550293\n",
      "Training Batch: 16762 Loss: 3598.090332\n",
      "Training Batch: 16763 Loss: 3496.365234\n",
      "Training Batch: 16764 Loss: 3513.481934\n",
      "Training Batch: 16765 Loss: 3355.570068\n",
      "Training Batch: 16766 Loss: 3538.471191\n",
      "Training Batch: 16767 Loss: 3517.005859\n",
      "Training Batch: 16768 Loss: 3484.828613\n",
      "Training Batch: 16769 Loss: 3457.517578\n",
      "Training Batch: 16770 Loss: 3439.174561\n",
      "Training Batch: 16771 Loss: 3431.890625\n",
      "Training Batch: 16772 Loss: 3400.427002\n",
      "Training Batch: 16773 Loss: 3385.894043\n",
      "Training Batch: 16774 Loss: 3383.027832\n",
      "Training Batch: 16775 Loss: 3358.463135\n",
      "Training Batch: 16776 Loss: 3523.595215\n",
      "Training Batch: 16777 Loss: 3466.868652\n",
      "Training Batch: 16778 Loss: 3371.947754\n",
      "Training Batch: 16779 Loss: 3403.672852\n",
      "Training Batch: 16780 Loss: 3682.021973\n",
      "Training Batch: 16781 Loss: 3462.554688\n",
      "Training Batch: 16782 Loss: 3559.469727\n",
      "Training Batch: 16783 Loss: 3686.982910\n",
      "Training Batch: 16784 Loss: 3545.253906\n",
      "Training Batch: 16785 Loss: 3561.597900\n",
      "Training Batch: 16786 Loss: 3601.673340\n",
      "Training Batch: 16787 Loss: 3598.467773\n",
      "Training Batch: 16788 Loss: 3363.331543\n",
      "Training Batch: 16789 Loss: 3367.431152\n",
      "Training Batch: 16790 Loss: 3414.254395\n",
      "Training Batch: 16791 Loss: 3446.417480\n",
      "Training Batch: 16792 Loss: 3406.933105\n",
      "Training Batch: 16793 Loss: 3388.740234\n",
      "Training Batch: 16794 Loss: 3740.389404\n",
      "Training Batch: 16795 Loss: 3582.527832\n",
      "Training Batch: 16796 Loss: 3427.240234\n",
      "Training Batch: 16797 Loss: 3402.060547\n",
      "Training Batch: 16798 Loss: 3381.511230\n",
      "Training Batch: 16799 Loss: 3407.155273\n",
      "Training Batch: 16800 Loss: 3389.989746\n",
      "Training Batch: 16801 Loss: 3441.399658\n",
      "Training Batch: 16802 Loss: 3428.666260\n",
      "Training Batch: 16803 Loss: 3460.545898\n",
      "Training Batch: 16804 Loss: 3687.801758\n",
      "Training Batch: 16805 Loss: 3678.143555\n",
      "Training Batch: 16806 Loss: 3508.073730\n",
      "Training Batch: 16807 Loss: 3447.613770\n",
      "Training Batch: 16808 Loss: 3488.442139\n",
      "Training Batch: 16809 Loss: 3557.194824\n",
      "Training Batch: 16810 Loss: 3470.738525\n",
      "Training Batch: 16811 Loss: 4426.061035\n",
      "Training Batch: 16812 Loss: 4283.895508\n",
      "Training Batch: 16813 Loss: 3734.789795\n",
      "Training Batch: 16814 Loss: 3523.935547\n",
      "Training Batch: 16815 Loss: 3527.475586\n",
      "Training Batch: 16816 Loss: 3555.614258\n",
      "Training Batch: 16817 Loss: 3601.413574\n",
      "Training Batch: 16818 Loss: 3404.484375\n",
      "Training Batch: 16819 Loss: 3380.767578\n",
      "Training Batch: 16820 Loss: 3445.612305\n",
      "Training Batch: 16821 Loss: 3449.312500\n",
      "Training Batch: 16822 Loss: 3543.173828\n",
      "Training Batch: 16823 Loss: 3565.488281\n",
      "Training Batch: 16824 Loss: 3399.712402\n",
      "Training Batch: 16825 Loss: 3349.921631\n",
      "Training Batch: 16826 Loss: 3486.367188\n",
      "Training Batch: 16827 Loss: 3446.371582\n",
      "Training Batch: 16828 Loss: 3517.465088\n",
      "Training Batch: 16829 Loss: 3442.592529\n",
      "Training Batch: 16830 Loss: 3535.322510\n",
      "Training Batch: 16831 Loss: 3477.678223\n",
      "Training Batch: 16832 Loss: 3451.697754\n",
      "Training Batch: 16833 Loss: 3598.036621\n",
      "Training Batch: 16834 Loss: 3343.765137\n",
      "Training Batch: 16835 Loss: 3338.108887\n",
      "Training Batch: 16836 Loss: 3395.765625\n",
      "Training Batch: 16837 Loss: 3463.692383\n",
      "Training Batch: 16838 Loss: 3329.722900\n",
      "Training Batch: 16839 Loss: 3652.566895\n",
      "Training Batch: 16840 Loss: 3463.607422\n",
      "Training Batch: 16841 Loss: 3409.574707\n",
      "Training Batch: 16842 Loss: 3494.536377\n",
      "Training Batch: 16843 Loss: 3409.719727\n",
      "Training Batch: 16844 Loss: 3443.875000\n",
      "Training Batch: 16845 Loss: 3401.253906\n",
      "Training Batch: 16846 Loss: 3344.519043\n",
      "Training Batch: 16847 Loss: 3422.641602\n",
      "Training Batch: 16848 Loss: 3460.704834\n",
      "Training Batch: 16849 Loss: 3424.161865\n",
      "Training Batch: 16850 Loss: 3595.721191\n",
      "Training Batch: 16851 Loss: 3460.995605\n",
      "Training Batch: 16852 Loss: 3531.799072\n",
      "Training Batch: 16853 Loss: 3485.752930\n",
      "Training Batch: 16854 Loss: 3328.425537\n",
      "Training Batch: 16855 Loss: 3427.787598\n",
      "Training Batch: 16856 Loss: 3456.523438\n",
      "Training Batch: 16857 Loss: 3461.263672\n",
      "Training Batch: 16858 Loss: 3600.730469\n",
      "Training Batch: 16859 Loss: 3468.120117\n",
      "Training Batch: 16860 Loss: 3517.647705\n",
      "Training Batch: 16861 Loss: 3377.610352\n",
      "Training Batch: 16862 Loss: 3411.495850\n",
      "Training Batch: 16863 Loss: 3453.205078\n",
      "Training Batch: 16864 Loss: 3437.981445\n",
      "Training Batch: 16865 Loss: 3446.582275\n",
      "Training Batch: 16866 Loss: 3450.278076\n",
      "Training Batch: 16867 Loss: 3404.262695\n",
      "Training Batch: 16868 Loss: 3596.687744\n",
      "Training Batch: 16869 Loss: 3424.089844\n",
      "Training Batch: 16870 Loss: 3592.043457\n",
      "Training Batch: 16871 Loss: 3397.242188\n",
      "Training Batch: 16872 Loss: 3460.005859\n",
      "Training Batch: 16873 Loss: 3383.423096\n",
      "Training Batch: 16874 Loss: 3350.506836\n",
      "Training Batch: 16875 Loss: 3346.287109\n",
      "Training Batch: 16876 Loss: 3359.828613\n",
      "Training Batch: 16877 Loss: 3436.396973\n",
      "Training Batch: 16878 Loss: 3520.172852\n",
      "Training Batch: 16879 Loss: 3520.755859\n",
      "Training Batch: 16880 Loss: 3400.031250\n",
      "Training Batch: 16881 Loss: 3423.564697\n",
      "Training Batch: 16882 Loss: 3442.819336\n",
      "Training Batch: 16883 Loss: 3354.659180\n",
      "Training Batch: 16884 Loss: 3454.467041\n",
      "Training Batch: 16885 Loss: 3441.145996\n",
      "Training Batch: 16886 Loss: 3465.089844\n",
      "Training Batch: 16887 Loss: 3515.203125\n",
      "Training Batch: 16888 Loss: 3543.552979\n",
      "Training Batch: 16889 Loss: 3503.676270\n",
      "Training Batch: 16890 Loss: 3491.481934\n",
      "Training Batch: 16891 Loss: 3668.679688\n",
      "Training Batch: 16892 Loss: 3538.737793\n",
      "Training Batch: 16893 Loss: 3549.617188\n",
      "Training Batch: 16894 Loss: 3726.112793\n",
      "Training Batch: 16895 Loss: 3421.272949\n",
      "Training Batch: 16896 Loss: 3501.395508\n",
      "Training Batch: 16897 Loss: 3547.501221\n",
      "Training Batch: 16898 Loss: 3405.329102\n",
      "Training Batch: 16899 Loss: 3348.084473\n",
      "Training Batch: 16900 Loss: 3457.891113\n",
      "Training Batch: 16901 Loss: 3549.228271\n",
      "Training Batch: 16902 Loss: 3284.452637\n",
      "Training Batch: 16903 Loss: 3429.337402\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Batch: 16904 Loss: 3641.937256\n",
      "Training Batch: 16905 Loss: 3548.732178\n",
      "Training Batch: 16906 Loss: 4084.944580\n",
      "Training Batch: 16907 Loss: 3723.105469\n",
      "Training Batch: 16908 Loss: 3581.612549\n",
      "Training Batch: 16909 Loss: 3434.965332\n",
      "Training Batch: 16910 Loss: 3597.689941\n",
      "Training Batch: 16911 Loss: 3442.393066\n",
      "Training Batch: 16912 Loss: 3399.142822\n",
      "Training Batch: 16913 Loss: 3683.542969\n",
      "Training Batch: 16914 Loss: 3452.355957\n",
      "Training Batch: 16915 Loss: 3355.418457\n",
      "Training Batch: 16916 Loss: 3331.833496\n",
      "Training Batch: 16917 Loss: 3513.119873\n",
      "Training Batch: 16918 Loss: 3440.805664\n",
      "Training Batch: 16919 Loss: 3509.957031\n",
      "Training Batch: 16920 Loss: 3554.400635\n",
      "Training Batch: 16921 Loss: 3717.307129\n",
      "Training Batch: 16922 Loss: 3479.835938\n",
      "Training Batch: 16923 Loss: 3437.238037\n",
      "Training Batch: 16924 Loss: 3420.171387\n",
      "Training Batch: 16925 Loss: 3516.175781\n",
      "Training Batch: 16926 Loss: 3478.677979\n",
      "Training Batch: 16927 Loss: 3476.266846\n",
      "Training Batch: 16928 Loss: 3447.179199\n",
      "Training Batch: 16929 Loss: 3531.486816\n",
      "Training Batch: 16930 Loss: 3661.208252\n",
      "Training Batch: 16931 Loss: 3481.656738\n",
      "Training Batch: 16932 Loss: 3505.106445\n",
      "Training Batch: 16933 Loss: 3526.018311\n",
      "Training Batch: 16934 Loss: 3530.277832\n",
      "Training Batch: 16935 Loss: 3529.301758\n",
      "Training Batch: 16936 Loss: 3462.072021\n",
      "Training Batch: 16937 Loss: 3339.041504\n",
      "Training Batch: 16938 Loss: 3371.021973\n",
      "Training Batch: 16939 Loss: 3416.004883\n",
      "Training Batch: 16940 Loss: 3406.774902\n",
      "Training Batch: 16941 Loss: 3590.480957\n",
      "Training Batch: 16942 Loss: 3448.859375\n",
      "Training Batch: 16943 Loss: 3428.249023\n",
      "Training Batch: 16944 Loss: 3625.835938\n",
      "Training Batch: 16945 Loss: 3470.170898\n",
      "Training Batch: 16946 Loss: 3429.043457\n",
      "Training Batch: 16947 Loss: 3337.832031\n",
      "Training Batch: 16948 Loss: 3573.486328\n",
      "Training Batch: 16949 Loss: 4438.425293\n",
      "Training Batch: 16950 Loss: 4049.158691\n",
      "Training Batch: 16951 Loss: 3752.286133\n",
      "Training Batch: 16952 Loss: 3961.495605\n",
      "Training Batch: 16953 Loss: 3731.105469\n",
      "Training Batch: 16954 Loss: 3561.719238\n",
      "Training Batch: 16955 Loss: 3924.676758\n",
      "Training Batch: 16956 Loss: 3595.067383\n",
      "Training Batch: 16957 Loss: 3447.210938\n",
      "Training Batch: 16958 Loss: 3454.340332\n",
      "Training Batch: 16959 Loss: 3648.490723\n",
      "Training Batch: 16960 Loss: 3458.243164\n",
      "Training Batch: 16961 Loss: 3457.696533\n",
      "Training Batch: 16962 Loss: 3456.888184\n",
      "Training Batch: 16963 Loss: 3495.657471\n",
      "Training Batch: 16964 Loss: 3602.380859\n",
      "Training Batch: 16965 Loss: 3437.102051\n",
      "Training Batch: 16966 Loss: 3463.098389\n",
      "Training Batch: 16967 Loss: 3546.071533\n",
      "Training Batch: 16968 Loss: 3394.563965\n",
      "Training Batch: 16969 Loss: 3588.340820\n",
      "Training Batch: 16970 Loss: 3528.238037\n",
      "Training Batch: 16971 Loss: 3721.024414\n",
      "Training Batch: 16972 Loss: 3364.726074\n",
      "Training Batch: 16973 Loss: 3581.769287\n",
      "Training Batch: 16974 Loss: 4135.006836\n",
      "Training Batch: 16975 Loss: 4105.900391\n",
      "Training Batch: 16976 Loss: 3903.035400\n",
      "Training Batch: 16977 Loss: 3702.525391\n",
      "Training Batch: 16978 Loss: 3618.791016\n",
      "Training Batch: 16979 Loss: 3495.686768\n",
      "Training Batch: 16980 Loss: 3424.163086\n",
      "Training Batch: 16981 Loss: 3328.875000\n",
      "Training Batch: 16982 Loss: 3444.451172\n",
      "Training Batch: 16983 Loss: 3534.725586\n",
      "Training Batch: 16984 Loss: 3627.606201\n",
      "Training Batch: 16985 Loss: 3542.146973\n",
      "Training Batch: 16986 Loss: 3632.969238\n",
      "Training Batch: 16987 Loss: 3687.298340\n",
      "Training Batch: 16988 Loss: 3478.715820\n",
      "Training Batch: 16989 Loss: 3765.338135\n",
      "Training Batch: 16990 Loss: 3501.116211\n",
      "Training Batch: 16991 Loss: 3423.317627\n",
      "Training Batch: 16992 Loss: 3392.419434\n",
      "Training Batch: 16993 Loss: 3495.059082\n",
      "Training Batch: 16994 Loss: 3445.433350\n",
      "Training Batch: 16995 Loss: 3396.600586\n",
      "Training Batch: 16996 Loss: 3370.182129\n",
      "Training Batch: 16997 Loss: 3502.026855\n",
      "Training Batch: 16998 Loss: 3504.304688\n",
      "Training Batch: 16999 Loss: 3564.417480\n",
      "Training Batch: 17000 Loss: 3609.135254\n",
      "Training Batch: 17001 Loss: 3483.320312\n",
      "Training Batch: 17002 Loss: 3552.255859\n",
      "Training Batch: 17003 Loss: 3600.807129\n",
      "Training Batch: 17004 Loss: 3626.077148\n",
      "Training Batch: 17005 Loss: 3423.275391\n",
      "Training Batch: 17006 Loss: 3407.004150\n",
      "Training Batch: 17007 Loss: 3392.506592\n",
      "Training Batch: 17008 Loss: 3342.018066\n",
      "Training Batch: 17009 Loss: 3488.529053\n",
      "Training Batch: 17010 Loss: 3540.120605\n",
      "Training Batch: 17011 Loss: 3427.196777\n",
      "Training Batch: 17012 Loss: 3687.910400\n",
      "Training Batch: 17013 Loss: 3496.563477\n",
      "Training Batch: 17014 Loss: 3601.143555\n",
      "Training Batch: 17015 Loss: 3606.884277\n",
      "Training Batch: 17016 Loss: 3388.564941\n",
      "Training Batch: 17017 Loss: 3379.256348\n",
      "Training Batch: 17018 Loss: 3360.657715\n",
      "Training Batch: 17019 Loss: 3461.782715\n",
      "Training Batch: 17020 Loss: 3361.909180\n",
      "Training Batch: 17021 Loss: 3467.395020\n",
      "Training Batch: 17022 Loss: 3337.948486\n",
      "Training Batch: 17023 Loss: 3784.498779\n",
      "Training Batch: 17024 Loss: 3755.997559\n",
      "Training Batch: 17025 Loss: 3459.708984\n",
      "Training Batch: 17026 Loss: 3363.710693\n",
      "Training Batch: 17027 Loss: 3431.262939\n",
      "Training Batch: 17028 Loss: 3444.837646\n",
      "Training Batch: 17029 Loss: 3536.076660\n",
      "Training Batch: 17030 Loss: 3360.779297\n",
      "Training Batch: 17031 Loss: 3421.338867\n",
      "Training Batch: 17032 Loss: 3648.270996\n",
      "Training Batch: 17033 Loss: 3756.686523\n",
      "Training Batch: 17034 Loss: 3539.731445\n",
      "Training Batch: 17035 Loss: 3416.801270\n",
      "Training Batch: 17036 Loss: 3620.536865\n",
      "Training Batch: 17037 Loss: 3520.961182\n",
      "Training Batch: 17038 Loss: 3556.135010\n",
      "Training Batch: 17039 Loss: 3716.442383\n",
      "Training Batch: 17040 Loss: 3390.791504\n",
      "Training Batch: 17041 Loss: 3501.649170\n",
      "Training Batch: 17042 Loss: 3415.548828\n",
      "Training Batch: 17043 Loss: 3586.733398\n",
      "Training Batch: 17044 Loss: 3537.225586\n",
      "Training Batch: 17045 Loss: 3451.583496\n",
      "Training Batch: 17046 Loss: 3538.291504\n",
      "Training Batch: 17047 Loss: 3358.303711\n",
      "Training Batch: 17048 Loss: 3552.721191\n",
      "Training Batch: 17049 Loss: 3545.282471\n",
      "Training Batch: 17050 Loss: 3586.115723\n",
      "Training Batch: 17051 Loss: 3425.276367\n",
      "Training Batch: 17052 Loss: 3458.676270\n",
      "Training Batch: 17053 Loss: 3656.445068\n",
      "Training Batch: 17054 Loss: 3379.618408\n",
      "Training Batch: 17055 Loss: 3481.983643\n",
      "Training Batch: 17056 Loss: 3357.280762\n",
      "Training Batch: 17057 Loss: 3397.718750\n",
      "Training Batch: 17058 Loss: 3311.510742\n",
      "Training Batch: 17059 Loss: 3509.875000\n",
      "Training Batch: 17060 Loss: 3419.647461\n",
      "Training Batch: 17061 Loss: 3465.850342\n",
      "Training Batch: 17062 Loss: 3457.336670\n",
      "Training Batch: 17063 Loss: 3405.504150\n",
      "Training Batch: 17064 Loss: 3936.840820\n",
      "Training Batch: 17065 Loss: 3631.352295\n",
      "Training Batch: 17066 Loss: 3479.279053\n",
      "Training Batch: 17067 Loss: 3478.267578\n",
      "Training Batch: 17068 Loss: 3462.377930\n",
      "Training Batch: 17069 Loss: 3516.832031\n",
      "Training Batch: 17070 Loss: 3564.699707\n",
      "Training Batch: 17071 Loss: 3675.981934\n",
      "Training Batch: 17072 Loss: 3528.761719\n",
      "Training Batch: 17073 Loss: 3338.401855\n",
      "Training Batch: 17074 Loss: 3407.539062\n",
      "Training Batch: 17075 Loss: 3420.978760\n",
      "Training Batch: 17076 Loss: 3456.859131\n",
      "Training Batch: 17077 Loss: 3531.787109\n",
      "Training Batch: 17078 Loss: 3719.109375\n",
      "Training Batch: 17079 Loss: 3506.735596\n",
      "Training Batch: 17080 Loss: 3568.620117\n",
      "Training Batch: 17081 Loss: 3363.344238\n",
      "Training Batch: 17082 Loss: 3357.770020\n",
      "Training Batch: 17083 Loss: 3558.386719\n",
      "Training Batch: 17084 Loss: 3546.198730\n",
      "Training Batch: 17085 Loss: 3477.655273\n",
      "Training Batch: 17086 Loss: 3445.087891\n",
      "Training Batch: 17087 Loss: 3317.675049\n",
      "Training Batch: 17088 Loss: 3361.043457\n",
      "Training Batch: 17089 Loss: 3594.354004\n",
      "Training Batch: 17090 Loss: 3377.145508\n",
      "Training Batch: 17091 Loss: 3344.626953\n",
      "Training Batch: 17092 Loss: 3345.642822\n",
      "Training Batch: 17093 Loss: 3411.491943\n",
      "Training Batch: 17094 Loss: 3361.722900\n",
      "Training Batch: 17095 Loss: 3435.825684\n",
      "Training Batch: 17096 Loss: 3398.605225\n",
      "Training Batch: 17097 Loss: 3408.411621\n",
      "Training Batch: 17098 Loss: 3562.009033\n",
      "Training Batch: 17099 Loss: 3557.373535\n",
      "Training Batch: 17100 Loss: 3499.884277\n",
      "Training Batch: 17101 Loss: 3381.954102\n",
      "Training Batch: 17102 Loss: 3578.796875\n",
      "Training Batch: 17103 Loss: 3422.826172\n",
      "Training Batch: 17104 Loss: 3486.548828\n",
      "Training Batch: 17105 Loss: 3392.030273\n",
      "Training Batch: 17106 Loss: 3635.955078\n",
      "Training Batch: 17107 Loss: 3468.694092\n",
      "Training Batch: 17108 Loss: 3411.021240\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Batch: 17109 Loss: 3527.023682\n",
      "Training Batch: 17110 Loss: 3665.927734\n",
      "Training Batch: 17111 Loss: 3489.529053\n",
      "Training Batch: 17112 Loss: 3544.314453\n",
      "Training Batch: 17113 Loss: 3509.808594\n",
      "Training Batch: 17114 Loss: 3346.841553\n",
      "Training Batch: 17115 Loss: 3550.735352\n",
      "Training Batch: 17116 Loss: 3677.949219\n",
      "Training Batch: 17117 Loss: 3629.160645\n",
      "Training Batch: 17118 Loss: 3434.529297\n",
      "Training Batch: 17119 Loss: 3388.079102\n",
      "Training Batch: 17120 Loss: 3408.491211\n",
      "Training Batch: 17121 Loss: 3455.086914\n",
      "Training Batch: 17122 Loss: 3400.121582\n",
      "Training Batch: 17123 Loss: 3509.529541\n",
      "Training Batch: 17124 Loss: 3679.631104\n",
      "Training Batch: 17125 Loss: 3523.134521\n",
      "Training Batch: 17126 Loss: 4102.550781\n",
      "Training Batch: 17127 Loss: 4475.810059\n",
      "Training Batch: 17128 Loss: 3461.474609\n",
      "Training Batch: 17129 Loss: 3507.305420\n",
      "Training Batch: 17130 Loss: 3608.936279\n",
      "Training Batch: 17131 Loss: 3492.133057\n",
      "Training Batch: 17132 Loss: 3717.086426\n",
      "Training Batch: 17133 Loss: 3470.140137\n",
      "Training Batch: 17134 Loss: 3465.271973\n",
      "Training Batch: 17135 Loss: 3532.758545\n",
      "Training Batch: 17136 Loss: 3411.267334\n",
      "Training Batch: 17137 Loss: 3483.863770\n",
      "Training Batch: 17138 Loss: 3491.809814\n",
      "Training Batch: 17139 Loss: 3337.666260\n",
      "Training Batch: 17140 Loss: 3366.655762\n",
      "Training Batch: 17141 Loss: 3423.753906\n",
      "Training Batch: 17142 Loss: 3468.057373\n",
      "Training Batch: 17143 Loss: 3533.239746\n",
      "Training Batch: 17144 Loss: 3382.126953\n",
      "Training Batch: 17145 Loss: 3340.584961\n",
      "Training Batch: 17146 Loss: 3396.145020\n",
      "Training Batch: 17147 Loss: 3365.214355\n",
      "Training Batch: 17148 Loss: 3401.621094\n",
      "Training Batch: 17149 Loss: 3626.917969\n",
      "Training Batch: 17150 Loss: 3485.088867\n",
      "Training Batch: 17151 Loss: 3473.877930\n",
      "Training Batch: 17152 Loss: 3557.249512\n",
      "Training Batch: 17153 Loss: 3493.411621\n",
      "Training Batch: 17154 Loss: 3415.337891\n",
      "Training Batch: 17155 Loss: 3571.046387\n",
      "Training Batch: 17156 Loss: 3565.359375\n",
      "Training Batch: 17157 Loss: 3497.211426\n",
      "Training Batch: 17158 Loss: 3464.179199\n",
      "Training Batch: 17159 Loss: 3507.306152\n",
      "Training Batch: 17160 Loss: 3386.009277\n",
      "Training Batch: 17161 Loss: 3507.517090\n",
      "Training Batch: 17162 Loss: 3499.179688\n",
      "Training Batch: 17163 Loss: 3613.510742\n",
      "Training Batch: 17164 Loss: 3474.136230\n",
      "Training Batch: 17165 Loss: 3342.437012\n",
      "Training Batch: 17166 Loss: 3387.867432\n",
      "Training Batch: 17167 Loss: 3482.537598\n",
      "Training Batch: 17168 Loss: 3340.414062\n",
      "Training Batch: 17169 Loss: 3567.022217\n",
      "Training Batch: 17170 Loss: 3439.246338\n",
      "Training Batch: 17171 Loss: 3398.986328\n",
      "Training Batch: 17172 Loss: 3371.652344\n",
      "Training Batch: 17173 Loss: 3355.345215\n",
      "Training Batch: 17174 Loss: 3473.253662\n",
      "Training Batch: 17175 Loss: 3428.447754\n",
      "Training Batch: 17176 Loss: 3419.535645\n",
      "Training Batch: 17177 Loss: 3481.874023\n",
      "Training Batch: 17178 Loss: 3376.126221\n",
      "Training Batch: 17179 Loss: 3365.523438\n",
      "Training Batch: 17180 Loss: 3372.335693\n",
      "Training Batch: 17181 Loss: 3362.657471\n",
      "Training Batch: 17182 Loss: 3476.409668\n",
      "Training Batch: 17183 Loss: 3428.830566\n",
      "Training Batch: 17184 Loss: 3383.015625\n",
      "Training Batch: 17185 Loss: 3493.273193\n",
      "Training Batch: 17186 Loss: 3484.290771\n",
      "Training Batch: 17187 Loss: 3669.735107\n",
      "Training Batch: 17188 Loss: 3764.279297\n",
      "Training Batch: 17189 Loss: 3538.393555\n",
      "Training Batch: 17190 Loss: 4006.688965\n",
      "Training Batch: 17191 Loss: 3839.482910\n",
      "Training Batch: 17192 Loss: 3509.162598\n",
      "Training Batch: 17193 Loss: 3910.959229\n",
      "Training Batch: 17194 Loss: 3743.352295\n",
      "Training Batch: 17195 Loss: 3442.040527\n",
      "Training Batch: 17196 Loss: 3555.959473\n",
      "Training Batch: 17197 Loss: 3371.721680\n",
      "Training Batch: 17198 Loss: 3410.647949\n",
      "Training Batch: 17199 Loss: 3402.895996\n",
      "Training Batch: 17200 Loss: 3357.180176\n",
      "Training Batch: 17201 Loss: 3381.353760\n",
      "Training Batch: 17202 Loss: 3359.651855\n",
      "Training Batch: 17203 Loss: 3429.123535\n",
      "Training Batch: 17204 Loss: 3357.802734\n",
      "Training Batch: 17205 Loss: 3324.112793\n",
      "Training Batch: 17206 Loss: 3400.927979\n",
      "Training Batch: 17207 Loss: 3434.770996\n",
      "Training Batch: 17208 Loss: 3350.808105\n",
      "Training Batch: 17209 Loss: 3358.442383\n",
      "Training Batch: 17210 Loss: 3518.381104\n",
      "Training Batch: 17211 Loss: 3399.149170\n",
      "Training Batch: 17212 Loss: 3613.731934\n",
      "Training Batch: 17213 Loss: 3552.574219\n",
      "Training Batch: 17214 Loss: 3584.363281\n",
      "Training Batch: 17215 Loss: 3492.598633\n",
      "Training Batch: 17216 Loss: 3405.123535\n",
      "Training Batch: 17217 Loss: 4016.375000\n",
      "Training Batch: 17218 Loss: 3458.035156\n",
      "Training Batch: 17219 Loss: 3477.485840\n",
      "Training Batch: 17220 Loss: 3764.088867\n",
      "Training Batch: 17221 Loss: 3643.480469\n",
      "Training Batch: 17222 Loss: 3475.227539\n",
      "Training Batch: 17223 Loss: 3372.599365\n",
      "Training Batch: 17224 Loss: 3529.520508\n",
      "Training Batch: 17225 Loss: 3378.583496\n",
      "Training Batch: 17226 Loss: 3412.802734\n",
      "Training Batch: 17227 Loss: 3515.751221\n",
      "Training Batch: 17228 Loss: 3362.656250\n",
      "Training Batch: 17229 Loss: 3533.118896\n",
      "Training Batch: 17230 Loss: 3394.982910\n",
      "Training Batch: 17231 Loss: 3393.604980\n",
      "Training Batch: 17232 Loss: 3302.005859\n",
      "Training Batch: 17233 Loss: 3741.435059\n",
      "Training Batch: 17234 Loss: 3515.672607\n",
      "Training Batch: 17235 Loss: 3446.629150\n",
      "Training Batch: 17236 Loss: 3364.800781\n",
      "Training Batch: 17237 Loss: 3506.603271\n",
      "Training Batch: 17238 Loss: 3466.670410\n",
      "Training Batch: 17239 Loss: 3348.816650\n",
      "Training Batch: 17240 Loss: 3442.026855\n",
      "Training Batch: 17241 Loss: 3389.889648\n",
      "Training Batch: 17242 Loss: 3473.427734\n",
      "Training Batch: 17243 Loss: 3421.525879\n",
      "Training Batch: 17244 Loss: 3451.647949\n",
      "Training Batch: 17245 Loss: 3655.279785\n",
      "Training Batch: 17246 Loss: 3440.610352\n",
      "Training Batch: 17247 Loss: 3361.081299\n",
      "Training Batch: 17248 Loss: 3415.271973\n",
      "Training Batch: 17249 Loss: 3355.850098\n",
      "Training Batch: 17250 Loss: 3406.884277\n",
      "Training Batch: 17251 Loss: 3360.778564\n",
      "Training Batch: 17252 Loss: 3536.505859\n",
      "Training Batch: 17253 Loss: 3390.559814\n",
      "Training Batch: 17254 Loss: 3469.017578\n",
      "Training Batch: 17255 Loss: 3616.281006\n",
      "Training Batch: 17256 Loss: 3451.053223\n",
      "Training Batch: 17257 Loss: 3471.561768\n",
      "Training Batch: 17258 Loss: 3479.309570\n",
      "Training Batch: 17259 Loss: 3678.240479\n",
      "Training Batch: 17260 Loss: 3528.483398\n",
      "Training Batch: 17261 Loss: 3365.007812\n",
      "Training Batch: 17262 Loss: 3412.666260\n",
      "Training Batch: 17263 Loss: 3393.928467\n",
      "Training Batch: 17264 Loss: 3378.615723\n",
      "Training Batch: 17265 Loss: 3496.600586\n",
      "Training Batch: 17266 Loss: 3840.989746\n",
      "Training Batch: 17267 Loss: 3451.763916\n",
      "Training Batch: 17268 Loss: 3523.007324\n",
      "Training Batch: 17269 Loss: 3620.514648\n",
      "Training Batch: 17270 Loss: 3439.568359\n",
      "Training Batch: 17271 Loss: 3462.548340\n",
      "Training Batch: 17272 Loss: 3405.410156\n",
      "Training Batch: 17273 Loss: 3419.479248\n",
      "Training Batch: 17274 Loss: 3395.939697\n",
      "Training Batch: 17275 Loss: 3562.969971\n",
      "Training Batch: 17276 Loss: 3492.608398\n",
      "Training Batch: 17277 Loss: 3512.770752\n",
      "Training Batch: 17278 Loss: 3407.501953\n",
      "Training Batch: 17279 Loss: 3463.018555\n",
      "Training Batch: 17280 Loss: 3431.525635\n",
      "Training Batch: 17281 Loss: 3424.385254\n",
      "Training Batch: 17282 Loss: 3355.514160\n",
      "Training Batch: 17283 Loss: 3412.919922\n",
      "Training Batch: 17284 Loss: 3378.556641\n",
      "Training Batch: 17285 Loss: 3451.657471\n",
      "Training Batch: 17286 Loss: 3715.287109\n",
      "Training Batch: 17287 Loss: 3572.468262\n",
      "Training Batch: 17288 Loss: 3450.700684\n",
      "Training Batch: 17289 Loss: 3455.036865\n",
      "Training Batch: 17290 Loss: 3481.930664\n",
      "Training Batch: 17291 Loss: 3351.640137\n",
      "Training Batch: 17292 Loss: 3408.402832\n",
      "Training Batch: 17293 Loss: 3553.936768\n",
      "Training Batch: 17294 Loss: 3356.363037\n",
      "Training Batch: 17295 Loss: 3513.455322\n",
      "Training Batch: 17296 Loss: 3673.064941\n",
      "Training Batch: 17297 Loss: 3389.772949\n",
      "Training Batch: 17298 Loss: 3581.346680\n",
      "Training Batch: 17299 Loss: 3481.146484\n",
      "Training Batch: 17300 Loss: 3352.053711\n",
      "Training Batch: 17301 Loss: 3373.374023\n",
      "Training Batch: 17302 Loss: 3633.090576\n",
      "Training Batch: 17303 Loss: 3351.848633\n",
      "Training Batch: 17304 Loss: 3546.141113\n",
      "Training Batch: 17305 Loss: 3502.970703\n",
      "Training Batch: 17306 Loss: 3449.348633\n",
      "Training Batch: 17307 Loss: 3441.670898\n",
      "Training Batch: 17308 Loss: 3603.913086\n",
      "Training Batch: 17309 Loss: 3496.531494\n",
      "Training Batch: 17310 Loss: 3488.770996\n",
      "Training Batch: 17311 Loss: 3572.944824\n",
      "Training Batch: 17312 Loss: 3417.179443\n",
      "Training Batch: 17313 Loss: 3372.013184\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Batch: 17314 Loss: 3386.705566\n",
      "Training Batch: 17315 Loss: 3455.691406\n",
      "Training Batch: 17316 Loss: 3495.462402\n",
      "Training Batch: 17317 Loss: 3378.147705\n",
      "Training Batch: 17318 Loss: 3401.189453\n",
      "Training Batch: 17319 Loss: 3655.861816\n",
      "Training Batch: 17320 Loss: 3532.018555\n",
      "Training Batch: 17321 Loss: 3658.571777\n",
      "Training Batch: 17322 Loss: 3380.395996\n",
      "Training Batch: 17323 Loss: 3382.373535\n",
      "Training Batch: 17324 Loss: 3393.909668\n",
      "Training Batch: 17325 Loss: 3648.658691\n",
      "Training Batch: 17326 Loss: 3453.005371\n",
      "Training Batch: 17327 Loss: 3386.633057\n",
      "Training Batch: 17328 Loss: 3414.862061\n",
      "Training Batch: 17329 Loss: 3485.762207\n",
      "Training Batch: 17330 Loss: 3459.516846\n",
      "Training Batch: 17331 Loss: 3376.850586\n",
      "Training Batch: 17332 Loss: 3383.469727\n",
      "Training Batch: 17333 Loss: 3576.861572\n",
      "Training Batch: 17334 Loss: 3653.441895\n",
      "Training Batch: 17335 Loss: 3467.304688\n",
      "Training Batch: 17336 Loss: 3442.042480\n",
      "Training Batch: 17337 Loss: 3377.717529\n",
      "Training Batch: 17338 Loss: 3418.416748\n",
      "Training Batch: 17339 Loss: 3492.576416\n",
      "Training Batch: 17340 Loss: 3394.380859\n",
      "Training Batch: 17341 Loss: 3362.023193\n",
      "Training Batch: 17342 Loss: 3265.841309\n",
      "Training Batch: 17343 Loss: 3332.770020\n",
      "Training Batch: 17344 Loss: 3352.810547\n",
      "Training Batch: 17345 Loss: 3469.387695\n",
      "Training Batch: 17346 Loss: 3463.155762\n",
      "Training Batch: 17347 Loss: 3452.901855\n",
      "Training Batch: 17348 Loss: 3557.268311\n",
      "Training Batch: 17349 Loss: 3380.230469\n",
      "Training Batch: 17350 Loss: 3433.379395\n",
      "Training Batch: 17351 Loss: 3435.964111\n",
      "Training Batch: 17352 Loss: 3522.115479\n",
      "Training Batch: 17353 Loss: 3466.229492\n",
      "Training Batch: 17354 Loss: 3707.771484\n",
      "Training Batch: 17355 Loss: 3403.763672\n",
      "Training Batch: 17356 Loss: 3403.084473\n",
      "Training Batch: 17357 Loss: 3497.317383\n",
      "Training Batch: 17358 Loss: 3357.770020\n",
      "Training Batch: 17359 Loss: 3681.465332\n",
      "Training Batch: 17360 Loss: 3359.390625\n",
      "Training Batch: 17361 Loss: 3643.534180\n",
      "Training Batch: 17362 Loss: 3339.378662\n",
      "Training Batch: 17363 Loss: 3444.301758\n",
      "Training Batch: 17364 Loss: 3607.919434\n",
      "Training Batch: 17365 Loss: 3444.892578\n",
      "Training Batch: 17366 Loss: 3383.427246\n",
      "Training Batch: 17367 Loss: 3454.137207\n",
      "Training Batch: 17368 Loss: 3571.042480\n",
      "Training Batch: 17369 Loss: 3742.663818\n",
      "Training Batch: 17370 Loss: 3562.565918\n",
      "Training Batch: 17371 Loss: 3356.312500\n",
      "Training Batch: 17372 Loss: 3405.812744\n",
      "Training Batch: 17373 Loss: 3384.402344\n",
      "Training Batch: 17374 Loss: 3801.434570\n",
      "Training Batch: 17375 Loss: 3797.938965\n",
      "Training Batch: 17376 Loss: 3749.295654\n",
      "Training Batch: 17377 Loss: 3572.732422\n",
      "Training Batch: 17378 Loss: 3440.225342\n",
      "Training Batch: 17379 Loss: 3586.955566\n",
      "Training Batch: 17380 Loss: 3497.056641\n",
      "Training Batch: 17381 Loss: 3515.318848\n",
      "Training Batch: 17382 Loss: 3355.629395\n",
      "Training Batch: 17383 Loss: 3483.996094\n",
      "Training Batch: 17384 Loss: 3463.873779\n",
      "Training Batch: 17385 Loss: 3545.302490\n",
      "Training Batch: 17386 Loss: 3561.581055\n",
      "Training Batch: 17387 Loss: 3518.439697\n",
      "Training Batch: 17388 Loss: 3459.863281\n",
      "Training Batch: 17389 Loss: 3510.566406\n",
      "Training Batch: 17390 Loss: 3554.379395\n",
      "Training Batch: 17391 Loss: 3356.829102\n",
      "Training Batch: 17392 Loss: 3497.713867\n",
      "Training Batch: 17393 Loss: 3547.106934\n",
      "Training Batch: 17394 Loss: 3435.058838\n",
      "Training Batch: 17395 Loss: 3497.211426\n",
      "Training Batch: 17396 Loss: 3536.321533\n",
      "Training Batch: 17397 Loss: 3425.802734\n",
      "Training Batch: 17398 Loss: 3668.837402\n",
      "Training Batch: 17399 Loss: 3401.600342\n",
      "Training Batch: 17400 Loss: 3552.565918\n",
      "Training Batch: 17401 Loss: 3606.180420\n",
      "Training Batch: 17402 Loss: 3543.905762\n",
      "Training Batch: 17403 Loss: 3496.791504\n",
      "Training Batch: 17404 Loss: 3481.193604\n",
      "Training Batch: 17405 Loss: 3452.469238\n",
      "Training Batch: 17406 Loss: 3448.619141\n",
      "Training Batch: 17407 Loss: 3618.686035\n",
      "Training Batch: 17408 Loss: 3388.999268\n",
      "Training Batch: 17409 Loss: 3463.578125\n",
      "Training Batch: 17410 Loss: 3409.210449\n",
      "Training Batch: 17411 Loss: 3450.773682\n",
      "Training Batch: 17412 Loss: 3362.555176\n",
      "Training Batch: 17413 Loss: 3315.847900\n",
      "Training Batch: 17414 Loss: 3336.513672\n",
      "Training Batch: 17415 Loss: 3357.894287\n",
      "Training Batch: 17416 Loss: 3352.951660\n",
      "Training Batch: 17417 Loss: 3390.312988\n",
      "Training Batch: 17418 Loss: 3419.522461\n",
      "Training Batch: 17419 Loss: 3487.241699\n",
      "Training Batch: 17420 Loss: 3463.354980\n",
      "Training Batch: 17421 Loss: 3355.264648\n",
      "Training Batch: 17422 Loss: 3408.459473\n",
      "Training Batch: 17423 Loss: 3398.909180\n",
      "Training Batch: 17424 Loss: 3337.060547\n",
      "Training Batch: 17425 Loss: 3507.799561\n",
      "Training Batch: 17426 Loss: 3584.291748\n",
      "Training Batch: 17427 Loss: 3421.817383\n",
      "Training Batch: 17428 Loss: 3391.991455\n",
      "Training Batch: 17429 Loss: 3402.486328\n",
      "Training Batch: 17430 Loss: 3317.944336\n",
      "Training Batch: 17431 Loss: 3481.787109\n",
      "Training Batch: 17432 Loss: 3632.019287\n",
      "Training Batch: 17433 Loss: 3301.717285\n",
      "Training Batch: 17434 Loss: 3399.690430\n",
      "Training Batch: 17435 Loss: 3392.947266\n",
      "Training Batch: 17436 Loss: 3418.102783\n",
      "Training Batch: 17437 Loss: 3300.292969\n",
      "Training Batch: 17438 Loss: 3380.378906\n",
      "Training Batch: 17439 Loss: 3408.086670\n",
      "Training Batch: 17440 Loss: 3376.232422\n",
      "Training Batch: 17441 Loss: 3438.188721\n",
      "Training Batch: 17442 Loss: 3493.912842\n",
      "Training Batch: 17443 Loss: 3499.245850\n",
      "Training Batch: 17444 Loss: 3719.476074\n",
      "Training Batch: 17445 Loss: 3657.187256\n",
      "Training Batch: 17446 Loss: 3535.233398\n",
      "Training Batch: 17447 Loss: 3434.706055\n",
      "Training Batch: 17448 Loss: 3376.677246\n",
      "Training Batch: 17449 Loss: 3661.633301\n",
      "Training Batch: 17450 Loss: 3508.687500\n",
      "Training Batch: 17451 Loss: 3383.226562\n",
      "Training Batch: 17452 Loss: 3666.027344\n",
      "Training Batch: 17453 Loss: 3424.598633\n",
      "Training Batch: 17454 Loss: 3444.430176\n",
      "Training Batch: 17455 Loss: 3404.063965\n",
      "Training Batch: 17456 Loss: 3414.768555\n",
      "Training Batch: 17457 Loss: 3562.603516\n",
      "Training Batch: 17458 Loss: 3411.282715\n",
      "Training Batch: 17459 Loss: 3356.464844\n",
      "Training Batch: 17460 Loss: 3429.649414\n",
      "Training Batch: 17461 Loss: 3519.319824\n",
      "Training Batch: 17462 Loss: 3468.853516\n",
      "Training Batch: 17463 Loss: 3469.512207\n",
      "Training Batch: 17464 Loss: 3375.977051\n",
      "Training Batch: 17465 Loss: 3315.753662\n",
      "Training Batch: 17466 Loss: 3410.230713\n",
      "Training Batch: 17467 Loss: 3384.692871\n",
      "Training Batch: 17468 Loss: 3399.053955\n",
      "Training Batch: 17469 Loss: 3411.600098\n",
      "Training Batch: 17470 Loss: 3464.334961\n",
      "Training Batch: 17471 Loss: 3629.191895\n",
      "Training Batch: 17472 Loss: 3492.133789\n",
      "Training Batch: 17473 Loss: 3465.295410\n",
      "Training Batch: 17474 Loss: 3302.411377\n",
      "Training Batch: 17475 Loss: 3465.830566\n",
      "Training Batch: 17476 Loss: 3360.755859\n",
      "Training Batch: 17477 Loss: 3431.063477\n",
      "Training Batch: 17478 Loss: 3253.586914\n",
      "Training Batch: 17479 Loss: 3329.934570\n",
      "Training Batch: 17480 Loss: 3397.785156\n",
      "Training Batch: 17481 Loss: 3386.379639\n",
      "Training Batch: 17482 Loss: 3501.222412\n",
      "Training Batch: 17483 Loss: 3571.494385\n",
      "Training Batch: 17484 Loss: 3388.649902\n",
      "Training Batch: 17485 Loss: 3514.179199\n",
      "Training Batch: 17486 Loss: 3340.838379\n",
      "Training Batch: 17487 Loss: 3420.055176\n",
      "Training Batch: 17488 Loss: 3753.579590\n",
      "Training Batch: 17489 Loss: 3590.766602\n",
      "Training Batch: 17490 Loss: 3463.391113\n",
      "Training Batch: 17491 Loss: 3513.364746\n",
      "Training Batch: 17492 Loss: 3318.949219\n",
      "Training Batch: 17493 Loss: 3651.254883\n",
      "Training Batch: 17494 Loss: 3474.307373\n",
      "Training Batch: 17495 Loss: 3385.689941\n",
      "Training Batch: 17496 Loss: 3300.007812\n",
      "Training Batch: 17497 Loss: 3664.645508\n",
      "Training Batch: 17498 Loss: 3449.628906\n",
      "Training Batch: 17499 Loss: 3374.110107\n",
      "Training Batch: 17500 Loss: 3420.568359\n",
      "Training Batch: 17501 Loss: 3476.558105\n",
      "Training Batch: 17502 Loss: 3386.817627\n",
      "Training Batch: 17503 Loss: 3439.409668\n",
      "Training Batch: 17504 Loss: 3425.929199\n",
      "Training Batch: 17505 Loss: 3406.422363\n",
      "Training Batch: 17506 Loss: 3454.418945\n",
      "Training Batch: 17507 Loss: 3520.176270\n",
      "Training Batch: 17508 Loss: 3554.250488\n",
      "Training Batch: 17509 Loss: 3627.395752\n",
      "Training Batch: 17510 Loss: 3398.292480\n",
      "Training Batch: 17511 Loss: 3717.117432\n",
      "Training Batch: 17512 Loss: 3579.809570\n",
      "Training Batch: 17513 Loss: 3288.255859\n",
      "Training Batch: 17514 Loss: 3554.850098\n",
      "Training Batch: 17515 Loss: 3573.127441\n",
      "Training Batch: 17516 Loss: 3447.298828\n",
      "Training Batch: 17517 Loss: 3478.180176\n",
      "Training Batch: 17518 Loss: 3364.339844\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Batch: 17519 Loss: 3387.950195\n",
      "Training Batch: 17520 Loss: 3665.596436\n",
      "Training Batch: 17521 Loss: 3433.906494\n",
      "Training Batch: 17522 Loss: 3352.968262\n",
      "Training Batch: 17523 Loss: 3358.038574\n",
      "Training Batch: 17524 Loss: 3404.755371\n",
      "Training Batch: 17525 Loss: 3512.242188\n",
      "Training Batch: 17526 Loss: 3408.000732\n",
      "Training Batch: 17527 Loss: 3306.726807\n",
      "Training Batch: 17528 Loss: 3676.665283\n",
      "Training Batch: 17529 Loss: 3523.673828\n",
      "Training Batch: 17530 Loss: 3450.604492\n",
      "Training Batch: 17531 Loss: 3461.592529\n",
      "Training Batch: 17532 Loss: 3483.844238\n",
      "Training Batch: 17533 Loss: 3341.735107\n",
      "Training Batch: 17534 Loss: 3434.538818\n",
      "Training Batch: 17535 Loss: 3462.520020\n",
      "Training Batch: 17536 Loss: 3401.766846\n",
      "Training Batch: 17537 Loss: 3753.489258\n",
      "Training Batch: 17538 Loss: 3423.378662\n",
      "Training Batch: 17539 Loss: 3460.347656\n",
      "Training Batch: 17540 Loss: 3442.035645\n",
      "Training Batch: 17541 Loss: 3341.650879\n",
      "Training Batch: 17542 Loss: 3389.119629\n",
      "Training Batch: 17543 Loss: 3349.678711\n",
      "Training Batch: 17544 Loss: 3396.112305\n",
      "Training Batch: 17545 Loss: 3401.057617\n",
      "Training Batch: 17546 Loss: 3503.979980\n",
      "Training Batch: 17547 Loss: 3362.246582\n",
      "Training Batch: 17548 Loss: 3857.691406\n",
      "Training Batch: 17549 Loss: 3545.150879\n",
      "Training Batch: 17550 Loss: 3396.767090\n",
      "Training Batch: 17551 Loss: 3425.551758\n",
      "Training Batch: 17552 Loss: 3507.121582\n",
      "Training Batch: 17553 Loss: 3445.877930\n",
      "Training Batch: 17554 Loss: 3330.082031\n",
      "Training Batch: 17555 Loss: 3366.382568\n",
      "Training Batch: 17556 Loss: 3395.871826\n",
      "Training Batch: 17557 Loss: 3368.601562\n",
      "Training Batch: 17558 Loss: 3799.446289\n",
      "Training Batch: 17559 Loss: 3861.001953\n",
      "Training Batch: 17560 Loss: 3499.006592\n",
      "Training Batch: 17561 Loss: 3466.206299\n",
      "Training Batch: 17562 Loss: 3510.523438\n",
      "Training Batch: 17563 Loss: 3732.901367\n",
      "Training Batch: 17564 Loss: 3513.678711\n",
      "Training Batch: 17565 Loss: 3470.339844\n",
      "Training Batch: 17566 Loss: 3543.932617\n",
      "Training Batch: 17567 Loss: 3507.229004\n",
      "Training Batch: 17568 Loss: 3517.874023\n",
      "Training Batch: 17569 Loss: 3363.944336\n",
      "Training Batch: 17570 Loss: 3370.141602\n",
      "Training Batch: 17571 Loss: 3357.251953\n",
      "Training Batch: 17572 Loss: 3608.316895\n",
      "Training Batch: 17573 Loss: 3459.126953\n",
      "Training Batch: 17574 Loss: 3444.969727\n",
      "Training Batch: 17575 Loss: 3446.917480\n",
      "Training Batch: 17576 Loss: 3407.200195\n",
      "Training Batch: 17577 Loss: 3389.843994\n",
      "Training Batch: 17578 Loss: 3337.350586\n",
      "Training Batch: 17579 Loss: 3326.929688\n",
      "Training Batch: 17580 Loss: 3393.118164\n",
      "Training Batch: 17581 Loss: 3427.519531\n",
      "Training Batch: 17582 Loss: 3504.857422\n",
      "Training Batch: 17583 Loss: 3362.729248\n",
      "Training Batch: 17584 Loss: 3412.986816\n",
      "Training Batch: 17585 Loss: 3316.757812\n",
      "Training Batch: 17586 Loss: 3382.996826\n",
      "Training Batch: 17587 Loss: 3343.348145\n",
      "Training Batch: 17588 Loss: 3417.617676\n",
      "Training Batch: 17589 Loss: 3513.732178\n",
      "Training Batch: 17590 Loss: 3486.859863\n",
      "Training Batch: 17591 Loss: 3547.944580\n",
      "Training Batch: 17592 Loss: 3410.110840\n",
      "Training Batch: 17593 Loss: 3440.731934\n",
      "Training Batch: 17594 Loss: 3542.724609\n",
      "Training Batch: 17595 Loss: 3280.407959\n",
      "Training Batch: 17596 Loss: 3436.166748\n",
      "Training Batch: 17597 Loss: 3403.358887\n",
      "Training Batch: 17598 Loss: 3497.222656\n",
      "Training Batch: 17599 Loss: 3394.027832\n",
      "Training Batch: 17600 Loss: 3464.543945\n",
      "Training Batch: 17601 Loss: 3363.318848\n",
      "Training Batch: 17602 Loss: 3309.668945\n",
      "Training Batch: 17603 Loss: 3609.575195\n",
      "Training Batch: 17604 Loss: 3323.650391\n",
      "Training Batch: 17605 Loss: 3376.765625\n",
      "Training Batch: 17606 Loss: 3377.200684\n",
      "Training Batch: 17607 Loss: 3285.934082\n",
      "Training Batch: 17608 Loss: 3446.268066\n",
      "Training Batch: 17609 Loss: 3521.533691\n",
      "Training Batch: 17610 Loss: 3612.572266\n",
      "Training Batch: 17611 Loss: 3457.764160\n",
      "Training Batch: 17612 Loss: 3535.760498\n",
      "Training Batch: 17613 Loss: 3441.301514\n",
      "Training Batch: 17614 Loss: 3416.501953\n",
      "Training Batch: 17615 Loss: 3358.745361\n",
      "Training Batch: 17616 Loss: 3322.735352\n",
      "Training Batch: 17617 Loss: 3379.849854\n",
      "Training Batch: 17618 Loss: 3371.049072\n",
      "Training Batch: 17619 Loss: 3384.698730\n",
      "Training Batch: 17620 Loss: 3433.151367\n",
      "Training Batch: 17621 Loss: 3440.612305\n",
      "Training Batch: 17622 Loss: 3341.467773\n",
      "Training Batch: 17623 Loss: 3594.297852\n",
      "Training Batch: 17624 Loss: 3374.214355\n",
      "Training Batch: 17625 Loss: 3305.978516\n",
      "Training Batch: 17626 Loss: 3510.299316\n",
      "Training Batch: 17627 Loss: 3482.011230\n",
      "Training Batch: 17628 Loss: 3581.992188\n",
      "Training Batch: 17629 Loss: 3572.905762\n",
      "Training Batch: 17630 Loss: 3421.152100\n",
      "Training Batch: 17631 Loss: 3499.205078\n",
      "Training Batch: 17632 Loss: 3415.895264\n",
      "Training Batch: 17633 Loss: 3563.737305\n",
      "Training Batch: 17634 Loss: 3453.230469\n",
      "Training Batch: 17635 Loss: 3351.998291\n",
      "Training Batch: 17636 Loss: 3413.814209\n",
      "Training Batch: 17637 Loss: 3428.866699\n",
      "Training Batch: 17638 Loss: 3467.747559\n",
      "Training Batch: 17639 Loss: 3452.915039\n",
      "Training Batch: 17640 Loss: 3386.004639\n",
      "Training Batch: 17641 Loss: 3574.373047\n",
      "Training Batch: 17642 Loss: 3369.598145\n",
      "Training Batch: 17643 Loss: 3339.166016\n",
      "Training Batch: 17644 Loss: 3338.196777\n",
      "Training Batch: 17645 Loss: 3344.455078\n",
      "Training Batch: 17646 Loss: 3392.913818\n",
      "Training Batch: 17647 Loss: 3424.568115\n",
      "Training Batch: 17648 Loss: 3599.818848\n",
      "Training Batch: 17649 Loss: 3468.100586\n",
      "Training Batch: 17650 Loss: 3331.722168\n",
      "Training Batch: 17651 Loss: 3323.034668\n",
      "Training Batch: 17652 Loss: 3329.383789\n",
      "Training Batch: 17653 Loss: 3502.673340\n",
      "Training Batch: 17654 Loss: 3416.832031\n",
      "Training Batch: 17655 Loss: 3404.152344\n",
      "Training Batch: 17656 Loss: 3309.966553\n",
      "Training Batch: 17657 Loss: 3624.917969\n",
      "Training Batch: 17658 Loss: 3537.498535\n",
      "Training Batch: 17659 Loss: 3366.122070\n",
      "Training Batch: 17660 Loss: 3317.411133\n",
      "Training Batch: 17661 Loss: 3386.693359\n",
      "Training Batch: 17662 Loss: 3453.188721\n",
      "Training Batch: 17663 Loss: 3369.020996\n",
      "Training Batch: 17664 Loss: 3396.701660\n",
      "Training Batch: 17665 Loss: 3339.291504\n",
      "Training Batch: 17666 Loss: 3618.026367\n",
      "Training Batch: 17667 Loss: 3478.044434\n",
      "Training Batch: 17668 Loss: 3335.154785\n",
      "Training Batch: 17669 Loss: 3358.160645\n",
      "Training Batch: 17670 Loss: 3309.771973\n",
      "Training Batch: 17671 Loss: 3409.529297\n",
      "Training Batch: 17672 Loss: 3519.574707\n",
      "Training Batch: 17673 Loss: 3420.081299\n",
      "Training Batch: 17674 Loss: 3403.917480\n",
      "Training Batch: 17675 Loss: 3334.912354\n",
      "Training Batch: 17676 Loss: 3289.214844\n",
      "Training Batch: 17677 Loss: 3316.284668\n",
      "Training Batch: 17678 Loss: 3492.765625\n",
      "Training Batch: 17679 Loss: 3333.699219\n",
      "Training Batch: 17680 Loss: 3387.969971\n",
      "Training Batch: 17681 Loss: 3407.737305\n",
      "Training Batch: 17682 Loss: 3286.305664\n",
      "Training Batch: 17683 Loss: 3317.475098\n",
      "Training Batch: 17684 Loss: 3403.570068\n",
      "Training Batch: 17685 Loss: 3468.308594\n",
      "Training Batch: 17686 Loss: 3439.130371\n",
      "Training Batch: 17687 Loss: 3313.859863\n",
      "Training Batch: 17688 Loss: 3418.407227\n",
      "Training Batch: 17689 Loss: 3363.872070\n",
      "Training Batch: 17690 Loss: 3397.294678\n",
      "Training Batch: 17691 Loss: 3360.146973\n",
      "Training Batch: 17692 Loss: 3510.046631\n",
      "Training Batch: 17693 Loss: 3423.333008\n",
      "Training Batch: 17694 Loss: 3526.738281\n",
      "Training Batch: 17695 Loss: 3439.714600\n",
      "Training Batch: 17696 Loss: 3460.850830\n",
      "Training Batch: 17697 Loss: 3396.916260\n",
      "Training Batch: 17698 Loss: 3432.244385\n",
      "Training Batch: 17699 Loss: 3368.808105\n",
      "Training Batch: 17700 Loss: 3400.068115\n",
      "Training Batch: 17701 Loss: 3348.286133\n",
      "Training Batch: 17702 Loss: 3456.024414\n",
      "Training Batch: 17703 Loss: 3542.709473\n",
      "Training Batch: 17704 Loss: 3419.586426\n",
      "Training Batch: 17705 Loss: 3362.230957\n",
      "Training Batch: 17706 Loss: 3461.882568\n",
      "Training Batch: 17707 Loss: 3275.896973\n",
      "Training Batch: 17708 Loss: 3382.943604\n",
      "Training Batch: 17709 Loss: 3338.291504\n",
      "Training Batch: 17710 Loss: 3419.830811\n",
      "Training Batch: 17711 Loss: 3647.628418\n",
      "Training Batch: 17712 Loss: 3410.485840\n",
      "Training Batch: 17713 Loss: 3535.733398\n",
      "Training Batch: 17714 Loss: 3936.471924\n",
      "Training Batch: 17715 Loss: 3590.900146\n",
      "Training Batch: 17716 Loss: 3471.642578\n",
      "Training Batch: 17717 Loss: 3516.750977\n",
      "Training Batch: 17718 Loss: 3385.229492\n",
      "Training Batch: 17719 Loss: 3434.458496\n",
      "Training Batch: 17720 Loss: 3327.622559\n",
      "Training Batch: 17721 Loss: 3510.985352\n",
      "Training Batch: 17722 Loss: 3478.853271\n",
      "Training Batch: 17723 Loss: 3484.701660\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Batch: 17724 Loss: 3530.246582\n",
      "Training Batch: 17725 Loss: 3463.432129\n",
      "Training Batch: 17726 Loss: 3585.376465\n",
      "Training Batch: 17727 Loss: 3454.595947\n",
      "Training Batch: 17728 Loss: 3827.045898\n",
      "Training Batch: 17729 Loss: 3941.684570\n",
      "Training Batch: 17730 Loss: 3570.029297\n",
      "Training Batch: 17731 Loss: 3599.501953\n",
      "Training Batch: 17732 Loss: 3436.812256\n",
      "Training Batch: 17733 Loss: 3453.487305\n",
      "Training Batch: 17734 Loss: 3445.804443\n",
      "Training Batch: 17735 Loss: 3751.799805\n",
      "Training Batch: 17736 Loss: 3707.904541\n",
      "Training Batch: 17737 Loss: 3601.954590\n",
      "Training Batch: 17738 Loss: 3341.097168\n",
      "Training Batch: 17739 Loss: 3322.584961\n",
      "Training Batch: 17740 Loss: 3395.612549\n",
      "Training Batch: 17741 Loss: 3474.703613\n",
      "Training Batch: 17742 Loss: 3421.116211\n",
      "Training Batch: 17743 Loss: 3607.989746\n",
      "Training Batch: 17744 Loss: 3447.340332\n",
      "Training Batch: 17745 Loss: 3646.595703\n",
      "Training Batch: 17746 Loss: 3632.995605\n",
      "Training Batch: 17747 Loss: 3474.567627\n",
      "Training Batch: 17748 Loss: 3372.327148\n",
      "Training Batch: 17749 Loss: 3326.069092\n",
      "Training Batch: 17750 Loss: 3410.497314\n",
      "Training Batch: 17751 Loss: 3487.821777\n",
      "Training Batch: 17752 Loss: 3307.779785\n",
      "Training Batch: 17753 Loss: 3355.266602\n",
      "Training Batch: 17754 Loss: 3275.689453\n",
      "Training Batch: 17755 Loss: 3407.682617\n",
      "Training Batch: 17756 Loss: 3390.769775\n",
      "Training Batch: 17757 Loss: 3447.726562\n",
      "Training Batch: 17758 Loss: 3317.757812\n",
      "Training Batch: 17759 Loss: 3504.079834\n",
      "Training Batch: 17760 Loss: 3480.234375\n",
      "Training Batch: 17761 Loss: 3309.841309\n",
      "Training Batch: 17762 Loss: 3443.629639\n",
      "Training Batch: 17763 Loss: 3390.378906\n",
      "Training Batch: 17764 Loss: 3375.202148\n",
      "Training Batch: 17765 Loss: 4045.452148\n",
      "Training Batch: 17766 Loss: 3811.853516\n",
      "Training Batch: 17767 Loss: 3525.742920\n",
      "Training Batch: 17768 Loss: 3457.462402\n",
      "Training Batch: 17769 Loss: 3332.399414\n",
      "Training Batch: 17770 Loss: 3985.829346\n",
      "Training Batch: 17771 Loss: 3769.571533\n",
      "Training Batch: 17772 Loss: 3521.277344\n",
      "Training Batch: 17773 Loss: 3523.051514\n",
      "Training Batch: 17774 Loss: 3664.021973\n",
      "Training Batch: 17775 Loss: 3426.052246\n",
      "Training Batch: 17776 Loss: 3407.560547\n",
      "Training Batch: 17777 Loss: 3367.024414\n",
      "Training Batch: 17778 Loss: 3526.050049\n",
      "Training Batch: 17779 Loss: 3332.624756\n",
      "Training Batch: 17780 Loss: 3377.474121\n",
      "Training Batch: 17781 Loss: 3420.236328\n",
      "Training Batch: 17782 Loss: 3425.058350\n",
      "Training Batch: 17783 Loss: 3471.599609\n",
      "Training Batch: 17784 Loss: 3547.249023\n",
      "Training Batch: 17785 Loss: 3622.836182\n",
      "Training Batch: 17786 Loss: 3454.802002\n",
      "Training Batch: 17787 Loss: 3466.081543\n",
      "Training Batch: 17788 Loss: 3347.684082\n",
      "Training Batch: 17789 Loss: 3463.581055\n",
      "Training Batch: 17790 Loss: 3555.283691\n",
      "Training Batch: 17791 Loss: 3477.725586\n",
      "Training Batch: 17792 Loss: 3564.945801\n",
      "Training Batch: 17793 Loss: 3441.473877\n",
      "Training Batch: 17794 Loss: 3413.843750\n",
      "Training Batch: 17795 Loss: 3364.470215\n",
      "Training Batch: 17796 Loss: 3353.158203\n",
      "Training Batch: 17797 Loss: 3373.070801\n",
      "Training Batch: 17798 Loss: 3404.316162\n",
      "Training Batch: 17799 Loss: 3559.221680\n",
      "Training Batch: 17800 Loss: 3489.453369\n",
      "Training Batch: 17801 Loss: 3440.051514\n",
      "Training Batch: 17802 Loss: 3462.506836\n",
      "Training Batch: 17803 Loss: 3578.381348\n",
      "Training Batch: 17804 Loss: 3565.803711\n",
      "Training Batch: 17805 Loss: 3423.378906\n",
      "Training Batch: 17806 Loss: 3528.057129\n",
      "Training Batch: 17807 Loss: 3432.979004\n",
      "Training Batch: 17808 Loss: 3378.958496\n",
      "Training Batch: 17809 Loss: 3382.349609\n",
      "Training Batch: 17810 Loss: 3396.850586\n",
      "Training Batch: 17811 Loss: 3439.607178\n",
      "Training Batch: 17812 Loss: 3470.595459\n",
      "Training Batch: 17813 Loss: 3469.884766\n",
      "Training Batch: 17814 Loss: 3343.334229\n",
      "Training Batch: 17815 Loss: 3362.526367\n",
      "Training Batch: 17816 Loss: 3339.624268\n",
      "Training Batch: 17817 Loss: 3431.246582\n",
      "Training Batch: 17818 Loss: 3371.077148\n",
      "Training Batch: 17819 Loss: 3483.806885\n",
      "Training Batch: 17820 Loss: 3397.657227\n",
      "Training Batch: 17821 Loss: 3391.941406\n",
      "Training Batch: 17822 Loss: 3344.449707\n",
      "Training Batch: 17823 Loss: 3458.280518\n",
      "Training Batch: 17824 Loss: 3496.164062\n",
      "Training Batch: 17825 Loss: 3423.080078\n",
      "Training Batch: 17826 Loss: 3320.536377\n",
      "Training Batch: 17827 Loss: 3544.179443\n",
      "Training Batch: 17828 Loss: 3688.937500\n",
      "Training Batch: 17829 Loss: 3509.948242\n",
      "Training Batch: 17830 Loss: 3487.365723\n",
      "Training Batch: 17831 Loss: 3368.860840\n",
      "Training Batch: 17832 Loss: 3439.842041\n",
      "Training Batch: 17833 Loss: 3407.159912\n",
      "Training Batch: 17834 Loss: 3358.026855\n",
      "Training Batch: 17835 Loss: 3417.477539\n",
      "Training Batch: 17836 Loss: 3413.965332\n",
      "Training Batch: 17837 Loss: 3492.967529\n",
      "Training Batch: 17838 Loss: 3398.766846\n",
      "Training Batch: 17839 Loss: 3307.231934\n",
      "Training Batch: 17840 Loss: 3362.343506\n",
      "Training Batch: 17841 Loss: 3422.858887\n",
      "Training Batch: 17842 Loss: 3606.579102\n",
      "Training Batch: 17843 Loss: 3592.949707\n",
      "Training Batch: 17844 Loss: 3659.740234\n",
      "Training Batch: 17845 Loss: 3444.090332\n",
      "Training Batch: 17846 Loss: 3377.598633\n",
      "Training Batch: 17847 Loss: 3349.713379\n",
      "Training Batch: 17848 Loss: 3398.221436\n",
      "Training Batch: 17849 Loss: 3440.533691\n",
      "Training Batch: 17850 Loss: 3412.509033\n",
      "Training Batch: 17851 Loss: 3364.139404\n",
      "Training Batch: 17852 Loss: 3485.465576\n",
      "Training Batch: 17853 Loss: 3322.028564\n",
      "Training Batch: 17854 Loss: 3495.994141\n",
      "Training Batch: 17855 Loss: 3310.504150\n",
      "Training Batch: 17856 Loss: 3466.943115\n",
      "Training Batch: 17857 Loss: 3331.944824\n",
      "Training Batch: 17858 Loss: 3380.007812\n",
      "Training Batch: 17859 Loss: 3395.684082\n",
      "Training Batch: 17860 Loss: 3596.935059\n",
      "Training Batch: 17861 Loss: 3358.593506\n",
      "Training Batch: 17862 Loss: 3344.708984\n",
      "Training Batch: 17863 Loss: 3374.064453\n",
      "Training Batch: 17864 Loss: 3417.096680\n",
      "Training Batch: 17865 Loss: 3509.062012\n",
      "Training Batch: 17866 Loss: 3537.541016\n",
      "Training Batch: 17867 Loss: 3358.648926\n",
      "Training Batch: 17868 Loss: 3558.498535\n",
      "Training Batch: 17869 Loss: 3509.916016\n",
      "Training Batch: 17870 Loss: 3377.102539\n",
      "Training Batch: 17871 Loss: 3291.088379\n",
      "Training Batch: 17872 Loss: 3402.848145\n",
      "Training Batch: 17873 Loss: 3536.600586\n",
      "Training Batch: 17874 Loss: 3572.850586\n",
      "Training Batch: 17875 Loss: 3391.709961\n",
      "Training Batch: 17876 Loss: 3358.311523\n",
      "Training Batch: 17877 Loss: 3370.568359\n",
      "Training Batch: 17878 Loss: 3513.284180\n",
      "Training Batch: 17879 Loss: 3441.341797\n",
      "Training Batch: 17880 Loss: 4004.976807\n",
      "Training Batch: 17881 Loss: 3450.563477\n",
      "Training Batch: 17882 Loss: 3418.070068\n",
      "Training Batch: 17883 Loss: 3321.076172\n",
      "Training Batch: 17884 Loss: 3386.878174\n",
      "Training Batch: 17885 Loss: 3565.086426\n",
      "Training Batch: 17886 Loss: 3468.182617\n",
      "Training Batch: 17887 Loss: 3358.592041\n",
      "Training Batch: 17888 Loss: 3552.284912\n",
      "Training Batch: 17889 Loss: 3480.768311\n",
      "Training Batch: 17890 Loss: 3440.045898\n",
      "Training Batch: 17891 Loss: 3378.201904\n",
      "Training Batch: 17892 Loss: 3432.675781\n",
      "Training Batch: 17893 Loss: 3383.277344\n",
      "Training Batch: 17894 Loss: 3367.511719\n",
      "Training Batch: 17895 Loss: 3368.774414\n",
      "Training Batch: 17896 Loss: 3541.310059\n",
      "Training Batch: 17897 Loss: 3433.210205\n",
      "Training Batch: 17898 Loss: 3334.185059\n",
      "Training Batch: 17899 Loss: 3335.596191\n",
      "Training Batch: 17900 Loss: 3433.059814\n",
      "Training Batch: 17901 Loss: 3369.909180\n",
      "Training Batch: 17902 Loss: 3468.586914\n",
      "Training Batch: 17903 Loss: 3491.224365\n",
      "Training Batch: 17904 Loss: 3441.615234\n",
      "Training Batch: 17905 Loss: 3460.890625\n",
      "Training Batch: 17906 Loss: 3409.504150\n",
      "Training Batch: 17907 Loss: 3670.131836\n",
      "Training Batch: 17908 Loss: 3414.346680\n",
      "Training Batch: 17909 Loss: 3327.195801\n",
      "Training Batch: 17910 Loss: 3335.991699\n",
      "Training Batch: 17911 Loss: 3374.615479\n",
      "Training Batch: 17912 Loss: 3377.814453\n",
      "Training Batch: 17913 Loss: 3409.946289\n",
      "Training Batch: 17914 Loss: 3495.803467\n",
      "Training Batch: 17915 Loss: 3386.889648\n",
      "Training Batch: 17916 Loss: 3426.835205\n",
      "Training Batch: 17917 Loss: 3480.691406\n",
      "Training Batch: 17918 Loss: 3350.060547\n",
      "Training Batch: 17919 Loss: 3407.714600\n",
      "Training Batch: 17920 Loss: 3433.831787\n",
      "Training Batch: 17921 Loss: 3413.021484\n",
      "Training Batch: 17922 Loss: 3466.459961\n",
      "Training Batch: 17923 Loss: 3431.101562\n",
      "Training Batch: 17924 Loss: 3543.698975\n",
      "Training Batch: 17925 Loss: 3499.574463\n",
      "Training Batch: 17926 Loss: 3428.880859\n",
      "Training Batch: 17927 Loss: 3613.680176\n",
      "Training Batch: 17928 Loss: 3527.925293\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Batch: 17929 Loss: 3349.488770\n",
      "Training Batch: 17930 Loss: 3398.654297\n",
      "Training Batch: 17931 Loss: 3479.051025\n",
      "Training Batch: 17932 Loss: 3348.715332\n",
      "Training Batch: 17933 Loss: 3290.024414\n",
      "Training Batch: 17934 Loss: 3459.230225\n",
      "Training Batch: 17935 Loss: 3384.750732\n",
      "Training Batch: 17936 Loss: 3318.769043\n",
      "Training Batch: 17937 Loss: 3375.827637\n",
      "Training Batch: 17938 Loss: 3397.816406\n",
      "Training Batch: 17939 Loss: 3461.781494\n",
      "Training Batch: 17940 Loss: 3634.781006\n",
      "Training Batch: 17941 Loss: 3680.893066\n",
      "Training Batch: 17942 Loss: 3440.484619\n",
      "Training Batch: 17943 Loss: 3349.617188\n",
      "Training Batch: 17944 Loss: 3342.192871\n",
      "Training Batch: 17945 Loss: 3385.973145\n",
      "Training Batch: 17946 Loss: 3634.712402\n",
      "Training Batch: 17947 Loss: 3375.011475\n",
      "Training Batch: 17948 Loss: 3407.164062\n",
      "Training Batch: 17949 Loss: 3448.998535\n",
      "Training Batch: 17950 Loss: 3506.734863\n",
      "Training Batch: 17951 Loss: 3519.531982\n",
      "Training Batch: 17952 Loss: 3510.779785\n",
      "Training Batch: 17953 Loss: 3413.916992\n",
      "Training Batch: 17954 Loss: 3374.036133\n",
      "Training Batch: 17955 Loss: 3435.992676\n",
      "Training Batch: 17956 Loss: 3426.079102\n",
      "Training Batch: 17957 Loss: 3351.483643\n",
      "Training Batch: 17958 Loss: 3528.558838\n",
      "Training Batch: 17959 Loss: 3571.241455\n",
      "Training Batch: 17960 Loss: 3457.103516\n",
      "Training Batch: 17961 Loss: 3325.425293\n",
      "Training Batch: 17962 Loss: 3414.516602\n",
      "Training Batch: 17963 Loss: 3356.486572\n",
      "Training Batch: 17964 Loss: 3359.771484\n",
      "Training Batch: 17965 Loss: 3510.688965\n",
      "Training Batch: 17966 Loss: 3500.231689\n",
      "Training Batch: 17967 Loss: 3387.992676\n",
      "Training Batch: 17968 Loss: 3489.810547\n",
      "Training Batch: 17969 Loss: 3484.486816\n",
      "Training Batch: 17970 Loss: 3416.707275\n",
      "Training Batch: 17971 Loss: 3373.641113\n",
      "Training Batch: 17972 Loss: 3380.864502\n",
      "Training Batch: 17973 Loss: 3386.740723\n",
      "Training Batch: 17974 Loss: 3311.243652\n",
      "Training Batch: 17975 Loss: 3295.832275\n",
      "Training Batch: 17976 Loss: 3480.588867\n",
      "Training Batch: 17977 Loss: 3440.889648\n",
      "Training Batch: 17978 Loss: 3597.613525\n",
      "Training Batch: 17979 Loss: 3516.830078\n",
      "Training Batch: 17980 Loss: 3665.354980\n",
      "Training Batch: 17981 Loss: 3419.860107\n",
      "Training Batch: 17982 Loss: 3445.176514\n",
      "Training Batch: 17983 Loss: 3380.350586\n",
      "Training Batch: 17984 Loss: 3406.499023\n",
      "Training Batch: 17985 Loss: 3560.571777\n",
      "Training Batch: 17986 Loss: 3467.967285\n",
      "Training Batch: 17987 Loss: 3368.209229\n",
      "Training Batch: 17988 Loss: 3499.276367\n",
      "Training Batch: 17989 Loss: 3346.921631\n",
      "Training Batch: 17990 Loss: 3340.277344\n",
      "Training Batch: 17991 Loss: 3413.979492\n",
      "Training Batch: 17992 Loss: 3427.372070\n",
      "Training Batch: 17993 Loss: 3394.396729\n",
      "Training Batch: 17994 Loss: 3323.157715\n",
      "Training Batch: 17995 Loss: 3474.831787\n",
      "Training Batch: 17996 Loss: 3566.907715\n",
      "Training Batch: 17997 Loss: 3335.804688\n",
      "Training Batch: 17998 Loss: 3632.554688\n",
      "Training Batch: 17999 Loss: 3655.288574\n",
      "Training Batch: 18000 Loss: 3435.482910\n",
      "Training Batch: 18001 Loss: 3440.898926\n",
      "Training Batch: 18002 Loss: 3375.785889\n",
      "Training Batch: 18003 Loss: 3448.063721\n",
      "Training Batch: 18004 Loss: 3373.041504\n",
      "Training Batch: 18005 Loss: 3430.233643\n",
      "Training Batch: 18006 Loss: 3431.277344\n",
      "Training Batch: 18007 Loss: 3460.867188\n",
      "Training Batch: 18008 Loss: 3341.615723\n",
      "Training Batch: 18009 Loss: 3279.121826\n",
      "Training Batch: 18010 Loss: 3324.138184\n",
      "Training Batch: 18011 Loss: 3383.993164\n",
      "Training Batch: 18012 Loss: 3294.139893\n",
      "Training Batch: 18013 Loss: 3538.560547\n",
      "Training Batch: 18014 Loss: 3480.578857\n",
      "Training Batch: 18015 Loss: 3260.251953\n",
      "Training Batch: 18016 Loss: 3276.065430\n",
      "Training Batch: 18017 Loss: 3203.024658\n",
      "Training Batch: 18018 Loss: 3282.372559\n",
      "Training Batch: 18019 Loss: 3426.869629\n",
      "Training Batch: 18020 Loss: 3297.543945\n",
      "Training Batch: 18021 Loss: 3392.657959\n",
      "Training Batch: 18022 Loss: 3588.134766\n",
      "Training Batch: 18023 Loss: 3457.715332\n",
      "Training Batch: 18024 Loss: 3560.926514\n",
      "Training Batch: 18025 Loss: 3374.708008\n",
      "Training Batch: 18026 Loss: 3516.015625\n",
      "Training Batch: 18027 Loss: 3441.512695\n",
      "Training Batch: 18028 Loss: 3502.403564\n",
      "Training Batch: 18029 Loss: 3442.483398\n",
      "Training Batch: 18030 Loss: 3563.960693\n",
      "Training Batch: 18031 Loss: 3383.704346\n",
      "Training Batch: 18032 Loss: 3463.469482\n",
      "Training Batch: 18033 Loss: 3561.818359\n",
      "Training Batch: 18034 Loss: 3723.995605\n",
      "Training Batch: 18035 Loss: 3363.452148\n",
      "Training Batch: 18036 Loss: 3463.960449\n",
      "Training Batch: 18037 Loss: 3486.964355\n",
      "Training Batch: 18038 Loss: 3507.939941\n",
      "Training Batch: 18039 Loss: 3387.108154\n",
      "Training Batch: 18040 Loss: 3450.989746\n",
      "Training Batch: 18041 Loss: 3367.847168\n",
      "Training Batch: 18042 Loss: 3421.677246\n",
      "Training Batch: 18043 Loss: 3390.667969\n",
      "Training Batch: 18044 Loss: 3342.870117\n",
      "Training Batch: 18045 Loss: 3503.607422\n",
      "Training Batch: 18046 Loss: 3320.495117\n",
      "Training Batch: 18047 Loss: 3417.818848\n",
      "Training Batch: 18048 Loss: 3438.181152\n",
      "Training Batch: 18049 Loss: 3457.133057\n",
      "Training Batch: 18050 Loss: 3347.340088\n",
      "Training Batch: 18051 Loss: 3553.369629\n",
      "Training Batch: 18052 Loss: 3334.379150\n",
      "Training Batch: 18053 Loss: 3522.407227\n",
      "Training Batch: 18054 Loss: 3378.458252\n",
      "Training Batch: 18055 Loss: 3352.697754\n",
      "Training Batch: 18056 Loss: 3316.313477\n",
      "Training Batch: 18057 Loss: 3555.863037\n",
      "Training Batch: 18058 Loss: 3395.771240\n",
      "Training Batch: 18059 Loss: 3366.057373\n",
      "Training Batch: 18060 Loss: 3385.756348\n",
      "Training Batch: 18061 Loss: 3296.744141\n",
      "Training Batch: 18062 Loss: 3389.034180\n",
      "Training Batch: 18063 Loss: 3527.175537\n",
      "Training Batch: 18064 Loss: 3317.051758\n",
      "Training Batch: 18065 Loss: 3312.986816\n",
      "Training Batch: 18066 Loss: 3337.608887\n",
      "Training Batch: 18067 Loss: 3388.611328\n",
      "Training Batch: 18068 Loss: 3512.670898\n",
      "Training Batch: 18069 Loss: 3497.259277\n",
      "Training Batch: 18070 Loss: 3784.662354\n",
      "Training Batch: 18071 Loss: 3731.036133\n",
      "Training Batch: 18072 Loss: 3678.141357\n",
      "Training Batch: 18073 Loss: 3501.605957\n",
      "Training Batch: 18074 Loss: 3438.448242\n",
      "Training Batch: 18075 Loss: 3376.758057\n",
      "Training Batch: 18076 Loss: 3377.166992\n",
      "Training Batch: 18077 Loss: 3339.802246\n",
      "Training Batch: 18078 Loss: 4295.506836\n",
      "Training Batch: 18079 Loss: 4792.392090\n",
      "Training Batch: 18080 Loss: 4067.805176\n",
      "Training Batch: 18081 Loss: 4200.629883\n",
      "Training Batch: 18082 Loss: 4278.887695\n",
      "Training Batch: 18083 Loss: 3845.874023\n",
      "Training Batch: 18084 Loss: 3558.572266\n",
      "Training Batch: 18085 Loss: 3448.273926\n",
      "Training Batch: 18086 Loss: 3442.346680\n",
      "Training Batch: 18087 Loss: 3427.396484\n",
      "Training Batch: 18088 Loss: 3358.636230\n",
      "Training Batch: 18089 Loss: 3439.333496\n",
      "Training Batch: 18090 Loss: 3595.653320\n",
      "Training Batch: 18091 Loss: 3630.067627\n",
      "Training Batch: 18092 Loss: 3580.109863\n",
      "Training Batch: 18093 Loss: 3441.047852\n",
      "Training Batch: 18094 Loss: 3769.034668\n",
      "Training Batch: 18095 Loss: 3480.454102\n",
      "Training Batch: 18096 Loss: 3446.814941\n",
      "Training Batch: 18097 Loss: 3452.129883\n",
      "Training Batch: 18098 Loss: 4044.838379\n",
      "Training Batch: 18099 Loss: 3443.238281\n",
      "Training Batch: 18100 Loss: 3523.819336\n",
      "Training Batch: 18101 Loss: 3465.213379\n",
      "Training Batch: 18102 Loss: 3428.150879\n",
      "Training Batch: 18103 Loss: 3464.011230\n",
      "Training Batch: 18104 Loss: 3390.398682\n",
      "Training Batch: 18105 Loss: 3547.525635\n",
      "Training Batch: 18106 Loss: 3608.269043\n",
      "Training Batch: 18107 Loss: 3364.806152\n",
      "Training Batch: 18108 Loss: 3498.083496\n",
      "Training Batch: 18109 Loss: 3658.054932\n",
      "Training Batch: 18110 Loss: 3702.959473\n",
      "Training Batch: 18111 Loss: 3704.409668\n",
      "Training Batch: 18112 Loss: 3795.960205\n",
      "Training Batch: 18113 Loss: 3742.949707\n",
      "Training Batch: 18114 Loss: 3360.689453\n",
      "Training Batch: 18115 Loss: 3379.128174\n",
      "Training Batch: 18116 Loss: 3467.042480\n",
      "Training Batch: 18117 Loss: 3462.217285\n",
      "Training Batch: 18118 Loss: 3462.416504\n",
      "Training Batch: 18119 Loss: 3372.887207\n",
      "Training Batch: 18120 Loss: 3552.798828\n",
      "Training Batch: 18121 Loss: 3501.193848\n",
      "Training Batch: 18122 Loss: 3408.315430\n",
      "Training Batch: 18123 Loss: 3418.304932\n",
      "Training Batch: 18124 Loss: 3599.580078\n",
      "Training Batch: 18125 Loss: 3459.358887\n",
      "Training Batch: 18126 Loss: 3343.689941\n",
      "Training Batch: 18127 Loss: 3361.474121\n",
      "Training Batch: 18128 Loss: 3435.822510\n",
      "Training Batch: 18129 Loss: 3339.067383\n",
      "Training Batch: 18130 Loss: 3478.874023\n",
      "Training Batch: 18131 Loss: 3359.642090\n",
      "Training Batch: 18132 Loss: 3431.321045\n",
      "Training Batch: 18133 Loss: 3505.429932\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Batch: 18134 Loss: 3381.805176\n",
      "Training Batch: 18135 Loss: 3396.217773\n",
      "Training Batch: 18136 Loss: 3318.433594\n",
      "Training Batch: 18137 Loss: 3401.791748\n",
      "Training Batch: 18138 Loss: 3461.231445\n",
      "Training Batch: 18139 Loss: 3455.404297\n",
      "Training Batch: 18140 Loss: 3502.561523\n",
      "Training Batch: 18141 Loss: 3454.633789\n",
      "Training Batch: 18142 Loss: 3339.824219\n",
      "Training Batch: 18143 Loss: 3407.379883\n",
      "Training Batch: 18144 Loss: 3429.627197\n",
      "Training Batch: 18145 Loss: 3469.053223\n",
      "Training Batch: 18146 Loss: 3369.714355\n",
      "Training Batch: 18147 Loss: 3541.079834\n",
      "Training Batch: 18148 Loss: 3295.697021\n",
      "Training Batch: 18149 Loss: 3433.782715\n",
      "Training Batch: 18150 Loss: 3352.506836\n",
      "Training Batch: 18151 Loss: 3408.153320\n",
      "Training Batch: 18152 Loss: 3461.615723\n",
      "Training Batch: 18153 Loss: 3536.851074\n",
      "Training Batch: 18154 Loss: 3433.064941\n",
      "Training Batch: 18155 Loss: 3412.370117\n",
      "Training Batch: 18156 Loss: 3522.410645\n",
      "Training Batch: 18157 Loss: 3422.032715\n",
      "Training Batch: 18158 Loss: 3475.199707\n",
      "Training Batch: 18159 Loss: 3444.576172\n",
      "Training Batch: 18160 Loss: 3337.980713\n",
      "Training Batch: 18161 Loss: 3383.940186\n",
      "Training Batch: 18162 Loss: 3416.342285\n",
      "Training Batch: 18163 Loss: 3565.668945\n",
      "Training Batch: 18164 Loss: 3374.928223\n",
      "Training Batch: 18165 Loss: 3299.598389\n",
      "Training Batch: 18166 Loss: 3369.271484\n",
      "Training Batch: 18167 Loss: 3585.352051\n",
      "Training Batch: 18168 Loss: 3461.849854\n",
      "Training Batch: 18169 Loss: 3453.938232\n",
      "Training Batch: 18170 Loss: 3451.156738\n",
      "Training Batch: 18171 Loss: 3447.026855\n",
      "Training Batch: 18172 Loss: 3511.032715\n",
      "Training Batch: 18173 Loss: 3465.412598\n",
      "Training Batch: 18174 Loss: 3418.041504\n",
      "Training Batch: 18175 Loss: 3620.234375\n",
      "Training Batch: 18176 Loss: 3397.554199\n",
      "Training Batch: 18177 Loss: 3625.103271\n",
      "Training Batch: 18178 Loss: 3407.961914\n",
      "Training Batch: 18179 Loss: 3562.374268\n",
      "Training Batch: 18180 Loss: 3348.354492\n",
      "Training Batch: 18181 Loss: 3440.862061\n",
      "Training Batch: 18182 Loss: 3535.901367\n",
      "Training Batch: 18183 Loss: 3355.626953\n",
      "Training Batch: 18184 Loss: 3547.442871\n",
      "Training Batch: 18185 Loss: 3386.928223\n",
      "Training Batch: 18186 Loss: 3343.553711\n",
      "Training Batch: 18187 Loss: 3549.840088\n",
      "Training Batch: 18188 Loss: 3407.609375\n",
      "Training Batch: 18189 Loss: 3361.267822\n",
      "Training Batch: 18190 Loss: 3329.118164\n",
      "Training Batch: 18191 Loss: 3388.327393\n",
      "Training Batch: 18192 Loss: 3342.593262\n",
      "Training Batch: 18193 Loss: 3325.140625\n",
      "Training Batch: 18194 Loss: 3697.274414\n",
      "Training Batch: 18195 Loss: 3435.423828\n",
      "Training Batch: 18196 Loss: 3454.606445\n",
      "Training Batch: 18197 Loss: 3433.077148\n",
      "Training Batch: 18198 Loss: 3535.812744\n",
      "Training Batch: 18199 Loss: 3434.145508\n",
      "Training Batch: 18200 Loss: 3272.915527\n",
      "Training Batch: 18201 Loss: 3387.894531\n",
      "Training Batch: 18202 Loss: 3411.455811\n",
      "Training Batch: 18203 Loss: 3302.091797\n",
      "Training Batch: 18204 Loss: 3404.389648\n",
      "Training Batch: 18205 Loss: 3421.862793\n",
      "Training Batch: 18206 Loss: 3410.236328\n",
      "Training Batch: 18207 Loss: 3492.682373\n",
      "Training Batch: 18208 Loss: 3449.068359\n",
      "Training Batch: 18209 Loss: 3346.588135\n",
      "Training Batch: 18210 Loss: 3541.518066\n",
      "Training Batch: 18211 Loss: 3552.497070\n",
      "Training Batch: 18212 Loss: 3440.844727\n",
      "Training Batch: 18213 Loss: 3364.031250\n",
      "Training Batch: 18214 Loss: 3537.968262\n",
      "Training Batch: 18215 Loss: 3416.495605\n",
      "Training Batch: 18216 Loss: 3422.559814\n",
      "Training Batch: 18217 Loss: 3449.359863\n",
      "Training Batch: 18218 Loss: 3700.538086\n",
      "Training Batch: 18219 Loss: 3427.454102\n",
      "Training Batch: 18220 Loss: 3470.862305\n",
      "Training Batch: 18221 Loss: 3455.117188\n",
      "Training Batch: 18222 Loss: 3519.300293\n",
      "Training Batch: 18223 Loss: 3335.511719\n",
      "Training Batch: 18224 Loss: 3479.592285\n",
      "Training Batch: 18225 Loss: 3354.739258\n",
      "Training Batch: 18226 Loss: 3317.205322\n",
      "Training Batch: 18227 Loss: 3229.043213\n",
      "Training Batch: 18228 Loss: 3441.034180\n",
      "Training Batch: 18229 Loss: 3317.675293\n",
      "Training Batch: 18230 Loss: 3360.122070\n",
      "Training Batch: 18231 Loss: 3368.979248\n",
      "Training Batch: 18232 Loss: 3533.261230\n",
      "Training Batch: 18233 Loss: 3367.506104\n",
      "Training Batch: 18234 Loss: 3350.171387\n",
      "Training Batch: 18235 Loss: 3471.216064\n",
      "Training Batch: 18236 Loss: 3428.829346\n",
      "Training Batch: 18237 Loss: 3307.116211\n",
      "Training Batch: 18238 Loss: 3343.788574\n",
      "Training Batch: 18239 Loss: 3319.349365\n",
      "Training Batch: 18240 Loss: 3668.451660\n",
      "Training Batch: 18241 Loss: 3425.412109\n",
      "Training Batch: 18242 Loss: 3439.109375\n",
      "Training Batch: 18243 Loss: 3628.593750\n",
      "Training Batch: 18244 Loss: 3347.114258\n",
      "Training Batch: 18245 Loss: 3441.479004\n",
      "Training Batch: 18246 Loss: 3525.266602\n",
      "Training Batch: 18247 Loss: 3381.686035\n",
      "Training Batch: 18248 Loss: 3381.088135\n",
      "Training Batch: 18249 Loss: 3389.794678\n",
      "Training Batch: 18250 Loss: 3641.402832\n",
      "Training Batch: 18251 Loss: 3311.791016\n",
      "Training Batch: 18252 Loss: 3500.059082\n",
      "Training Batch: 18253 Loss: 3318.658203\n",
      "Training Batch: 18254 Loss: 3299.684814\n",
      "Training Batch: 18255 Loss: 3350.604004\n",
      "Training Batch: 18256 Loss: 3354.909424\n",
      "Training Batch: 18257 Loss: 3420.238281\n",
      "Training Batch: 18258 Loss: 3519.898438\n",
      "Training Batch: 18259 Loss: 3294.242432\n",
      "Training Batch: 18260 Loss: 3423.487549\n",
      "Training Batch: 18261 Loss: 3437.310791\n",
      "Training Batch: 18262 Loss: 3687.851074\n",
      "Training Batch: 18263 Loss: 3847.660156\n",
      "Training Batch: 18264 Loss: 3474.394531\n",
      "Training Batch: 18265 Loss: 3509.848633\n",
      "Training Batch: 18266 Loss: 3410.549072\n",
      "Training Batch: 18267 Loss: 3614.395752\n",
      "Training Batch: 18268 Loss: 3507.669189\n",
      "Training Batch: 18269 Loss: 3430.820312\n",
      "Training Batch: 18270 Loss: 3556.956055\n",
      "Training Batch: 18271 Loss: 3415.840820\n",
      "Training Batch: 18272 Loss: 3456.613770\n",
      "Training Batch: 18273 Loss: 3749.850830\n",
      "Training Batch: 18274 Loss: 3452.675781\n",
      "Training Batch: 18275 Loss: 3666.039551\n",
      "Training Batch: 18276 Loss: 3454.455078\n",
      "Training Batch: 18277 Loss: 3343.116699\n",
      "Training Batch: 18278 Loss: 3348.427002\n",
      "Training Batch: 18279 Loss: 3420.291016\n",
      "Training Batch: 18280 Loss: 3439.715820\n",
      "Training Batch: 18281 Loss: 3453.662598\n",
      "Training Batch: 18282 Loss: 3343.758301\n",
      "Training Batch: 18283 Loss: 3606.876953\n",
      "Training Batch: 18284 Loss: 3420.120361\n",
      "Training Batch: 18285 Loss: 3844.410645\n",
      "Training Batch: 18286 Loss: 3872.695557\n",
      "Training Batch: 18287 Loss: 3420.476562\n",
      "Training Batch: 18288 Loss: 3477.748047\n",
      "Training Batch: 18289 Loss: 3311.164551\n",
      "Training Batch: 18290 Loss: 3533.946045\n",
      "Training Batch: 18291 Loss: 3552.968262\n",
      "Training Batch: 18292 Loss: 3494.245605\n",
      "Training Batch: 18293 Loss: 3570.776855\n",
      "Training Batch: 18294 Loss: 3344.890137\n",
      "Training Batch: 18295 Loss: 3410.956299\n",
      "Training Batch: 18296 Loss: 3479.348145\n",
      "Training Batch: 18297 Loss: 3507.769775\n",
      "Training Batch: 18298 Loss: 3361.915283\n",
      "Training Batch: 18299 Loss: 3484.831543\n",
      "Training Batch: 18300 Loss: 3362.029785\n",
      "Training Batch: 18301 Loss: 3433.394531\n",
      "Training Batch: 18302 Loss: 3340.815430\n",
      "Training Batch: 18303 Loss: 3394.591309\n",
      "Training Batch: 18304 Loss: 3497.494141\n",
      "Training Batch: 18305 Loss: 3533.474609\n",
      "Training Batch: 18306 Loss: 3352.206543\n",
      "Training Batch: 18307 Loss: 3340.259033\n",
      "Training Batch: 18308 Loss: 3355.382812\n",
      "Training Batch: 18309 Loss: 3383.890137\n",
      "Training Batch: 18310 Loss: 3346.925293\n",
      "Training Batch: 18311 Loss: 3344.030029\n",
      "Training Batch: 18312 Loss: 3286.777344\n",
      "Training Batch: 18313 Loss: 3355.946777\n",
      "Training Batch: 18314 Loss: 3464.697754\n",
      "Training Batch: 18315 Loss: 3461.883057\n",
      "Training Batch: 18316 Loss: 3368.003906\n",
      "Training Batch: 18317 Loss: 3441.720215\n",
      "Training Batch: 18318 Loss: 3288.804688\n",
      "Training Batch: 18319 Loss: 3321.506836\n",
      "Training Batch: 18320 Loss: 3228.784180\n",
      "Training Batch: 18321 Loss: 3401.969971\n",
      "Training Batch: 18322 Loss: 3346.112305\n",
      "Training Batch: 18323 Loss: 3422.476562\n",
      "Training Batch: 18324 Loss: 3347.854492\n",
      "Training Batch: 18325 Loss: 3324.847168\n",
      "Training Batch: 18326 Loss: 3307.955078\n",
      "Training Batch: 18327 Loss: 3434.097168\n",
      "Training Batch: 18328 Loss: 3386.804688\n",
      "Training Batch: 18329 Loss: 3421.950684\n",
      "Training Batch: 18330 Loss: 3389.320312\n",
      "Training Batch: 18331 Loss: 4123.514648\n",
      "Training Batch: 18332 Loss: 3361.278076\n",
      "Training Batch: 18333 Loss: 3305.434814\n",
      "Training Batch: 18334 Loss: 3421.722656\n",
      "Training Batch: 18335 Loss: 3344.794434\n",
      "Training Batch: 18336 Loss: 3609.324707\n",
      "Training Batch: 18337 Loss: 3375.594482\n",
      "Training Batch: 18338 Loss: 3399.206543\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Batch: 18339 Loss: 3472.106689\n",
      "Training Batch: 18340 Loss: 3363.022461\n",
      "Training Batch: 18341 Loss: 3581.805664\n",
      "Training Batch: 18342 Loss: 3491.579102\n",
      "Training Batch: 18343 Loss: 3472.080811\n",
      "Training Batch: 18344 Loss: 3419.362793\n",
      "Training Batch: 18345 Loss: 3437.440430\n",
      "Training Batch: 18346 Loss: 3360.687012\n",
      "Training Batch: 18347 Loss: 3344.476562\n",
      "Training Batch: 18348 Loss: 3348.820801\n",
      "Training Batch: 18349 Loss: 3479.458984\n",
      "Training Batch: 18350 Loss: 3431.214844\n",
      "Training Batch: 18351 Loss: 3612.698730\n",
      "Training Batch: 18352 Loss: 3544.463379\n",
      "Training Batch: 18353 Loss: 3329.829590\n",
      "Training Batch: 18354 Loss: 3318.771240\n",
      "Training Batch: 18355 Loss: 3399.904785\n",
      "Training Batch: 18356 Loss: 3455.701660\n",
      "Training Batch: 18357 Loss: 3442.770996\n",
      "Training Batch: 18358 Loss: 3536.836670\n",
      "Training Batch: 18359 Loss: 3458.507080\n",
      "Training Batch: 18360 Loss: 3517.115723\n",
      "Training Batch: 18361 Loss: 3341.674316\n",
      "Training Batch: 18362 Loss: 3424.628906\n",
      "Training Batch: 18363 Loss: 3481.107910\n",
      "Training Batch: 18364 Loss: 3405.352051\n",
      "Training Batch: 18365 Loss: 3530.902344\n",
      "Training Batch: 18366 Loss: 3419.779297\n",
      "Training Batch: 18367 Loss: 3429.976074\n",
      "Training Batch: 18368 Loss: 3411.723389\n",
      "Training Batch: 18369 Loss: 3366.991211\n",
      "Training Batch: 18370 Loss: 3510.736816\n",
      "Training Batch: 18371 Loss: 3421.127441\n",
      "Training Batch: 18372 Loss: 3497.420898\n",
      "Training Batch: 18373 Loss: 3468.382812\n",
      "Training Batch: 18374 Loss: 3402.663818\n",
      "Training Batch: 18375 Loss: 3510.410889\n",
      "Training Batch: 18376 Loss: 3999.606934\n",
      "Training Batch: 18377 Loss: 3710.524902\n",
      "Training Batch: 18378 Loss: 3505.365723\n",
      "Training Batch: 18379 Loss: 3324.783447\n",
      "Training Batch: 18380 Loss: 3431.850098\n",
      "Training Batch: 18381 Loss: 3340.337891\n",
      "Training Batch: 18382 Loss: 3454.558838\n",
      "Training Batch: 18383 Loss: 3413.616699\n",
      "Training Batch: 18384 Loss: 3465.313965\n",
      "Training Batch: 18385 Loss: 3442.191406\n",
      "Training Batch: 18386 Loss: 3412.367676\n",
      "Training Batch: 18387 Loss: 3436.648193\n",
      "Training Batch: 18388 Loss: 3437.775879\n",
      "Training Batch: 18389 Loss: 4111.427734\n",
      "Training Batch: 18390 Loss: 3668.919922\n",
      "Training Batch: 18391 Loss: 3596.492188\n",
      "Training Batch: 18392 Loss: 3477.468018\n",
      "Training Batch: 18393 Loss: 3600.922363\n",
      "Training Batch: 18394 Loss: 3478.870605\n",
      "Training Batch: 18395 Loss: 3384.169922\n",
      "Training Batch: 18396 Loss: 3498.586426\n",
      "Training Batch: 18397 Loss: 3400.162109\n",
      "Training Batch: 18398 Loss: 3337.627441\n",
      "Training Batch: 18399 Loss: 3360.446045\n",
      "Training Batch: 18400 Loss: 3311.565186\n",
      "Training Batch: 18401 Loss: 3474.654785\n",
      "Training Batch: 18402 Loss: 3575.793213\n",
      "Training Batch: 18403 Loss: 3411.697266\n",
      "Training Batch: 18404 Loss: 3485.876953\n",
      "Training Batch: 18405 Loss: 3400.636963\n",
      "Training Batch: 18406 Loss: 3421.139893\n",
      "Training Batch: 18407 Loss: 3376.642090\n",
      "Training Batch: 18408 Loss: 3410.480469\n",
      "Training Batch: 18409 Loss: 3413.863525\n",
      "Training Batch: 18410 Loss: 3431.187012\n",
      "Training Batch: 18411 Loss: 3402.130859\n",
      "Training Batch: 18412 Loss: 3604.240234\n",
      "Training Batch: 18413 Loss: 3403.183105\n",
      "Training Batch: 18414 Loss: 3875.774658\n",
      "Training Batch: 18415 Loss: 3759.132812\n",
      "Training Batch: 18416 Loss: 3683.946533\n",
      "Training Batch: 18417 Loss: 3429.849854\n",
      "Training Batch: 18418 Loss: 3835.814453\n",
      "Training Batch: 18419 Loss: 3649.101562\n",
      "Training Batch: 18420 Loss: 3331.037109\n",
      "Training Batch: 18421 Loss: 3431.721191\n",
      "Training Batch: 18422 Loss: 3524.963867\n",
      "Training Batch: 18423 Loss: 3510.607422\n",
      "Training Batch: 18424 Loss: 3419.257324\n",
      "Training Batch: 18425 Loss: 3374.338867\n",
      "Training Batch: 18426 Loss: 3434.268066\n",
      "Training Batch: 18427 Loss: 3411.941406\n",
      "Training Batch: 18428 Loss: 3584.707764\n",
      "Training Batch: 18429 Loss: 3386.148193\n",
      "Training Batch: 18430 Loss: 3450.162598\n",
      "Training Batch: 18431 Loss: 3533.417969\n",
      "Training Batch: 18432 Loss: 3398.878418\n",
      "Training Batch: 18433 Loss: 3445.133789\n",
      "Training Batch: 18434 Loss: 3350.969727\n",
      "Training Batch: 18435 Loss: 3362.419922\n",
      "Training Batch: 18436 Loss: 3368.062988\n",
      "Training Batch: 18437 Loss: 3312.962891\n",
      "Training Batch: 18438 Loss: 3370.362793\n",
      "Training Batch: 18439 Loss: 3352.285645\n",
      "Training Batch: 18440 Loss: 3588.113770\n",
      "Training Batch: 18441 Loss: 3431.673096\n",
      "Training Batch: 18442 Loss: 3325.948730\n",
      "Training Batch: 18443 Loss: 3266.002930\n",
      "Training Batch: 18444 Loss: 3407.964844\n",
      "Training Batch: 18445 Loss: 3686.852295\n",
      "Training Batch: 18446 Loss: 3488.535156\n",
      "Training Batch: 18447 Loss: 3789.089111\n",
      "Training Batch: 18448 Loss: 3374.832520\n",
      "Training Batch: 18449 Loss: 3457.343750\n",
      "Training Batch: 18450 Loss: 3544.373535\n",
      "Training Batch: 18451 Loss: 3379.987061\n",
      "Training Batch: 18452 Loss: 3310.727539\n",
      "Training Batch: 18453 Loss: 3382.428467\n",
      "Training Batch: 18454 Loss: 3402.632324\n",
      "Training Batch: 18455 Loss: 3399.204102\n",
      "Training Batch: 18456 Loss: 3386.870117\n",
      "Training Batch: 18457 Loss: 3659.262207\n",
      "Training Batch: 18458 Loss: 3849.577148\n",
      "Training Batch: 18459 Loss: 3626.392090\n",
      "Training Batch: 18460 Loss: 3422.367188\n",
      "Training Batch: 18461 Loss: 3501.673584\n",
      "Training Batch: 18462 Loss: 3623.289062\n",
      "Training Batch: 18463 Loss: 3474.977783\n",
      "Training Batch: 18464 Loss: 3425.397949\n",
      "Training Batch: 18465 Loss: 3651.745605\n",
      "Training Batch: 18466 Loss: 3513.863770\n",
      "Training Batch: 18467 Loss: 3575.802490\n",
      "Training Batch: 18468 Loss: 3345.184570\n",
      "Training Batch: 18469 Loss: 3401.918945\n",
      "Training Batch: 18470 Loss: 3395.522949\n",
      "Training Batch: 18471 Loss: 3406.196045\n",
      "Training Batch: 18472 Loss: 3520.307617\n",
      "Training Batch: 18473 Loss: 3377.466309\n",
      "Training Batch: 18474 Loss: 3871.744141\n",
      "Training Batch: 18475 Loss: 3590.720703\n",
      "Training Batch: 18476 Loss: 3325.938965\n",
      "Training Batch: 18477 Loss: 3481.832031\n",
      "Training Batch: 18478 Loss: 3665.542969\n",
      "Training Batch: 18479 Loss: 3621.809570\n",
      "Training Batch: 18480 Loss: 3544.066406\n",
      "Training Batch: 18481 Loss: 3369.510254\n",
      "Training Batch: 18482 Loss: 3498.583008\n",
      "Training Batch: 18483 Loss: 3435.586914\n",
      "Training Batch: 18484 Loss: 3343.010742\n",
      "Training Batch: 18485 Loss: 3346.882812\n",
      "Training Batch: 18486 Loss: 3407.040039\n",
      "Training Batch: 18487 Loss: 3513.395508\n",
      "Training Batch: 18488 Loss: 3398.708984\n",
      "Training Batch: 18489 Loss: 3385.586426\n",
      "Training Batch: 18490 Loss: 3317.087891\n",
      "Training Batch: 18491 Loss: 3364.634766\n",
      "Training Batch: 18492 Loss: 3377.188477\n",
      "Training Batch: 18493 Loss: 3386.268066\n",
      "Training Batch: 18494 Loss: 3428.675537\n",
      "Training Batch: 18495 Loss: 3354.676758\n",
      "Training Batch: 18496 Loss: 3330.354004\n",
      "Training Batch: 18497 Loss: 3568.380615\n",
      "Training Batch: 18498 Loss: 4029.687012\n",
      "Training Batch: 18499 Loss: 3438.499023\n",
      "Training Batch: 18500 Loss: 3423.929199\n",
      "Training Batch: 18501 Loss: 3460.714844\n",
      "Training Batch: 18502 Loss: 3305.043945\n",
      "Training Batch: 18503 Loss: 3304.496338\n",
      "Training Batch: 18504 Loss: 3425.202148\n",
      "Training Batch: 18505 Loss: 3335.487305\n",
      "Training Batch: 18506 Loss: 3321.211426\n",
      "Training Batch: 18507 Loss: 3465.583496\n",
      "Training Batch: 18508 Loss: 3242.761230\n",
      "Training Batch: 18509 Loss: 3487.084717\n",
      "Training Batch: 18510 Loss: 3405.907715\n",
      "Training Batch: 18511 Loss: 3325.809814\n",
      "Training Batch: 18512 Loss: 3378.017822\n",
      "Training Batch: 18513 Loss: 3516.314941\n",
      "Training Batch: 18514 Loss: 3719.400146\n",
      "Training Batch: 18515 Loss: 3521.085693\n",
      "Training Batch: 18516 Loss: 3477.018555\n",
      "Training Batch: 18517 Loss: 3512.241699\n",
      "Training Batch: 18518 Loss: 3314.227295\n",
      "Training Batch: 18519 Loss: 3288.351562\n",
      "Training Batch: 18520 Loss: 3321.657959\n",
      "Training Batch: 18521 Loss: 3418.035889\n",
      "Training Batch: 18522 Loss: 3360.599121\n",
      "Training Batch: 18523 Loss: 3405.410156\n",
      "Training Batch: 18524 Loss: 3298.459473\n",
      "Training Batch: 18525 Loss: 3586.035156\n",
      "Training Batch: 18526 Loss: 3524.937988\n",
      "Training Batch: 18527 Loss: 3530.935547\n",
      "Training Batch: 18528 Loss: 3420.483398\n",
      "Training Batch: 18529 Loss: 3360.654297\n",
      "Training Batch: 18530 Loss: 3366.386719\n",
      "Training Batch: 18531 Loss: 3282.300781\n",
      "Training Batch: 18532 Loss: 3404.388428\n",
      "Training Batch: 18533 Loss: 3313.322754\n",
      "Training Batch: 18534 Loss: 3405.794189\n",
      "Training Batch: 18535 Loss: 3539.038574\n",
      "Training Batch: 18536 Loss: 3376.339844\n",
      "Training Batch: 18537 Loss: 3596.363281\n",
      "Training Batch: 18538 Loss: 3739.584473\n",
      "Training Batch: 18539 Loss: 3385.092285\n",
      "Training Batch: 18540 Loss: 3329.480957\n",
      "Training Batch: 18541 Loss: 3420.755859\n",
      "Training Batch: 18542 Loss: 3240.917480\n",
      "Training Batch: 18543 Loss: 3589.714355\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Batch: 18544 Loss: 3422.965820\n",
      "Training Batch: 18545 Loss: 3300.265381\n",
      "Training Batch: 18546 Loss: 3492.717773\n",
      "Training Batch: 18547 Loss: 3537.561279\n",
      "Training Batch: 18548 Loss: 3376.109375\n",
      "Training Batch: 18549 Loss: 3692.910645\n",
      "Training Batch: 18550 Loss: 3461.958252\n",
      "Training Batch: 18551 Loss: 3472.942383\n",
      "Training Batch: 18552 Loss: 3409.548584\n",
      "Training Batch: 18553 Loss: 3508.951172\n",
      "Training Batch: 18554 Loss: 3590.529053\n",
      "Training Batch: 18555 Loss: 3422.495605\n",
      "Training Batch: 18556 Loss: 3449.638184\n",
      "Training Batch: 18557 Loss: 3365.813965\n",
      "Training Batch: 18558 Loss: 3480.223877\n",
      "Training Batch: 18559 Loss: 3422.171875\n",
      "Training Batch: 18560 Loss: 3314.058594\n",
      "Training Batch: 18561 Loss: 3372.587402\n",
      "Training Batch: 18562 Loss: 3514.076660\n",
      "Training Batch: 18563 Loss: 3427.276123\n",
      "Training Batch: 18564 Loss: 3500.688721\n",
      "Training Batch: 18565 Loss: 3599.932129\n",
      "Training Batch: 18566 Loss: 3382.353516\n",
      "Training Batch: 18567 Loss: 3581.363037\n",
      "Training Batch: 18568 Loss: 3747.949219\n",
      "Training Batch: 18569 Loss: 3631.786133\n",
      "Training Batch: 18570 Loss: 3462.970215\n",
      "Training Batch: 18571 Loss: 3395.166992\n",
      "Training Batch: 18572 Loss: 3418.551758\n",
      "Training Batch: 18573 Loss: 3635.383789\n",
      "Training Batch: 18574 Loss: 3732.288574\n",
      "Training Batch: 18575 Loss: 3547.214355\n",
      "Training Batch: 18576 Loss: 3587.296631\n",
      "Training Batch: 18577 Loss: 3616.342285\n",
      "Training Batch: 18578 Loss: 3370.876465\n",
      "Training Batch: 18579 Loss: 3378.216064\n",
      "Training Batch: 18580 Loss: 3427.763184\n",
      "Training Batch: 18581 Loss: 3476.320801\n",
      "Training Batch: 18582 Loss: 3396.097168\n",
      "Training Batch: 18583 Loss: 3344.604492\n",
      "Training Batch: 18584 Loss: 3352.243896\n",
      "Training Batch: 18585 Loss: 3360.218750\n",
      "Training Batch: 18586 Loss: 3382.041504\n",
      "Training Batch: 18587 Loss: 3480.517578\n",
      "Training Batch: 18588 Loss: 3517.892578\n",
      "Training Batch: 18589 Loss: 3295.780273\n",
      "Training Batch: 18590 Loss: 3429.269531\n",
      "Training Batch: 18591 Loss: 3296.400635\n",
      "Training Batch: 18592 Loss: 3299.648926\n",
      "Training Batch: 18593 Loss: 3421.939453\n",
      "Training Batch: 18594 Loss: 3290.138672\n",
      "Training Batch: 18595 Loss: 3272.062988\n",
      "Training Batch: 18596 Loss: 3302.168945\n",
      "Training Batch: 18597 Loss: 3484.449219\n",
      "Training Batch: 18598 Loss: 3306.688232\n",
      "Training Batch: 18599 Loss: 3439.362793\n",
      "Training Batch: 18600 Loss: 3500.953857\n",
      "Training Batch: 18601 Loss: 3413.138184\n",
      "Training Batch: 18602 Loss: 3401.718506\n",
      "Training Batch: 18603 Loss: 3382.525879\n",
      "Training Batch: 18604 Loss: 3294.492188\n",
      "Training Batch: 18605 Loss: 3345.100586\n",
      "Training Batch: 18606 Loss: 3372.804199\n",
      "Training Batch: 18607 Loss: 3335.142090\n",
      "Training Batch: 18608 Loss: 3509.134033\n",
      "Training Batch: 18609 Loss: 3386.686035\n",
      "Training Batch: 18610 Loss: 3544.225830\n",
      "Training Batch: 18611 Loss: 3515.953857\n",
      "Training Batch: 18612 Loss: 3408.560303\n",
      "Training Batch: 18613 Loss: 3298.803223\n",
      "Training Batch: 18614 Loss: 3505.716797\n",
      "Training Batch: 18615 Loss: 3579.345703\n",
      "Training Batch: 18616 Loss: 3428.747559\n",
      "Training Batch: 18617 Loss: 3388.725830\n",
      "Training Batch: 18618 Loss: 3446.006348\n",
      "Training Batch: 18619 Loss: 3463.844238\n",
      "Training Batch: 18620 Loss: 3482.851074\n",
      "Training Batch: 18621 Loss: 3338.130859\n",
      "Training Batch: 18622 Loss: 3419.804688\n",
      "Training Batch: 18623 Loss: 3417.545898\n",
      "Training Batch: 18624 Loss: 3421.006836\n",
      "Training Batch: 18625 Loss: 3473.213867\n",
      "Training Batch: 18626 Loss: 3417.673584\n",
      "Training Batch: 18627 Loss: 3291.268555\n",
      "Training Batch: 18628 Loss: 3367.748535\n",
      "Training Batch: 18629 Loss: 3365.233643\n",
      "Training Batch: 18630 Loss: 3416.800537\n",
      "Training Batch: 18631 Loss: 3330.749512\n",
      "Training Batch: 18632 Loss: 3450.696777\n",
      "Training Batch: 18633 Loss: 3424.861328\n",
      "Training Batch: 18634 Loss: 3318.562012\n",
      "Training Batch: 18635 Loss: 3307.688477\n",
      "Training Batch: 18636 Loss: 3374.912109\n",
      "Training Batch: 18637 Loss: 3300.603516\n",
      "Training Batch: 18638 Loss: 3360.956055\n",
      "Training Batch: 18639 Loss: 3622.217285\n",
      "Training Batch: 18640 Loss: 3548.304688\n",
      "Training Batch: 18641 Loss: 3417.106445\n",
      "Training Batch: 18642 Loss: 3522.016602\n",
      "Training Batch: 18643 Loss: 3718.602051\n",
      "Training Batch: 18644 Loss: 3724.462402\n",
      "Training Batch: 18645 Loss: 3384.981934\n",
      "Training Batch: 18646 Loss: 3465.584473\n",
      "Training Batch: 18647 Loss: 3553.067383\n",
      "Training Batch: 18648 Loss: 3607.416260\n",
      "Training Batch: 18649 Loss: 3553.026855\n",
      "Training Batch: 18650 Loss: 3578.114014\n",
      "Training Batch: 18651 Loss: 3461.599854\n",
      "Training Batch: 18652 Loss: 3354.064453\n",
      "Training Batch: 18653 Loss: 3474.102051\n",
      "Training Batch: 18654 Loss: 3348.805420\n",
      "Training Batch: 18655 Loss: 3308.907715\n",
      "Training Batch: 18656 Loss: 3448.436279\n",
      "Training Batch: 18657 Loss: 3446.614014\n",
      "Training Batch: 18658 Loss: 3484.685547\n",
      "Training Batch: 18659 Loss: 3477.130371\n",
      "Training Batch: 18660 Loss: 3508.354492\n",
      "Training Batch: 18661 Loss: 3344.216064\n",
      "Training Batch: 18662 Loss: 3448.369873\n",
      "Training Batch: 18663 Loss: 3323.157471\n",
      "Training Batch: 18664 Loss: 3374.152832\n",
      "Training Batch: 18665 Loss: 3425.930664\n",
      "Training Batch: 18666 Loss: 3343.708984\n",
      "Training Batch: 18667 Loss: 3354.515137\n",
      "Training Batch: 18668 Loss: 3372.199707\n",
      "Training Batch: 18669 Loss: 3380.925781\n",
      "Training Batch: 18670 Loss: 3401.583984\n",
      "Training Batch: 18671 Loss: 3327.485840\n",
      "Training Batch: 18672 Loss: 3335.138672\n",
      "Training Batch: 18673 Loss: 3365.934570\n",
      "Training Batch: 18674 Loss: 3419.612305\n",
      "Training Batch: 18675 Loss: 3277.844727\n",
      "Training Batch: 18676 Loss: 3421.321777\n",
      "Training Batch: 18677 Loss: 3464.542480\n",
      "Training Batch: 18678 Loss: 3579.558594\n",
      "Training Batch: 18679 Loss: 3347.746094\n",
      "Training Batch: 18680 Loss: 3363.067871\n",
      "Training Batch: 18681 Loss: 3299.651123\n",
      "Training Batch: 18682 Loss: 3521.978516\n",
      "Training Batch: 18683 Loss: 3305.471436\n",
      "Training Batch: 18684 Loss: 3506.922607\n",
      "Training Batch: 18685 Loss: 3529.097656\n",
      "Training Batch: 18686 Loss: 3351.101562\n",
      "Training Batch: 18687 Loss: 3426.269287\n",
      "Training Batch: 18688 Loss: 3364.347656\n",
      "Training Batch: 18689 Loss: 3486.945557\n",
      "Training Batch: 18690 Loss: 3464.233154\n",
      "Training Batch: 18691 Loss: 3327.660645\n",
      "Training Batch: 18692 Loss: 3359.318115\n",
      "Training Batch: 18693 Loss: 3434.603271\n",
      "Training Batch: 18694 Loss: 3416.676514\n",
      "Training Batch: 18695 Loss: 3451.145508\n",
      "Training Batch: 18696 Loss: 3452.023438\n",
      "Training Batch: 18697 Loss: 3612.500000\n",
      "Training Batch: 18698 Loss: 3525.505859\n",
      "Training Batch: 18699 Loss: 3583.639648\n",
      "Training Batch: 18700 Loss: 3316.083496\n",
      "Training Batch: 18701 Loss: 3486.857666\n",
      "Training Batch: 18702 Loss: 3406.523193\n",
      "Training Batch: 18703 Loss: 3718.164551\n",
      "Training Batch: 18704 Loss: 3434.387939\n",
      "Training Batch: 18705 Loss: 3419.781494\n",
      "Training Batch: 18706 Loss: 3379.972656\n",
      "Training Batch: 18707 Loss: 3386.015137\n",
      "Training Batch: 18708 Loss: 3349.724121\n",
      "Training Batch: 18709 Loss: 3563.103027\n",
      "Training Batch: 18710 Loss: 3416.976562\n",
      "Training Batch: 18711 Loss: 3472.226074\n",
      "Training Batch: 18712 Loss: 3377.934326\n",
      "Training Batch: 18713 Loss: 3327.911133\n",
      "Training Batch: 18714 Loss: 3398.032227\n",
      "Training Batch: 18715 Loss: 3370.954102\n",
      "Training Batch: 18716 Loss: 3437.102783\n",
      "Training Batch: 18717 Loss: 3410.271484\n",
      "Training Batch: 18718 Loss: 4160.641602\n",
      "Training Batch: 18719 Loss: 3622.000732\n",
      "Training Batch: 18720 Loss: 3420.635254\n",
      "Training Batch: 18721 Loss: 3316.050781\n",
      "Training Batch: 18722 Loss: 3543.182373\n",
      "Training Batch: 18723 Loss: 3520.633789\n",
      "Training Batch: 18724 Loss: 3476.557129\n",
      "Training Batch: 18725 Loss: 3527.233154\n",
      "Training Batch: 18726 Loss: 3355.080566\n",
      "Training Batch: 18727 Loss: 3402.526367\n",
      "Training Batch: 18728 Loss: 3668.279297\n",
      "Training Batch: 18729 Loss: 3349.025879\n",
      "Training Batch: 18730 Loss: 3466.206543\n",
      "Training Batch: 18731 Loss: 3355.134033\n",
      "Training Batch: 18732 Loss: 3333.885254\n",
      "Training Batch: 18733 Loss: 3609.321289\n",
      "Training Batch: 18734 Loss: 3452.875000\n",
      "Training Batch: 18735 Loss: 3553.471436\n",
      "Training Batch: 18736 Loss: 3529.015625\n",
      "Training Batch: 18737 Loss: 3382.417969\n",
      "Training Batch: 18738 Loss: 3335.571777\n",
      "Training Batch: 18739 Loss: 3389.645020\n",
      "Training Batch: 18740 Loss: 3385.543701\n",
      "Training Batch: 18741 Loss: 3343.228516\n",
      "Training Batch: 18742 Loss: 3330.721191\n",
      "Training Batch: 18743 Loss: 3286.120117\n",
      "Training Batch: 18744 Loss: 3529.805176\n",
      "Training Batch: 18745 Loss: 3389.346191\n",
      "Training Batch: 18746 Loss: 3462.847168\n",
      "Training Batch: 18747 Loss: 3479.453857\n",
      "Training Batch: 18748 Loss: 3374.038330\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Batch: 18749 Loss: 3343.279785\n",
      "Training Batch: 18750 Loss: 3352.493164\n",
      "Training Batch: 18751 Loss: 3379.269043\n",
      "Training Batch: 18752 Loss: 3355.571289\n",
      "Training Batch: 18753 Loss: 3573.096191\n",
      "Training Batch: 18754 Loss: 3421.288086\n",
      "Training Batch: 18755 Loss: 3260.030762\n",
      "Training Batch: 18756 Loss: 3375.811523\n",
      "Training Batch: 18757 Loss: 3344.964844\n",
      "Training Batch: 18758 Loss: 3356.062012\n",
      "Training Batch: 18759 Loss: 3475.480957\n",
      "Training Batch: 18760 Loss: 3537.416748\n",
      "Training Batch: 18761 Loss: 3542.442871\n",
      "Training Batch: 18762 Loss: 3893.900391\n",
      "Training Batch: 18763 Loss: 3451.153320\n",
      "Training Batch: 18764 Loss: 3401.498291\n",
      "Training Batch: 18765 Loss: 3400.570801\n",
      "Training Batch: 18766 Loss: 3524.762207\n",
      "Training Batch: 18767 Loss: 3354.704346\n",
      "Training Batch: 18768 Loss: 3574.489014\n",
      "Training Batch: 18769 Loss: 3451.462891\n",
      "Training Batch: 18770 Loss: 3409.044922\n",
      "Training Batch: 18771 Loss: 3391.444824\n",
      "Training Batch: 18772 Loss: 3336.380371\n",
      "Training Batch: 18773 Loss: 3428.579346\n",
      "Training Batch: 18774 Loss: 3457.209473\n",
      "Training Batch: 18775 Loss: 3343.326172\n",
      "Training Batch: 18776 Loss: 3321.564453\n",
      "Training Batch: 18777 Loss: 3515.863770\n",
      "Training Batch: 18778 Loss: 3722.185059\n",
      "Training Batch: 18779 Loss: 3359.548340\n",
      "Training Batch: 18780 Loss: 3444.553223\n",
      "Training Batch: 18781 Loss: 3382.914307\n",
      "Training Batch: 18782 Loss: 3294.721191\n",
      "Training Batch: 18783 Loss: 3390.331055\n",
      "Training Batch: 18784 Loss: 3344.696289\n",
      "Training Batch: 18785 Loss: 3378.733154\n",
      "Training Batch: 18786 Loss: 3293.712402\n",
      "Training Batch: 18787 Loss: 3368.581543\n",
      "Training Batch: 18788 Loss: 3452.063232\n",
      "Training Batch: 18789 Loss: 3311.025879\n",
      "Training Batch: 18790 Loss: 3383.308105\n",
      "Training Batch: 18791 Loss: 3308.053467\n",
      "Training Batch: 18792 Loss: 3397.761719\n",
      "Training Batch: 18793 Loss: 3357.687012\n",
      "Training Batch: 18794 Loss: 3332.246094\n",
      "Training Batch: 18795 Loss: 3343.324219\n",
      "Training Batch: 18796 Loss: 3330.741211\n",
      "Training Batch: 18797 Loss: 3306.622559\n",
      "Training Batch: 18798 Loss: 3420.561768\n",
      "Training Batch: 18799 Loss: 3273.690918\n",
      "Training Batch: 18800 Loss: 3394.687500\n",
      "Training Batch: 18801 Loss: 3272.324707\n",
      "Training Batch: 18802 Loss: 3283.592041\n",
      "Training Batch: 18803 Loss: 3344.814697\n",
      "Training Batch: 18804 Loss: 3770.281250\n",
      "Training Batch: 18805 Loss: 3414.228027\n",
      "Training Batch: 18806 Loss: 3276.015625\n",
      "Training Batch: 18807 Loss: 3346.895020\n",
      "Training Batch: 18808 Loss: 3351.559082\n",
      "Training Batch: 18809 Loss: 3322.577637\n",
      "Training Batch: 18810 Loss: 3313.027832\n",
      "Training Batch: 18811 Loss: 3365.752441\n",
      "Training Batch: 18812 Loss: 3287.807617\n",
      "Training Batch: 18813 Loss: 3385.135742\n",
      "Training Batch: 18814 Loss: 3329.833496\n",
      "Training Batch: 18815 Loss: 3376.922852\n",
      "Training Batch: 18816 Loss: 3418.818848\n",
      "Training Batch: 18817 Loss: 3377.952881\n",
      "Training Batch: 18818 Loss: 3335.143066\n",
      "Training Batch: 18819 Loss: 3388.127441\n",
      "Training Batch: 18820 Loss: 3444.582520\n",
      "Training Batch: 18821 Loss: 3292.048340\n",
      "Training Batch: 18822 Loss: 3411.728516\n",
      "Training Batch: 18823 Loss: 3443.355469\n",
      "Training Batch: 18824 Loss: 3324.784180\n",
      "Training Batch: 18825 Loss: 3338.415039\n",
      "Training Batch: 18826 Loss: 3397.826660\n",
      "Training Batch: 18827 Loss: 3510.390625\n",
      "Training Batch: 18828 Loss: 3430.529785\n",
      "Training Batch: 18829 Loss: 3482.687256\n",
      "Training Batch: 18830 Loss: 3499.097656\n",
      "Training Batch: 18831 Loss: 3374.862793\n",
      "Training Batch: 18832 Loss: 3335.901367\n",
      "Training Batch: 18833 Loss: 3449.760254\n",
      "Training Batch: 18834 Loss: 3400.626221\n",
      "Training Batch: 18835 Loss: 3539.534912\n",
      "Training Batch: 18836 Loss: 3325.548584\n",
      "Training Batch: 18837 Loss: 3432.189453\n",
      "Training Batch: 18838 Loss: 3381.287598\n",
      "Training Batch: 18839 Loss: 3434.432129\n",
      "Training Batch: 18840 Loss: 3453.267090\n",
      "Training Batch: 18841 Loss: 3364.293457\n",
      "Training Batch: 18842 Loss: 3397.944824\n",
      "Training Batch: 18843 Loss: 3398.847900\n",
      "Training Batch: 18844 Loss: 3432.688965\n",
      "Training Batch: 18845 Loss: 3510.756836\n",
      "Training Batch: 18846 Loss: 3340.548584\n",
      "Training Batch: 18847 Loss: 3415.841797\n",
      "Training Batch: 18848 Loss: 3554.767578\n",
      "Training Batch: 18849 Loss: 3463.174072\n",
      "Training Batch: 18850 Loss: 3368.136230\n",
      "Training Batch: 18851 Loss: 3479.263672\n",
      "Training Batch: 18852 Loss: 3274.657227\n",
      "Training Batch: 18853 Loss: 3403.688965\n",
      "Training Batch: 18854 Loss: 3451.258545\n",
      "Training Batch: 18855 Loss: 3584.352539\n",
      "Training Batch: 18856 Loss: 3440.210449\n",
      "Training Batch: 18857 Loss: 3452.675293\n",
      "Training Batch: 18858 Loss: 3416.764160\n",
      "Training Batch: 18859 Loss: 3480.152344\n",
      "Training Batch: 18860 Loss: 3246.685547\n",
      "Training Batch: 18861 Loss: 3377.889648\n",
      "Training Batch: 18862 Loss: 3401.676025\n",
      "Training Batch: 18863 Loss: 3472.418945\n",
      "Training Batch: 18864 Loss: 3339.366211\n",
      "Training Batch: 18865 Loss: 3384.526855\n",
      "Training Batch: 18866 Loss: 3405.272217\n",
      "Training Batch: 18867 Loss: 3644.807617\n",
      "Training Batch: 18868 Loss: 3244.121582\n",
      "Training Batch: 18869 Loss: 3395.222168\n",
      "Training Batch: 18870 Loss: 3401.693848\n",
      "Training Batch: 18871 Loss: 3371.715820\n",
      "Training Batch: 18872 Loss: 3388.376709\n",
      "Training Batch: 18873 Loss: 3369.108398\n",
      "Training Batch: 18874 Loss: 3627.026855\n",
      "Training Batch: 18875 Loss: 3546.468750\n",
      "Training Batch: 18876 Loss: 3368.753906\n",
      "Training Batch: 18877 Loss: 3492.187012\n",
      "Training Batch: 18878 Loss: 3358.444824\n",
      "Training Batch: 18879 Loss: 3395.849609\n",
      "Training Batch: 18880 Loss: 3403.937012\n",
      "Training Batch: 18881 Loss: 3488.581055\n",
      "Training Batch: 18882 Loss: 3545.360840\n",
      "Training Batch: 18883 Loss: 3317.709473\n",
      "Training Batch: 18884 Loss: 3422.277344\n",
      "Training Batch: 18885 Loss: 3389.841553\n",
      "Training Batch: 18886 Loss: 3457.946289\n",
      "Training Batch: 18887 Loss: 3422.116699\n",
      "Training Batch: 18888 Loss: 3365.264893\n",
      "Training Batch: 18889 Loss: 3414.098633\n",
      "Training Batch: 18890 Loss: 3362.897461\n",
      "Training Batch: 18891 Loss: 3428.270996\n",
      "Training Batch: 18892 Loss: 3403.580566\n",
      "Training Batch: 18893 Loss: 3241.661621\n",
      "Training Batch: 18894 Loss: 3528.243164\n",
      "Training Batch: 18895 Loss: 3316.833984\n",
      "Training Batch: 18896 Loss: 3360.762451\n",
      "Training Batch: 18897 Loss: 3305.708496\n",
      "Training Batch: 18898 Loss: 3412.599609\n",
      "Training Batch: 18899 Loss: 3351.124512\n",
      "Training Batch: 18900 Loss: 3338.171875\n",
      "Training Batch: 18901 Loss: 3275.686523\n",
      "Training Batch: 18902 Loss: 3386.388672\n",
      "Training Batch: 18903 Loss: 3430.386230\n",
      "Training Batch: 18904 Loss: 3412.606934\n",
      "Training Batch: 18905 Loss: 3393.092773\n",
      "Training Batch: 18906 Loss: 3406.071045\n",
      "Training Batch: 18907 Loss: 3414.662109\n",
      "Training Batch: 18908 Loss: 3569.511230\n",
      "Training Batch: 18909 Loss: 3386.650879\n",
      "Training Batch: 18910 Loss: 3412.763672\n",
      "Training Batch: 18911 Loss: 3361.731445\n",
      "Training Batch: 18912 Loss: 3441.563965\n",
      "Training Batch: 18913 Loss: 3499.954102\n",
      "Training Batch: 18914 Loss: 3390.028809\n",
      "Training Batch: 18915 Loss: 3542.688965\n",
      "Training Batch: 18916 Loss: 3335.016113\n",
      "Training Batch: 18917 Loss: 3395.940918\n",
      "Training Batch: 18918 Loss: 3432.568359\n",
      "Training Batch: 18919 Loss: 3455.839355\n",
      "Training Batch: 18920 Loss: 3349.856445\n",
      "Training Batch: 18921 Loss: 3387.387207\n",
      "Training Batch: 18922 Loss: 3526.691895\n",
      "Training Batch: 18923 Loss: 3443.769531\n",
      "Training Batch: 18924 Loss: 3614.244629\n",
      "Training Batch: 18925 Loss: 3482.070801\n",
      "Training Batch: 18926 Loss: 3416.312988\n",
      "Training Batch: 18927 Loss: 3262.975098\n",
      "Training Batch: 18928 Loss: 3324.664307\n",
      "Training Batch: 18929 Loss: 3370.992676\n",
      "Training Batch: 18930 Loss: 3252.388184\n",
      "Training Batch: 18931 Loss: 3254.125977\n",
      "Training Batch: 18932 Loss: 3382.092041\n",
      "Training Batch: 18933 Loss: 3456.045410\n",
      "Training Batch: 18934 Loss: 3518.740479\n",
      "Training Batch: 18935 Loss: 3461.752686\n",
      "Training Batch: 18936 Loss: 3453.105957\n",
      "Training Batch: 18937 Loss: 3360.477051\n",
      "Training Batch: 18938 Loss: 3302.794678\n",
      "Training Batch: 18939 Loss: 3467.399902\n",
      "Training Batch: 18940 Loss: 3357.734375\n",
      "Training Batch: 18941 Loss: 3416.229248\n",
      "Training Batch: 18942 Loss: 3409.716309\n",
      "Training Batch: 18943 Loss: 3449.625488\n",
      "Training Batch: 18944 Loss: 3562.944580\n",
      "Training Batch: 18945 Loss: 3490.285645\n",
      "Training Batch: 18946 Loss: 3364.490967\n",
      "Training Batch: 18947 Loss: 3554.114746\n",
      "Training Batch: 18948 Loss: 3341.546387\n",
      "Training Batch: 18949 Loss: 3348.915527\n",
      "Training Batch: 18950 Loss: 3450.683105\n",
      "Training Batch: 18951 Loss: 3361.047852\n",
      "Training Batch: 18952 Loss: 3367.403320\n",
      "Training Batch: 18953 Loss: 3398.605957\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Batch: 18954 Loss: 3388.860352\n",
      "Training Batch: 18955 Loss: 3774.813965\n",
      "Training Batch: 18956 Loss: 3317.708008\n",
      "Training Batch: 18957 Loss: 3390.578613\n",
      "Training Batch: 18958 Loss: 3384.976562\n",
      "Training Batch: 18959 Loss: 3254.269043\n",
      "Training Batch: 18960 Loss: 3291.352539\n",
      "Training Batch: 18961 Loss: 3286.081055\n",
      "Training Batch: 18962 Loss: 3316.701904\n",
      "Training Batch: 18963 Loss: 3335.165039\n",
      "Training Batch: 18964 Loss: 3538.611816\n",
      "Training Batch: 18965 Loss: 3288.121582\n",
      "Training Batch: 18966 Loss: 3359.837891\n",
      "Training Batch: 18967 Loss: 3440.075195\n",
      "Training Batch: 18968 Loss: 3390.106934\n",
      "Training Batch: 18969 Loss: 3340.830078\n",
      "Training Batch: 18970 Loss: 3270.495117\n",
      "Training Batch: 18971 Loss: 3458.290039\n",
      "Training Batch: 18972 Loss: 3517.139648\n",
      "Training Batch: 18973 Loss: 3372.663330\n",
      "Training Batch: 18974 Loss: 3582.724121\n",
      "Training Batch: 18975 Loss: 3450.708740\n",
      "Training Batch: 18976 Loss: 3440.193604\n",
      "Training Batch: 18977 Loss: 3381.359619\n",
      "Training Batch: 18978 Loss: 3955.600586\n",
      "Training Batch: 18979 Loss: 3956.335938\n",
      "Training Batch: 18980 Loss: 3626.745117\n",
      "Training Batch: 18981 Loss: 3428.191895\n",
      "Training Batch: 18982 Loss: 3584.332520\n",
      "Training Batch: 18983 Loss: 3364.509766\n",
      "Training Batch: 18984 Loss: 3606.703125\n",
      "Training Batch: 18985 Loss: 3510.979492\n",
      "Training Batch: 18986 Loss: 3498.586670\n",
      "Training Batch: 18987 Loss: 3569.339355\n",
      "Training Batch: 18988 Loss: 3514.039551\n",
      "Training Batch: 18989 Loss: 3361.855469\n",
      "Training Batch: 18990 Loss: 3373.889160\n",
      "Training Batch: 18991 Loss: 3243.946533\n",
      "Training Batch: 18992 Loss: 3312.578613\n",
      "Training Batch: 18993 Loss: 3394.631836\n",
      "Training Batch: 18994 Loss: 3430.137207\n",
      "Training Batch: 18995 Loss: 3377.644775\n",
      "Training Batch: 18996 Loss: 3798.480469\n",
      "Training Batch: 18997 Loss: 3427.175781\n",
      "Training Batch: 18998 Loss: 3463.296387\n",
      "Training Batch: 18999 Loss: 3598.122559\n",
      "Training Batch: 19000 Loss: 3369.098145\n",
      "Training Batch: 19001 Loss: 3340.077148\n",
      "Training Batch: 19002 Loss: 3337.172607\n",
      "Training Batch: 19003 Loss: 3403.746094\n",
      "Training Batch: 19004 Loss: 3457.400879\n",
      "Training Batch: 19005 Loss: 3429.284668\n",
      "Training Batch: 19006 Loss: 3437.603516\n",
      "Training Batch: 19007 Loss: 3355.242920\n",
      "Training Batch: 19008 Loss: 3347.833984\n",
      "Training Batch: 19009 Loss: 3432.894043\n",
      "Training Batch: 19010 Loss: 3775.141602\n",
      "Training Batch: 19011 Loss: 3556.318848\n",
      "Training Batch: 19012 Loss: 3373.471680\n",
      "Training Batch: 19013 Loss: 3583.830078\n",
      "Training Batch: 19014 Loss: 3568.534180\n",
      "Training Batch: 19015 Loss: 3380.948730\n",
      "Training Batch: 19016 Loss: 3434.021484\n",
      "Training Batch: 19017 Loss: 3381.839355\n",
      "Training Batch: 19018 Loss: 3546.526855\n",
      "Training Batch: 19019 Loss: 3723.491455\n",
      "Training Batch: 19020 Loss: 3507.400391\n",
      "Training Batch: 19021 Loss: 3489.259277\n",
      "Training Batch: 19022 Loss: 3499.000000\n",
      "Training Batch: 19023 Loss: 3253.248779\n",
      "Training Batch: 19024 Loss: 3516.775146\n",
      "Training Batch: 19025 Loss: 3294.104004\n",
      "Training Batch: 19026 Loss: 3475.533691\n",
      "Training Batch: 19027 Loss: 3220.548828\n",
      "Training Batch: 19028 Loss: 3439.689453\n",
      "Training Batch: 19029 Loss: 3378.876465\n",
      "Training Batch: 19030 Loss: 3527.064453\n",
      "Training Batch: 19031 Loss: 3400.442383\n",
      "Training Batch: 19032 Loss: 3261.682617\n",
      "Training Batch: 19033 Loss: 3491.513672\n",
      "Training Batch: 19034 Loss: 3381.496826\n",
      "Training Batch: 19035 Loss: 3547.167480\n",
      "Training Batch: 19036 Loss: 3450.381348\n",
      "Training Batch: 19037 Loss: 3293.473145\n",
      "Training Batch: 19038 Loss: 3447.108154\n",
      "Training Batch: 19039 Loss: 3244.942383\n",
      "Training Batch: 19040 Loss: 3422.612793\n",
      "Training Batch: 19041 Loss: 3495.822266\n",
      "Training Batch: 19042 Loss: 3328.032715\n",
      "Training Batch: 19043 Loss: 4045.642822\n",
      "Training Batch: 19044 Loss: 4009.004883\n",
      "Training Batch: 19045 Loss: 3926.381836\n",
      "Training Batch: 19046 Loss: 3713.330566\n",
      "Training Batch: 19047 Loss: 3498.200684\n",
      "Training Batch: 19048 Loss: 3810.229980\n",
      "Training Batch: 19049 Loss: 3647.275391\n",
      "Training Batch: 19050 Loss: 3556.462402\n",
      "Training Batch: 19051 Loss: 3465.341797\n",
      "Training Batch: 19052 Loss: 3816.307861\n",
      "Training Batch: 19053 Loss: 3649.275635\n",
      "Training Batch: 19054 Loss: 3752.458496\n",
      "Training Batch: 19055 Loss: 3397.377197\n",
      "Training Batch: 19056 Loss: 3401.818359\n",
      "Training Batch: 19057 Loss: 3464.093750\n",
      "Training Batch: 19058 Loss: 3474.588867\n",
      "Training Batch: 19059 Loss: 3407.468750\n",
      "Training Batch: 19060 Loss: 3388.045410\n",
      "Training Batch: 19061 Loss: 3322.690674\n",
      "Training Batch: 19062 Loss: 3402.229004\n",
      "Training Batch: 19063 Loss: 3486.883545\n",
      "Training Batch: 19064 Loss: 3331.027832\n",
      "Training Batch: 19065 Loss: 3399.136719\n",
      "Training Batch: 19066 Loss: 3439.068848\n",
      "Training Batch: 19067 Loss: 3472.878906\n",
      "Training Batch: 19068 Loss: 3374.077393\n",
      "Training Batch: 19069 Loss: 3347.601562\n",
      "Training Batch: 19070 Loss: 3326.863281\n",
      "Training Batch: 19071 Loss: 3415.508301\n",
      "Training Batch: 19072 Loss: 3396.912109\n",
      "Training Batch: 19073 Loss: 3361.053223\n",
      "Training Batch: 19074 Loss: 3305.436035\n",
      "Training Batch: 19075 Loss: 3341.483887\n",
      "Training Batch: 19076 Loss: 3366.494141\n",
      "Training Batch: 19077 Loss: 3321.745605\n",
      "Training Batch: 19078 Loss: 3363.629883\n",
      "Training Batch: 19079 Loss: 3314.738770\n",
      "Training Batch: 19080 Loss: 3294.187988\n",
      "Training Batch: 19081 Loss: 3483.611328\n",
      "Training Batch: 19082 Loss: 3322.676758\n",
      "Training Batch: 19083 Loss: 3299.148438\n",
      "Training Batch: 19084 Loss: 3322.949219\n",
      "Training Batch: 19085 Loss: 3322.187500\n",
      "Training Batch: 19086 Loss: 3446.483398\n",
      "Training Batch: 19087 Loss: 3387.544434\n",
      "Training Batch: 19088 Loss: 3471.261719\n",
      "Training Batch: 19089 Loss: 3483.889893\n",
      "Training Batch: 19090 Loss: 3482.294434\n",
      "Training Batch: 19091 Loss: 3381.877930\n",
      "Training Batch: 19092 Loss: 3649.596191\n",
      "Training Batch: 19093 Loss: 3590.358398\n",
      "Training Batch: 19094 Loss: 3495.041748\n",
      "Training Batch: 19095 Loss: 3524.912598\n",
      "Training Batch: 19096 Loss: 3441.148926\n",
      "Training Batch: 19097 Loss: 3432.972168\n",
      "Training Batch: 19098 Loss: 3307.648926\n",
      "Training Batch: 19099 Loss: 3434.811523\n",
      "Training Batch: 19100 Loss: 3401.335693\n",
      "Training Batch: 19101 Loss: 3360.384033\n",
      "Training Batch: 19102 Loss: 3362.318115\n",
      "Training Batch: 19103 Loss: 3345.597412\n",
      "Training Batch: 19104 Loss: 3252.670410\n",
      "Training Batch: 19105 Loss: 3383.875977\n",
      "Training Batch: 19106 Loss: 3701.624023\n",
      "Training Batch: 19107 Loss: 3424.243408\n",
      "Training Batch: 19108 Loss: 3455.367920\n",
      "Training Batch: 19109 Loss: 3373.471191\n",
      "Training Batch: 19110 Loss: 3303.708496\n",
      "Training Batch: 19111 Loss: 3465.966309\n",
      "Training Batch: 19112 Loss: 3378.197266\n",
      "Training Batch: 19113 Loss: 3396.232422\n",
      "Training Batch: 19114 Loss: 3467.759277\n",
      "Training Batch: 19115 Loss: 3450.364502\n",
      "Training Batch: 19116 Loss: 3921.760254\n",
      "Training Batch: 19117 Loss: 3713.529541\n",
      "Training Batch: 19118 Loss: 3586.519287\n",
      "Training Batch: 19119 Loss: 3698.929443\n",
      "Training Batch: 19120 Loss: 3408.805176\n",
      "Training Batch: 19121 Loss: 3374.857666\n",
      "Training Batch: 19122 Loss: 3397.144043\n",
      "Training Batch: 19123 Loss: 3429.861572\n",
      "Training Batch: 19124 Loss: 3473.607910\n",
      "Training Batch: 19125 Loss: 3343.206543\n",
      "Training Batch: 19126 Loss: 3371.779297\n",
      "Training Batch: 19127 Loss: 3452.698242\n",
      "Training Batch: 19128 Loss: 3362.085693\n",
      "Training Batch: 19129 Loss: 3376.264404\n",
      "Training Batch: 19130 Loss: 3345.340332\n",
      "Training Batch: 19131 Loss: 3271.921875\n",
      "Training Batch: 19132 Loss: 3343.541992\n",
      "Training Batch: 19133 Loss: 3390.357910\n",
      "Training Batch: 19134 Loss: 3450.742920\n",
      "Training Batch: 19135 Loss: 3368.930176\n",
      "Training Batch: 19136 Loss: 3447.145752\n",
      "Training Batch: 19137 Loss: 3561.932861\n",
      "Training Batch: 19138 Loss: 3508.352051\n",
      "Training Batch: 19139 Loss: 3351.101562\n",
      "Training Batch: 19140 Loss: 3503.307129\n",
      "Training Batch: 19141 Loss: 3442.801270\n",
      "Training Batch: 19142 Loss: 3339.672363\n",
      "Training Batch: 19143 Loss: 3388.657227\n",
      "Training Batch: 19144 Loss: 3390.135498\n",
      "Training Batch: 19145 Loss: 3677.211670\n",
      "Training Batch: 19146 Loss: 3366.291504\n",
      "Training Batch: 19147 Loss: 3384.188965\n",
      "Training Batch: 19148 Loss: 3345.810059\n",
      "Training Batch: 19149 Loss: 3463.674805\n",
      "Training Batch: 19150 Loss: 3448.658691\n",
      "Training Batch: 19151 Loss: 3539.008301\n",
      "Training Batch: 19152 Loss: 3443.123535\n",
      "Training Batch: 19153 Loss: 3690.268555\n",
      "Training Batch: 19154 Loss: 3447.539062\n",
      "Training Batch: 19155 Loss: 3295.259277\n",
      "Training Batch: 19156 Loss: 3414.488281\n",
      "Training Batch: 19157 Loss: 3455.139648\n",
      "Training Batch: 19158 Loss: 3417.956787\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Batch: 19159 Loss: 3251.393066\n",
      "Training Batch: 19160 Loss: 3321.592285\n",
      "Training Batch: 19161 Loss: 3374.162598\n",
      "Training Batch: 19162 Loss: 3306.113525\n",
      "Training Batch: 19163 Loss: 3463.431641\n",
      "Training Batch: 19164 Loss: 3354.094482\n",
      "Training Batch: 19165 Loss: 3444.858887\n",
      "Training Batch: 19166 Loss: 3364.708496\n",
      "Training Batch: 19167 Loss: 3487.443115\n",
      "Training Batch: 19168 Loss: 3541.551270\n",
      "Training Batch: 19169 Loss: 3736.539551\n",
      "Training Batch: 19170 Loss: 3686.368164\n",
      "Training Batch: 19171 Loss: 3532.217773\n",
      "Training Batch: 19172 Loss: 3703.429443\n",
      "Training Batch: 19173 Loss: 3790.901611\n",
      "Training Batch: 19174 Loss: 3735.929932\n",
      "Training Batch: 19175 Loss: 3557.190430\n",
      "Training Batch: 19176 Loss: 3905.079834\n",
      "Training Batch: 19177 Loss: 3889.628662\n",
      "Training Batch: 19178 Loss: 3460.070312\n",
      "Training Batch: 19179 Loss: 3402.569336\n",
      "Training Batch: 19180 Loss: 3380.598633\n",
      "Training Batch: 19181 Loss: 3352.420410\n",
      "Training Batch: 19182 Loss: 3627.947754\n",
      "Training Batch: 19183 Loss: 3330.830566\n",
      "Training Batch: 19184 Loss: 3467.955566\n",
      "Training Batch: 19185 Loss: 3417.958252\n",
      "Training Batch: 19186 Loss: 3292.375977\n",
      "Training Batch: 19187 Loss: 3397.497070\n",
      "Training Batch: 19188 Loss: 3312.435303\n",
      "Training Batch: 19189 Loss: 3385.750977\n",
      "Training Batch: 19190 Loss: 3323.436035\n",
      "Training Batch: 19191 Loss: 3239.945801\n",
      "Training Batch: 19192 Loss: 3590.791504\n",
      "Training Batch: 19193 Loss: 3344.192383\n",
      "Training Batch: 19194 Loss: 3454.121094\n",
      "Training Batch: 19195 Loss: 3353.287109\n",
      "Training Batch: 19196 Loss: 3323.202637\n",
      "Training Batch: 19197 Loss: 3350.701660\n",
      "Training Batch: 19198 Loss: 3371.784180\n",
      "Training Batch: 19199 Loss: 3546.140625\n",
      "Training Batch: 19200 Loss: 3339.725830\n",
      "Training Batch: 19201 Loss: 3439.015869\n",
      "Training Batch: 19202 Loss: 3500.354248\n",
      "Training Batch: 19203 Loss: 3462.268555\n",
      "Training Batch: 19204 Loss: 3307.838867\n",
      "Training Batch: 19205 Loss: 3401.172852\n",
      "Training Batch: 19206 Loss: 3435.715820\n",
      "Training Batch: 19207 Loss: 3637.926270\n",
      "Training Batch: 19208 Loss: 3516.886719\n",
      "Training Batch: 19209 Loss: 3416.220215\n",
      "Training Batch: 19210 Loss: 3462.693848\n",
      "Training Batch: 19211 Loss: 3367.491455\n",
      "Training Batch: 19212 Loss: 3329.152588\n",
      "Training Batch: 19213 Loss: 3366.516602\n",
      "Training Batch: 19214 Loss: 3266.440430\n",
      "Training Batch: 19215 Loss: 3324.561035\n",
      "Training Batch: 19216 Loss: 3270.420410\n",
      "Training Batch: 19217 Loss: 3337.876709\n",
      "Training Batch: 19218 Loss: 3446.839844\n",
      "Training Batch: 19219 Loss: 3314.273438\n",
      "Training Batch: 19220 Loss: 3358.708008\n",
      "Training Batch: 19221 Loss: 3345.145996\n",
      "Training Batch: 19222 Loss: 3278.941406\n",
      "Training Batch: 19223 Loss: 3358.225586\n",
      "Training Batch: 19224 Loss: 3386.850342\n",
      "Training Batch: 19225 Loss: 3373.151855\n",
      "Training Batch: 19226 Loss: 3389.153809\n",
      "Training Batch: 19227 Loss: 3249.866699\n",
      "Training Batch: 19228 Loss: 3394.383301\n",
      "Training Batch: 19229 Loss: 3282.786621\n",
      "Training Batch: 19230 Loss: 3472.880859\n",
      "Training Batch: 19231 Loss: 3341.482910\n",
      "Training Batch: 19232 Loss: 3586.794922\n",
      "Training Batch: 19233 Loss: 3486.809570\n",
      "Training Batch: 19234 Loss: 3535.493408\n",
      "Training Batch: 19235 Loss: 3407.856689\n",
      "Training Batch: 19236 Loss: 3417.970703\n",
      "Training Batch: 19237 Loss: 3334.311768\n",
      "Training Batch: 19238 Loss: 3266.628662\n",
      "Training Batch: 19239 Loss: 3349.314209\n",
      "Training Batch: 19240 Loss: 3512.616211\n",
      "Training Batch: 19241 Loss: 3455.838623\n",
      "Training Batch: 19242 Loss: 3471.270996\n",
      "Training Batch: 19243 Loss: 3423.490479\n",
      "Training Batch: 19244 Loss: 3407.792480\n",
      "Training Batch: 19245 Loss: 3400.672363\n",
      "Training Batch: 19246 Loss: 3243.136719\n",
      "Training Batch: 19247 Loss: 3384.133301\n",
      "Training Batch: 19248 Loss: 3309.371338\n",
      "Training Batch: 19249 Loss: 3505.112061\n",
      "Training Batch: 19250 Loss: 3413.463623\n",
      "Training Batch: 19251 Loss: 3384.994385\n",
      "Training Batch: 19252 Loss: 3362.745605\n",
      "Training Batch: 19253 Loss: 3296.513916\n",
      "Training Batch: 19254 Loss: 3395.106934\n",
      "Training Batch: 19255 Loss: 3407.252930\n",
      "Training Batch: 19256 Loss: 3555.608887\n",
      "Training Batch: 19257 Loss: 3512.918701\n",
      "Training Batch: 19258 Loss: 3330.030762\n",
      "Training Batch: 19259 Loss: 3358.450684\n",
      "Training Batch: 19260 Loss: 3384.877441\n",
      "Training Batch: 19261 Loss: 3384.643066\n",
      "Training Batch: 19262 Loss: 3355.805664\n",
      "Training Batch: 19263 Loss: 3558.170654\n",
      "Training Batch: 19264 Loss: 3288.166992\n",
      "Training Batch: 19265 Loss: 3340.366943\n",
      "Training Batch: 19266 Loss: 3484.848633\n",
      "Training Batch: 19267 Loss: 3656.534180\n",
      "Training Batch: 19268 Loss: 3555.528076\n",
      "Training Batch: 19269 Loss: 3625.406738\n",
      "Training Batch: 19270 Loss: 3581.504395\n",
      "Training Batch: 19271 Loss: 3446.171875\n",
      "Training Batch: 19272 Loss: 3544.544434\n",
      "Training Batch: 19273 Loss: 3480.496094\n",
      "Training Batch: 19274 Loss: 3573.355957\n",
      "Training Batch: 19275 Loss: 3595.276367\n",
      "Training Batch: 19276 Loss: 3582.669922\n",
      "Training Batch: 19277 Loss: 3525.520020\n",
      "Training Batch: 19278 Loss: 3649.569336\n",
      "Training Batch: 19279 Loss: 3374.162842\n",
      "Training Batch: 19280 Loss: 3463.222168\n",
      "Training Batch: 19281 Loss: 3550.831543\n",
      "Training Batch: 19282 Loss: 3441.433105\n",
      "Training Batch: 19283 Loss: 3380.800293\n",
      "Training Batch: 19284 Loss: 3456.878418\n",
      "Training Batch: 19285 Loss: 3298.260254\n",
      "Training Batch: 19286 Loss: 3541.912354\n",
      "Training Batch: 19287 Loss: 3622.795654\n",
      "Training Batch: 19288 Loss: 3529.883301\n",
      "Training Batch: 19289 Loss: 3395.681641\n",
      "Training Batch: 19290 Loss: 3430.857910\n",
      "Training Batch: 19291 Loss: 3605.599121\n",
      "Training Batch: 19292 Loss: 3431.885742\n",
      "Training Batch: 19293 Loss: 3474.825195\n",
      "Training Batch: 19294 Loss: 3638.173584\n",
      "Training Batch: 19295 Loss: 3574.271973\n",
      "Training Batch: 19296 Loss: 3621.846924\n",
      "Training Batch: 19297 Loss: 4196.649902\n",
      "Training Batch: 19298 Loss: 3549.417236\n",
      "Training Batch: 19299 Loss: 3508.742188\n",
      "Training Batch: 19300 Loss: 3447.061523\n",
      "Training Batch: 19301 Loss: 3412.353027\n",
      "Training Batch: 19302 Loss: 3436.777100\n",
      "Training Batch: 19303 Loss: 3375.246582\n",
      "Training Batch: 19304 Loss: 3365.454346\n",
      "Training Batch: 19305 Loss: 3402.697021\n",
      "Training Batch: 19306 Loss: 3467.030762\n",
      "Training Batch: 19307 Loss: 3313.427246\n",
      "Training Batch: 19308 Loss: 3436.408691\n",
      "Training Batch: 19309 Loss: 3387.812012\n",
      "Training Batch: 19310 Loss: 3378.399902\n",
      "Training Batch: 19311 Loss: 3368.047852\n",
      "Training Batch: 19312 Loss: 3475.913086\n",
      "Training Batch: 19313 Loss: 3392.814941\n",
      "Training Batch: 19314 Loss: 3426.654297\n",
      "Training Batch: 19315 Loss: 3429.588867\n",
      "Training Batch: 19316 Loss: 3304.319092\n",
      "Training Batch: 19317 Loss: 3418.807129\n",
      "Training Batch: 19318 Loss: 3391.521973\n",
      "Training Batch: 19319 Loss: 3827.751221\n",
      "Training Batch: 19320 Loss: 3351.892090\n",
      "Training Batch: 19321 Loss: 3418.590820\n",
      "Training Batch: 19322 Loss: 3459.306152\n",
      "Training Batch: 19323 Loss: 3343.438232\n",
      "Training Batch: 19324 Loss: 3514.350098\n",
      "Training Batch: 19325 Loss: 3432.235352\n",
      "Training Batch: 19326 Loss: 3348.182617\n",
      "Training Batch: 19327 Loss: 3308.894531\n",
      "Training Batch: 19328 Loss: 3357.547363\n",
      "Training Batch: 19329 Loss: 3436.940430\n",
      "Training Batch: 19330 Loss: 3585.450439\n",
      "Training Batch: 19331 Loss: 3452.512695\n",
      "Training Batch: 19332 Loss: 3370.059814\n",
      "Training Batch: 19333 Loss: 3483.482422\n",
      "Training Batch: 19334 Loss: 3322.299805\n",
      "Training Batch: 19335 Loss: 3411.480957\n",
      "Training Batch: 19336 Loss: 3234.224121\n",
      "Training Batch: 19337 Loss: 3437.137695\n",
      "Training Batch: 19338 Loss: 3368.848633\n",
      "Training Batch: 19339 Loss: 3436.608398\n",
      "Training Batch: 19340 Loss: 3519.926270\n",
      "Training Batch: 19341 Loss: 3423.168945\n",
      "Training Batch: 19342 Loss: 3415.041748\n",
      "Training Batch: 19343 Loss: 3319.263916\n",
      "Training Batch: 19344 Loss: 3350.019531\n",
      "Training Batch: 19345 Loss: 3374.576172\n",
      "Training Batch: 19346 Loss: 3312.158203\n",
      "Training Batch: 19347 Loss: 3435.536377\n",
      "Training Batch: 19348 Loss: 3369.887695\n",
      "Training Batch: 19349 Loss: 3380.226074\n",
      "Training Batch: 19350 Loss: 3293.905273\n",
      "Training Batch: 19351 Loss: 3390.296875\n",
      "Training Batch: 19352 Loss: 3278.924072\n",
      "Training Batch: 19353 Loss: 3719.840820\n",
      "Training Batch: 19354 Loss: 3389.989990\n",
      "Training Batch: 19355 Loss: 3235.836426\n",
      "Training Batch: 19356 Loss: 3399.226074\n",
      "Training Batch: 19357 Loss: 3453.043457\n",
      "Training Batch: 19358 Loss: 3279.928711\n",
      "Training Batch: 19359 Loss: 3412.365723\n",
      "Training Batch: 19360 Loss: 3407.156738\n",
      "Training Batch: 19361 Loss: 3440.186035\n",
      "Training Batch: 19362 Loss: 3760.813965\n",
      "Training Batch: 19363 Loss: 3361.884277\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Batch: 19364 Loss: 3406.752930\n",
      "Training Batch: 19365 Loss: 3595.035645\n",
      "Training Batch: 19366 Loss: 3429.031738\n",
      "Training Batch: 19367 Loss: 3376.463135\n",
      "Training Batch: 19368 Loss: 3308.095215\n",
      "Training Batch: 19369 Loss: 3271.778320\n",
      "Training Batch: 19370 Loss: 3416.068604\n",
      "Training Batch: 19371 Loss: 3508.192871\n",
      "Training Batch: 19372 Loss: 3413.653809\n",
      "Training Batch: 19373 Loss: 3361.993652\n",
      "Training Batch: 19374 Loss: 3416.108887\n",
      "Training Batch: 19375 Loss: 3418.688721\n",
      "Training Batch: 19376 Loss: 3397.163086\n",
      "Training Batch: 19377 Loss: 3459.799561\n",
      "Training Batch: 19378 Loss: 3264.936523\n",
      "Training Batch: 19379 Loss: 3510.760742\n",
      "Training Batch: 19380 Loss: 3679.646973\n",
      "Training Batch: 19381 Loss: 3343.244629\n",
      "Training Batch: 19382 Loss: 3502.889160\n",
      "Training Batch: 19383 Loss: 3521.268066\n",
      "Training Batch: 19384 Loss: 3388.653320\n",
      "Training Batch: 19385 Loss: 3679.930664\n",
      "Training Batch: 19386 Loss: 3416.034668\n",
      "Training Batch: 19387 Loss: 3336.469971\n",
      "Training Batch: 19388 Loss: 3428.255859\n",
      "Training Batch: 19389 Loss: 3247.846191\n",
      "Training Batch: 19390 Loss: 3339.320068\n",
      "Training Batch: 19391 Loss: 3304.206299\n",
      "Training Batch: 19392 Loss: 3495.392578\n",
      "Training Batch: 19393 Loss: 3401.200195\n",
      "Training Batch: 19394 Loss: 3364.030762\n",
      "Training Batch: 19395 Loss: 3311.035889\n",
      "Training Batch: 19396 Loss: 3297.254883\n",
      "Training Batch: 19397 Loss: 3242.146973\n",
      "Training Batch: 19398 Loss: 3207.228027\n",
      "Training Batch: 19399 Loss: 3278.255859\n",
      "Training Batch: 19400 Loss: 3396.851074\n",
      "Training Batch: 19401 Loss: 3336.201660\n",
      "Training Batch: 19402 Loss: 3402.081543\n",
      "Training Batch: 19403 Loss: 3441.104004\n",
      "Training Batch: 19404 Loss: 3401.184570\n",
      "Training Batch: 19405 Loss: 3385.151855\n",
      "Training Batch: 19406 Loss: 3436.295166\n",
      "Training Batch: 19407 Loss: 3231.884277\n",
      "Training Batch: 19408 Loss: 3461.468750\n",
      "Training Batch: 19409 Loss: 3322.303711\n",
      "Training Batch: 19410 Loss: 3347.904297\n",
      "Training Batch: 19411 Loss: 3316.079102\n",
      "Training Batch: 19412 Loss: 3265.970215\n",
      "Training Batch: 19413 Loss: 3324.834961\n",
      "Training Batch: 19414 Loss: 3257.509277\n",
      "Training Batch: 19415 Loss: 3286.255859\n",
      "Training Batch: 19416 Loss: 3359.518311\n",
      "Training Batch: 19417 Loss: 3291.688965\n",
      "Training Batch: 19418 Loss: 3430.579590\n",
      "Training Batch: 19419 Loss: 3352.819092\n",
      "Training Batch: 19420 Loss: 3346.298584\n",
      "Training Batch: 19421 Loss: 3392.084229\n",
      "Training Batch: 19422 Loss: 3367.340576\n",
      "Training Batch: 19423 Loss: 3333.346191\n",
      "Training Batch: 19424 Loss: 3424.172852\n",
      "Training Batch: 19425 Loss: 3289.208496\n",
      "Training Batch: 19426 Loss: 3353.154297\n",
      "Training Batch: 19427 Loss: 3264.672852\n",
      "Training Batch: 19428 Loss: 3269.197021\n",
      "Training Batch: 19429 Loss: 3291.225098\n",
      "Training Batch: 19430 Loss: 3357.382324\n",
      "Training Batch: 19431 Loss: 3330.319336\n",
      "Training Batch: 19432 Loss: 3350.115967\n",
      "Training Batch: 19433 Loss: 3427.357178\n",
      "Training Batch: 19434 Loss: 3346.431152\n",
      "Training Batch: 19435 Loss: 3257.365234\n",
      "Training Batch: 19436 Loss: 3321.609131\n",
      "Training Batch: 19437 Loss: 3271.813477\n",
      "Training Batch: 19438 Loss: 3286.700684\n",
      "Training Batch: 19439 Loss: 3296.581055\n",
      "Training Batch: 19440 Loss: 3317.081299\n",
      "Training Batch: 19441 Loss: 3333.102051\n",
      "Training Batch: 19442 Loss: 3471.650635\n",
      "Training Batch: 19443 Loss: 3237.931152\n",
      "Training Batch: 19444 Loss: 3302.785156\n",
      "Training Batch: 19445 Loss: 3409.288086\n",
      "Training Batch: 19446 Loss: 3262.801270\n",
      "Training Batch: 19447 Loss: 3222.771973\n",
      "Training Batch: 19448 Loss: 3424.691406\n",
      "Training Batch: 19449 Loss: 3372.895996\n",
      "Training Batch: 19450 Loss: 3252.934570\n",
      "Training Batch: 19451 Loss: 3260.441406\n",
      "Training Batch: 19452 Loss: 3521.928223\n",
      "Training Batch: 19453 Loss: 3378.232910\n",
      "Training Batch: 19454 Loss: 3299.411621\n",
      "Training Batch: 19455 Loss: 3364.542480\n",
      "Training Batch: 19456 Loss: 3413.580078\n",
      "Training Batch: 19457 Loss: 3348.593750\n",
      "Training Batch: 19458 Loss: 3459.349121\n",
      "Training Batch: 19459 Loss: 3340.336426\n",
      "Training Batch: 19460 Loss: 3350.235596\n",
      "Training Batch: 19461 Loss: 3332.154541\n",
      "Training Batch: 19462 Loss: 3595.599609\n",
      "Training Batch: 19463 Loss: 3386.139404\n",
      "Training Batch: 19464 Loss: 3412.068848\n",
      "Training Batch: 19465 Loss: 3412.848633\n",
      "Training Batch: 19466 Loss: 3422.569580\n",
      "Training Batch: 19467 Loss: 3442.621094\n",
      "Training Batch: 19468 Loss: 3497.037109\n",
      "Training Batch: 19469 Loss: 3565.271240\n",
      "Training Batch: 19470 Loss: 3356.750977\n",
      "Training Batch: 19471 Loss: 3548.807373\n",
      "Training Batch: 19472 Loss: 3324.256836\n",
      "Training Batch: 19473 Loss: 3259.912109\n",
      "Training Batch: 19474 Loss: 3392.856689\n",
      "Training Batch: 19475 Loss: 3388.645508\n",
      "Training Batch: 19476 Loss: 3423.799316\n",
      "Training Batch: 19477 Loss: 3339.624756\n",
      "Training Batch: 19478 Loss: 3403.083008\n",
      "Training Batch: 19479 Loss: 3413.521973\n",
      "Training Batch: 19480 Loss: 3383.087891\n",
      "Training Batch: 19481 Loss: 3414.025391\n",
      "Training Batch: 19482 Loss: 3421.500000\n",
      "Training Batch: 19483 Loss: 3302.686279\n",
      "Training Batch: 19484 Loss: 3544.418457\n",
      "Training Batch: 19485 Loss: 3356.908691\n",
      "Training Batch: 19486 Loss: 3367.179688\n",
      "Training Batch: 19487 Loss: 3397.317871\n",
      "Training Batch: 19488 Loss: 3330.860840\n",
      "Training Batch: 19489 Loss: 3339.682373\n",
      "Training Batch: 19490 Loss: 3315.193359\n",
      "Training Batch: 19491 Loss: 3318.481934\n",
      "Training Batch: 19492 Loss: 3433.799561\n",
      "Training Batch: 19493 Loss: 3410.388672\n",
      "Training Batch: 19494 Loss: 3295.207031\n",
      "Training Batch: 19495 Loss: 3305.420410\n",
      "Training Batch: 19496 Loss: 3331.242676\n",
      "Training Batch: 19497 Loss: 3411.054199\n",
      "Training Batch: 19498 Loss: 3615.305176\n",
      "Training Batch: 19499 Loss: 3376.518066\n",
      "Training Batch: 19500 Loss: 3426.756348\n",
      "Training Batch: 19501 Loss: 3649.148438\n",
      "Training Batch: 19502 Loss: 3357.537598\n",
      "Training Batch: 19503 Loss: 3386.089111\n",
      "Training Batch: 19504 Loss: 3271.249023\n",
      "Training Batch: 19505 Loss: 3298.166016\n",
      "Training Batch: 19506 Loss: 3325.904541\n",
      "Training Batch: 19507 Loss: 3576.169434\n",
      "Training Batch: 19508 Loss: 3287.999023\n",
      "Training Batch: 19509 Loss: 3498.336914\n",
      "Training Batch: 19510 Loss: 3554.269043\n",
      "Training Batch: 19511 Loss: 3389.237549\n",
      "Training Batch: 19512 Loss: 3291.646484\n",
      "Training Batch: 19513 Loss: 3435.715820\n",
      "Training Batch: 19514 Loss: 3450.702637\n",
      "Training Batch: 19515 Loss: 3384.367676\n",
      "Training Batch: 19516 Loss: 3631.981934\n",
      "Training Batch: 19517 Loss: 3635.441895\n",
      "Training Batch: 19518 Loss: 3378.871582\n",
      "Training Batch: 19519 Loss: 3339.664795\n",
      "Training Batch: 19520 Loss: 3353.238525\n",
      "Training Batch: 19521 Loss: 3396.674805\n",
      "Training Batch: 19522 Loss: 3236.803223\n",
      "Training Batch: 19523 Loss: 3277.539551\n",
      "Training Batch: 19524 Loss: 3422.981934\n",
      "Training Batch: 19525 Loss: 3951.891113\n",
      "Training Batch: 19526 Loss: 3498.216797\n",
      "Training Batch: 19527 Loss: 3482.763184\n",
      "Training Batch: 19528 Loss: 3323.420898\n",
      "Training Batch: 19529 Loss: 3694.423340\n",
      "Training Batch: 19530 Loss: 3966.150635\n",
      "Training Batch: 19531 Loss: 3421.902344\n",
      "Training Batch: 19532 Loss: 3618.519043\n",
      "Training Batch: 19533 Loss: 3598.313477\n",
      "Training Batch: 19534 Loss: 3897.102783\n",
      "Training Batch: 19535 Loss: 3435.171875\n",
      "Training Batch: 19536 Loss: 3361.767578\n",
      "Training Batch: 19537 Loss: 3372.714844\n",
      "Training Batch: 19538 Loss: 3600.474121\n",
      "Training Batch: 19539 Loss: 3600.504150\n",
      "Training Batch: 19540 Loss: 3608.618408\n",
      "Training Batch: 19541 Loss: 3318.333984\n",
      "Training Batch: 19542 Loss: 3385.658691\n",
      "Training Batch: 19543 Loss: 3444.493652\n",
      "Training Batch: 19544 Loss: 3427.512207\n",
      "Training Batch: 19545 Loss: 3717.654541\n",
      "Training Batch: 19546 Loss: 3485.764893\n",
      "Training Batch: 19547 Loss: 3631.643799\n",
      "Training Batch: 19548 Loss: 3432.409668\n",
      "Training Batch: 19549 Loss: 3431.775879\n",
      "Training Batch: 19550 Loss: 3351.966797\n",
      "Training Batch: 19551 Loss: 3290.467041\n",
      "Training Batch: 19552 Loss: 3384.019043\n",
      "Training Batch: 19553 Loss: 3382.945801\n",
      "Training Batch: 19554 Loss: 3447.397461\n",
      "Training Batch: 19555 Loss: 3405.648438\n",
      "Training Batch: 19556 Loss: 3568.010742\n",
      "Training Batch: 19557 Loss: 3492.995850\n",
      "Training Batch: 19558 Loss: 3312.623779\n",
      "Training Batch: 19559 Loss: 3398.056641\n",
      "Training Batch: 19560 Loss: 3367.803711\n",
      "Training Batch: 19561 Loss: 3326.441650\n",
      "Training Batch: 19562 Loss: 3298.403076\n",
      "Training Batch: 19563 Loss: 3342.081543\n",
      "Training Batch: 19564 Loss: 3481.152100\n",
      "Training Batch: 19565 Loss: 3362.433594\n",
      "Training Batch: 19566 Loss: 3522.424316\n",
      "Training Batch: 19567 Loss: 3358.493896\n",
      "Training Batch: 19568 Loss: 3645.858887\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Batch: 19569 Loss: 3597.902344\n",
      "Training Batch: 19570 Loss: 3290.950195\n",
      "Training Batch: 19571 Loss: 3408.430176\n",
      "Training Batch: 19572 Loss: 3365.660400\n",
      "Training Batch: 19573 Loss: 3482.877441\n",
      "Training Batch: 19574 Loss: 3325.290771\n",
      "Training Batch: 19575 Loss: 3500.585449\n",
      "Training Batch: 19576 Loss: 3436.838135\n",
      "Training Batch: 19577 Loss: 3394.212158\n",
      "Training Batch: 19578 Loss: 3427.913574\n",
      "Training Batch: 19579 Loss: 3353.969238\n",
      "Training Batch: 19580 Loss: 3390.864014\n",
      "Training Batch: 19581 Loss: 3515.668945\n",
      "Training Batch: 19582 Loss: 3347.367676\n",
      "Training Batch: 19583 Loss: 3376.306396\n",
      "Training Batch: 19584 Loss: 3366.870605\n",
      "Training Batch: 19585 Loss: 3479.232910\n",
      "Training Batch: 19586 Loss: 3310.923096\n",
      "Training Batch: 19587 Loss: 3293.270752\n",
      "Training Batch: 19588 Loss: 3407.023926\n",
      "Training Batch: 19589 Loss: 3478.812012\n",
      "Training Batch: 19590 Loss: 3349.360840\n",
      "Training Batch: 19591 Loss: 3360.292480\n",
      "Training Batch: 19592 Loss: 3486.073730\n",
      "Training Batch: 19593 Loss: 3288.204346\n",
      "Training Batch: 19594 Loss: 3349.646973\n",
      "Training Batch: 19595 Loss: 3298.774414\n",
      "Training Batch: 19596 Loss: 3349.560059\n",
      "Training Batch: 19597 Loss: 3330.514160\n",
      "Training Batch: 19598 Loss: 3288.074707\n",
      "Training Batch: 19599 Loss: 3429.636230\n",
      "Training Batch: 19600 Loss: 3301.833984\n",
      "Training Batch: 19601 Loss: 3263.349609\n",
      "Training Batch: 19602 Loss: 3499.209229\n",
      "Training Batch: 19603 Loss: 3360.325684\n",
      "Training Batch: 19604 Loss: 3403.347656\n",
      "Training Batch: 19605 Loss: 3377.864746\n",
      "Training Batch: 19606 Loss: 3349.576904\n",
      "Training Batch: 19607 Loss: 3388.236328\n",
      "Training Batch: 19608 Loss: 3503.640137\n",
      "Training Batch: 19609 Loss: 3347.547363\n",
      "Training Batch: 19610 Loss: 3293.584717\n",
      "Training Batch: 19611 Loss: 3557.246582\n",
      "Training Batch: 19612 Loss: 3416.027832\n",
      "Training Batch: 19613 Loss: 3252.161377\n",
      "Training Batch: 19614 Loss: 3436.565918\n",
      "Training Batch: 19615 Loss: 3466.205078\n",
      "Training Batch: 19616 Loss: 3673.689697\n",
      "Training Batch: 19617 Loss: 3494.696289\n",
      "Training Batch: 19618 Loss: 3338.197266\n",
      "Training Batch: 19619 Loss: 3322.129639\n",
      "Training Batch: 19620 Loss: 3339.704834\n",
      "Training Batch: 19621 Loss: 3646.980225\n",
      "Training Batch: 19622 Loss: 3601.022461\n",
      "Training Batch: 19623 Loss: 3506.907227\n",
      "Training Batch: 19624 Loss: 3417.077393\n",
      "Training Batch: 19625 Loss: 3577.678711\n",
      "Training Batch: 19626 Loss: 3378.959717\n",
      "Training Batch: 19627 Loss: 3362.877441\n",
      "Training Batch: 19628 Loss: 3305.348877\n",
      "Training Batch: 19629 Loss: 3302.282227\n",
      "Training Batch: 19630 Loss: 3374.925049\n",
      "Training Batch: 19631 Loss: 3301.809570\n",
      "Training Batch: 19632 Loss: 3446.730225\n",
      "Training Batch: 19633 Loss: 3358.461914\n",
      "Training Batch: 19634 Loss: 3408.402832\n",
      "Training Batch: 19635 Loss: 3237.818359\n",
      "Training Batch: 19636 Loss: 3308.700684\n",
      "Training Batch: 19637 Loss: 3370.132812\n",
      "Training Batch: 19638 Loss: 3422.343994\n",
      "Training Batch: 19639 Loss: 3272.727783\n",
      "Training Batch: 19640 Loss: 3276.590576\n",
      "Training Batch: 19641 Loss: 3320.238770\n",
      "Training Batch: 19642 Loss: 3400.534180\n",
      "Training Batch: 19643 Loss: 3292.462891\n",
      "Training Batch: 19644 Loss: 3386.367676\n",
      "Training Batch: 19645 Loss: 3528.353516\n",
      "Training Batch: 19646 Loss: 3453.067871\n",
      "Training Batch: 19647 Loss: 3369.250488\n",
      "Training Batch: 19648 Loss: 3691.869629\n",
      "Training Batch: 19649 Loss: 3366.198730\n",
      "Training Batch: 19650 Loss: 3450.054688\n",
      "Training Batch: 19651 Loss: 3553.020020\n",
      "Training Batch: 19652 Loss: 3293.437500\n",
      "Training Batch: 19653 Loss: 3339.927246\n",
      "Training Batch: 19654 Loss: 3395.031738\n",
      "Training Batch: 19655 Loss: 3211.078125\n",
      "Training Batch: 19656 Loss: 3380.503418\n",
      "Training Batch: 19657 Loss: 3209.767578\n",
      "Training Batch: 19658 Loss: 3280.986572\n",
      "Training Batch: 19659 Loss: 3300.888184\n",
      "Training Batch: 19660 Loss: 3429.067383\n",
      "Training Batch: 19661 Loss: 3294.511719\n",
      "Training Batch: 19662 Loss: 3291.744141\n",
      "Training Batch: 19663 Loss: 3440.533691\n",
      "Training Batch: 19664 Loss: 3429.572754\n",
      "Training Batch: 19665 Loss: 3250.750000\n",
      "Training Batch: 19666 Loss: 3395.123535\n",
      "Training Batch: 19667 Loss: 3461.107422\n",
      "Training Batch: 19668 Loss: 3375.926270\n",
      "Training Batch: 19669 Loss: 3252.193848\n",
      "Training Batch: 19670 Loss: 3470.661133\n",
      "Training Batch: 19671 Loss: 3397.374512\n",
      "Training Batch: 19672 Loss: 3466.671387\n",
      "Training Batch: 19673 Loss: 3455.543213\n",
      "Training Batch: 19674 Loss: 3374.458252\n",
      "Training Batch: 19675 Loss: 3188.957520\n",
      "Training Batch: 19676 Loss: 3322.667480\n",
      "Training Batch: 19677 Loss: 3572.882812\n",
      "Training Batch: 19678 Loss: 3421.446289\n",
      "Training Batch: 19679 Loss: 3342.488525\n",
      "Training Batch: 19680 Loss: 3529.969971\n",
      "Training Batch: 19681 Loss: 3382.686035\n",
      "Training Batch: 19682 Loss: 3779.353516\n",
      "Training Batch: 19683 Loss: 3575.814697\n",
      "Training Batch: 19684 Loss: 3286.816406\n",
      "Training Batch: 19685 Loss: 3590.157227\n",
      "Training Batch: 19686 Loss: 3435.971191\n",
      "Training Batch: 19687 Loss: 3451.849121\n",
      "Training Batch: 19688 Loss: 3446.658936\n",
      "Training Batch: 19689 Loss: 3473.173096\n",
      "Training Batch: 19690 Loss: 3449.061279\n",
      "Training Batch: 19691 Loss: 3412.175781\n",
      "Training Batch: 19692 Loss: 3320.855713\n",
      "Training Batch: 19693 Loss: 3322.591309\n",
      "Training Batch: 19694 Loss: 3281.316895\n",
      "Training Batch: 19695 Loss: 3302.349609\n",
      "Training Batch: 19696 Loss: 3440.825195\n",
      "Training Batch: 19697 Loss: 3366.829102\n",
      "Training Batch: 19698 Loss: 3549.847656\n",
      "Training Batch: 19699 Loss: 3365.991699\n",
      "Training Batch: 19700 Loss: 3370.866211\n",
      "Training Batch: 19701 Loss: 3303.701416\n",
      "Training Batch: 19702 Loss: 3317.923828\n",
      "Training Batch: 19703 Loss: 3407.205078\n",
      "Training Batch: 19704 Loss: 3389.829102\n",
      "Training Batch: 19705 Loss: 3388.605469\n",
      "Training Batch: 19706 Loss: 3398.141602\n",
      "Training Batch: 19707 Loss: 3380.618896\n",
      "Training Batch: 19708 Loss: 3411.204834\n",
      "Training Batch: 19709 Loss: 3753.335938\n",
      "Training Batch: 19710 Loss: 3806.520020\n",
      "Training Batch: 19711 Loss: 3441.126953\n",
      "Training Batch: 19712 Loss: 3415.264648\n",
      "Training Batch: 19713 Loss: 3442.056641\n",
      "Training Batch: 19714 Loss: 3506.919922\n",
      "Training Batch: 19715 Loss: 3365.562500\n",
      "Training Batch: 19716 Loss: 3411.691650\n",
      "Training Batch: 19717 Loss: 3339.074707\n",
      "Training Batch: 19718 Loss: 3306.305176\n",
      "Training Batch: 19719 Loss: 3351.020996\n",
      "Training Batch: 19720 Loss: 3396.954102\n",
      "Training Batch: 19721 Loss: 3361.028320\n",
      "Training Batch: 19722 Loss: 3385.566162\n",
      "Training Batch: 19723 Loss: 3344.827148\n",
      "Training Batch: 19724 Loss: 3500.286377\n",
      "Training Batch: 19725 Loss: 3363.643555\n",
      "Training Batch: 19726 Loss: 3234.187744\n",
      "Training Batch: 19727 Loss: 3323.738281\n",
      "Training Batch: 19728 Loss: 3333.532715\n",
      "Training Batch: 19729 Loss: 3382.561035\n",
      "Training Batch: 19730 Loss: 3367.745117\n",
      "Training Batch: 19731 Loss: 3477.317627\n",
      "Training Batch: 19732 Loss: 3343.859863\n",
      "Training Batch: 19733 Loss: 3332.078613\n",
      "Training Batch: 19734 Loss: 3388.958008\n",
      "Training Batch: 19735 Loss: 3356.014160\n",
      "Training Batch: 19736 Loss: 3424.134277\n",
      "Training Batch: 19737 Loss: 3290.375977\n",
      "Training Batch: 19738 Loss: 3369.648926\n",
      "Training Batch: 19739 Loss: 3394.970459\n",
      "Training Batch: 19740 Loss: 3495.092285\n",
      "Training Batch: 19741 Loss: 3312.667969\n",
      "Training Batch: 19742 Loss: 3207.614746\n",
      "Training Batch: 19743 Loss: 3404.768066\n",
      "Training Batch: 19744 Loss: 3465.156738\n",
      "Training Batch: 19745 Loss: 3403.934570\n",
      "Training Batch: 19746 Loss: 3356.234375\n",
      "Training Batch: 19747 Loss: 3467.501465\n",
      "Training Batch: 19748 Loss: 3320.177246\n",
      "Training Batch: 19749 Loss: 3376.493164\n",
      "Training Batch: 19750 Loss: 3287.033203\n",
      "Training Batch: 19751 Loss: 3462.450684\n",
      "Training Batch: 19752 Loss: 3519.256592\n",
      "Training Batch: 19753 Loss: 3446.910889\n",
      "Training Batch: 19754 Loss: 3622.423828\n",
      "Training Batch: 19755 Loss: 3544.277832\n",
      "Training Batch: 19756 Loss: 3385.087402\n",
      "Training Batch: 19757 Loss: 3372.010254\n",
      "Training Batch: 19758 Loss: 3544.654053\n",
      "Training Batch: 19759 Loss: 3405.103516\n",
      "Training Batch: 19760 Loss: 3440.926758\n",
      "Training Batch: 19761 Loss: 3381.503906\n",
      "Training Batch: 19762 Loss: 3497.836914\n",
      "Training Batch: 19763 Loss: 3601.207031\n",
      "Training Batch: 19764 Loss: 3492.809082\n",
      "Training Batch: 19765 Loss: 3385.737305\n",
      "Training Batch: 19766 Loss: 3398.655762\n",
      "Training Batch: 19767 Loss: 3416.496582\n",
      "Training Batch: 19768 Loss: 3364.308105\n",
      "Training Batch: 19769 Loss: 3283.939941\n",
      "Training Batch: 19770 Loss: 3383.630371\n",
      "Training Batch: 19771 Loss: 3422.859375\n",
      "Training Batch: 19772 Loss: 3416.556641\n",
      "Training Batch: 19773 Loss: 3518.630859\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Batch: 19774 Loss: 3338.241943\n",
      "Training Batch: 19775 Loss: 3360.127197\n",
      "Training Batch: 19776 Loss: 3422.184326\n",
      "Training Batch: 19777 Loss: 3417.897949\n",
      "Training Batch: 19778 Loss: 3462.490723\n",
      "Training Batch: 19779 Loss: 3310.236572\n",
      "Training Batch: 19780 Loss: 3380.250244\n",
      "Training Batch: 19781 Loss: 3311.152344\n",
      "Training Batch: 19782 Loss: 3445.249268\n",
      "Training Batch: 19783 Loss: 3527.095703\n",
      "Training Batch: 19784 Loss: 3545.916016\n",
      "Training Batch: 19785 Loss: 3408.107422\n",
      "Training Batch: 19786 Loss: 3363.885254\n",
      "Training Batch: 19787 Loss: 3384.436035\n",
      "Training Batch: 19788 Loss: 3375.268311\n",
      "Training Batch: 19789 Loss: 3418.014160\n",
      "Training Batch: 19790 Loss: 3500.903809\n",
      "Training Batch: 19791 Loss: 3419.980957\n",
      "Training Batch: 19792 Loss: 3592.917480\n",
      "Training Batch: 19793 Loss: 3351.745117\n",
      "Training Batch: 19794 Loss: 3448.273926\n",
      "Training Batch: 19795 Loss: 3402.939209\n",
      "Training Batch: 19796 Loss: 3384.403809\n",
      "Training Batch: 19797 Loss: 3383.331543\n",
      "Training Batch: 19798 Loss: 3298.350586\n",
      "Training Batch: 19799 Loss: 3438.075684\n",
      "Training Batch: 19800 Loss: 3275.497070\n",
      "Training Batch: 19801 Loss: 3382.859131\n",
      "Training Batch: 19802 Loss: 3269.689941\n",
      "Training Batch: 19803 Loss: 3320.080078\n",
      "Training Batch: 19804 Loss: 3526.269531\n",
      "Training Batch: 19805 Loss: 3458.324951\n",
      "Training Batch: 19806 Loss: 3403.209473\n",
      "Training Batch: 19807 Loss: 3434.572266\n",
      "Training Batch: 19808 Loss: 3340.562012\n",
      "Training Batch: 19809 Loss: 3549.152344\n",
      "Training Batch: 19810 Loss: 3478.463379\n",
      "Training Batch: 19811 Loss: 3614.785156\n",
      "Training Batch: 19812 Loss: 3397.222168\n",
      "Training Batch: 19813 Loss: 3548.567383\n",
      "Training Batch: 19814 Loss: 3435.103516\n",
      "Training Batch: 19815 Loss: 3371.449951\n",
      "Training Batch: 19816 Loss: 3304.205566\n",
      "Training Batch: 19817 Loss: 3366.580811\n",
      "Training Batch: 19818 Loss: 3290.591064\n",
      "Training Batch: 19819 Loss: 3259.438477\n",
      "Training Batch: 19820 Loss: 3309.529297\n",
      "Training Batch: 19821 Loss: 3346.291504\n",
      "Training Batch: 19822 Loss: 3309.075195\n",
      "Training Batch: 19823 Loss: 3312.731445\n",
      "Training Batch: 19824 Loss: 3343.346924\n",
      "Training Batch: 19825 Loss: 3494.765137\n",
      "Training Batch: 19826 Loss: 3249.647461\n",
      "Training Batch: 19827 Loss: 3591.621338\n",
      "Training Batch: 19828 Loss: 3423.974854\n",
      "Training Batch: 19829 Loss: 3275.437256\n",
      "Training Batch: 19830 Loss: 3710.718262\n",
      "Training Batch: 19831 Loss: 3561.354492\n",
      "Training Batch: 19832 Loss: 3389.252441\n",
      "Training Batch: 19833 Loss: 3365.084717\n",
      "Training Batch: 19834 Loss: 3348.986816\n",
      "Training Batch: 19835 Loss: 3296.941895\n",
      "Training Batch: 19836 Loss: 3371.162598\n",
      "Training Batch: 19837 Loss: 3259.243896\n",
      "Training Batch: 19838 Loss: 3255.279785\n",
      "Training Batch: 19839 Loss: 3247.185791\n",
      "Training Batch: 19840 Loss: 3307.942871\n",
      "Training Batch: 19841 Loss: 3274.696289\n",
      "Training Batch: 19842 Loss: 3317.080811\n",
      "Training Batch: 19843 Loss: 3390.393066\n",
      "Training Batch: 19844 Loss: 3313.089355\n",
      "Training Batch: 19845 Loss: 3351.541748\n",
      "Training Batch: 19846 Loss: 3350.692139\n",
      "Training Batch: 19847 Loss: 3325.742920\n",
      "Training Batch: 19848 Loss: 3367.753418\n",
      "Training Batch: 19849 Loss: 3304.696289\n",
      "Training Batch: 19850 Loss: 3344.232422\n",
      "Training Batch: 19851 Loss: 3286.496582\n",
      "Training Batch: 19852 Loss: 3387.963867\n",
      "Training Batch: 19853 Loss: 3410.230225\n",
      "Training Batch: 19854 Loss: 3419.376953\n",
      "Training Batch: 19855 Loss: 3243.328125\n",
      "Training Batch: 19856 Loss: 3384.057373\n",
      "Training Batch: 19857 Loss: 3410.472656\n",
      "Training Batch: 19858 Loss: 3599.797852\n",
      "Training Batch: 19859 Loss: 3402.455078\n",
      "Training Batch: 19860 Loss: 3433.819824\n",
      "Training Batch: 19861 Loss: 3497.275635\n",
      "Training Batch: 19862 Loss: 3430.042480\n",
      "Training Batch: 19863 Loss: 3405.137939\n",
      "Training Batch: 19864 Loss: 3402.687988\n",
      "Training Batch: 19865 Loss: 3375.731689\n",
      "Training Batch: 19866 Loss: 3376.210938\n",
      "Training Batch: 19867 Loss: 3337.452148\n",
      "Training Batch: 19868 Loss: 3643.500732\n",
      "Training Batch: 19869 Loss: 3442.140625\n",
      "Training Batch: 19870 Loss: 3427.073975\n",
      "Training Batch: 19871 Loss: 3478.914062\n",
      "Training Batch: 19872 Loss: 3565.688965\n",
      "Training Batch: 19873 Loss: 3342.646484\n",
      "Training Batch: 19874 Loss: 3420.051025\n",
      "Training Batch: 19875 Loss: 3517.623779\n",
      "Training Batch: 19876 Loss: 3513.850098\n",
      "Training Batch: 19877 Loss: 3386.995117\n",
      "Training Batch: 19878 Loss: 3448.367188\n",
      "Training Batch: 19879 Loss: 3465.400391\n",
      "Training Batch: 19880 Loss: 3468.940430\n",
      "Training Batch: 19881 Loss: 3282.134033\n",
      "Training Batch: 19882 Loss: 3573.473145\n",
      "Training Batch: 19883 Loss: 3349.341064\n",
      "Training Batch: 19884 Loss: 3487.807129\n",
      "Training Batch: 19885 Loss: 3306.189453\n",
      "Training Batch: 19886 Loss: 3284.278564\n",
      "Training Batch: 19887 Loss: 3337.176270\n",
      "Training Batch: 19888 Loss: 3594.469238\n",
      "Training Batch: 19889 Loss: 3353.515381\n",
      "Training Batch: 19890 Loss: 3346.252441\n",
      "Training Batch: 19891 Loss: 3376.574707\n",
      "Training Batch: 19892 Loss: 3421.906738\n",
      "Training Batch: 19893 Loss: 3332.927246\n",
      "Training Batch: 19894 Loss: 3427.693848\n",
      "Training Batch: 19895 Loss: 3327.188477\n",
      "Training Batch: 19896 Loss: 3287.905273\n",
      "Training Batch: 19897 Loss: 3345.633057\n",
      "Training Batch: 19898 Loss: 3321.447021\n",
      "Training Batch: 19899 Loss: 3523.030518\n",
      "Training Batch: 19900 Loss: 3319.237305\n",
      "Training Batch: 19901 Loss: 3318.437988\n",
      "Training Batch: 19902 Loss: 3404.284180\n",
      "Training Batch: 19903 Loss: 3303.903320\n",
      "Training Batch: 19904 Loss: 3239.718018\n",
      "Training Batch: 19905 Loss: 3421.979248\n",
      "Training Batch: 19906 Loss: 3323.185547\n",
      "Training Batch: 19907 Loss: 3284.197754\n",
      "Training Batch: 19908 Loss: 3555.402344\n",
      "Training Batch: 19909 Loss: 3535.472168\n",
      "Training Batch: 19910 Loss: 3468.916260\n",
      "Training Batch: 19911 Loss: 3345.369629\n",
      "Training Batch: 19912 Loss: 3583.038818\n",
      "Training Batch: 19913 Loss: 3408.319336\n",
      "Training Batch: 19914 Loss: 3457.042969\n",
      "Training Batch: 19915 Loss: 3605.688721\n",
      "Training Batch: 19916 Loss: 3410.145264\n",
      "Training Batch: 19917 Loss: 3451.535400\n",
      "Training Batch: 19918 Loss: 3464.977539\n",
      "Training Batch: 19919 Loss: 3381.904297\n",
      "Training Batch: 19920 Loss: 3481.707031\n",
      "Training Batch: 19921 Loss: 3423.274414\n",
      "Training Batch: 19922 Loss: 3436.213867\n",
      "Training Batch: 19923 Loss: 3352.526855\n",
      "Training Batch: 19924 Loss: 3399.514648\n",
      "Training Batch: 19925 Loss: 3358.401367\n",
      "Training Batch: 19926 Loss: 3317.835938\n",
      "Training Batch: 19927 Loss: 3281.541748\n",
      "Training Batch: 19928 Loss: 3277.781250\n",
      "Training Batch: 19929 Loss: 3364.155762\n",
      "Training Batch: 19930 Loss: 3281.748047\n",
      "Training Batch: 19931 Loss: 3239.422852\n",
      "Training Batch: 19932 Loss: 3837.200439\n",
      "Training Batch: 19933 Loss: 3352.003906\n",
      "Training Batch: 19934 Loss: 3461.449219\n",
      "Training Batch: 19935 Loss: 3393.416260\n",
      "Training Batch: 19936 Loss: 3331.788574\n",
      "Training Batch: 19937 Loss: 3610.752930\n",
      "Training Batch: 19938 Loss: 3467.388184\n",
      "Training Batch: 19939 Loss: 3402.545898\n",
      "Training Batch: 19940 Loss: 3497.861328\n",
      "Training Batch: 19941 Loss: 3333.065918\n",
      "Training Batch: 19942 Loss: 3344.139648\n",
      "Training Batch: 19943 Loss: 3256.086670\n",
      "Training Batch: 19944 Loss: 3421.787598\n",
      "Training Batch: 19945 Loss: 3454.294434\n",
      "Training Batch: 19946 Loss: 3440.004883\n",
      "Training Batch: 19947 Loss: 3386.523438\n",
      "Training Batch: 19948 Loss: 3492.203613\n",
      "Training Batch: 19949 Loss: 3270.869141\n",
      "Training Batch: 19950 Loss: 3325.300049\n",
      "Training Batch: 19951 Loss: 3347.183594\n",
      "Training Batch: 19952 Loss: 3511.145996\n",
      "Training Batch: 19953 Loss: 3320.619141\n",
      "Training Batch: 19954 Loss: 3362.878906\n",
      "Training Batch: 19955 Loss: 3424.683105\n",
      "Training Batch: 19956 Loss: 3258.116211\n",
      "Training Batch: 19957 Loss: 3348.659668\n",
      "Training Batch: 19958 Loss: 3407.073730\n",
      "Training Batch: 19959 Loss: 3323.121582\n",
      "Training Batch: 19960 Loss: 3447.933594\n",
      "Training Batch: 19961 Loss: 3351.093262\n",
      "Training Batch: 19962 Loss: 3298.094971\n",
      "Training Batch: 19963 Loss: 3458.012451\n",
      "Training Batch: 19964 Loss: 3430.791260\n",
      "Training Batch: 19965 Loss: 3418.615234\n",
      "Training Batch: 19966 Loss: 3345.111328\n",
      "Training Batch: 19967 Loss: 3430.152344\n",
      "Training Batch: 19968 Loss: 3554.120117\n",
      "Training Batch: 19969 Loss: 3380.104980\n",
      "Training Batch: 19970 Loss: 3315.101562\n",
      "Training Batch: 19971 Loss: 3443.371582\n",
      "Training Batch: 19972 Loss: 3456.095215\n",
      "Training Batch: 19973 Loss: 3423.084961\n",
      "Training Batch: 19974 Loss: 3391.519531\n",
      "Training Batch: 19975 Loss: 3349.738770\n",
      "Training Batch: 19976 Loss: 3333.596436\n",
      "Training Batch: 19977 Loss: 3488.237793\n",
      "Training Batch: 19978 Loss: 3472.518311\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Batch: 19979 Loss: 3393.027588\n",
      "Training Batch: 19980 Loss: 3312.894043\n",
      "Training Batch: 19981 Loss: 3289.055664\n",
      "Training Batch: 19982 Loss: 3517.317871\n",
      "Training Batch: 19983 Loss: 3321.615479\n",
      "Training Batch: 19984 Loss: 3475.090576\n",
      "Training Batch: 19985 Loss: 3370.343262\n",
      "Training Batch: 19986 Loss: 3409.146484\n",
      "Training Batch: 19987 Loss: 3456.582520\n",
      "Training Batch: 19988 Loss: 3281.522949\n",
      "Training Batch: 19989 Loss: 3274.927246\n",
      "Training Batch: 19990 Loss: 3337.279785\n",
      "Training Batch: 19991 Loss: 3451.039062\n",
      "Training Batch: 19992 Loss: 3361.380127\n",
      "Training Batch: 19993 Loss: 3398.863281\n",
      "Training Batch: 19994 Loss: 3356.197998\n",
      "Training Batch: 19995 Loss: 3537.944824\n",
      "Training Batch: 19996 Loss: 3589.421143\n",
      "Training Batch: 19997 Loss: 3994.167480\n",
      "Training Batch: 19998 Loss: 3490.748535\n",
      "Training Batch: 19999 Loss: 3435.247559\n",
      "Training Batch: 20000 Loss: 3399.155273\n",
      "<class 'numpy.ndarray'>\n",
      "Validation Batch: 1 Loss: 3334.432373\n",
      "<class 'numpy.ndarray'>\n",
      "Validation Batch: 2 Loss: 3320.030762\n",
      "<class 'numpy.ndarray'>\n",
      "Validation Batch: 3 Loss: 3340.310547\n",
      "<class 'numpy.ndarray'>\n",
      "Validation Batch: 4 Loss: 3331.072266\n",
      "<class 'numpy.ndarray'>\n",
      "Validation Batch: 5 Loss: 3351.786621\n",
      "<class 'numpy.ndarray'>\n",
      "Validation Batch: 6 Loss: 3481.537109\n",
      "<class 'numpy.ndarray'>\n",
      "Validation Batch: 7 Loss: 3328.973145\n",
      "<class 'numpy.ndarray'>\n",
      "Validation Batch: 8 Loss: 3343.026611\n",
      "<class 'numpy.ndarray'>\n",
      "Validation Batch: 9 Loss: 3340.949219\n",
      "<class 'numpy.ndarray'>\n",
      "Validation Batch: 10 Loss: 3417.755859\n",
      "<class 'numpy.ndarray'>\n",
      "Validation Batch: 11 Loss: 3366.206055\n",
      "<class 'numpy.ndarray'>\n",
      "Validation Batch: 12 Loss: 3348.732666\n",
      "<class 'numpy.ndarray'>\n",
      "Validation Batch: 13 Loss: 3299.966797\n",
      "<class 'numpy.ndarray'>\n",
      "Validation Batch: 14 Loss: 3422.572510\n",
      "<class 'numpy.ndarray'>\n",
      "Validation Batch: 15 Loss: 3345.565918\n",
      "<class 'numpy.ndarray'>\n",
      "Validation Batch: 16 Loss: 3347.413330\n",
      "<class 'numpy.ndarray'>\n",
      "Validation Batch: 17 Loss: 3436.518555\n",
      "<class 'numpy.ndarray'>\n",
      "Validation Batch: 18 Loss: 3832.689209\n",
      "<class 'numpy.ndarray'>\n",
      "Validation Batch: 19 Loss: 3325.458984\n",
      "<class 'numpy.ndarray'>\n",
      "Validation Batch: 20 Loss: 3348.490967\n",
      "<class 'numpy.ndarray'>\n",
      "Validation Batch: 21 Loss: 3521.375244\n",
      "<class 'numpy.ndarray'>\n",
      "Validation Batch: 22 Loss: 3274.380859\n",
      "<class 'numpy.ndarray'>\n",
      "Validation Batch: 23 Loss: 3499.041504\n",
      "<class 'numpy.ndarray'>\n",
      "Validation Batch: 24 Loss: 3295.657227\n",
      "<class 'numpy.ndarray'>\n",
      "Validation Batch: 25 Loss: 3296.156006\n",
      "<class 'numpy.ndarray'>\n",
      "Validation Batch: 26 Loss: 3335.667480\n",
      "<class 'numpy.ndarray'>\n",
      "Validation Batch: 27 Loss: 3406.102051\n",
      "<class 'numpy.ndarray'>\n",
      "Validation Batch: 28 Loss: 3389.553223\n",
      "<class 'numpy.ndarray'>\n",
      "Validation Batch: 29 Loss: 3347.796387\n",
      "<class 'numpy.ndarray'>\n",
      "Validation Batch: 30 Loss: 3374.806396\n",
      "<class 'numpy.ndarray'>\n",
      "Validation Batch: 31 Loss: 3461.901855\n",
      "<class 'numpy.ndarray'>\n",
      "Validation Batch: 32 Loss: 3305.141357\n",
      "<class 'numpy.ndarray'>\n",
      "Validation Batch: 33 Loss: 3257.783691\n",
      "<class 'numpy.ndarray'>\n",
      "Validation Batch: 34 Loss: 3307.284180\n",
      "<class 'numpy.ndarray'>\n",
      "Validation Batch: 35 Loss: 3511.871826\n",
      "<class 'numpy.ndarray'>\n",
      "Validation Batch: 36 Loss: 3302.835449\n",
      "<class 'numpy.ndarray'>\n",
      "Validation Batch: 37 Loss: 3377.317871\n",
      "<class 'numpy.ndarray'>\n",
      "Validation Batch: 38 Loss: 3389.664551\n",
      "<class 'numpy.ndarray'>\n",
      "Validation Batch: 39 Loss: 3417.331055\n",
      "<class 'numpy.ndarray'>\n",
      "Validation Batch: 40 Loss: 3355.841309\n",
      "<class 'numpy.ndarray'>\n",
      "Validation Batch: 41 Loss: 3328.540039\n",
      "<class 'numpy.ndarray'>\n",
      "Validation Batch: 42 Loss: 3321.410156\n",
      "<class 'numpy.ndarray'>\n",
      "Validation Batch: 43 Loss: 3373.103516\n",
      "<class 'numpy.ndarray'>\n",
      "Validation Batch: 44 Loss: 3307.573730\n",
      "<class 'numpy.ndarray'>\n",
      "Validation Batch: 45 Loss: 3320.583496\n",
      "<class 'numpy.ndarray'>\n",
      "Validation Batch: 46 Loss: 3290.735352\n",
      "<class 'numpy.ndarray'>\n",
      "Validation Batch: 47 Loss: 3555.647461\n",
      "<class 'numpy.ndarray'>\n",
      "Validation Batch: 48 Loss: 3434.758301\n",
      "<class 'numpy.ndarray'>\n",
      "Validation Batch: 49 Loss: 3325.360352\n",
      "<class 'numpy.ndarray'>\n",
      "Validation Batch: 50 Loss: 3464.863281\n",
      "<class 'numpy.ndarray'>\n",
      "Validation Batch: 51 Loss: 3523.812012\n",
      "<class 'numpy.ndarray'>\n",
      "Validation Batch: 52 Loss: 3313.287598\n",
      "<class 'numpy.ndarray'>\n",
      "Validation Batch: 53 Loss: 3444.552734\n",
      "<class 'numpy.ndarray'>\n",
      "Validation Batch: 54 Loss: 3422.478271\n",
      "<class 'numpy.ndarray'>\n",
      "Validation Batch: 55 Loss: 3397.777832\n",
      "<class 'numpy.ndarray'>\n",
      "Validation Batch: 56 Loss: 3289.082520\n",
      "<class 'numpy.ndarray'>\n",
      "Validation Batch: 57 Loss: 3419.865723\n",
      "<class 'numpy.ndarray'>\n",
      "Validation Batch: 58 Loss: 3304.745117\n",
      "<class 'numpy.ndarray'>\n",
      "Validation Batch: 59 Loss: 3409.553711\n",
      "<class 'numpy.ndarray'>\n",
      "Validation Batch: 60 Loss: 3293.324707\n",
      "<class 'numpy.ndarray'>\n",
      "Validation Batch: 61 Loss: 3264.930176\n",
      "<class 'numpy.ndarray'>\n",
      "Validation Batch: 62 Loss: 3272.820068\n",
      "<class 'numpy.ndarray'>\n",
      "Validation Batch: 63 Loss: 3428.049561\n",
      "<class 'numpy.ndarray'>\n",
      "Validation Batch: 64 Loss: 3445.372559\n",
      "<class 'numpy.ndarray'>\n",
      "Validation Batch: 65 Loss: 3284.131348\n",
      "<class 'numpy.ndarray'>\n",
      "Validation Batch: 66 Loss: 3384.739502\n",
      "<class 'numpy.ndarray'>\n",
      "Validation Batch: 67 Loss: 3315.876709\n",
      "<class 'numpy.ndarray'>\n",
      "Validation Batch: 68 Loss: 3272.760498\n",
      "<class 'numpy.ndarray'>\n",
      "Validation Batch: 69 Loss: 3364.449707\n",
      "<class 'numpy.ndarray'>\n",
      "Validation Batch: 70 Loss: 3395.368652\n",
      "<class 'numpy.ndarray'>\n",
      "Validation Batch: 71 Loss: 3411.307617\n",
      "<class 'numpy.ndarray'>\n",
      "Validation Batch: 72 Loss: 3337.534668\n",
      "<class 'numpy.ndarray'>\n",
      "Validation Batch: 73 Loss: 3385.002930\n",
      "<class 'numpy.ndarray'>\n",
      "Validation Batch: 74 Loss: 3378.650879\n",
      "<class 'numpy.ndarray'>\n",
      "Validation Batch: 75 Loss: 3491.741211\n",
      "<class 'numpy.ndarray'>\n",
      "Validation Batch: 76 Loss: 3363.146484\n",
      "<class 'numpy.ndarray'>\n",
      "Validation Batch: 77 Loss: 3312.830811\n",
      "<class 'numpy.ndarray'>\n",
      "Validation Batch: 78 Loss: 3371.335449\n",
      "<class 'numpy.ndarray'>\n",
      "Validation Batch: 79 Loss: 3310.762451\n",
      "<class 'numpy.ndarray'>\n",
      "Validation Batch: 80 Loss: 3357.769287\n",
      "<class 'numpy.ndarray'>\n",
      "Validation Batch: 81 Loss: 3316.210449\n",
      "<class 'numpy.ndarray'>\n",
      "Validation Batch: 82 Loss: 3229.531982\n",
      "<class 'numpy.ndarray'>\n",
      "Validation Batch: 83 Loss: 3314.274414\n",
      "<class 'numpy.ndarray'>\n",
      "Validation Batch: 84 Loss: 3355.669434\n",
      "<class 'numpy.ndarray'>\n",
      "Validation Batch: 85 Loss: 3596.943604\n",
      "<class 'numpy.ndarray'>\n",
      "Validation Batch: 86 Loss: 3340.621582\n",
      "<class 'numpy.ndarray'>\n",
      "Validation Batch: 87 Loss: 3355.798340\n",
      "<class 'numpy.ndarray'>\n",
      "Validation Batch: 88 Loss: 3345.619385\n",
      "<class 'numpy.ndarray'>\n",
      "Validation Batch: 89 Loss: 3298.817383\n",
      "<class 'numpy.ndarray'>\n",
      "Validation Batch: 90 Loss: 3303.299805\n",
      "<class 'numpy.ndarray'>\n",
      "Validation Batch: 91 Loss: 3309.078613\n",
      "<class 'numpy.ndarray'>\n",
      "Validation Batch: 92 Loss: 3356.072266\n",
      "<class 'numpy.ndarray'>\n",
      "Validation Batch: 93 Loss: 3273.323242\n",
      "<class 'numpy.ndarray'>\n",
      "Validation Batch: 94 Loss: 3239.052246\n",
      "<class 'numpy.ndarray'>\n",
      "Validation Batch: 95 Loss: 3385.105225\n",
      "<class 'numpy.ndarray'>\n",
      "Validation Batch: 96 Loss: 3339.186768\n",
      "<class 'numpy.ndarray'>\n",
      "Validation Batch: 97 Loss: 3367.001465\n",
      "<class 'numpy.ndarray'>\n",
      "Validation Batch: 98 Loss: 3317.390381\n",
      "<class 'numpy.ndarray'>\n",
      "Validation Batch: 99 Loss: 3375.440918\n",
      "<class 'numpy.ndarray'>\n",
      "Validation Batch: 100 Loss: 3365.985840\n",
      "<class 'numpy.ndarray'>\n",
      "Validation Batch: 101 Loss: 3338.270996\n",
      "<class 'numpy.ndarray'>\n",
      "Validation Batch: 102 Loss: 3436.987305\n",
      "<class 'numpy.ndarray'>\n",
      "Validation Batch: 103 Loss: 3352.870117\n",
      "<class 'numpy.ndarray'>\n",
      "Validation Batch: 104 Loss: 3439.112793\n",
      "<class 'numpy.ndarray'>\n",
      "Validation Batch: 105 Loss: 3408.093262\n",
      "<class 'numpy.ndarray'>\n",
      "Validation Batch: 106 Loss: 3339.795410\n",
      "<class 'numpy.ndarray'>\n",
      "Validation Batch: 107 Loss: 3251.771484\n",
      "<class 'numpy.ndarray'>\n",
      "Validation Batch: 108 Loss: 3300.877441\n",
      "<class 'numpy.ndarray'>\n",
      "Validation Batch: 109 Loss: 3240.769531\n",
      "<class 'numpy.ndarray'>\n",
      "Validation Batch: 110 Loss: 3435.680908\n",
      "<class 'numpy.ndarray'>\n",
      "Validation Batch: 111 Loss: 3386.270020\n",
      "<class 'numpy.ndarray'>\n",
      "Validation Batch: 112 Loss: 3303.928711\n",
      "<class 'numpy.ndarray'>\n",
      "Validation Batch: 113 Loss: 3425.177002\n",
      "<class 'numpy.ndarray'>\n",
      "Validation Batch: 114 Loss: 3303.447998\n",
      "<class 'numpy.ndarray'>\n",
      "Validation Batch: 115 Loss: 3412.054932\n",
      "<class 'numpy.ndarray'>\n",
      "Validation Batch: 116 Loss: 3539.479004\n",
      "<class 'numpy.ndarray'>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Batch: 117 Loss: 3363.712402\n",
      "<class 'numpy.ndarray'>\n",
      "Validation Batch: 118 Loss: 3344.965332\n",
      "<class 'numpy.ndarray'>\n",
      "Validation Batch: 119 Loss: 3303.089844\n",
      "<class 'numpy.ndarray'>\n",
      "Validation Batch: 120 Loss: 3298.099854\n",
      "<class 'numpy.ndarray'>\n",
      "Validation Batch: 121 Loss: 3290.246338\n",
      "<class 'numpy.ndarray'>\n",
      "Validation Batch: 122 Loss: 3345.664062\n",
      "<class 'numpy.ndarray'>\n",
      "Validation Batch: 123 Loss: 3341.660156\n",
      "<class 'numpy.ndarray'>\n",
      "Validation Batch: 124 Loss: 3378.948242\n",
      "<class 'numpy.ndarray'>\n",
      "Validation Batch: 125 Loss: 3441.299072\n",
      "<class 'numpy.ndarray'>\n",
      "Validation Batch: 126 Loss: 3369.729492\n",
      "<class 'numpy.ndarray'>\n",
      "Validation Batch: 127 Loss: 3393.756104\n",
      "<class 'numpy.ndarray'>\n",
      "Validation Batch: 128 Loss: 3346.885010\n",
      "<class 'numpy.ndarray'>\n",
      "Validation Batch: 129 Loss: 3424.407715\n",
      "<class 'numpy.ndarray'>\n",
      "Validation Batch: 130 Loss: 3403.540527\n",
      "<class 'numpy.ndarray'>\n",
      "Validation Batch: 131 Loss: 3404.294678\n",
      "<class 'numpy.ndarray'>\n",
      "Validation Batch: 132 Loss: 3369.114258\n",
      "<class 'numpy.ndarray'>\n",
      "Validation Batch: 133 Loss: 3395.575684\n",
      "<class 'numpy.ndarray'>\n",
      "Validation Batch: 134 Loss: 3270.033691\n",
      "<class 'numpy.ndarray'>\n",
      "Validation Batch: 135 Loss: 3293.172119\n",
      "<class 'numpy.ndarray'>\n",
      "Validation Batch: 136 Loss: 3404.378906\n",
      "<class 'numpy.ndarray'>\n",
      "Validation Batch: 137 Loss: 3798.151367\n",
      "<class 'numpy.ndarray'>\n",
      "Validation Batch: 138 Loss: 3403.531738\n",
      "<class 'numpy.ndarray'>\n",
      "Validation Batch: 139 Loss: 3270.275391\n",
      "<class 'numpy.ndarray'>\n",
      "Validation Batch: 140 Loss: 3297.120605\n",
      "<class 'numpy.ndarray'>\n",
      "Validation Batch: 141 Loss: 3315.004150\n",
      "<class 'numpy.ndarray'>\n",
      "Validation Batch: 142 Loss: 3347.542480\n",
      "<class 'numpy.ndarray'>\n",
      "Validation Batch: 143 Loss: 3342.791504\n",
      "<class 'numpy.ndarray'>\n",
      "Validation Batch: 144 Loss: 3298.870605\n",
      "<class 'numpy.ndarray'>\n",
      "Validation Batch: 145 Loss: 3273.332031\n",
      "<class 'numpy.ndarray'>\n",
      "Validation Batch: 146 Loss: 3397.845215\n",
      "<class 'numpy.ndarray'>\n",
      "Validation Batch: 147 Loss: 3468.201172\n",
      "<class 'numpy.ndarray'>\n",
      "Validation Batch: 148 Loss: 3235.298828\n",
      "<class 'numpy.ndarray'>\n",
      "Validation Batch: 149 Loss: 3342.509521\n",
      "<class 'numpy.ndarray'>\n",
      "Validation Batch: 150 Loss: 3271.777344\n",
      "<class 'numpy.ndarray'>\n",
      "Validation Batch: 151 Loss: 3431.882812\n",
      "<class 'numpy.ndarray'>\n",
      "Validation Batch: 152 Loss: 3329.209473\n",
      "<class 'numpy.ndarray'>\n",
      "Validation Batch: 153 Loss: 3372.119141\n",
      "<class 'numpy.ndarray'>\n",
      "Validation Batch: 154 Loss: 3293.728027\n",
      "<class 'numpy.ndarray'>\n",
      "Validation Batch: 155 Loss: 3416.881836\n",
      "<class 'numpy.ndarray'>\n",
      "Validation Batch: 156 Loss: 3515.020264\n",
      "<class 'numpy.ndarray'>\n",
      "Validation Batch: 157 Loss: 3507.798340\n",
      "<class 'numpy.ndarray'>\n",
      "Validation Batch: 158 Loss: 3336.365234\n",
      "<class 'numpy.ndarray'>\n",
      "Validation Batch: 159 Loss: 3392.524170\n",
      "<class 'numpy.ndarray'>\n",
      "Validation Batch: 160 Loss: 3348.675537\n",
      "<class 'numpy.ndarray'>\n",
      "Validation Batch: 161 Loss: 3330.687012\n",
      "<class 'numpy.ndarray'>\n",
      "Validation Batch: 162 Loss: 3269.191406\n",
      "<class 'numpy.ndarray'>\n",
      "Validation Batch: 163 Loss: 3326.509033\n",
      "<class 'numpy.ndarray'>\n",
      "Validation Batch: 164 Loss: 3382.945068\n",
      "<class 'numpy.ndarray'>\n",
      "Validation Batch: 165 Loss: 3257.645508\n",
      "<class 'numpy.ndarray'>\n",
      "Validation Batch: 166 Loss: 3290.655273\n",
      "<class 'numpy.ndarray'>\n",
      "Validation Batch: 167 Loss: 3834.212158\n",
      "<class 'numpy.ndarray'>\n",
      "Validation Batch: 168 Loss: 3343.939453\n",
      "<class 'numpy.ndarray'>\n",
      "Validation Batch: 169 Loss: 3346.840332\n",
      "<class 'numpy.ndarray'>\n",
      "Validation Batch: 170 Loss: 3317.893555\n",
      "<class 'numpy.ndarray'>\n",
      "Validation Batch: 171 Loss: 3343.959961\n",
      "<class 'numpy.ndarray'>\n",
      "Validation Batch: 172 Loss: 3425.528564\n",
      "<class 'numpy.ndarray'>\n",
      "Validation Batch: 173 Loss: 3510.827148\n",
      "<class 'numpy.ndarray'>\n",
      "Validation Batch: 174 Loss: 3327.747314\n",
      "<class 'numpy.ndarray'>\n",
      "Validation Batch: 175 Loss: 3376.530518\n",
      "<class 'numpy.ndarray'>\n",
      "Validation Batch: 176 Loss: 3428.511230\n",
      "<class 'numpy.ndarray'>\n",
      "Validation Batch: 177 Loss: 3313.197266\n",
      "<class 'numpy.ndarray'>\n",
      "Validation Batch: 178 Loss: 3417.988525\n",
      "<class 'numpy.ndarray'>\n",
      "Validation Batch: 179 Loss: 3459.349609\n",
      "<class 'numpy.ndarray'>\n",
      "Validation Batch: 180 Loss: 3269.205078\n",
      "<class 'numpy.ndarray'>\n",
      "Validation Batch: 181 Loss: 3361.830811\n",
      "<class 'numpy.ndarray'>\n",
      "Validation Batch: 182 Loss: 3371.989746\n",
      "<class 'numpy.ndarray'>\n",
      "Validation Batch: 183 Loss: 3361.047363\n",
      "<class 'numpy.ndarray'>\n",
      "Validation Batch: 184 Loss: 3374.247559\n",
      "<class 'numpy.ndarray'>\n",
      "Validation Batch: 185 Loss: 3307.957031\n",
      "<class 'numpy.ndarray'>\n",
      "Validation Batch: 186 Loss: 3265.300049\n",
      "<class 'numpy.ndarray'>\n",
      "Validation Batch: 187 Loss: 3411.358154\n",
      "<class 'numpy.ndarray'>\n",
      "Validation Batch: 188 Loss: 3391.281250\n",
      "<class 'numpy.ndarray'>\n",
      "Validation Batch: 189 Loss: 3311.240723\n",
      "<class 'numpy.ndarray'>\n",
      "Validation Batch: 190 Loss: 3302.883545\n",
      "<class 'numpy.ndarray'>\n",
      "Validation Batch: 191 Loss: 3410.787842\n",
      "<class 'numpy.ndarray'>\n",
      "Validation Batch: 192 Loss: 3318.829590\n",
      "<class 'numpy.ndarray'>\n",
      "Validation Batch: 193 Loss: 3383.943115\n",
      "<class 'numpy.ndarray'>\n",
      "Validation Batch: 194 Loss: 3413.250488\n",
      "<class 'numpy.ndarray'>\n",
      "Validation Batch: 195 Loss: 3334.488770\n",
      "<class 'numpy.ndarray'>\n",
      "Validation Batch: 196 Loss: 3417.616211\n",
      "<class 'numpy.ndarray'>\n",
      "Validation Batch: 197 Loss: 3401.922607\n",
      "<class 'numpy.ndarray'>\n",
      "Validation Batch: 198 Loss: 3398.503174\n",
      "<class 'numpy.ndarray'>\n",
      "Validation Batch: 199 Loss: 3261.284668\n",
      "<class 'numpy.ndarray'>\n",
      "Validation Batch: 200 Loss: 3289.225098\n",
      "<class 'numpy.ndarray'>\n",
      "Validation Batch: 201 Loss: 3286.766113\n",
      "<class 'numpy.ndarray'>\n",
      "Validation Batch: 202 Loss: 3366.279541\n",
      "<class 'numpy.ndarray'>\n",
      "Validation Batch: 203 Loss: 3394.451416\n",
      "<class 'numpy.ndarray'>\n",
      "Validation Batch: 204 Loss: 3419.849609\n",
      "<class 'numpy.ndarray'>\n",
      "Validation Batch: 205 Loss: 3444.645020\n",
      "<class 'numpy.ndarray'>\n",
      "Validation Batch: 206 Loss: 3404.669678\n",
      "<class 'numpy.ndarray'>\n",
      "Validation Batch: 207 Loss: 3344.599121\n",
      "<class 'numpy.ndarray'>\n",
      "Validation Batch: 208 Loss: 3373.685059\n",
      "<class 'numpy.ndarray'>\n",
      "Validation Batch: 209 Loss: 3325.465332\n",
      "<class 'numpy.ndarray'>\n",
      "Validation Batch: 210 Loss: 3353.708496\n",
      "<class 'numpy.ndarray'>\n",
      "Validation Batch: 211 Loss: 3316.184570\n",
      "<class 'numpy.ndarray'>\n",
      "Validation Batch: 212 Loss: 3325.648438\n",
      "<class 'numpy.ndarray'>\n",
      "Validation Batch: 213 Loss: 3297.531250\n",
      "<class 'numpy.ndarray'>\n",
      "Validation Batch: 214 Loss: 3265.301758\n",
      "<class 'numpy.ndarray'>\n",
      "Validation Batch: 215 Loss: 3458.969727\n",
      "<class 'numpy.ndarray'>\n",
      "Validation Batch: 216 Loss: 3275.186035\n",
      "<class 'numpy.ndarray'>\n",
      "Validation Batch: 217 Loss: 3328.655273\n",
      "<class 'numpy.ndarray'>\n",
      "Validation Batch: 218 Loss: 3320.679688\n",
      "<class 'numpy.ndarray'>\n",
      "Validation Batch: 219 Loss: 3301.861816\n",
      "<class 'numpy.ndarray'>\n",
      "Validation Batch: 220 Loss: 3358.814453\n",
      "<class 'numpy.ndarray'>\n",
      "Validation Batch: 221 Loss: 3353.087158\n",
      "<class 'numpy.ndarray'>\n",
      "Validation Batch: 222 Loss: 3296.019531\n",
      "<class 'numpy.ndarray'>\n",
      "Validation Batch: 223 Loss: 3357.973633\n",
      "<class 'numpy.ndarray'>\n",
      "Validation Batch: 224 Loss: 3338.923828\n",
      "<class 'numpy.ndarray'>\n",
      "Validation Batch: 225 Loss: 3302.050537\n",
      "<class 'numpy.ndarray'>\n",
      "Validation Batch: 226 Loss: 3366.005615\n",
      "<class 'numpy.ndarray'>\n",
      "Validation Batch: 227 Loss: 3339.938965\n",
      "<class 'numpy.ndarray'>\n",
      "Validation Batch: 228 Loss: 3339.923096\n",
      "<class 'numpy.ndarray'>\n",
      "Validation Batch: 229 Loss: 3477.819824\n",
      "<class 'numpy.ndarray'>\n",
      "Validation Batch: 230 Loss: 3359.136230\n",
      "<class 'numpy.ndarray'>\n",
      "Validation Batch: 231 Loss: 3325.856445\n",
      "<class 'numpy.ndarray'>\n",
      "Validation Batch: 232 Loss: 3343.854248\n",
      "<class 'numpy.ndarray'>\n",
      "Validation Batch: 233 Loss: 3358.505371\n",
      "<class 'numpy.ndarray'>\n",
      "Validation Batch: 234 Loss: 3395.674805\n",
      "<class 'numpy.ndarray'>\n",
      "Validation Batch: 235 Loss: 3745.562500\n",
      "<class 'numpy.ndarray'>\n",
      "Validation Batch: 236 Loss: 3338.769043\n",
      "<class 'numpy.ndarray'>\n",
      "Validation Batch: 237 Loss: 3350.565186\n",
      "<class 'numpy.ndarray'>\n",
      "Validation Batch: 238 Loss: 3482.282227\n",
      "<class 'numpy.ndarray'>\n",
      "Validation Batch: 239 Loss: 3344.854980\n",
      "<class 'numpy.ndarray'>\n",
      "Validation Batch: 240 Loss: 3385.496338\n",
      "<class 'numpy.ndarray'>\n",
      "Validation Batch: 241 Loss: 3365.471191\n",
      "<class 'numpy.ndarray'>\n",
      "Validation Batch: 242 Loss: 3463.840576\n",
      "<class 'numpy.ndarray'>\n",
      "Validation Batch: 243 Loss: 3291.979736\n",
      "<class 'numpy.ndarray'>\n",
      "Validation Batch: 244 Loss: 3382.353516\n",
      "<class 'numpy.ndarray'>\n",
      "Validation Batch: 245 Loss: 3482.465332\n",
      "<class 'numpy.ndarray'>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Batch: 246 Loss: 3296.966309\n",
      "<class 'numpy.ndarray'>\n",
      "Validation Batch: 247 Loss: 3282.734863\n",
      "<class 'numpy.ndarray'>\n",
      "Validation Batch: 248 Loss: 3428.076660\n",
      "<class 'numpy.ndarray'>\n",
      "Validation Batch: 249 Loss: 3227.844238\n",
      "<class 'numpy.ndarray'>\n",
      "Validation Batch: 250 Loss: 3331.953125\n",
      "Training Batch: 1 Loss: 3424.145020\n",
      "Training Batch: 2 Loss: 3602.854492\n",
      "Training Batch: 3 Loss: 3391.377930\n",
      "Training Batch: 4 Loss: 3560.333252\n",
      "Training Batch: 5 Loss: 3519.488770\n",
      "Training Batch: 6 Loss: 3357.333496\n",
      "Training Batch: 7 Loss: 3288.640869\n",
      "Training Batch: 8 Loss: 3319.344727\n",
      "Training Batch: 9 Loss: 3382.098633\n",
      "Training Batch: 10 Loss: 3450.063965\n",
      "Training Batch: 11 Loss: 3655.354492\n",
      "Training Batch: 12 Loss: 3359.013916\n",
      "Training Batch: 13 Loss: 3359.111572\n",
      "Training Batch: 14 Loss: 3294.796875\n",
      "Training Batch: 15 Loss: 3388.772461\n",
      "Training Batch: 16 Loss: 3560.325684\n",
      "Training Batch: 17 Loss: 3380.701416\n",
      "Training Batch: 18 Loss: 3446.173828\n",
      "Training Batch: 19 Loss: 3483.717773\n",
      "Training Batch: 20 Loss: 3377.775635\n",
      "Training Batch: 21 Loss: 3306.746094\n",
      "Training Batch: 22 Loss: 3371.050049\n",
      "Training Batch: 23 Loss: 3386.108643\n",
      "Training Batch: 24 Loss: 3476.059082\n",
      "Training Batch: 25 Loss: 3316.455078\n",
      "Training Batch: 26 Loss: 3482.734619\n",
      "Training Batch: 27 Loss: 3463.725098\n",
      "Training Batch: 28 Loss: 3494.074463\n",
      "Training Batch: 29 Loss: 3471.493164\n",
      "Training Batch: 30 Loss: 3280.612793\n",
      "Training Batch: 31 Loss: 3356.480957\n",
      "Training Batch: 32 Loss: 3254.186768\n",
      "Training Batch: 33 Loss: 3389.296631\n",
      "Training Batch: 34 Loss: 3405.140625\n",
      "Training Batch: 35 Loss: 3356.554199\n",
      "Training Batch: 36 Loss: 3299.723633\n",
      "Training Batch: 37 Loss: 3376.212891\n",
      "Training Batch: 38 Loss: 3437.047363\n",
      "Training Batch: 39 Loss: 3368.568848\n",
      "Training Batch: 40 Loss: 3472.929199\n",
      "Training Batch: 41 Loss: 3455.518555\n",
      "Training Batch: 42 Loss: 3319.673584\n",
      "Training Batch: 43 Loss: 3282.949951\n",
      "Training Batch: 44 Loss: 3285.579102\n",
      "Training Batch: 45 Loss: 3257.797607\n",
      "Training Batch: 46 Loss: 3366.098145\n",
      "Training Batch: 47 Loss: 3526.404785\n",
      "Training Batch: 48 Loss: 3326.671143\n",
      "Training Batch: 49 Loss: 3290.510986\n",
      "Training Batch: 50 Loss: 3246.884277\n",
      "Training Batch: 51 Loss: 3285.358643\n",
      "Training Batch: 52 Loss: 3237.293945\n",
      "Training Batch: 53 Loss: 3239.092773\n",
      "Training Batch: 54 Loss: 3282.253174\n",
      "Training Batch: 55 Loss: 3277.978027\n",
      "Training Batch: 56 Loss: 3459.163574\n",
      "Training Batch: 57 Loss: 3442.541504\n",
      "Training Batch: 58 Loss: 3492.875732\n",
      "Training Batch: 59 Loss: 3296.943848\n",
      "Training Batch: 60 Loss: 3280.873047\n",
      "Training Batch: 61 Loss: 3492.726074\n",
      "Training Batch: 62 Loss: 3479.708008\n",
      "Training Batch: 63 Loss: 3510.606934\n",
      "Training Batch: 64 Loss: 3281.553955\n",
      "Training Batch: 65 Loss: 3229.133789\n",
      "Training Batch: 66 Loss: 3265.237305\n",
      "Training Batch: 67 Loss: 3341.909424\n",
      "Training Batch: 68 Loss: 3337.405273\n",
      "Training Batch: 69 Loss: 3506.151855\n",
      "Training Batch: 70 Loss: 3238.047852\n",
      "Training Batch: 71 Loss: 3438.437012\n",
      "Training Batch: 72 Loss: 3313.202148\n",
      "Training Batch: 73 Loss: 3239.002197\n",
      "Training Batch: 74 Loss: 3351.355713\n",
      "Training Batch: 75 Loss: 3268.229492\n",
      "Training Batch: 76 Loss: 3342.310791\n",
      "Training Batch: 77 Loss: 3324.446045\n",
      "Training Batch: 78 Loss: 3367.888672\n",
      "Training Batch: 79 Loss: 3294.156494\n",
      "Training Batch: 80 Loss: 3340.132324\n",
      "Training Batch: 81 Loss: 3266.666992\n",
      "Training Batch: 82 Loss: 3412.468262\n",
      "Training Batch: 83 Loss: 3415.823730\n",
      "Training Batch: 84 Loss: 3238.729980\n",
      "Training Batch: 85 Loss: 3303.506348\n",
      "Training Batch: 86 Loss: 3292.354980\n",
      "Training Batch: 87 Loss: 3310.072266\n",
      "Training Batch: 88 Loss: 3315.447510\n",
      "Training Batch: 89 Loss: 3383.734619\n",
      "Training Batch: 90 Loss: 3429.156738\n",
      "Training Batch: 91 Loss: 3389.479492\n",
      "Training Batch: 92 Loss: 3347.399414\n",
      "Training Batch: 93 Loss: 3349.492676\n",
      "Training Batch: 94 Loss: 3206.926270\n",
      "Training Batch: 95 Loss: 3263.317139\n",
      "Training Batch: 96 Loss: 3338.804443\n",
      "Training Batch: 97 Loss: 3318.587891\n",
      "Training Batch: 98 Loss: 3584.252441\n",
      "Training Batch: 99 Loss: 3400.831543\n",
      "Training Batch: 100 Loss: 3326.101562\n",
      "Training Batch: 101 Loss: 3385.414307\n",
      "Training Batch: 102 Loss: 3180.498047\n",
      "Training Batch: 103 Loss: 3323.303711\n",
      "Training Batch: 104 Loss: 3167.320801\n",
      "Training Batch: 105 Loss: 3261.103027\n",
      "Training Batch: 106 Loss: 3329.547363\n",
      "Training Batch: 107 Loss: 3341.314697\n",
      "Training Batch: 108 Loss: 3228.003906\n",
      "Training Batch: 109 Loss: 3321.397949\n",
      "Training Batch: 110 Loss: 3734.846191\n",
      "Training Batch: 111 Loss: 3191.643066\n",
      "Training Batch: 112 Loss: 3279.247559\n",
      "Training Batch: 113 Loss: 3405.493164\n",
      "Training Batch: 114 Loss: 3225.831055\n",
      "Training Batch: 115 Loss: 3337.018066\n",
      "Training Batch: 116 Loss: 3470.854492\n",
      "Training Batch: 117 Loss: 3378.511719\n",
      "Training Batch: 118 Loss: 3602.640137\n",
      "Training Batch: 119 Loss: 3421.014160\n",
      "Training Batch: 120 Loss: 3390.932129\n",
      "Training Batch: 121 Loss: 3499.580078\n",
      "Training Batch: 122 Loss: 3392.751465\n",
      "Training Batch: 123 Loss: 3358.863770\n",
      "Training Batch: 124 Loss: 3325.575439\n",
      "Training Batch: 125 Loss: 3243.363281\n",
      "Training Batch: 126 Loss: 3284.415527\n",
      "Training Batch: 127 Loss: 3322.644531\n",
      "Training Batch: 128 Loss: 3329.022949\n",
      "Training Batch: 129 Loss: 3261.693359\n",
      "Training Batch: 130 Loss: 3298.545898\n",
      "Training Batch: 131 Loss: 3458.626953\n",
      "Training Batch: 132 Loss: 3369.068848\n",
      "Training Batch: 133 Loss: 3318.976807\n",
      "Training Batch: 134 Loss: 3251.048828\n",
      "Training Batch: 135 Loss: 3317.775391\n",
      "Training Batch: 136 Loss: 3412.353516\n",
      "Training Batch: 137 Loss: 3312.414551\n",
      "Training Batch: 138 Loss: 3453.123535\n",
      "Training Batch: 139 Loss: 3371.106201\n",
      "Training Batch: 140 Loss: 3229.042480\n",
      "Training Batch: 141 Loss: 3578.241211\n",
      "Training Batch: 142 Loss: 3259.457520\n",
      "Training Batch: 143 Loss: 3720.833008\n",
      "Training Batch: 144 Loss: 3388.368652\n",
      "Training Batch: 145 Loss: 3379.517822\n",
      "Training Batch: 146 Loss: 3309.155273\n",
      "Training Batch: 147 Loss: 3266.625977\n",
      "Training Batch: 148 Loss: 3498.890625\n",
      "Training Batch: 149 Loss: 3320.009277\n",
      "Training Batch: 150 Loss: 3393.218750\n",
      "Training Batch: 151 Loss: 3371.351318\n",
      "Training Batch: 152 Loss: 3336.958984\n",
      "Training Batch: 153 Loss: 3382.991211\n",
      "Training Batch: 154 Loss: 3315.156738\n",
      "Training Batch: 155 Loss: 3743.524414\n",
      "Training Batch: 156 Loss: 3824.658691\n",
      "Training Batch: 157 Loss: 3399.233887\n",
      "Training Batch: 158 Loss: 3358.631348\n",
      "Training Batch: 159 Loss: 3382.751953\n",
      "Training Batch: 160 Loss: 3294.838379\n",
      "Training Batch: 161 Loss: 3363.891602\n",
      "Training Batch: 162 Loss: 3344.553467\n",
      "Training Batch: 163 Loss: 3296.982910\n",
      "Training Batch: 164 Loss: 3519.600098\n",
      "Training Batch: 165 Loss: 3355.004150\n",
      "Training Batch: 166 Loss: 3355.081543\n",
      "Training Batch: 167 Loss: 3410.713379\n",
      "Training Batch: 168 Loss: 3323.862305\n",
      "Training Batch: 169 Loss: 3376.698730\n",
      "Training Batch: 170 Loss: 3453.709961\n",
      "Training Batch: 171 Loss: 3367.888428\n",
      "Training Batch: 172 Loss: 3242.471680\n",
      "Training Batch: 173 Loss: 3368.692383\n",
      "Training Batch: 174 Loss: 3327.097656\n",
      "Training Batch: 175 Loss: 3442.654541\n",
      "Training Batch: 176 Loss: 3323.495850\n",
      "Training Batch: 177 Loss: 3417.974609\n",
      "Training Batch: 178 Loss: 3368.461426\n",
      "Training Batch: 179 Loss: 3466.836426\n",
      "Training Batch: 180 Loss: 3392.631836\n",
      "Training Batch: 181 Loss: 3296.825439\n",
      "Training Batch: 182 Loss: 3264.645996\n",
      "Training Batch: 183 Loss: 3280.765137\n",
      "Training Batch: 184 Loss: 3359.487305\n",
      "Training Batch: 185 Loss: 3436.079590\n",
      "Training Batch: 186 Loss: 3526.258057\n",
      "Training Batch: 187 Loss: 3413.526855\n",
      "Training Batch: 188 Loss: 3299.238037\n",
      "Training Batch: 189 Loss: 3389.254395\n",
      "Training Batch: 190 Loss: 3552.457520\n",
      "Training Batch: 191 Loss: 3312.019531\n",
      "Training Batch: 192 Loss: 3497.464844\n",
      "Training Batch: 193 Loss: 3525.436035\n",
      "Training Batch: 194 Loss: 3376.373535\n",
      "Training Batch: 195 Loss: 3383.520752\n",
      "Training Batch: 196 Loss: 3324.416992\n",
      "Training Batch: 197 Loss: 3457.807861\n",
      "Training Batch: 198 Loss: 3258.920410\n",
      "Training Batch: 199 Loss: 3300.959961\n",
      "Training Batch: 200 Loss: 3359.674805\n",
      "Training Batch: 201 Loss: 3394.366211\n",
      "Training Batch: 202 Loss: 3205.106445\n",
      "Training Batch: 203 Loss: 3518.882324\n",
      "Training Batch: 204 Loss: 3389.465820\n",
      "Training Batch: 205 Loss: 3370.565918\n",
      "Training Batch: 206 Loss: 3350.056152\n",
      "Training Batch: 207 Loss: 3339.295410\n",
      "Training Batch: 208 Loss: 3247.593506\n",
      "Training Batch: 209 Loss: 3401.829102\n",
      "Training Batch: 210 Loss: 3451.762451\n",
      "Training Batch: 211 Loss: 3365.830566\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Batch: 212 Loss: 3267.288330\n",
      "Training Batch: 213 Loss: 3208.079590\n",
      "Training Batch: 214 Loss: 3458.679688\n",
      "Training Batch: 215 Loss: 3367.868164\n",
      "Training Batch: 216 Loss: 3536.513184\n",
      "Training Batch: 217 Loss: 3431.943359\n",
      "Training Batch: 218 Loss: 3569.142822\n",
      "Training Batch: 219 Loss: 3361.930176\n",
      "Training Batch: 220 Loss: 3340.177734\n",
      "Training Batch: 221 Loss: 3497.712402\n",
      "Training Batch: 222 Loss: 3293.990234\n",
      "Training Batch: 223 Loss: 3336.893799\n",
      "Training Batch: 224 Loss: 3252.757324\n",
      "Training Batch: 225 Loss: 3336.700928\n",
      "Training Batch: 226 Loss: 3290.463379\n",
      "Training Batch: 227 Loss: 3249.808594\n",
      "Training Batch: 228 Loss: 3351.313477\n",
      "Training Batch: 229 Loss: 3220.003906\n",
      "Training Batch: 230 Loss: 3310.707275\n",
      "Training Batch: 231 Loss: 3380.913086\n",
      "Training Batch: 232 Loss: 3220.955078\n",
      "Training Batch: 233 Loss: 3384.043457\n",
      "Training Batch: 234 Loss: 3322.303223\n",
      "Training Batch: 235 Loss: 3456.251953\n",
      "Training Batch: 236 Loss: 3358.158691\n",
      "Training Batch: 237 Loss: 3422.505859\n",
      "Training Batch: 238 Loss: 3304.591797\n",
      "Training Batch: 239 Loss: 3410.448486\n",
      "Training Batch: 240 Loss: 3289.210938\n",
      "Training Batch: 241 Loss: 3294.118164\n",
      "Training Batch: 242 Loss: 3302.099121\n",
      "Training Batch: 243 Loss: 3295.346191\n",
      "Training Batch: 244 Loss: 3279.886230\n",
      "Training Batch: 245 Loss: 3382.014893\n",
      "Training Batch: 246 Loss: 3371.132324\n",
      "Training Batch: 247 Loss: 3427.665039\n",
      "Training Batch: 248 Loss: 3325.730957\n",
      "Training Batch: 249 Loss: 3402.416504\n",
      "Training Batch: 250 Loss: 3330.077881\n",
      "Training Batch: 251 Loss: 3393.390137\n",
      "Training Batch: 252 Loss: 3318.463867\n",
      "Training Batch: 253 Loss: 3226.493164\n",
      "Training Batch: 254 Loss: 3272.222168\n",
      "Training Batch: 255 Loss: 3348.529297\n",
      "Training Batch: 256 Loss: 3321.745117\n",
      "Training Batch: 257 Loss: 3565.733398\n",
      "Training Batch: 258 Loss: 3450.550293\n",
      "Training Batch: 259 Loss: 3286.479492\n",
      "Training Batch: 260 Loss: 3419.477051\n",
      "Training Batch: 261 Loss: 3292.042969\n",
      "Training Batch: 262 Loss: 3389.139404\n",
      "Training Batch: 263 Loss: 3325.818848\n",
      "Training Batch: 264 Loss: 3408.162598\n",
      "Training Batch: 265 Loss: 3337.972656\n",
      "Training Batch: 266 Loss: 3412.528809\n",
      "Training Batch: 267 Loss: 3279.294434\n",
      "Training Batch: 268 Loss: 3328.503418\n",
      "Training Batch: 269 Loss: 3620.356445\n",
      "Training Batch: 270 Loss: 3561.668701\n",
      "Training Batch: 271 Loss: 3327.954590\n",
      "Training Batch: 272 Loss: 3506.175781\n",
      "Training Batch: 273 Loss: 3438.055176\n",
      "Training Batch: 274 Loss: 3339.569824\n",
      "Training Batch: 275 Loss: 3358.194092\n",
      "Training Batch: 276 Loss: 3320.872559\n",
      "Training Batch: 277 Loss: 3353.713379\n",
      "Training Batch: 278 Loss: 3331.312500\n",
      "Training Batch: 279 Loss: 3366.685791\n",
      "Training Batch: 280 Loss: 3322.715332\n",
      "Training Batch: 281 Loss: 3653.625977\n",
      "Training Batch: 282 Loss: 3364.306152\n",
      "Training Batch: 283 Loss: 3391.956299\n",
      "Training Batch: 284 Loss: 3350.907715\n",
      "Training Batch: 285 Loss: 3295.013184\n",
      "Training Batch: 286 Loss: 3347.517090\n",
      "Training Batch: 287 Loss: 3433.507812\n",
      "Training Batch: 288 Loss: 3378.142578\n",
      "Training Batch: 289 Loss: 3370.636719\n",
      "Training Batch: 290 Loss: 3293.514648\n",
      "Training Batch: 291 Loss: 3387.330566\n",
      "Training Batch: 292 Loss: 3300.335205\n",
      "Training Batch: 293 Loss: 3536.648682\n",
      "Training Batch: 294 Loss: 3559.673584\n",
      "Training Batch: 295 Loss: 3380.322266\n",
      "Training Batch: 296 Loss: 3354.450684\n",
      "Training Batch: 297 Loss: 3447.330566\n",
      "Training Batch: 298 Loss: 3280.137207\n",
      "Training Batch: 299 Loss: 3372.439941\n",
      "Training Batch: 300 Loss: 3351.870605\n",
      "Training Batch: 301 Loss: 3553.146484\n",
      "Training Batch: 302 Loss: 3352.059082\n",
      "Training Batch: 303 Loss: 3335.517822\n",
      "Training Batch: 304 Loss: 3590.383057\n",
      "Training Batch: 305 Loss: 3311.461426\n",
      "Training Batch: 306 Loss: 3470.211914\n",
      "Training Batch: 307 Loss: 3417.257568\n",
      "Training Batch: 308 Loss: 3343.863770\n",
      "Training Batch: 309 Loss: 3436.358398\n",
      "Training Batch: 310 Loss: 3342.018066\n",
      "Training Batch: 311 Loss: 3335.596680\n",
      "Training Batch: 312 Loss: 3334.542480\n",
      "Training Batch: 313 Loss: 3246.018555\n",
      "Training Batch: 314 Loss: 3235.127441\n",
      "Training Batch: 315 Loss: 3199.739990\n",
      "Training Batch: 316 Loss: 3232.196045\n",
      "Training Batch: 317 Loss: 3307.226562\n",
      "Training Batch: 318 Loss: 3337.521973\n",
      "Training Batch: 319 Loss: 3273.951172\n",
      "Training Batch: 320 Loss: 3242.155029\n",
      "Training Batch: 321 Loss: 3312.716309\n",
      "Training Batch: 322 Loss: 3308.397461\n",
      "Training Batch: 323 Loss: 3378.219971\n",
      "Training Batch: 324 Loss: 3466.362305\n",
      "Training Batch: 325 Loss: 3272.555908\n",
      "Training Batch: 326 Loss: 3178.862793\n",
      "Training Batch: 327 Loss: 3295.524902\n",
      "Training Batch: 328 Loss: 3261.726562\n",
      "Training Batch: 329 Loss: 3314.661377\n",
      "Training Batch: 330 Loss: 3417.432373\n",
      "Training Batch: 331 Loss: 3302.136230\n",
      "Training Batch: 332 Loss: 3420.896240\n",
      "Training Batch: 333 Loss: 3414.992676\n",
      "Training Batch: 334 Loss: 3465.236328\n",
      "Training Batch: 335 Loss: 3486.626953\n",
      "Training Batch: 336 Loss: 3421.607422\n",
      "Training Batch: 337 Loss: 3396.198730\n",
      "Training Batch: 338 Loss: 3372.693848\n",
      "Training Batch: 339 Loss: 3603.960205\n",
      "Training Batch: 340 Loss: 3334.280762\n",
      "Training Batch: 341 Loss: 3359.870117\n",
      "Training Batch: 342 Loss: 3513.241211\n",
      "Training Batch: 343 Loss: 3519.657715\n",
      "Training Batch: 344 Loss: 3308.969238\n",
      "Training Batch: 345 Loss: 3384.424072\n",
      "Training Batch: 346 Loss: 3471.010010\n",
      "Training Batch: 347 Loss: 3291.561768\n",
      "Training Batch: 348 Loss: 3340.286865\n",
      "Training Batch: 349 Loss: 3309.111328\n",
      "Training Batch: 350 Loss: 3363.453125\n",
      "Training Batch: 351 Loss: 3331.156250\n",
      "Training Batch: 352 Loss: 3391.070801\n",
      "Training Batch: 353 Loss: 3368.493164\n",
      "Training Batch: 354 Loss: 3388.038574\n",
      "Training Batch: 355 Loss: 3417.618164\n",
      "Training Batch: 356 Loss: 3241.962402\n",
      "Training Batch: 357 Loss: 3655.009033\n",
      "Training Batch: 358 Loss: 3609.380371\n",
      "Training Batch: 359 Loss: 3799.916504\n",
      "Training Batch: 360 Loss: 3434.229004\n",
      "Training Batch: 361 Loss: 3585.821289\n",
      "Training Batch: 362 Loss: 3448.927246\n",
      "Training Batch: 363 Loss: 3764.290039\n",
      "Training Batch: 364 Loss: 3618.452637\n",
      "Training Batch: 365 Loss: 3957.228516\n",
      "Training Batch: 366 Loss: 3707.489990\n",
      "Training Batch: 367 Loss: 3399.807373\n",
      "Training Batch: 368 Loss: 3600.119141\n",
      "Training Batch: 369 Loss: 3453.365234\n",
      "Training Batch: 370 Loss: 3462.938477\n",
      "Training Batch: 371 Loss: 3614.609375\n",
      "Training Batch: 372 Loss: 3334.872070\n",
      "Training Batch: 373 Loss: 3484.807861\n",
      "Training Batch: 374 Loss: 3442.504883\n",
      "Training Batch: 375 Loss: 3647.563965\n",
      "Training Batch: 376 Loss: 3429.744141\n",
      "Training Batch: 377 Loss: 3624.738281\n",
      "Training Batch: 378 Loss: 3714.617676\n",
      "Training Batch: 379 Loss: 3547.451416\n",
      "Training Batch: 380 Loss: 3431.257080\n",
      "Training Batch: 381 Loss: 3388.185303\n",
      "Training Batch: 382 Loss: 3306.157471\n",
      "Training Batch: 383 Loss: 3413.345703\n",
      "Training Batch: 384 Loss: 3419.385986\n",
      "Training Batch: 385 Loss: 3257.597656\n",
      "Training Batch: 386 Loss: 3358.701172\n",
      "Training Batch: 387 Loss: 3537.403320\n",
      "Training Batch: 388 Loss: 3303.831543\n",
      "Training Batch: 389 Loss: 3347.810791\n",
      "Training Batch: 390 Loss: 3322.792725\n",
      "Training Batch: 391 Loss: 3225.644287\n",
      "Training Batch: 392 Loss: 3349.787109\n",
      "Training Batch: 393 Loss: 3248.640869\n",
      "Training Batch: 394 Loss: 3384.720215\n",
      "Training Batch: 395 Loss: 3367.827148\n",
      "Training Batch: 396 Loss: 3250.912598\n",
      "Training Batch: 397 Loss: 3613.017090\n",
      "Training Batch: 398 Loss: 3295.999512\n",
      "Training Batch: 399 Loss: 3232.322998\n",
      "Training Batch: 400 Loss: 3276.019043\n",
      "Training Batch: 401 Loss: 3247.531250\n",
      "Training Batch: 402 Loss: 3427.096191\n",
      "Training Batch: 403 Loss: 3727.372559\n",
      "Training Batch: 404 Loss: 3314.924561\n",
      "Training Batch: 405 Loss: 3461.373535\n",
      "Training Batch: 406 Loss: 3454.609619\n",
      "Training Batch: 407 Loss: 3473.212891\n",
      "Training Batch: 408 Loss: 3387.617676\n",
      "Training Batch: 409 Loss: 3321.684570\n",
      "Training Batch: 410 Loss: 3373.039551\n",
      "Training Batch: 411 Loss: 3370.739258\n",
      "Training Batch: 412 Loss: 3364.715820\n",
      "Training Batch: 413 Loss: 3465.958008\n",
      "Training Batch: 414 Loss: 3244.178223\n",
      "Training Batch: 415 Loss: 3256.317139\n",
      "Training Batch: 416 Loss: 3235.305908\n",
      "Training Batch: 417 Loss: 3357.613525\n",
      "Training Batch: 418 Loss: 3347.279785\n",
      "Training Batch: 419 Loss: 3424.512695\n",
      "Training Batch: 420 Loss: 3248.479980\n",
      "Training Batch: 421 Loss: 3338.803223\n",
      "Training Batch: 422 Loss: 3409.773438\n",
      "Training Batch: 423 Loss: 3561.604004\n",
      "Training Batch: 424 Loss: 3345.633789\n",
      "Training Batch: 425 Loss: 3291.246094\n",
      "Training Batch: 426 Loss: 3414.364258\n",
      "Training Batch: 427 Loss: 3258.695312\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Batch: 428 Loss: 3303.888916\n",
      "Training Batch: 429 Loss: 3417.787354\n",
      "Training Batch: 430 Loss: 3266.040039\n",
      "Training Batch: 431 Loss: 3310.628906\n",
      "Training Batch: 432 Loss: 3342.255859\n",
      "Training Batch: 433 Loss: 3530.336914\n",
      "Training Batch: 434 Loss: 3683.387207\n",
      "Training Batch: 435 Loss: 3678.249512\n",
      "Training Batch: 436 Loss: 3434.717285\n",
      "Training Batch: 437 Loss: 3465.229004\n",
      "Training Batch: 438 Loss: 3546.773438\n",
      "Training Batch: 439 Loss: 3525.218750\n",
      "Training Batch: 440 Loss: 3293.860840\n",
      "Training Batch: 441 Loss: 3281.200684\n",
      "Training Batch: 442 Loss: 3310.361328\n",
      "Training Batch: 443 Loss: 3334.930664\n",
      "Training Batch: 444 Loss: 3871.893555\n",
      "Training Batch: 445 Loss: 3845.175537\n",
      "Training Batch: 446 Loss: 3438.062012\n",
      "Training Batch: 447 Loss: 3349.584961\n",
      "Training Batch: 448 Loss: 3275.259766\n",
      "Training Batch: 449 Loss: 3332.094238\n",
      "Training Batch: 450 Loss: 3367.520996\n",
      "Training Batch: 451 Loss: 3302.023438\n",
      "Training Batch: 452 Loss: 3364.921387\n",
      "Training Batch: 453 Loss: 3336.866943\n",
      "Training Batch: 454 Loss: 3409.393799\n",
      "Training Batch: 455 Loss: 3295.854492\n",
      "Training Batch: 456 Loss: 3422.154541\n",
      "Training Batch: 457 Loss: 3444.742188\n",
      "Training Batch: 458 Loss: 3364.605469\n",
      "Training Batch: 459 Loss: 3384.419434\n",
      "Training Batch: 460 Loss: 3332.434082\n",
      "Training Batch: 461 Loss: 3364.628418\n",
      "Training Batch: 462 Loss: 3262.156738\n",
      "Training Batch: 463 Loss: 3443.929932\n",
      "Training Batch: 464 Loss: 3421.269043\n",
      "Training Batch: 465 Loss: 3316.179688\n",
      "Training Batch: 466 Loss: 3363.420654\n",
      "Training Batch: 467 Loss: 3447.558350\n",
      "Training Batch: 468 Loss: 4476.013184\n",
      "Training Batch: 469 Loss: 3830.933594\n",
      "Training Batch: 470 Loss: 3386.413086\n",
      "Training Batch: 471 Loss: 3352.039307\n",
      "Training Batch: 472 Loss: 3373.210693\n",
      "Training Batch: 473 Loss: 3263.335938\n",
      "Training Batch: 474 Loss: 3464.694824\n",
      "Training Batch: 475 Loss: 3355.969727\n",
      "Training Batch: 476 Loss: 3387.094482\n",
      "Training Batch: 477 Loss: 3503.869629\n",
      "Training Batch: 478 Loss: 3494.414795\n",
      "Training Batch: 479 Loss: 3333.884277\n",
      "Training Batch: 480 Loss: 3509.143066\n",
      "Training Batch: 481 Loss: 3323.766846\n",
      "Training Batch: 482 Loss: 3273.650391\n",
      "Training Batch: 483 Loss: 3315.818604\n",
      "Training Batch: 484 Loss: 3426.723877\n",
      "Training Batch: 485 Loss: 3392.337402\n",
      "Training Batch: 486 Loss: 3380.359863\n",
      "Training Batch: 487 Loss: 3311.572021\n",
      "Training Batch: 488 Loss: 3380.045166\n",
      "Training Batch: 489 Loss: 3401.614258\n",
      "Training Batch: 490 Loss: 3389.390137\n",
      "Training Batch: 491 Loss: 3423.803467\n",
      "Training Batch: 492 Loss: 3491.327637\n",
      "Training Batch: 493 Loss: 3275.145508\n",
      "Training Batch: 494 Loss: 3445.047852\n",
      "Training Batch: 495 Loss: 3408.638184\n",
      "Training Batch: 496 Loss: 3385.840820\n",
      "Training Batch: 497 Loss: 3481.616943\n",
      "Training Batch: 498 Loss: 3514.120605\n",
      "Training Batch: 499 Loss: 3397.486816\n",
      "Training Batch: 500 Loss: 3344.162598\n",
      "Training Batch: 501 Loss: 3390.359375\n",
      "Training Batch: 502 Loss: 3313.240723\n",
      "Training Batch: 503 Loss: 3465.490967\n",
      "Training Batch: 504 Loss: 3336.220703\n",
      "Training Batch: 505 Loss: 3317.295410\n",
      "Training Batch: 506 Loss: 3437.233154\n",
      "Training Batch: 507 Loss: 3533.495361\n",
      "Training Batch: 508 Loss: 3261.274902\n",
      "Training Batch: 509 Loss: 3315.107422\n",
      "Training Batch: 510 Loss: 3336.345947\n",
      "Training Batch: 511 Loss: 3229.876953\n",
      "Training Batch: 512 Loss: 3299.775391\n",
      "Training Batch: 513 Loss: 3332.371094\n",
      "Training Batch: 514 Loss: 3420.024414\n",
      "Training Batch: 515 Loss: 3211.515625\n",
      "Training Batch: 516 Loss: 3346.074707\n",
      "Training Batch: 517 Loss: 3308.516357\n",
      "Training Batch: 518 Loss: 3291.099365\n",
      "Training Batch: 519 Loss: 3429.056152\n",
      "Training Batch: 520 Loss: 3403.790039\n",
      "Training Batch: 521 Loss: 3432.041260\n",
      "Training Batch: 522 Loss: 3412.378174\n",
      "Training Batch: 523 Loss: 3628.909912\n",
      "Training Batch: 524 Loss: 3585.445312\n",
      "Training Batch: 525 Loss: 3572.035645\n",
      "Training Batch: 526 Loss: 3568.770508\n",
      "Training Batch: 527 Loss: 3581.676025\n",
      "Training Batch: 528 Loss: 3326.781738\n",
      "Training Batch: 529 Loss: 3256.206543\n",
      "Training Batch: 530 Loss: 3425.383789\n",
      "Training Batch: 531 Loss: 3349.746094\n",
      "Training Batch: 532 Loss: 3344.824707\n",
      "Training Batch: 533 Loss: 3338.763672\n",
      "Training Batch: 534 Loss: 3283.582520\n",
      "Training Batch: 535 Loss: 3346.125977\n",
      "Training Batch: 536 Loss: 3334.363770\n",
      "Training Batch: 537 Loss: 3203.159668\n",
      "Training Batch: 538 Loss: 3330.383301\n",
      "Training Batch: 539 Loss: 3297.907471\n",
      "Training Batch: 540 Loss: 3267.876709\n",
      "Training Batch: 541 Loss: 3311.752441\n",
      "Training Batch: 542 Loss: 3230.606445\n",
      "Training Batch: 543 Loss: 3390.616943\n",
      "Training Batch: 544 Loss: 3248.158203\n",
      "Training Batch: 545 Loss: 3404.334717\n",
      "Training Batch: 546 Loss: 3341.979980\n",
      "Training Batch: 547 Loss: 3369.205811\n",
      "Training Batch: 548 Loss: 3268.216797\n",
      "Training Batch: 549 Loss: 3367.189453\n",
      "Training Batch: 550 Loss: 3317.816650\n",
      "Training Batch: 551 Loss: 3279.268799\n",
      "Training Batch: 552 Loss: 3308.350586\n",
      "Training Batch: 553 Loss: 3358.756348\n",
      "Training Batch: 554 Loss: 3175.775391\n",
      "Training Batch: 555 Loss: 3252.280273\n",
      "Training Batch: 556 Loss: 3822.180664\n",
      "Training Batch: 557 Loss: 3450.753418\n",
      "Training Batch: 558 Loss: 3311.603516\n",
      "Training Batch: 559 Loss: 3253.727539\n",
      "Training Batch: 560 Loss: 3289.829346\n",
      "Training Batch: 561 Loss: 3413.323730\n",
      "Training Batch: 562 Loss: 3284.453613\n",
      "Training Batch: 563 Loss: 3274.016113\n",
      "Training Batch: 564 Loss: 3330.432129\n",
      "Training Batch: 565 Loss: 3390.051270\n",
      "Training Batch: 566 Loss: 3622.643555\n",
      "Training Batch: 567 Loss: 3427.757324\n",
      "Training Batch: 568 Loss: 3540.506836\n",
      "Training Batch: 569 Loss: 3629.232910\n",
      "Training Batch: 570 Loss: 3424.666504\n",
      "Training Batch: 571 Loss: 3344.135742\n",
      "Training Batch: 572 Loss: 3290.486328\n",
      "Training Batch: 573 Loss: 3528.582275\n",
      "Training Batch: 574 Loss: 3798.873779\n",
      "Training Batch: 575 Loss: 3473.173828\n",
      "Training Batch: 576 Loss: 3505.142578\n",
      "Training Batch: 577 Loss: 3674.977539\n",
      "Training Batch: 578 Loss: 3333.605469\n",
      "Training Batch: 579 Loss: 3343.434570\n",
      "Training Batch: 580 Loss: 3306.862793\n",
      "Training Batch: 581 Loss: 3445.133789\n",
      "Training Batch: 582 Loss: 3420.139404\n",
      "Training Batch: 583 Loss: 3452.608887\n",
      "Training Batch: 584 Loss: 3300.480469\n",
      "Training Batch: 585 Loss: 3350.598389\n",
      "Training Batch: 586 Loss: 3375.796875\n",
      "Training Batch: 587 Loss: 3373.269531\n",
      "Training Batch: 588 Loss: 3457.940186\n",
      "Training Batch: 589 Loss: 3498.744385\n",
      "Training Batch: 590 Loss: 3302.864746\n",
      "Training Batch: 591 Loss: 3285.817383\n",
      "Training Batch: 592 Loss: 3486.305664\n",
      "Training Batch: 593 Loss: 3534.841309\n",
      "Training Batch: 594 Loss: 3433.445557\n",
      "Training Batch: 595 Loss: 3831.992188\n",
      "Training Batch: 596 Loss: 3628.095703\n",
      "Training Batch: 597 Loss: 3316.321777\n",
      "Training Batch: 598 Loss: 3430.408691\n",
      "Training Batch: 599 Loss: 3346.856445\n",
      "Training Batch: 600 Loss: 3437.937500\n",
      "Training Batch: 601 Loss: 3384.428467\n",
      "Training Batch: 602 Loss: 3384.043457\n",
      "Training Batch: 603 Loss: 3283.606445\n",
      "Training Batch: 604 Loss: 3535.450928\n",
      "Training Batch: 605 Loss: 3978.627930\n",
      "Training Batch: 606 Loss: 3515.615234\n",
      "Training Batch: 607 Loss: 3546.706055\n",
      "Training Batch: 608 Loss: 3432.499023\n",
      "Training Batch: 609 Loss: 3342.449951\n",
      "Training Batch: 610 Loss: 3362.314453\n",
      "Training Batch: 611 Loss: 3277.306396\n",
      "Training Batch: 612 Loss: 3334.726074\n",
      "Training Batch: 613 Loss: 3345.021973\n",
      "Training Batch: 614 Loss: 3306.326172\n",
      "Training Batch: 615 Loss: 3225.703613\n",
      "Training Batch: 616 Loss: 3263.600830\n",
      "Training Batch: 617 Loss: 3275.443115\n",
      "Training Batch: 618 Loss: 3348.780518\n",
      "Training Batch: 619 Loss: 3434.967773\n",
      "Training Batch: 620 Loss: 3385.349365\n",
      "Training Batch: 621 Loss: 3294.887207\n",
      "Training Batch: 622 Loss: 3313.563721\n",
      "Training Batch: 623 Loss: 3698.417969\n",
      "Training Batch: 624 Loss: 3439.513184\n",
      "Training Batch: 625 Loss: 3509.743164\n",
      "Training Batch: 626 Loss: 3473.231201\n",
      "Training Batch: 627 Loss: 3298.983887\n",
      "Training Batch: 628 Loss: 3456.269531\n",
      "Training Batch: 629 Loss: 3478.120850\n",
      "Training Batch: 630 Loss: 3373.025879\n",
      "Training Batch: 631 Loss: 3284.116699\n",
      "Training Batch: 632 Loss: 3495.694824\n",
      "Training Batch: 633 Loss: 3271.576660\n",
      "Training Batch: 634 Loss: 3288.364258\n",
      "Training Batch: 635 Loss: 3299.446289\n",
      "Training Batch: 636 Loss: 3448.120117\n",
      "Training Batch: 637 Loss: 3467.853516\n",
      "Training Batch: 638 Loss: 3403.809570\n",
      "Training Batch: 639 Loss: 3660.634277\n",
      "Training Batch: 640 Loss: 3309.680908\n",
      "Training Batch: 641 Loss: 3312.594727\n",
      "Training Batch: 642 Loss: 3381.996582\n",
      "Training Batch: 643 Loss: 3179.641357\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Batch: 644 Loss: 3429.481445\n",
      "Training Batch: 645 Loss: 3394.388184\n",
      "Training Batch: 646 Loss: 3733.726562\n",
      "Training Batch: 647 Loss: 3331.755859\n",
      "Training Batch: 648 Loss: 3394.990723\n",
      "Training Batch: 649 Loss: 3298.943848\n",
      "Training Batch: 650 Loss: 3521.161621\n",
      "Training Batch: 651 Loss: 3355.589844\n",
      "Training Batch: 652 Loss: 3268.165039\n",
      "Training Batch: 653 Loss: 3320.614258\n",
      "Training Batch: 654 Loss: 3362.702637\n",
      "Training Batch: 655 Loss: 3447.791992\n",
      "Training Batch: 656 Loss: 3402.151367\n",
      "Training Batch: 657 Loss: 3296.804688\n",
      "Training Batch: 658 Loss: 3651.032227\n",
      "Training Batch: 659 Loss: 3359.313477\n",
      "Training Batch: 660 Loss: 3293.378418\n",
      "Training Batch: 661 Loss: 3299.638672\n",
      "Training Batch: 662 Loss: 3306.432129\n",
      "Training Batch: 663 Loss: 3264.521484\n",
      "Training Batch: 664 Loss: 3299.742432\n",
      "Training Batch: 665 Loss: 3443.785645\n",
      "Training Batch: 666 Loss: 3262.198730\n",
      "Training Batch: 667 Loss: 3210.028809\n",
      "Training Batch: 668 Loss: 3200.383545\n",
      "Training Batch: 669 Loss: 3247.158203\n",
      "Training Batch: 670 Loss: 3270.652100\n",
      "Training Batch: 671 Loss: 3416.180664\n",
      "Training Batch: 672 Loss: 3320.368164\n",
      "Training Batch: 673 Loss: 3335.540039\n",
      "Training Batch: 674 Loss: 3304.721680\n",
      "Training Batch: 675 Loss: 3331.916748\n",
      "Training Batch: 676 Loss: 3268.523926\n",
      "Training Batch: 677 Loss: 3264.871582\n",
      "Training Batch: 678 Loss: 3293.802734\n",
      "Training Batch: 679 Loss: 3286.495117\n",
      "Training Batch: 680 Loss: 3279.715820\n",
      "Training Batch: 681 Loss: 3341.271240\n",
      "Training Batch: 682 Loss: 3301.716309\n",
      "Training Batch: 683 Loss: 3472.806885\n",
      "Training Batch: 684 Loss: 3221.961182\n",
      "Training Batch: 685 Loss: 3226.622070\n",
      "Training Batch: 686 Loss: 3349.233154\n",
      "Training Batch: 687 Loss: 3268.639404\n",
      "Training Batch: 688 Loss: 3315.196777\n",
      "Training Batch: 689 Loss: 3522.731934\n",
      "Training Batch: 690 Loss: 3208.689941\n",
      "Training Batch: 691 Loss: 3402.401611\n",
      "Training Batch: 692 Loss: 3583.178223\n",
      "Training Batch: 693 Loss: 3429.114746\n",
      "Training Batch: 694 Loss: 3314.234375\n",
      "Training Batch: 695 Loss: 3189.528076\n",
      "Training Batch: 696 Loss: 3390.916992\n",
      "Training Batch: 697 Loss: 3485.020508\n",
      "Training Batch: 698 Loss: 3413.349609\n",
      "Training Batch: 699 Loss: 3476.037354\n",
      "Training Batch: 700 Loss: 3368.335449\n",
      "Training Batch: 701 Loss: 3433.470215\n",
      "Training Batch: 702 Loss: 3388.376465\n",
      "Training Batch: 703 Loss: 3428.128418\n",
      "Training Batch: 704 Loss: 3291.987305\n",
      "Training Batch: 705 Loss: 3315.917969\n",
      "Training Batch: 706 Loss: 3307.886719\n",
      "Training Batch: 707 Loss: 3314.558838\n",
      "Training Batch: 708 Loss: 3318.151367\n",
      "Training Batch: 709 Loss: 3424.969238\n",
      "Training Batch: 710 Loss: 3275.123291\n",
      "Training Batch: 711 Loss: 3246.827881\n",
      "Training Batch: 712 Loss: 3450.994141\n",
      "Training Batch: 713 Loss: 3418.457520\n",
      "Training Batch: 714 Loss: 3409.971191\n",
      "Training Batch: 715 Loss: 3199.565430\n",
      "Training Batch: 716 Loss: 3287.487793\n",
      "Training Batch: 717 Loss: 3450.726562\n",
      "Training Batch: 718 Loss: 3471.742188\n",
      "Training Batch: 719 Loss: 3280.579590\n",
      "Training Batch: 720 Loss: 3455.660156\n",
      "Training Batch: 721 Loss: 3302.743652\n",
      "Training Batch: 722 Loss: 3407.559570\n",
      "Training Batch: 723 Loss: 3352.254883\n",
      "Training Batch: 724 Loss: 3312.529297\n",
      "Training Batch: 725 Loss: 3421.331055\n",
      "Training Batch: 726 Loss: 3551.392090\n",
      "Training Batch: 727 Loss: 3323.957031\n",
      "Training Batch: 728 Loss: 3308.831543\n",
      "Training Batch: 729 Loss: 3206.656250\n",
      "Training Batch: 730 Loss: 3661.029297\n",
      "Training Batch: 731 Loss: 3324.981689\n",
      "Training Batch: 732 Loss: 3386.860352\n",
      "Training Batch: 733 Loss: 3346.608887\n",
      "Training Batch: 734 Loss: 3274.385010\n",
      "Training Batch: 735 Loss: 3317.651855\n",
      "Training Batch: 736 Loss: 3465.617676\n",
      "Training Batch: 737 Loss: 3383.306152\n",
      "Training Batch: 738 Loss: 3373.337646\n",
      "Training Batch: 739 Loss: 3401.558594\n",
      "Training Batch: 740 Loss: 3316.666504\n",
      "Training Batch: 741 Loss: 3324.807617\n",
      "Training Batch: 742 Loss: 3161.442871\n",
      "Training Batch: 743 Loss: 3478.855957\n",
      "Training Batch: 744 Loss: 3373.710449\n",
      "Training Batch: 745 Loss: 3466.940918\n",
      "Training Batch: 746 Loss: 3488.758789\n",
      "Training Batch: 747 Loss: 3584.951172\n",
      "Training Batch: 748 Loss: 3437.315918\n",
      "Training Batch: 749 Loss: 3270.406250\n",
      "Training Batch: 750 Loss: 3245.062988\n",
      "Training Batch: 751 Loss: 3665.081543\n",
      "Training Batch: 752 Loss: 3486.857422\n",
      "Training Batch: 753 Loss: 3625.831543\n",
      "Training Batch: 754 Loss: 3399.218994\n",
      "Training Batch: 755 Loss: 3364.434814\n",
      "Training Batch: 756 Loss: 3338.782715\n",
      "Training Batch: 757 Loss: 3476.596680\n",
      "Training Batch: 758 Loss: 3335.220215\n",
      "Training Batch: 759 Loss: 3230.716309\n",
      "Training Batch: 760 Loss: 3413.831055\n",
      "Training Batch: 761 Loss: 3259.960938\n",
      "Training Batch: 762 Loss: 3205.862061\n",
      "Training Batch: 763 Loss: 3347.507324\n",
      "Training Batch: 764 Loss: 3328.097168\n",
      "Training Batch: 765 Loss: 3344.706055\n",
      "Training Batch: 766 Loss: 3310.091797\n",
      "Training Batch: 767 Loss: 3347.769043\n",
      "Training Batch: 768 Loss: 3274.326172\n",
      "Training Batch: 769 Loss: 3255.382080\n",
      "Training Batch: 770 Loss: 3286.030273\n",
      "Training Batch: 771 Loss: 3442.264893\n",
      "Training Batch: 772 Loss: 3264.718262\n",
      "Training Batch: 773 Loss: 3376.668945\n",
      "Training Batch: 774 Loss: 3254.854736\n",
      "Training Batch: 775 Loss: 3331.566895\n",
      "Training Batch: 776 Loss: 3273.126953\n",
      "Training Batch: 777 Loss: 3321.187988\n",
      "Training Batch: 778 Loss: 3378.108887\n",
      "Training Batch: 779 Loss: 3481.351074\n",
      "Training Batch: 780 Loss: 3255.395020\n",
      "Training Batch: 781 Loss: 3490.864746\n",
      "Training Batch: 782 Loss: 3691.211914\n",
      "Training Batch: 783 Loss: 3359.512695\n",
      "Training Batch: 784 Loss: 3333.027344\n",
      "Training Batch: 785 Loss: 3212.622070\n",
      "Training Batch: 786 Loss: 3369.376953\n",
      "Training Batch: 787 Loss: 3265.676025\n",
      "Training Batch: 788 Loss: 3348.689697\n",
      "Training Batch: 789 Loss: 3231.597168\n",
      "Training Batch: 790 Loss: 3269.164062\n",
      "Training Batch: 791 Loss: 3359.709961\n",
      "Training Batch: 792 Loss: 3311.398438\n",
      "Training Batch: 793 Loss: 3366.232422\n",
      "Training Batch: 794 Loss: 3248.625000\n",
      "Training Batch: 795 Loss: 3581.821533\n",
      "Training Batch: 796 Loss: 3445.491699\n",
      "Training Batch: 797 Loss: 3289.430664\n",
      "Training Batch: 798 Loss: 3387.135010\n",
      "Training Batch: 799 Loss: 3440.030273\n",
      "Training Batch: 800 Loss: 3413.226074\n",
      "Training Batch: 801 Loss: 3516.460693\n",
      "Training Batch: 802 Loss: 3377.697754\n",
      "Training Batch: 803 Loss: 3473.195312\n",
      "Training Batch: 804 Loss: 3408.987793\n",
      "Training Batch: 805 Loss: 3281.145996\n",
      "Training Batch: 806 Loss: 3382.028320\n",
      "Training Batch: 807 Loss: 3310.450195\n",
      "Training Batch: 808 Loss: 3621.509277\n",
      "Training Batch: 809 Loss: 3416.475586\n",
      "Training Batch: 810 Loss: 3318.951660\n",
      "Training Batch: 811 Loss: 3330.894531\n",
      "Training Batch: 812 Loss: 3221.422119\n",
      "Training Batch: 813 Loss: 3261.416748\n",
      "Training Batch: 814 Loss: 3225.493652\n",
      "Training Batch: 815 Loss: 3598.134766\n",
      "Training Batch: 816 Loss: 3561.630859\n",
      "Training Batch: 817 Loss: 3316.095215\n",
      "Training Batch: 818 Loss: 3583.595215\n",
      "Training Batch: 819 Loss: 3346.749268\n",
      "Training Batch: 820 Loss: 3283.003906\n",
      "Training Batch: 821 Loss: 3404.312500\n",
      "Training Batch: 822 Loss: 3347.085693\n",
      "Training Batch: 823 Loss: 3366.236328\n",
      "Training Batch: 824 Loss: 3310.353027\n",
      "Training Batch: 825 Loss: 3285.414062\n",
      "Training Batch: 826 Loss: 3383.276123\n",
      "Training Batch: 827 Loss: 3498.125732\n",
      "Training Batch: 828 Loss: 3435.229980\n",
      "Training Batch: 829 Loss: 3815.193848\n",
      "Training Batch: 830 Loss: 3272.015625\n",
      "Training Batch: 831 Loss: 3454.948242\n",
      "Training Batch: 832 Loss: 3223.619141\n",
      "Training Batch: 833 Loss: 3491.581787\n",
      "Training Batch: 834 Loss: 3210.231934\n",
      "Training Batch: 835 Loss: 3577.267578\n",
      "Training Batch: 836 Loss: 3385.293213\n",
      "Training Batch: 837 Loss: 3297.707520\n",
      "Training Batch: 838 Loss: 3258.784668\n",
      "Training Batch: 839 Loss: 3313.537109\n",
      "Training Batch: 840 Loss: 3373.134033\n",
      "Training Batch: 841 Loss: 3418.691406\n",
      "Training Batch: 842 Loss: 3270.009766\n",
      "Training Batch: 843 Loss: 3374.547852\n",
      "Training Batch: 844 Loss: 3223.499268\n",
      "Training Batch: 845 Loss: 3229.748535\n",
      "Training Batch: 846 Loss: 3320.291992\n",
      "Training Batch: 847 Loss: 3371.647949\n",
      "Training Batch: 848 Loss: 3313.123535\n",
      "Training Batch: 849 Loss: 3419.910645\n",
      "Training Batch: 850 Loss: 3352.391602\n",
      "Training Batch: 851 Loss: 3329.241211\n",
      "Training Batch: 852 Loss: 3331.150146\n",
      "Training Batch: 853 Loss: 3398.246582\n",
      "Training Batch: 854 Loss: 3229.496826\n",
      "Training Batch: 855 Loss: 3264.802734\n",
      "Training Batch: 856 Loss: 3210.342285\n",
      "Training Batch: 857 Loss: 3329.457764\n",
      "Training Batch: 858 Loss: 3378.219727\n",
      "Training Batch: 859 Loss: 3368.106445\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Batch: 860 Loss: 3392.619385\n",
      "Training Batch: 861 Loss: 3400.238770\n",
      "Training Batch: 862 Loss: 3301.310303\n",
      "Training Batch: 863 Loss: 3302.047363\n",
      "Training Batch: 864 Loss: 3307.877930\n",
      "Training Batch: 865 Loss: 3236.140625\n",
      "Training Batch: 866 Loss: 3238.948242\n",
      "Training Batch: 867 Loss: 3355.049316\n",
      "Training Batch: 868 Loss: 3251.776367\n",
      "Training Batch: 869 Loss: 3183.742676\n",
      "Training Batch: 870 Loss: 3525.205078\n",
      "Training Batch: 871 Loss: 3304.306152\n",
      "Training Batch: 872 Loss: 3357.940918\n",
      "Training Batch: 873 Loss: 3259.603516\n",
      "Training Batch: 874 Loss: 3236.533203\n",
      "Training Batch: 875 Loss: 3359.666992\n",
      "Training Batch: 876 Loss: 3340.737305\n",
      "Training Batch: 877 Loss: 3194.103027\n",
      "Training Batch: 878 Loss: 3279.006836\n",
      "Training Batch: 879 Loss: 3231.575684\n",
      "Training Batch: 880 Loss: 3332.927490\n",
      "Training Batch: 881 Loss: 3214.110352\n",
      "Training Batch: 882 Loss: 3371.788330\n",
      "Training Batch: 883 Loss: 3503.441895\n",
      "Training Batch: 884 Loss: 3272.228271\n",
      "Training Batch: 885 Loss: 3224.660400\n",
      "Training Batch: 886 Loss: 3283.775879\n",
      "Training Batch: 887 Loss: 3412.666992\n",
      "Training Batch: 888 Loss: 3342.180176\n",
      "Training Batch: 889 Loss: 3240.259277\n",
      "Training Batch: 890 Loss: 3345.257324\n",
      "Training Batch: 891 Loss: 3744.661621\n",
      "Training Batch: 892 Loss: 3242.426270\n",
      "Training Batch: 893 Loss: 3449.965332\n",
      "Training Batch: 894 Loss: 3471.687500\n",
      "Training Batch: 895 Loss: 3268.101562\n",
      "Training Batch: 896 Loss: 3366.965332\n",
      "Training Batch: 897 Loss: 3366.740234\n",
      "Training Batch: 898 Loss: 3490.899902\n",
      "Training Batch: 899 Loss: 3810.426270\n",
      "Training Batch: 900 Loss: 3416.135742\n",
      "Training Batch: 901 Loss: 3509.345703\n",
      "Training Batch: 902 Loss: 3977.846924\n",
      "Training Batch: 903 Loss: 3485.522461\n",
      "Training Batch: 904 Loss: 3362.022461\n",
      "Training Batch: 905 Loss: 3256.528320\n",
      "Training Batch: 906 Loss: 3202.541016\n",
      "Training Batch: 907 Loss: 3222.493164\n",
      "Training Batch: 908 Loss: 3332.249756\n",
      "Training Batch: 909 Loss: 3253.791016\n",
      "Training Batch: 910 Loss: 3247.857178\n",
      "Training Batch: 911 Loss: 3267.053955\n",
      "Training Batch: 912 Loss: 3211.806152\n",
      "Training Batch: 913 Loss: 3389.795166\n",
      "Training Batch: 914 Loss: 3297.490234\n",
      "Training Batch: 915 Loss: 3288.214844\n",
      "Training Batch: 916 Loss: 3382.289551\n",
      "Training Batch: 917 Loss: 3360.918945\n",
      "Training Batch: 918 Loss: 3538.292480\n",
      "Training Batch: 919 Loss: 3303.897217\n",
      "Training Batch: 920 Loss: 3592.511230\n",
      "Training Batch: 921 Loss: 3599.924316\n",
      "Training Batch: 922 Loss: 3434.345947\n",
      "Training Batch: 923 Loss: 3503.867676\n",
      "Training Batch: 924 Loss: 3566.087891\n",
      "Training Batch: 925 Loss: 3409.708496\n",
      "Training Batch: 926 Loss: 3410.758057\n",
      "Training Batch: 927 Loss: 3368.710449\n",
      "Training Batch: 928 Loss: 3569.081055\n",
      "Training Batch: 929 Loss: 3491.395508\n",
      "Training Batch: 930 Loss: 3609.099121\n",
      "Training Batch: 931 Loss: 3424.809082\n",
      "Training Batch: 932 Loss: 3275.046875\n",
      "Training Batch: 933 Loss: 3282.306152\n",
      "Training Batch: 934 Loss: 3287.006104\n",
      "Training Batch: 935 Loss: 3262.978271\n",
      "Training Batch: 936 Loss: 3285.683105\n",
      "Training Batch: 937 Loss: 3424.175293\n",
      "Training Batch: 938 Loss: 3321.560791\n",
      "Training Batch: 939 Loss: 3493.162109\n",
      "Training Batch: 940 Loss: 3445.173340\n",
      "Training Batch: 941 Loss: 3543.518555\n",
      "Training Batch: 942 Loss: 3454.489258\n",
      "Training Batch: 943 Loss: 3481.994873\n",
      "Training Batch: 944 Loss: 3335.623535\n",
      "Training Batch: 945 Loss: 3247.782959\n",
      "Training Batch: 946 Loss: 3258.573242\n",
      "Training Batch: 947 Loss: 3306.081543\n",
      "Training Batch: 948 Loss: 3261.519043\n",
      "Training Batch: 949 Loss: 3363.343994\n",
      "Training Batch: 950 Loss: 3538.958984\n",
      "Training Batch: 951 Loss: 3301.665283\n",
      "Training Batch: 952 Loss: 3268.734375\n",
      "Training Batch: 953 Loss: 3532.504395\n",
      "Training Batch: 954 Loss: 3212.673340\n",
      "Training Batch: 955 Loss: 3260.635254\n",
      "Training Batch: 956 Loss: 3265.254395\n",
      "Training Batch: 957 Loss: 3213.981934\n",
      "Training Batch: 958 Loss: 3339.715332\n",
      "Training Batch: 959 Loss: 3400.284668\n",
      "Training Batch: 960 Loss: 3194.670410\n",
      "Training Batch: 961 Loss: 3254.611816\n",
      "Training Batch: 962 Loss: 3210.714844\n",
      "Training Batch: 963 Loss: 3269.948730\n",
      "Training Batch: 964 Loss: 3405.841064\n",
      "Training Batch: 965 Loss: 3407.664307\n",
      "Training Batch: 966 Loss: 3392.972168\n",
      "Training Batch: 967 Loss: 3343.370117\n",
      "Training Batch: 968 Loss: 3333.671875\n",
      "Training Batch: 969 Loss: 3265.746826\n",
      "Training Batch: 970 Loss: 3392.550293\n",
      "Training Batch: 971 Loss: 3292.253418\n",
      "Training Batch: 972 Loss: 3391.161865\n",
      "Training Batch: 973 Loss: 3194.992676\n",
      "Training Batch: 974 Loss: 3361.612793\n",
      "Training Batch: 975 Loss: 3216.145996\n",
      "Training Batch: 976 Loss: 3327.252441\n",
      "Training Batch: 977 Loss: 3437.372559\n",
      "Training Batch: 978 Loss: 3347.785156\n",
      "Training Batch: 979 Loss: 3309.186279\n",
      "Training Batch: 980 Loss: 3360.642090\n",
      "Training Batch: 981 Loss: 3292.156738\n",
      "Training Batch: 982 Loss: 3323.451416\n",
      "Training Batch: 983 Loss: 3419.287598\n",
      "Training Batch: 984 Loss: 3289.115967\n",
      "Training Batch: 985 Loss: 3320.637939\n",
      "Training Batch: 986 Loss: 3327.626465\n",
      "Training Batch: 987 Loss: 3185.929443\n",
      "Training Batch: 988 Loss: 3271.468262\n",
      "Training Batch: 989 Loss: 3218.624756\n",
      "Training Batch: 990 Loss: 3283.859863\n",
      "Training Batch: 991 Loss: 3324.512451\n",
      "Training Batch: 992 Loss: 3303.849609\n",
      "Training Batch: 993 Loss: 3330.832764\n",
      "Training Batch: 994 Loss: 3252.240723\n",
      "Training Batch: 995 Loss: 3285.721680\n",
      "Training Batch: 996 Loss: 3317.983398\n",
      "Training Batch: 997 Loss: 3325.978516\n",
      "Training Batch: 998 Loss: 3310.262695\n",
      "Training Batch: 999 Loss: 3380.181152\n",
      "Training Batch: 1000 Loss: 3412.587891\n",
      "Training Batch: 1001 Loss: 3351.602051\n",
      "Training Batch: 1002 Loss: 3308.577393\n",
      "Training Batch: 1003 Loss: 3274.351074\n",
      "Training Batch: 1004 Loss: 3314.234863\n",
      "Training Batch: 1005 Loss: 3345.604980\n",
      "Training Batch: 1006 Loss: 3369.898438\n",
      "Training Batch: 1007 Loss: 3319.956055\n",
      "Training Batch: 1008 Loss: 3371.908203\n",
      "Training Batch: 1009 Loss: 3288.171875\n",
      "Training Batch: 1010 Loss: 3200.433105\n",
      "Training Batch: 1011 Loss: 3414.224854\n",
      "Training Batch: 1012 Loss: 3420.884766\n",
      "Training Batch: 1013 Loss: 3355.547607\n",
      "Training Batch: 1014 Loss: 3353.802979\n",
      "Training Batch: 1015 Loss: 3306.896973\n",
      "Training Batch: 1016 Loss: 3240.007812\n",
      "Training Batch: 1017 Loss: 3335.944824\n",
      "Training Batch: 1018 Loss: 3378.587646\n",
      "Training Batch: 1019 Loss: 3440.648438\n",
      "Training Batch: 1020 Loss: 3304.483887\n",
      "Training Batch: 1021 Loss: 3274.415039\n",
      "Training Batch: 1022 Loss: 3296.896240\n",
      "Training Batch: 1023 Loss: 3233.578613\n",
      "Training Batch: 1024 Loss: 3350.441895\n",
      "Training Batch: 1025 Loss: 3254.415527\n",
      "Training Batch: 1026 Loss: 3446.632324\n",
      "Training Batch: 1027 Loss: 3391.036377\n",
      "Training Batch: 1028 Loss: 3157.537109\n",
      "Training Batch: 1029 Loss: 3243.413574\n",
      "Training Batch: 1030 Loss: 3354.354736\n",
      "Training Batch: 1031 Loss: 3366.345215\n",
      "Training Batch: 1032 Loss: 3365.088135\n",
      "Training Batch: 1033 Loss: 3306.593994\n",
      "Training Batch: 1034 Loss: 3354.752930\n",
      "Training Batch: 1035 Loss: 3399.614746\n",
      "Training Batch: 1036 Loss: 3389.920654\n",
      "Training Batch: 1037 Loss: 3421.502197\n",
      "Training Batch: 1038 Loss: 3267.484863\n",
      "Training Batch: 1039 Loss: 3358.104004\n",
      "Training Batch: 1040 Loss: 3237.826172\n",
      "Training Batch: 1041 Loss: 3455.050781\n",
      "Training Batch: 1042 Loss: 3211.110107\n",
      "Training Batch: 1043 Loss: 3350.475098\n",
      "Training Batch: 1044 Loss: 3519.677246\n",
      "Training Batch: 1045 Loss: 3284.840820\n",
      "Training Batch: 1046 Loss: 3459.706543\n",
      "Training Batch: 1047 Loss: 3267.769043\n",
      "Training Batch: 1048 Loss: 3342.052246\n",
      "Training Batch: 1049 Loss: 3386.368164\n",
      "Training Batch: 1050 Loss: 3313.373047\n",
      "Training Batch: 1051 Loss: 3671.761719\n",
      "Training Batch: 1052 Loss: 3288.115234\n",
      "Training Batch: 1053 Loss: 3239.053467\n",
      "Training Batch: 1054 Loss: 3332.050537\n",
      "Training Batch: 1055 Loss: 3438.413574\n",
      "Training Batch: 1056 Loss: 3435.244629\n",
      "Training Batch: 1057 Loss: 3428.648926\n",
      "Training Batch: 1058 Loss: 3319.125977\n",
      "Training Batch: 1059 Loss: 3375.372314\n",
      "Training Batch: 1060 Loss: 3355.357178\n",
      "Training Batch: 1061 Loss: 3330.619629\n",
      "Training Batch: 1062 Loss: 3410.980957\n",
      "Training Batch: 1063 Loss: 3348.350830\n",
      "Training Batch: 1064 Loss: 3378.973877\n",
      "Training Batch: 1065 Loss: 3506.554932\n",
      "Training Batch: 1066 Loss: 3633.993164\n",
      "Training Batch: 1067 Loss: 3287.437012\n",
      "Training Batch: 1068 Loss: 3910.594238\n",
      "Training Batch: 1069 Loss: 4060.416260\n",
      "Training Batch: 1070 Loss: 3600.147949\n",
      "Training Batch: 1071 Loss: 3467.286133\n",
      "Training Batch: 1072 Loss: 3355.663330\n",
      "Training Batch: 1073 Loss: 3461.829102\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Batch: 1074 Loss: 3582.768799\n",
      "Training Batch: 1075 Loss: 3427.126465\n",
      "Training Batch: 1076 Loss: 3256.963379\n",
      "Training Batch: 1077 Loss: 3267.335449\n",
      "Training Batch: 1078 Loss: 3272.708008\n",
      "Training Batch: 1079 Loss: 3416.715820\n",
      "Training Batch: 1080 Loss: 3342.314941\n",
      "Training Batch: 1081 Loss: 3329.451660\n",
      "Training Batch: 1082 Loss: 3322.637451\n",
      "Training Batch: 1083 Loss: 3334.280273\n",
      "Training Batch: 1084 Loss: 3250.686279\n",
      "Training Batch: 1085 Loss: 3556.648438\n",
      "Training Batch: 1086 Loss: 3500.319092\n",
      "Training Batch: 1087 Loss: 3245.294922\n",
      "Training Batch: 1088 Loss: 3425.864746\n",
      "Training Batch: 1089 Loss: 3625.410645\n",
      "Training Batch: 1090 Loss: 3511.909912\n",
      "Training Batch: 1091 Loss: 3741.455566\n",
      "Training Batch: 1092 Loss: 3399.128418\n",
      "Training Batch: 1093 Loss: 3372.430176\n",
      "Training Batch: 1094 Loss: 3284.112793\n",
      "Training Batch: 1095 Loss: 3405.569824\n",
      "Training Batch: 1096 Loss: 3326.101074\n",
      "Training Batch: 1097 Loss: 3256.159668\n",
      "Training Batch: 1098 Loss: 3228.219238\n",
      "Training Batch: 1099 Loss: 3269.670898\n",
      "Training Batch: 1100 Loss: 3363.868652\n",
      "Training Batch: 1101 Loss: 3324.684570\n",
      "Training Batch: 1102 Loss: 3417.035889\n",
      "Training Batch: 1103 Loss: 3243.768555\n",
      "Training Batch: 1104 Loss: 3310.715820\n",
      "Training Batch: 1105 Loss: 3248.770996\n",
      "Training Batch: 1106 Loss: 3365.015625\n",
      "Training Batch: 1107 Loss: 3343.156250\n",
      "Training Batch: 1108 Loss: 3425.801025\n",
      "Training Batch: 1109 Loss: 3309.014160\n",
      "Training Batch: 1110 Loss: 3365.442383\n",
      "Training Batch: 1111 Loss: 3317.357910\n",
      "Training Batch: 1112 Loss: 3302.514648\n",
      "Training Batch: 1113 Loss: 3299.059082\n",
      "Training Batch: 1114 Loss: 3332.972168\n",
      "Training Batch: 1115 Loss: 3244.767578\n",
      "Training Batch: 1116 Loss: 3504.601562\n",
      "Training Batch: 1117 Loss: 3328.033203\n",
      "Training Batch: 1118 Loss: 3541.894287\n",
      "Training Batch: 1119 Loss: 3331.511230\n",
      "Training Batch: 1120 Loss: 3289.403320\n",
      "Training Batch: 1121 Loss: 3315.976562\n",
      "Training Batch: 1122 Loss: 3464.355957\n",
      "Training Batch: 1123 Loss: 3344.088379\n",
      "Training Batch: 1124 Loss: 3396.913330\n",
      "Training Batch: 1125 Loss: 3349.963623\n",
      "Training Batch: 1126 Loss: 3308.844727\n",
      "Training Batch: 1127 Loss: 3339.524902\n",
      "Training Batch: 1128 Loss: 3341.002441\n",
      "Training Batch: 1129 Loss: 3429.172852\n",
      "Training Batch: 1130 Loss: 3335.350586\n",
      "Training Batch: 1131 Loss: 3339.599365\n",
      "Training Batch: 1132 Loss: 3365.425293\n",
      "Training Batch: 1133 Loss: 3263.768555\n",
      "Training Batch: 1134 Loss: 3445.259277\n",
      "Training Batch: 1135 Loss: 3304.285889\n",
      "Training Batch: 1136 Loss: 3269.707275\n",
      "Training Batch: 1137 Loss: 3338.860352\n",
      "Training Batch: 1138 Loss: 3330.960449\n",
      "Training Batch: 1139 Loss: 3306.949707\n",
      "Training Batch: 1140 Loss: 3354.162598\n",
      "Training Batch: 1141 Loss: 3291.318115\n",
      "Training Batch: 1142 Loss: 3276.972656\n",
      "Training Batch: 1143 Loss: 3472.821777\n",
      "Training Batch: 1144 Loss: 3440.666504\n",
      "Training Batch: 1145 Loss: 3278.191650\n",
      "Training Batch: 1146 Loss: 3425.738770\n",
      "Training Batch: 1147 Loss: 3376.824951\n",
      "Training Batch: 1148 Loss: 3296.914062\n",
      "Training Batch: 1149 Loss: 3317.148438\n",
      "Training Batch: 1150 Loss: 3410.067871\n",
      "Training Batch: 1151 Loss: 3479.967773\n",
      "Training Batch: 1152 Loss: 3455.817871\n",
      "Training Batch: 1153 Loss: 3383.708008\n",
      "Training Batch: 1154 Loss: 3443.634033\n",
      "Training Batch: 1155 Loss: 3322.802734\n",
      "Training Batch: 1156 Loss: 3745.270020\n",
      "Training Batch: 1157 Loss: 3315.157227\n",
      "Training Batch: 1158 Loss: 3256.222168\n",
      "Training Batch: 1159 Loss: 3335.401367\n",
      "Training Batch: 1160 Loss: 3304.689697\n",
      "Training Batch: 1161 Loss: 3407.867188\n",
      "Training Batch: 1162 Loss: 3448.508789\n",
      "Training Batch: 1163 Loss: 3270.643066\n",
      "Training Batch: 1164 Loss: 3362.467285\n",
      "Training Batch: 1165 Loss: 3401.132324\n",
      "Training Batch: 1166 Loss: 3406.931885\n",
      "Training Batch: 1167 Loss: 3274.820312\n",
      "Training Batch: 1168 Loss: 3309.897949\n",
      "Training Batch: 1169 Loss: 3314.552490\n",
      "Training Batch: 1170 Loss: 3173.130615\n",
      "Training Batch: 1171 Loss: 3247.692871\n",
      "Training Batch: 1172 Loss: 3368.027344\n",
      "Training Batch: 1173 Loss: 3305.908691\n",
      "Training Batch: 1174 Loss: 3434.515625\n",
      "Training Batch: 1175 Loss: 3423.163574\n",
      "Training Batch: 1176 Loss: 3526.631348\n",
      "Training Batch: 1177 Loss: 3360.915527\n",
      "Training Batch: 1178 Loss: 3446.744629\n",
      "Training Batch: 1179 Loss: 3396.219238\n",
      "Training Batch: 1180 Loss: 3401.968262\n",
      "Training Batch: 1181 Loss: 3368.554199\n",
      "Training Batch: 1182 Loss: 3396.335449\n",
      "Training Batch: 1183 Loss: 3264.150879\n",
      "Training Batch: 1184 Loss: 3433.063965\n",
      "Training Batch: 1185 Loss: 3324.042969\n",
      "Training Batch: 1186 Loss: 3710.084961\n",
      "Training Batch: 1187 Loss: 3381.219238\n",
      "Training Batch: 1188 Loss: 3264.231934\n",
      "Training Batch: 1189 Loss: 3552.626953\n",
      "Training Batch: 1190 Loss: 3531.428223\n",
      "Training Batch: 1191 Loss: 3332.347656\n",
      "Training Batch: 1192 Loss: 3428.727783\n",
      "Training Batch: 1193 Loss: 3291.100586\n",
      "Training Batch: 1194 Loss: 3277.694580\n",
      "Training Batch: 1195 Loss: 3292.077393\n",
      "Training Batch: 1196 Loss: 3324.203613\n",
      "Training Batch: 1197 Loss: 3385.345703\n",
      "Training Batch: 1198 Loss: 3298.789062\n",
      "Training Batch: 1199 Loss: 3256.482910\n",
      "Training Batch: 1200 Loss: 3282.993652\n",
      "Training Batch: 1201 Loss: 3292.374268\n",
      "Training Batch: 1202 Loss: 3362.910645\n",
      "Training Batch: 1203 Loss: 3415.728516\n",
      "Training Batch: 1204 Loss: 3270.167480\n",
      "Training Batch: 1205 Loss: 3358.623535\n",
      "Training Batch: 1206 Loss: 3365.390869\n",
      "Training Batch: 1207 Loss: 3411.209961\n",
      "Training Batch: 1208 Loss: 3493.981934\n",
      "Training Batch: 1209 Loss: 3406.336426\n",
      "Training Batch: 1210 Loss: 3417.720703\n",
      "Training Batch: 1211 Loss: 3606.655273\n",
      "Training Batch: 1212 Loss: 3689.495117\n",
      "Training Batch: 1213 Loss: 3313.857422\n",
      "Training Batch: 1214 Loss: 3211.222656\n",
      "Training Batch: 1215 Loss: 3439.866699\n",
      "Training Batch: 1216 Loss: 3315.893066\n",
      "Training Batch: 1217 Loss: 3367.105957\n",
      "Training Batch: 1218 Loss: 3253.864990\n",
      "Training Batch: 1219 Loss: 3331.067871\n",
      "Training Batch: 1220 Loss: 3206.030762\n",
      "Training Batch: 1221 Loss: 3331.067627\n",
      "Training Batch: 1222 Loss: 3449.024902\n",
      "Training Batch: 1223 Loss: 3156.512939\n",
      "Training Batch: 1224 Loss: 3269.285889\n",
      "Training Batch: 1225 Loss: 3442.038086\n",
      "Training Batch: 1226 Loss: 3352.832520\n",
      "Training Batch: 1227 Loss: 3281.724365\n",
      "Training Batch: 1228 Loss: 3348.521973\n",
      "Training Batch: 1229 Loss: 3292.318115\n",
      "Training Batch: 1230 Loss: 3357.075195\n",
      "Training Batch: 1231 Loss: 3280.533203\n",
      "Training Batch: 1232 Loss: 3390.560547\n",
      "Training Batch: 1233 Loss: 3237.884766\n",
      "Training Batch: 1234 Loss: 3275.830811\n",
      "Training Batch: 1235 Loss: 3288.962158\n",
      "Training Batch: 1236 Loss: 3219.259033\n",
      "Training Batch: 1237 Loss: 3276.776855\n",
      "Training Batch: 1238 Loss: 3353.655273\n",
      "Training Batch: 1239 Loss: 3466.949219\n",
      "Training Batch: 1240 Loss: 3285.307617\n",
      "Training Batch: 1241 Loss: 3214.096191\n",
      "Training Batch: 1242 Loss: 3292.354492\n",
      "Training Batch: 1243 Loss: 3184.012695\n",
      "Training Batch: 1244 Loss: 3320.385742\n",
      "Training Batch: 1245 Loss: 3280.349609\n",
      "Training Batch: 1246 Loss: 3305.554199\n",
      "Training Batch: 1247 Loss: 3251.563232\n",
      "Training Batch: 1248 Loss: 3300.150391\n",
      "Training Batch: 1249 Loss: 3478.562988\n",
      "Training Batch: 1250 Loss: 3558.171387\n",
      "Training Batch: 1251 Loss: 3302.112305\n",
      "Training Batch: 1252 Loss: 3344.056152\n",
      "Training Batch: 1253 Loss: 3228.217773\n",
      "Training Batch: 1254 Loss: 3267.738770\n",
      "Training Batch: 1255 Loss: 3261.879883\n",
      "Training Batch: 1256 Loss: 3289.457031\n",
      "Training Batch: 1257 Loss: 3313.155273\n",
      "Training Batch: 1258 Loss: 3222.532959\n",
      "Training Batch: 1259 Loss: 3276.701172\n",
      "Training Batch: 1260 Loss: 3247.724609\n",
      "Training Batch: 1261 Loss: 3408.893555\n",
      "Training Batch: 1262 Loss: 3208.581055\n",
      "Training Batch: 1263 Loss: 3411.236816\n",
      "Training Batch: 1264 Loss: 3300.096680\n",
      "Training Batch: 1265 Loss: 3222.489258\n",
      "Training Batch: 1266 Loss: 3324.705078\n",
      "Training Batch: 1267 Loss: 3272.066406\n",
      "Training Batch: 1268 Loss: 3378.386963\n",
      "Training Batch: 1269 Loss: 3247.749512\n",
      "Training Batch: 1270 Loss: 3313.068848\n",
      "Training Batch: 1271 Loss: 3294.911377\n",
      "Training Batch: 1272 Loss: 3205.913086\n",
      "Training Batch: 1273 Loss: 3231.620117\n",
      "Training Batch: 1274 Loss: 3379.863037\n",
      "Training Batch: 1275 Loss: 3234.820801\n",
      "Training Batch: 1276 Loss: 3222.665039\n",
      "Training Batch: 1277 Loss: 3188.046387\n",
      "Training Batch: 1278 Loss: 3377.361816\n",
      "Training Batch: 1279 Loss: 3315.063965\n",
      "Training Batch: 1280 Loss: 3300.645508\n",
      "Training Batch: 1281 Loss: 3304.258545\n",
      "Training Batch: 1282 Loss: 3259.239258\n",
      "Training Batch: 1283 Loss: 3315.224609\n",
      "Training Batch: 1284 Loss: 3385.364990\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Batch: 1285 Loss: 3338.617432\n",
      "Training Batch: 1286 Loss: 3197.882812\n",
      "Training Batch: 1287 Loss: 3368.487793\n",
      "Training Batch: 1288 Loss: 3448.238281\n",
      "Training Batch: 1289 Loss: 3323.973633\n",
      "Training Batch: 1290 Loss: 3353.132324\n",
      "Training Batch: 1291 Loss: 3497.472168\n",
      "Training Batch: 1292 Loss: 3465.689697\n",
      "Training Batch: 1293 Loss: 3355.226074\n",
      "Training Batch: 1294 Loss: 3482.560059\n",
      "Training Batch: 1295 Loss: 3270.848145\n",
      "Training Batch: 1296 Loss: 3241.133301\n",
      "Training Batch: 1297 Loss: 3239.618164\n",
      "Training Batch: 1298 Loss: 3306.920898\n",
      "Training Batch: 1299 Loss: 3283.916992\n",
      "Training Batch: 1300 Loss: 3367.314941\n",
      "Training Batch: 1301 Loss: 3293.117188\n",
      "Training Batch: 1302 Loss: 3277.740479\n",
      "Training Batch: 1303 Loss: 3537.513672\n",
      "Training Batch: 1304 Loss: 3407.925781\n",
      "Training Batch: 1305 Loss: 3397.031250\n",
      "Training Batch: 1306 Loss: 3246.899414\n",
      "Training Batch: 1307 Loss: 3657.469971\n",
      "Training Batch: 1308 Loss: 3464.545898\n",
      "Training Batch: 1309 Loss: 3295.129883\n",
      "Training Batch: 1310 Loss: 3406.714844\n",
      "Training Batch: 1311 Loss: 3302.594238\n",
      "Training Batch: 1312 Loss: 3402.046875\n",
      "Training Batch: 1313 Loss: 3399.742676\n",
      "Training Batch: 1314 Loss: 3328.328369\n",
      "Training Batch: 1315 Loss: 3315.487305\n",
      "Training Batch: 1316 Loss: 3417.647949\n",
      "Training Batch: 1317 Loss: 3282.821533\n",
      "Training Batch: 1318 Loss: 3212.095215\n",
      "Training Batch: 1319 Loss: 3472.370117\n",
      "Training Batch: 1320 Loss: 3252.449707\n",
      "Training Batch: 1321 Loss: 3247.041504\n",
      "Training Batch: 1322 Loss: 3636.639160\n",
      "Training Batch: 1323 Loss: 3508.624756\n",
      "Training Batch: 1324 Loss: 3315.808594\n",
      "Training Batch: 1325 Loss: 3422.410645\n",
      "Training Batch: 1326 Loss: 3293.729248\n",
      "Training Batch: 1327 Loss: 3355.602051\n",
      "Training Batch: 1328 Loss: 3214.050537\n",
      "Training Batch: 1329 Loss: 3332.763184\n",
      "Training Batch: 1330 Loss: 3362.097168\n",
      "Training Batch: 1331 Loss: 3404.573730\n",
      "Training Batch: 1332 Loss: 3281.207764\n",
      "Training Batch: 1333 Loss: 3207.078613\n",
      "Training Batch: 1334 Loss: 3409.955566\n",
      "Training Batch: 1335 Loss: 3253.776611\n",
      "Training Batch: 1336 Loss: 3338.470215\n",
      "Training Batch: 1337 Loss: 3340.326172\n",
      "Training Batch: 1338 Loss: 3363.685547\n",
      "Training Batch: 1339 Loss: 3451.746826\n",
      "Training Batch: 1340 Loss: 3462.279785\n",
      "Training Batch: 1341 Loss: 3285.105957\n",
      "Training Batch: 1342 Loss: 3297.670410\n",
      "Training Batch: 1343 Loss: 3291.314209\n",
      "Training Batch: 1344 Loss: 3405.010742\n",
      "Training Batch: 1345 Loss: 3269.892578\n",
      "Training Batch: 1346 Loss: 3382.732422\n",
      "Training Batch: 1347 Loss: 3445.961426\n",
      "Training Batch: 1348 Loss: 3475.489746\n",
      "Training Batch: 1349 Loss: 3266.009766\n",
      "Training Batch: 1350 Loss: 3309.256348\n",
      "Training Batch: 1351 Loss: 3248.165039\n",
      "Training Batch: 1352 Loss: 3342.292969\n",
      "Training Batch: 1353 Loss: 3317.765381\n",
      "Training Batch: 1354 Loss: 3324.977051\n",
      "Training Batch: 1355 Loss: 3397.944824\n",
      "Training Batch: 1356 Loss: 3281.404785\n",
      "Training Batch: 1357 Loss: 3248.559570\n",
      "Training Batch: 1358 Loss: 3379.229980\n",
      "Training Batch: 1359 Loss: 3341.859863\n",
      "Training Batch: 1360 Loss: 3469.619141\n",
      "Training Batch: 1361 Loss: 3311.616211\n",
      "Training Batch: 1362 Loss: 3187.822266\n",
      "Training Batch: 1363 Loss: 3259.609863\n",
      "Training Batch: 1364 Loss: 3262.982910\n",
      "Training Batch: 1365 Loss: 3353.258301\n",
      "Training Batch: 1366 Loss: 3248.633057\n",
      "Training Batch: 1367 Loss: 3226.817383\n",
      "Training Batch: 1368 Loss: 3242.753174\n",
      "Training Batch: 1369 Loss: 3262.515137\n",
      "Training Batch: 1370 Loss: 3429.155029\n",
      "Training Batch: 1371 Loss: 3364.745117\n",
      "Training Batch: 1372 Loss: 3311.892822\n",
      "Training Batch: 1373 Loss: 3323.658691\n",
      "Training Batch: 1374 Loss: 3430.705322\n",
      "Training Batch: 1375 Loss: 3323.461182\n",
      "Training Batch: 1376 Loss: 3247.034668\n",
      "Training Batch: 1377 Loss: 3254.658203\n",
      "Training Batch: 1378 Loss: 3219.570068\n",
      "Training Batch: 1379 Loss: 3262.719482\n",
      "Training Batch: 1380 Loss: 3153.271973\n",
      "Training Batch: 1381 Loss: 3752.606201\n",
      "Training Batch: 1382 Loss: 3943.271240\n",
      "Training Batch: 1383 Loss: 3471.395996\n",
      "Training Batch: 1384 Loss: 3299.185303\n",
      "Training Batch: 1385 Loss: 3471.634277\n",
      "Training Batch: 1386 Loss: 3393.971680\n",
      "Training Batch: 1387 Loss: 3381.276123\n",
      "Training Batch: 1388 Loss: 3250.399658\n",
      "Training Batch: 1389 Loss: 3312.194580\n",
      "Training Batch: 1390 Loss: 3307.943359\n",
      "Training Batch: 1391 Loss: 3251.052734\n",
      "Training Batch: 1392 Loss: 3345.004395\n",
      "Training Batch: 1393 Loss: 3336.042969\n",
      "Training Batch: 1394 Loss: 3321.614746\n",
      "Training Batch: 1395 Loss: 3264.720947\n",
      "Training Batch: 1396 Loss: 3381.181152\n",
      "Training Batch: 1397 Loss: 3226.125000\n",
      "Training Batch: 1398 Loss: 3603.254395\n",
      "Training Batch: 1399 Loss: 3652.436523\n",
      "Training Batch: 1400 Loss: 3409.633545\n",
      "Training Batch: 1401 Loss: 3491.958008\n",
      "Training Batch: 1402 Loss: 3382.145508\n",
      "Training Batch: 1403 Loss: 3354.200928\n",
      "Training Batch: 1404 Loss: 3402.265625\n",
      "Training Batch: 1405 Loss: 3385.772949\n",
      "Training Batch: 1406 Loss: 3277.232422\n",
      "Training Batch: 1407 Loss: 3767.706299\n",
      "Training Batch: 1408 Loss: 3562.990479\n",
      "Training Batch: 1409 Loss: 3298.005615\n",
      "Training Batch: 1410 Loss: 3388.012939\n",
      "Training Batch: 1411 Loss: 3391.849854\n",
      "Training Batch: 1412 Loss: 3483.247559\n",
      "Training Batch: 1413 Loss: 3562.087891\n",
      "Training Batch: 1414 Loss: 3378.259033\n",
      "Training Batch: 1415 Loss: 3317.661621\n",
      "Training Batch: 1416 Loss: 3232.118164\n",
      "Training Batch: 1417 Loss: 3412.840820\n",
      "Training Batch: 1418 Loss: 3259.703613\n",
      "Training Batch: 1419 Loss: 3212.482910\n",
      "Training Batch: 1420 Loss: 3498.479492\n",
      "Training Batch: 1421 Loss: 3460.520020\n",
      "Training Batch: 1422 Loss: 3267.234131\n",
      "Training Batch: 1423 Loss: 3403.077637\n",
      "Training Batch: 1424 Loss: 3268.212402\n",
      "Training Batch: 1425 Loss: 3218.451172\n",
      "Training Batch: 1426 Loss: 3248.587891\n",
      "Training Batch: 1427 Loss: 3283.152832\n",
      "Training Batch: 1428 Loss: 3278.781982\n",
      "Training Batch: 1429 Loss: 3309.712646\n",
      "Training Batch: 1430 Loss: 3504.067139\n",
      "Training Batch: 1431 Loss: 3481.680176\n",
      "Training Batch: 1432 Loss: 3367.869141\n",
      "Training Batch: 1433 Loss: 3241.329590\n",
      "Training Batch: 1434 Loss: 3441.946045\n",
      "Training Batch: 1435 Loss: 3279.004883\n",
      "Training Batch: 1436 Loss: 3440.432373\n",
      "Training Batch: 1437 Loss: 3335.352051\n",
      "Training Batch: 1438 Loss: 3280.245117\n",
      "Training Batch: 1439 Loss: 3187.822266\n",
      "Training Batch: 1440 Loss: 3205.671387\n",
      "Training Batch: 1441 Loss: 3512.608398\n",
      "Training Batch: 1442 Loss: 3231.056152\n",
      "Training Batch: 1443 Loss: 3261.758789\n",
      "Training Batch: 1444 Loss: 3481.593262\n",
      "Training Batch: 1445 Loss: 3267.492188\n",
      "Training Batch: 1446 Loss: 3535.656006\n",
      "Training Batch: 1447 Loss: 3322.833008\n",
      "Training Batch: 1448 Loss: 3507.118164\n",
      "Training Batch: 1449 Loss: 3285.684082\n",
      "Training Batch: 1450 Loss: 3397.244141\n",
      "Training Batch: 1451 Loss: 3255.623047\n",
      "Training Batch: 1452 Loss: 3443.219971\n",
      "Training Batch: 1453 Loss: 3349.136719\n",
      "Training Batch: 1454 Loss: 3284.458984\n",
      "Training Batch: 1455 Loss: 3481.033447\n",
      "Training Batch: 1456 Loss: 3452.627930\n",
      "Training Batch: 1457 Loss: 3265.495117\n",
      "Training Batch: 1458 Loss: 3239.589355\n",
      "Training Batch: 1459 Loss: 3253.664307\n",
      "Training Batch: 1460 Loss: 3418.389648\n",
      "Training Batch: 1461 Loss: 3301.955078\n",
      "Training Batch: 1462 Loss: 3214.255859\n",
      "Training Batch: 1463 Loss: 3409.790527\n",
      "Training Batch: 1464 Loss: 3406.389160\n",
      "Training Batch: 1465 Loss: 3322.457031\n",
      "Training Batch: 1466 Loss: 3226.190430\n",
      "Training Batch: 1467 Loss: 3225.315918\n",
      "Training Batch: 1468 Loss: 3231.498047\n",
      "Training Batch: 1469 Loss: 3257.133301\n",
      "Training Batch: 1470 Loss: 3254.109863\n",
      "Training Batch: 1471 Loss: 3290.113525\n",
      "Training Batch: 1472 Loss: 3284.123779\n",
      "Training Batch: 1473 Loss: 3289.612793\n",
      "Training Batch: 1474 Loss: 3369.765625\n",
      "Training Batch: 1475 Loss: 3496.655273\n",
      "Training Batch: 1476 Loss: 3245.572510\n",
      "Training Batch: 1477 Loss: 3339.260986\n",
      "Training Batch: 1478 Loss: 3257.665039\n",
      "Training Batch: 1479 Loss: 3217.795898\n",
      "Training Batch: 1480 Loss: 3297.712891\n",
      "Training Batch: 1481 Loss: 3222.818848\n",
      "Training Batch: 1482 Loss: 3354.217529\n",
      "Training Batch: 1483 Loss: 3279.070312\n",
      "Training Batch: 1484 Loss: 3223.323730\n",
      "Training Batch: 1485 Loss: 3302.312988\n",
      "Training Batch: 1486 Loss: 3275.521973\n",
      "Training Batch: 1487 Loss: 3267.729492\n",
      "Training Batch: 1488 Loss: 3271.115234\n",
      "Training Batch: 1489 Loss: 3473.188477\n",
      "Training Batch: 1490 Loss: 3294.938965\n",
      "Training Batch: 1491 Loss: 3310.958740\n",
      "Training Batch: 1492 Loss: 3500.176025\n",
      "Training Batch: 1493 Loss: 3457.407227\n",
      "Training Batch: 1494 Loss: 3611.467773\n",
      "Training Batch: 1495 Loss: 3379.332031\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Batch: 1496 Loss: 3408.900879\n",
      "Training Batch: 1497 Loss: 3289.242188\n",
      "Training Batch: 1498 Loss: 3244.104004\n",
      "Training Batch: 1499 Loss: 3330.250488\n",
      "Training Batch: 1500 Loss: 3387.012695\n",
      "Training Batch: 1501 Loss: 3399.916016\n",
      "Training Batch: 1502 Loss: 3400.154297\n",
      "Training Batch: 1503 Loss: 3308.532227\n",
      "Training Batch: 1504 Loss: 3351.045166\n",
      "Training Batch: 1505 Loss: 3410.216797\n",
      "Training Batch: 1506 Loss: 3775.174805\n",
      "Training Batch: 1507 Loss: 3381.514648\n",
      "Training Batch: 1508 Loss: 3259.356201\n",
      "Training Batch: 1509 Loss: 3245.574219\n",
      "Training Batch: 1510 Loss: 3291.129883\n",
      "Training Batch: 1511 Loss: 3281.940186\n",
      "Training Batch: 1512 Loss: 3360.350098\n",
      "Training Batch: 1513 Loss: 3279.482910\n",
      "Training Batch: 1514 Loss: 3240.961914\n",
      "Training Batch: 1515 Loss: 3231.129883\n",
      "Training Batch: 1516 Loss: 3297.398926\n",
      "Training Batch: 1517 Loss: 3346.053711\n",
      "Training Batch: 1518 Loss: 3583.235352\n",
      "Training Batch: 1519 Loss: 3528.054688\n",
      "Training Batch: 1520 Loss: 3291.543945\n",
      "Training Batch: 1521 Loss: 3292.592285\n",
      "Training Batch: 1522 Loss: 3313.170166\n",
      "Training Batch: 1523 Loss: 3421.030762\n",
      "Training Batch: 1524 Loss: 3322.937988\n",
      "Training Batch: 1525 Loss: 3250.790039\n",
      "Training Batch: 1526 Loss: 3312.807617\n",
      "Training Batch: 1527 Loss: 3478.826172\n",
      "Training Batch: 1528 Loss: 3354.162598\n",
      "Training Batch: 1529 Loss: 3441.584229\n",
      "Training Batch: 1530 Loss: 3361.104736\n",
      "Training Batch: 1531 Loss: 3230.566895\n",
      "Training Batch: 1532 Loss: 3559.522705\n",
      "Training Batch: 1533 Loss: 3342.297852\n",
      "Training Batch: 1534 Loss: 3440.736816\n",
      "Training Batch: 1535 Loss: 3524.362793\n",
      "Training Batch: 1536 Loss: 3246.312744\n",
      "Training Batch: 1537 Loss: 3276.756348\n",
      "Training Batch: 1538 Loss: 3334.368652\n",
      "Training Batch: 1539 Loss: 3230.268799\n",
      "Training Batch: 1540 Loss: 3267.652344\n",
      "Training Batch: 1541 Loss: 3274.732178\n",
      "Training Batch: 1542 Loss: 3390.444824\n",
      "Training Batch: 1543 Loss: 3357.723145\n",
      "Training Batch: 1544 Loss: 3450.644531\n",
      "Training Batch: 1545 Loss: 3331.692871\n",
      "Training Batch: 1546 Loss: 3270.874023\n",
      "Training Batch: 1547 Loss: 3401.270508\n",
      "Training Batch: 1548 Loss: 3275.304199\n",
      "Training Batch: 1549 Loss: 3319.575928\n",
      "Training Batch: 1550 Loss: 3410.894531\n",
      "Training Batch: 1551 Loss: 3546.345703\n",
      "Training Batch: 1552 Loss: 3293.663574\n",
      "Training Batch: 1553 Loss: 3451.740723\n",
      "Training Batch: 1554 Loss: 3295.045898\n",
      "Training Batch: 1555 Loss: 3355.851074\n",
      "Training Batch: 1556 Loss: 3272.997070\n",
      "Training Batch: 1557 Loss: 3274.757812\n",
      "Training Batch: 1558 Loss: 3396.508301\n",
      "Training Batch: 1559 Loss: 3415.181152\n",
      "Training Batch: 1560 Loss: 3209.410889\n",
      "Training Batch: 1561 Loss: 3346.327148\n",
      "Training Batch: 1562 Loss: 3263.875488\n",
      "Training Batch: 1563 Loss: 3196.919434\n",
      "Training Batch: 1564 Loss: 3256.025146\n",
      "Training Batch: 1565 Loss: 3238.129150\n",
      "Training Batch: 1566 Loss: 3377.450195\n",
      "Training Batch: 1567 Loss: 3371.717529\n",
      "Training Batch: 1568 Loss: 3390.515625\n",
      "Training Batch: 1569 Loss: 3330.649170\n",
      "Training Batch: 1570 Loss: 3349.444580\n",
      "Training Batch: 1571 Loss: 3405.607422\n",
      "Training Batch: 1572 Loss: 3337.670410\n",
      "Training Batch: 1573 Loss: 3271.079102\n",
      "Training Batch: 1574 Loss: 3225.976318\n",
      "Training Batch: 1575 Loss: 3260.719238\n",
      "Training Batch: 1576 Loss: 3296.847168\n",
      "Training Batch: 1577 Loss: 3254.604004\n",
      "Training Batch: 1578 Loss: 3399.186523\n",
      "Training Batch: 1579 Loss: 3239.669922\n",
      "Training Batch: 1580 Loss: 3352.029541\n",
      "Training Batch: 1581 Loss: 3235.326660\n",
      "Training Batch: 1582 Loss: 3250.337402\n",
      "Training Batch: 1583 Loss: 3222.403320\n",
      "Training Batch: 1584 Loss: 3282.015625\n",
      "Training Batch: 1585 Loss: 3321.966064\n",
      "Training Batch: 1586 Loss: 3263.453369\n",
      "Training Batch: 1587 Loss: 3390.360840\n",
      "Training Batch: 1588 Loss: 3274.031738\n",
      "Training Batch: 1589 Loss: 3455.223633\n",
      "Training Batch: 1590 Loss: 3363.231934\n",
      "Training Batch: 1591 Loss: 3287.510254\n",
      "Training Batch: 1592 Loss: 3339.195312\n",
      "Training Batch: 1593 Loss: 3227.517822\n",
      "Training Batch: 1594 Loss: 3257.078613\n",
      "Training Batch: 1595 Loss: 3269.037354\n",
      "Training Batch: 1596 Loss: 3215.283447\n",
      "Training Batch: 1597 Loss: 3244.812988\n",
      "Training Batch: 1598 Loss: 3339.771973\n",
      "Training Batch: 1599 Loss: 3368.415527\n",
      "Training Batch: 1600 Loss: 3309.285156\n",
      "Training Batch: 1601 Loss: 3324.761230\n",
      "Training Batch: 1602 Loss: 3333.506836\n",
      "Training Batch: 1603 Loss: 3368.344727\n",
      "Training Batch: 1604 Loss: 3373.345703\n",
      "Training Batch: 1605 Loss: 3552.128418\n",
      "Training Batch: 1606 Loss: 3328.464355\n",
      "Training Batch: 1607 Loss: 3519.514648\n",
      "Training Batch: 1608 Loss: 3279.686523\n",
      "Training Batch: 1609 Loss: 3347.540527\n",
      "Training Batch: 1610 Loss: 3997.874023\n",
      "Training Batch: 1611 Loss: 3410.860352\n",
      "Training Batch: 1612 Loss: 3286.424805\n",
      "Training Batch: 1613 Loss: 3376.077637\n",
      "Training Batch: 1614 Loss: 3380.067871\n",
      "Training Batch: 1615 Loss: 3335.788086\n",
      "Training Batch: 1616 Loss: 3356.265869\n",
      "Training Batch: 1617 Loss: 3485.373779\n",
      "Training Batch: 1618 Loss: 3343.796631\n",
      "Training Batch: 1619 Loss: 3180.199463\n",
      "Training Batch: 1620 Loss: 3450.815186\n",
      "Training Batch: 1621 Loss: 3514.768555\n",
      "Training Batch: 1622 Loss: 3395.960449\n",
      "Training Batch: 1623 Loss: 3507.512695\n",
      "Training Batch: 1624 Loss: 3756.655518\n",
      "Training Batch: 1625 Loss: 3425.594482\n",
      "Training Batch: 1626 Loss: 3380.694824\n",
      "Training Batch: 1627 Loss: 3280.852539\n",
      "Training Batch: 1628 Loss: 3200.963623\n",
      "Training Batch: 1629 Loss: 3190.776855\n",
      "Training Batch: 1630 Loss: 3496.880615\n",
      "Training Batch: 1631 Loss: 3457.726318\n",
      "Training Batch: 1632 Loss: 3228.076660\n",
      "Training Batch: 1633 Loss: 3384.115723\n",
      "Training Batch: 1634 Loss: 3328.920898\n",
      "Training Batch: 1635 Loss: 3357.508301\n",
      "Training Batch: 1636 Loss: 3682.545410\n",
      "Training Batch: 1637 Loss: 3226.108643\n",
      "Training Batch: 1638 Loss: 3326.265137\n",
      "Training Batch: 1639 Loss: 3624.231689\n",
      "Training Batch: 1640 Loss: 3489.229980\n",
      "Training Batch: 1641 Loss: 3274.439209\n",
      "Training Batch: 1642 Loss: 3290.452148\n",
      "Training Batch: 1643 Loss: 3314.010986\n",
      "Training Batch: 1644 Loss: 3304.858643\n",
      "Training Batch: 1645 Loss: 3264.335938\n",
      "Training Batch: 1646 Loss: 3327.074707\n",
      "Training Batch: 1647 Loss: 3216.958008\n",
      "Training Batch: 1648 Loss: 3303.209473\n",
      "Training Batch: 1649 Loss: 3562.973145\n",
      "Training Batch: 1650 Loss: 3362.215332\n",
      "Training Batch: 1651 Loss: 3296.514648\n",
      "Training Batch: 1652 Loss: 3319.785645\n",
      "Training Batch: 1653 Loss: 3430.691406\n",
      "Training Batch: 1654 Loss: 3340.160645\n",
      "Training Batch: 1655 Loss: 3456.052246\n",
      "Training Batch: 1656 Loss: 3385.967773\n",
      "Training Batch: 1657 Loss: 3346.301270\n",
      "Training Batch: 1658 Loss: 3330.412842\n",
      "Training Batch: 1659 Loss: 3387.371094\n",
      "Training Batch: 1660 Loss: 3247.640625\n",
      "Training Batch: 1661 Loss: 3324.014648\n",
      "Training Batch: 1662 Loss: 3359.145508\n",
      "Training Batch: 1663 Loss: 3258.409180\n",
      "Training Batch: 1664 Loss: 3275.070068\n",
      "Training Batch: 1665 Loss: 3397.792236\n",
      "Training Batch: 1666 Loss: 3219.916504\n",
      "Training Batch: 1667 Loss: 3261.109863\n",
      "Training Batch: 1668 Loss: 3257.198730\n",
      "Training Batch: 1669 Loss: 3215.249512\n",
      "Training Batch: 1670 Loss: 3284.811768\n",
      "Training Batch: 1671 Loss: 3344.479492\n",
      "Training Batch: 1672 Loss: 3497.225830\n",
      "Training Batch: 1673 Loss: 3347.927490\n",
      "Training Batch: 1674 Loss: 3230.192871\n",
      "Training Batch: 1675 Loss: 3261.188232\n",
      "Training Batch: 1676 Loss: 3278.326172\n",
      "Training Batch: 1677 Loss: 3229.855469\n",
      "Training Batch: 1678 Loss: 3358.919922\n",
      "Training Batch: 1679 Loss: 3268.166748\n",
      "Training Batch: 1680 Loss: 3252.415527\n",
      "Training Batch: 1681 Loss: 3190.135010\n",
      "Training Batch: 1682 Loss: 3376.353516\n",
      "Training Batch: 1683 Loss: 3360.768555\n",
      "Training Batch: 1684 Loss: 3424.107422\n",
      "Training Batch: 1685 Loss: 3356.382568\n",
      "Training Batch: 1686 Loss: 3412.594727\n",
      "Training Batch: 1687 Loss: 3247.778564\n",
      "Training Batch: 1688 Loss: 3407.751709\n",
      "Training Batch: 1689 Loss: 3337.756836\n",
      "Training Batch: 1690 Loss: 3358.758301\n",
      "Training Batch: 1691 Loss: 3348.738281\n",
      "Training Batch: 1692 Loss: 3359.621582\n",
      "Training Batch: 1693 Loss: 3471.250000\n",
      "Training Batch: 1694 Loss: 3337.994141\n",
      "Training Batch: 1695 Loss: 3374.650391\n",
      "Training Batch: 1696 Loss: 3697.896973\n",
      "Training Batch: 1697 Loss: 3353.452637\n",
      "Training Batch: 1698 Loss: 3348.002930\n",
      "Training Batch: 1699 Loss: 3328.559570\n",
      "Training Batch: 1700 Loss: 3265.897705\n",
      "Training Batch: 1701 Loss: 3399.153564\n",
      "Training Batch: 1702 Loss: 3410.975098\n",
      "Training Batch: 1703 Loss: 3471.196777\n",
      "Training Batch: 1704 Loss: 3313.064453\n",
      "Training Batch: 1705 Loss: 3290.167480\n",
      "Training Batch: 1706 Loss: 3351.344238\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Batch: 1707 Loss: 3418.124512\n",
      "Training Batch: 1708 Loss: 3331.589600\n",
      "Training Batch: 1709 Loss: 3267.608398\n",
      "Training Batch: 1710 Loss: 3319.336182\n",
      "Training Batch: 1711 Loss: 3287.427490\n",
      "Training Batch: 1712 Loss: 3268.075928\n",
      "Training Batch: 1713 Loss: 3272.812012\n",
      "Training Batch: 1714 Loss: 3351.550781\n",
      "Training Batch: 1715 Loss: 3334.460693\n",
      "Training Batch: 1716 Loss: 3369.898438\n",
      "Training Batch: 1717 Loss: 3256.806152\n",
      "Training Batch: 1718 Loss: 3716.941162\n",
      "Training Batch: 1719 Loss: 3518.358887\n",
      "Training Batch: 1720 Loss: 3294.088623\n",
      "Training Batch: 1721 Loss: 3423.698975\n",
      "Training Batch: 1722 Loss: 3469.201904\n",
      "Training Batch: 1723 Loss: 3228.741699\n",
      "Training Batch: 1724 Loss: 3420.414062\n",
      "Training Batch: 1725 Loss: 3397.900879\n",
      "Training Batch: 1726 Loss: 3363.723633\n",
      "Training Batch: 1727 Loss: 3394.276855\n",
      "Training Batch: 1728 Loss: 3457.395508\n",
      "Training Batch: 1729 Loss: 3301.344238\n",
      "Training Batch: 1730 Loss: 3252.262207\n",
      "Training Batch: 1731 Loss: 3314.106934\n",
      "Training Batch: 1732 Loss: 3261.412598\n",
      "Training Batch: 1733 Loss: 3213.843018\n",
      "Training Batch: 1734 Loss: 3437.659912\n",
      "Training Batch: 1735 Loss: 3488.139160\n",
      "Training Batch: 1736 Loss: 3308.464844\n",
      "Training Batch: 1737 Loss: 3572.685303\n",
      "Training Batch: 1738 Loss: 3344.668945\n",
      "Training Batch: 1739 Loss: 3496.736328\n",
      "Training Batch: 1740 Loss: 3428.237305\n",
      "Training Batch: 1741 Loss: 3366.021973\n",
      "Training Batch: 1742 Loss: 3506.831787\n",
      "Training Batch: 1743 Loss: 3444.908936\n",
      "Training Batch: 1744 Loss: 3380.613770\n",
      "Training Batch: 1745 Loss: 3277.797119\n",
      "Training Batch: 1746 Loss: 3446.809814\n",
      "Training Batch: 1747 Loss: 3252.072510\n",
      "Training Batch: 1748 Loss: 3490.894287\n",
      "Training Batch: 1749 Loss: 3382.129639\n",
      "Training Batch: 1750 Loss: 3413.717041\n",
      "Training Batch: 1751 Loss: 3464.049805\n",
      "Training Batch: 1752 Loss: 3264.739258\n",
      "Training Batch: 1753 Loss: 3332.282227\n",
      "Training Batch: 1754 Loss: 3279.083008\n",
      "Training Batch: 1755 Loss: 3234.885010\n",
      "Training Batch: 1756 Loss: 3389.447754\n",
      "Training Batch: 1757 Loss: 3416.087402\n",
      "Training Batch: 1758 Loss: 3480.535156\n",
      "Training Batch: 1759 Loss: 3413.643555\n",
      "Training Batch: 1760 Loss: 3472.700684\n",
      "Training Batch: 1761 Loss: 3342.415039\n",
      "Training Batch: 1762 Loss: 3437.045898\n",
      "Training Batch: 1763 Loss: 3426.601074\n",
      "Training Batch: 1764 Loss: 3346.766846\n",
      "Training Batch: 1765 Loss: 3368.203613\n",
      "Training Batch: 1766 Loss: 3254.251465\n",
      "Training Batch: 1767 Loss: 3250.901855\n",
      "Training Batch: 1768 Loss: 3351.620117\n",
      "Training Batch: 1769 Loss: 3332.181152\n",
      "Training Batch: 1770 Loss: 3388.171387\n",
      "Training Batch: 1771 Loss: 3481.089355\n",
      "Training Batch: 1772 Loss: 3152.982666\n",
      "Training Batch: 1773 Loss: 3295.627930\n",
      "Training Batch: 1774 Loss: 3484.926270\n",
      "Training Batch: 1775 Loss: 3444.267090\n",
      "Training Batch: 1776 Loss: 3408.500977\n",
      "Training Batch: 1777 Loss: 3395.400635\n",
      "Training Batch: 1778 Loss: 3541.986816\n",
      "Training Batch: 1779 Loss: 3573.871094\n",
      "Training Batch: 1780 Loss: 3379.525879\n",
      "Training Batch: 1781 Loss: 3517.582764\n",
      "Training Batch: 1782 Loss: 3403.019043\n",
      "Training Batch: 1783 Loss: 3352.984863\n",
      "Training Batch: 1784 Loss: 3380.302246\n",
      "Training Batch: 1785 Loss: 3280.700439\n",
      "Training Batch: 1786 Loss: 3297.952148\n",
      "Training Batch: 1787 Loss: 3376.976074\n",
      "Training Batch: 1788 Loss: 3315.600098\n",
      "Training Batch: 1789 Loss: 3373.420410\n",
      "Training Batch: 1790 Loss: 3242.139160\n",
      "Training Batch: 1791 Loss: 3404.031738\n",
      "Training Batch: 1792 Loss: 3350.216309\n",
      "Training Batch: 1793 Loss: 3326.019287\n",
      "Training Batch: 1794 Loss: 4651.749023\n",
      "Training Batch: 1795 Loss: 3750.045898\n",
      "Training Batch: 1796 Loss: 3362.447021\n",
      "Training Batch: 1797 Loss: 3498.324219\n",
      "Training Batch: 1798 Loss: 3858.865723\n",
      "Training Batch: 1799 Loss: 3526.592529\n",
      "Training Batch: 1800 Loss: 3377.421387\n",
      "Training Batch: 1801 Loss: 3361.987549\n",
      "Training Batch: 1802 Loss: 3404.369141\n",
      "Training Batch: 1803 Loss: 3268.717529\n",
      "Training Batch: 1804 Loss: 3247.291016\n",
      "Training Batch: 1805 Loss: 3222.640625\n",
      "Training Batch: 1806 Loss: 3329.292236\n",
      "Training Batch: 1807 Loss: 3342.938965\n",
      "Training Batch: 1808 Loss: 3618.964844\n",
      "Training Batch: 1809 Loss: 3274.108398\n",
      "Training Batch: 1810 Loss: 3326.882568\n",
      "Training Batch: 1811 Loss: 3406.366211\n",
      "Training Batch: 1812 Loss: 3262.989990\n",
      "Training Batch: 1813 Loss: 3278.233398\n",
      "Training Batch: 1814 Loss: 3274.496582\n",
      "Training Batch: 1815 Loss: 3255.044434\n",
      "Training Batch: 1816 Loss: 3199.851318\n",
      "Training Batch: 1817 Loss: 3322.439453\n",
      "Training Batch: 1818 Loss: 3239.778809\n",
      "Training Batch: 1819 Loss: 3412.572266\n",
      "Training Batch: 1820 Loss: 3402.489746\n",
      "Training Batch: 1821 Loss: 3442.957520\n",
      "Training Batch: 1822 Loss: 3294.379639\n",
      "Training Batch: 1823 Loss: 3319.796875\n",
      "Training Batch: 1824 Loss: 3282.812988\n",
      "Training Batch: 1825 Loss: 3468.683594\n",
      "Training Batch: 1826 Loss: 3273.663574\n",
      "Training Batch: 1827 Loss: 3402.476318\n",
      "Training Batch: 1828 Loss: 3702.196289\n",
      "Training Batch: 1829 Loss: 3424.288574\n",
      "Training Batch: 1830 Loss: 3342.632812\n",
      "Training Batch: 1831 Loss: 3235.237061\n",
      "Training Batch: 1832 Loss: 3259.078857\n",
      "Training Batch: 1833 Loss: 3304.576172\n",
      "Training Batch: 1834 Loss: 3396.886963\n",
      "Training Batch: 1835 Loss: 3317.434570\n",
      "Training Batch: 1836 Loss: 3387.793457\n",
      "Training Batch: 1837 Loss: 3237.909180\n",
      "Training Batch: 1838 Loss: 3280.675293\n",
      "Training Batch: 1839 Loss: 3288.905518\n",
      "Training Batch: 1840 Loss: 3299.171875\n",
      "Training Batch: 1841 Loss: 3414.490234\n",
      "Training Batch: 1842 Loss: 3630.307861\n",
      "Training Batch: 1843 Loss: 3315.653320\n",
      "Training Batch: 1844 Loss: 3625.482178\n",
      "Training Batch: 1845 Loss: 3269.894531\n",
      "Training Batch: 1846 Loss: 3206.119629\n",
      "Training Batch: 1847 Loss: 3332.777832\n",
      "Training Batch: 1848 Loss: 3360.686523\n",
      "Training Batch: 1849 Loss: 3310.452637\n",
      "Training Batch: 1850 Loss: 3296.281738\n",
      "Training Batch: 1851 Loss: 3224.100586\n",
      "Training Batch: 1852 Loss: 3208.093750\n",
      "Training Batch: 1853 Loss: 3356.397705\n",
      "Training Batch: 1854 Loss: 3316.149170\n",
      "Training Batch: 1855 Loss: 3411.786133\n",
      "Training Batch: 1856 Loss: 3444.162598\n",
      "Training Batch: 1857 Loss: 3257.128662\n",
      "Training Batch: 1858 Loss: 3339.321777\n",
      "Training Batch: 1859 Loss: 3241.472168\n",
      "Training Batch: 1860 Loss: 3326.465332\n",
      "Training Batch: 1861 Loss: 3562.156494\n",
      "Training Batch: 1862 Loss: 3270.319336\n",
      "Training Batch: 1863 Loss: 3716.614258\n",
      "Training Batch: 1864 Loss: 3624.528564\n",
      "Training Batch: 1865 Loss: 3466.412109\n",
      "Training Batch: 1866 Loss: 3467.276367\n",
      "Training Batch: 1867 Loss: 3584.170410\n",
      "Training Batch: 1868 Loss: 3336.140625\n",
      "Training Batch: 1869 Loss: 3800.860352\n",
      "Training Batch: 1870 Loss: 3627.936279\n",
      "Training Batch: 1871 Loss: 3407.941895\n",
      "Training Batch: 1872 Loss: 3451.342773\n",
      "Training Batch: 1873 Loss: 3554.652832\n",
      "Training Batch: 1874 Loss: 3346.260742\n",
      "Training Batch: 1875 Loss: 3422.233643\n",
      "Training Batch: 1876 Loss: 3208.199707\n",
      "Training Batch: 1877 Loss: 3460.650146\n",
      "Training Batch: 1878 Loss: 3251.766846\n",
      "Training Batch: 1879 Loss: 3526.592773\n",
      "Training Batch: 1880 Loss: 3420.515869\n",
      "Training Batch: 1881 Loss: 3218.496094\n",
      "Training Batch: 1882 Loss: 3399.808594\n",
      "Training Batch: 1883 Loss: 3207.490967\n",
      "Training Batch: 1884 Loss: 3278.742188\n",
      "Training Batch: 1885 Loss: 3357.499512\n",
      "Training Batch: 1886 Loss: 3329.277832\n",
      "Training Batch: 1887 Loss: 3139.214844\n",
      "Training Batch: 1888 Loss: 3332.556152\n",
      "Training Batch: 1889 Loss: 3256.723633\n",
      "Training Batch: 1890 Loss: 3341.380859\n",
      "Training Batch: 1891 Loss: 3353.413086\n",
      "Training Batch: 1892 Loss: 3309.348145\n",
      "Training Batch: 1893 Loss: 3668.773926\n",
      "Training Batch: 1894 Loss: 3251.963379\n",
      "Training Batch: 1895 Loss: 3590.889648\n",
      "Training Batch: 1896 Loss: 3359.497559\n",
      "Training Batch: 1897 Loss: 3476.318115\n",
      "Training Batch: 1898 Loss: 3449.278320\n",
      "Training Batch: 1899 Loss: 3384.841797\n",
      "Training Batch: 1900 Loss: 3349.627686\n",
      "Training Batch: 1901 Loss: 3366.897949\n",
      "Training Batch: 1902 Loss: 3294.748779\n",
      "Training Batch: 1903 Loss: 3354.699219\n",
      "Training Batch: 1904 Loss: 3397.875977\n",
      "Training Batch: 1905 Loss: 3350.828125\n",
      "Training Batch: 1906 Loss: 3498.526611\n",
      "Training Batch: 1907 Loss: 3660.421387\n",
      "Training Batch: 1908 Loss: 3504.658203\n",
      "Training Batch: 1909 Loss: 3689.001953\n",
      "Training Batch: 1910 Loss: 3251.575928\n",
      "Training Batch: 1911 Loss: 3321.798340\n",
      "Training Batch: 1912 Loss: 3445.335938\n",
      "Training Batch: 1913 Loss: 3391.778809\n",
      "Training Batch: 1914 Loss: 3317.670898\n",
      "Training Batch: 1915 Loss: 3312.172852\n",
      "Training Batch: 1916 Loss: 3387.737793\n",
      "Training Batch: 1917 Loss: 3326.750000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Batch: 1918 Loss: 3177.133789\n",
      "Training Batch: 1919 Loss: 3639.831543\n",
      "Training Batch: 1920 Loss: 3342.203125\n",
      "Training Batch: 1921 Loss: 3301.359619\n",
      "Training Batch: 1922 Loss: 3237.132812\n",
      "Training Batch: 1923 Loss: 3303.870117\n",
      "Training Batch: 1924 Loss: 3354.043457\n",
      "Training Batch: 1925 Loss: 3284.617188\n",
      "Training Batch: 1926 Loss: 3294.924316\n",
      "Training Batch: 1927 Loss: 3615.231934\n",
      "Training Batch: 1928 Loss: 3639.893555\n",
      "Training Batch: 1929 Loss: 3451.128906\n",
      "Training Batch: 1930 Loss: 3314.044434\n",
      "Training Batch: 1931 Loss: 3337.561768\n",
      "Training Batch: 1932 Loss: 3233.741943\n",
      "Training Batch: 1933 Loss: 3321.796387\n",
      "Training Batch: 1934 Loss: 3410.496094\n",
      "Training Batch: 1935 Loss: 3558.974121\n",
      "Training Batch: 1936 Loss: 3653.124512\n",
      "Training Batch: 1937 Loss: 3369.342285\n",
      "Training Batch: 1938 Loss: 3299.468262\n",
      "Training Batch: 1939 Loss: 3300.390381\n",
      "Training Batch: 1940 Loss: 3605.366455\n",
      "Training Batch: 1941 Loss: 3467.287109\n",
      "Training Batch: 1942 Loss: 3283.596680\n",
      "Training Batch: 1943 Loss: 3327.378418\n",
      "Training Batch: 1944 Loss: 3249.432861\n",
      "Training Batch: 1945 Loss: 3176.546387\n",
      "Training Batch: 1946 Loss: 3284.497559\n",
      "Training Batch: 1947 Loss: 3329.082031\n",
      "Training Batch: 1948 Loss: 3263.878418\n",
      "Training Batch: 1949 Loss: 3318.071289\n",
      "Training Batch: 1950 Loss: 3357.195801\n",
      "Training Batch: 1951 Loss: 3236.228027\n",
      "Training Batch: 1952 Loss: 3188.745117\n",
      "Training Batch: 1953 Loss: 3214.682617\n",
      "Training Batch: 1954 Loss: 3257.838135\n",
      "Training Batch: 1955 Loss: 3465.550049\n",
      "Training Batch: 1956 Loss: 3524.852539\n",
      "Training Batch: 1957 Loss: 3722.746582\n",
      "Training Batch: 1958 Loss: 3460.864258\n",
      "Training Batch: 1959 Loss: 3676.484375\n",
      "Training Batch: 1960 Loss: 3381.580566\n",
      "Training Batch: 1961 Loss: 3341.632080\n",
      "Training Batch: 1962 Loss: 3243.669678\n",
      "Training Batch: 1963 Loss: 3264.626465\n",
      "Training Batch: 1964 Loss: 3358.407471\n",
      "Training Batch: 1965 Loss: 3228.317139\n",
      "Training Batch: 1966 Loss: 3249.146973\n",
      "Training Batch: 1967 Loss: 3265.845215\n",
      "Training Batch: 1968 Loss: 3612.761719\n",
      "Training Batch: 1969 Loss: 3349.083008\n",
      "Training Batch: 1970 Loss: 3353.569336\n",
      "Training Batch: 1971 Loss: 3272.701660\n",
      "Training Batch: 1972 Loss: 3384.046875\n",
      "Training Batch: 1973 Loss: 3237.625488\n",
      "Training Batch: 1974 Loss: 3290.283447\n",
      "Training Batch: 1975 Loss: 3207.538330\n",
      "Training Batch: 1976 Loss: 3333.419922\n",
      "Training Batch: 1977 Loss: 3245.484131\n",
      "Training Batch: 1978 Loss: 3299.684082\n",
      "Training Batch: 1979 Loss: 3490.035156\n",
      "Training Batch: 1980 Loss: 3340.031738\n",
      "Training Batch: 1981 Loss: 3401.065430\n",
      "Training Batch: 1982 Loss: 3200.425293\n",
      "Training Batch: 1983 Loss: 3292.809082\n",
      "Training Batch: 1984 Loss: 3551.416504\n",
      "Training Batch: 1985 Loss: 3260.069824\n",
      "Training Batch: 1986 Loss: 3406.078125\n",
      "Training Batch: 1987 Loss: 3281.094727\n",
      "Training Batch: 1988 Loss: 3370.117676\n",
      "Training Batch: 1989 Loss: 3253.126953\n",
      "Training Batch: 1990 Loss: 3259.445312\n",
      "Training Batch: 1991 Loss: 3271.373779\n",
      "Training Batch: 1992 Loss: 3285.467285\n",
      "Training Batch: 1993 Loss: 3223.252930\n",
      "Training Batch: 1994 Loss: 3300.375488\n",
      "Training Batch: 1995 Loss: 3344.860107\n",
      "Training Batch: 1996 Loss: 3348.403564\n",
      "Training Batch: 1997 Loss: 3657.094238\n",
      "Training Batch: 1998 Loss: 3379.512695\n",
      "Training Batch: 1999 Loss: 3392.953125\n",
      "Training Batch: 2000 Loss: 3243.470215\n",
      "Training Batch: 2001 Loss: 3382.136230\n",
      "Training Batch: 2002 Loss: 3354.683838\n",
      "Training Batch: 2003 Loss: 3390.347168\n",
      "Training Batch: 2004 Loss: 3278.874512\n",
      "Training Batch: 2005 Loss: 3451.738770\n",
      "Training Batch: 2006 Loss: 3521.780518\n",
      "Training Batch: 2007 Loss: 3313.102295\n",
      "Training Batch: 2008 Loss: 3286.275635\n",
      "Training Batch: 2009 Loss: 3330.655762\n",
      "Training Batch: 2010 Loss: 3295.360840\n",
      "Training Batch: 2011 Loss: 3332.982422\n",
      "Training Batch: 2012 Loss: 3323.153320\n",
      "Training Batch: 2013 Loss: 3263.685791\n",
      "Training Batch: 2014 Loss: 3278.830566\n",
      "Training Batch: 2015 Loss: 3386.596191\n",
      "Training Batch: 2016 Loss: 3323.339111\n",
      "Training Batch: 2017 Loss: 3162.969482\n",
      "Training Batch: 2018 Loss: 3215.548828\n",
      "Training Batch: 2019 Loss: 3222.308838\n",
      "Training Batch: 2020 Loss: 3307.774658\n",
      "Training Batch: 2021 Loss: 3430.746094\n",
      "Training Batch: 2022 Loss: 3341.533203\n",
      "Training Batch: 2023 Loss: 3473.761230\n",
      "Training Batch: 2024 Loss: 3327.990234\n",
      "Training Batch: 2025 Loss: 3197.371826\n",
      "Training Batch: 2026 Loss: 3327.489990\n",
      "Training Batch: 2027 Loss: 3225.035156\n",
      "Training Batch: 2028 Loss: 3367.288818\n",
      "Training Batch: 2029 Loss: 3409.979004\n",
      "Training Batch: 2030 Loss: 3319.479980\n",
      "Training Batch: 2031 Loss: 3304.750977\n",
      "Training Batch: 2032 Loss: 3279.490723\n",
      "Training Batch: 2033 Loss: 3329.023193\n",
      "Training Batch: 2034 Loss: 3505.018066\n",
      "Training Batch: 2035 Loss: 3343.791504\n",
      "Training Batch: 2036 Loss: 3225.796387\n",
      "Training Batch: 2037 Loss: 3280.577637\n",
      "Training Batch: 2038 Loss: 3295.334717\n",
      "Training Batch: 2039 Loss: 3396.376709\n",
      "Training Batch: 2040 Loss: 3468.023682\n",
      "Training Batch: 2041 Loss: 3143.428223\n",
      "Training Batch: 2042 Loss: 3307.660156\n",
      "Training Batch: 2043 Loss: 3292.893555\n",
      "Training Batch: 2044 Loss: 3324.743652\n",
      "Training Batch: 2045 Loss: 3272.467773\n",
      "Training Batch: 2046 Loss: 3199.695312\n",
      "Training Batch: 2047 Loss: 3279.059570\n",
      "Training Batch: 2048 Loss: 3309.636719\n",
      "Training Batch: 2049 Loss: 3234.344727\n",
      "Training Batch: 2050 Loss: 3379.112549\n",
      "Training Batch: 2051 Loss: 3229.049072\n",
      "Training Batch: 2052 Loss: 3488.867676\n",
      "Training Batch: 2053 Loss: 3355.934570\n",
      "Training Batch: 2054 Loss: 3365.746338\n",
      "Training Batch: 2055 Loss: 3278.545410\n",
      "Training Batch: 2056 Loss: 3359.616455\n",
      "Training Batch: 2057 Loss: 3321.772461\n",
      "Training Batch: 2058 Loss: 3261.085449\n",
      "Training Batch: 2059 Loss: 3397.350586\n",
      "Training Batch: 2060 Loss: 3383.228027\n",
      "Training Batch: 2061 Loss: 3314.799072\n",
      "Training Batch: 2062 Loss: 3237.708984\n",
      "Training Batch: 2063 Loss: 3220.736572\n",
      "Training Batch: 2064 Loss: 3447.217773\n",
      "Training Batch: 2065 Loss: 3284.516357\n",
      "Training Batch: 2066 Loss: 3263.379395\n",
      "Training Batch: 2067 Loss: 3249.380859\n",
      "Training Batch: 2068 Loss: 3227.877930\n",
      "Training Batch: 2069 Loss: 3323.312500\n",
      "Training Batch: 2070 Loss: 3199.671875\n",
      "Training Batch: 2071 Loss: 3323.655518\n",
      "Training Batch: 2072 Loss: 3304.266357\n",
      "Training Batch: 2073 Loss: 3464.663818\n",
      "Training Batch: 2074 Loss: 3415.616455\n",
      "Training Batch: 2075 Loss: 4860.589844\n",
      "Training Batch: 2076 Loss: 3631.843750\n",
      "Training Batch: 2077 Loss: 3472.037109\n",
      "Training Batch: 2078 Loss: 3440.552002\n",
      "Training Batch: 2079 Loss: 3481.093018\n",
      "Training Batch: 2080 Loss: 3405.921387\n",
      "Training Batch: 2081 Loss: 3372.089844\n",
      "Training Batch: 2082 Loss: 3407.450684\n",
      "Training Batch: 2083 Loss: 3272.340820\n",
      "Training Batch: 2084 Loss: 3391.301758\n",
      "Training Batch: 2085 Loss: 3322.163574\n",
      "Training Batch: 2086 Loss: 3722.434570\n",
      "Training Batch: 2087 Loss: 3674.079346\n",
      "Training Batch: 2088 Loss: 3370.998535\n",
      "Training Batch: 2089 Loss: 3498.805420\n",
      "Training Batch: 2090 Loss: 3270.512207\n",
      "Training Batch: 2091 Loss: 3782.352783\n",
      "Training Batch: 2092 Loss: 4192.436523\n",
      "Training Batch: 2093 Loss: 3613.625000\n",
      "Training Batch: 2094 Loss: 3500.922852\n",
      "Training Batch: 2095 Loss: 3719.395996\n",
      "Training Batch: 2096 Loss: 3311.203125\n",
      "Training Batch: 2097 Loss: 3348.511719\n",
      "Training Batch: 2098 Loss: 3339.934814\n",
      "Training Batch: 2099 Loss: 3233.004395\n",
      "Training Batch: 2100 Loss: 3335.583496\n",
      "Training Batch: 2101 Loss: 3354.341309\n",
      "Training Batch: 2102 Loss: 3256.969238\n",
      "Training Batch: 2103 Loss: 3294.642334\n",
      "Training Batch: 2104 Loss: 3246.728516\n",
      "Training Batch: 2105 Loss: 3262.520020\n",
      "Training Batch: 2106 Loss: 3190.474121\n",
      "Training Batch: 2107 Loss: 3238.649414\n",
      "Training Batch: 2108 Loss: 3198.035156\n",
      "Training Batch: 2109 Loss: 3246.171875\n",
      "Training Batch: 2110 Loss: 3253.999023\n",
      "Training Batch: 2111 Loss: 3515.481445\n",
      "Training Batch: 2112 Loss: 3365.957031\n",
      "Training Batch: 2113 Loss: 3327.770508\n",
      "Training Batch: 2114 Loss: 3294.022705\n",
      "Training Batch: 2115 Loss: 3272.164062\n",
      "Training Batch: 2116 Loss: 3606.994141\n",
      "Training Batch: 2117 Loss: 3430.449707\n",
      "Training Batch: 2118 Loss: 3326.730957\n",
      "Training Batch: 2119 Loss: 3312.889160\n",
      "Training Batch: 2120 Loss: 3384.185547\n",
      "Training Batch: 2121 Loss: 3310.759033\n",
      "Training Batch: 2122 Loss: 3236.263672\n",
      "Training Batch: 2123 Loss: 3232.411865\n",
      "Training Batch: 2124 Loss: 3220.115234\n",
      "Training Batch: 2125 Loss: 3334.215820\n",
      "Training Batch: 2126 Loss: 3259.739258\n",
      "Training Batch: 2127 Loss: 3288.098145\n",
      "Training Batch: 2128 Loss: 3388.440918\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Batch: 2129 Loss: 3374.930664\n",
      "Training Batch: 2130 Loss: 3614.329102\n",
      "Training Batch: 2131 Loss: 3456.078125\n",
      "Training Batch: 2132 Loss: 3533.379395\n",
      "Training Batch: 2133 Loss: 3418.460205\n",
      "Training Batch: 2134 Loss: 3406.759277\n",
      "Training Batch: 2135 Loss: 3287.072266\n",
      "Training Batch: 2136 Loss: 3311.680908\n",
      "Training Batch: 2137 Loss: 3327.038086\n",
      "Training Batch: 2138 Loss: 3389.339844\n",
      "Training Batch: 2139 Loss: 3398.411621\n",
      "Training Batch: 2140 Loss: 3427.888184\n",
      "Training Batch: 2141 Loss: 3396.601562\n",
      "Training Batch: 2142 Loss: 3722.930664\n",
      "Training Batch: 2143 Loss: 3579.138916\n",
      "Training Batch: 2144 Loss: 3408.023193\n",
      "Training Batch: 2145 Loss: 3382.304688\n",
      "Training Batch: 2146 Loss: 3340.022705\n",
      "Training Batch: 2147 Loss: 3263.284668\n",
      "Training Batch: 2148 Loss: 3189.727539\n",
      "Training Batch: 2149 Loss: 3477.326660\n",
      "Training Batch: 2150 Loss: 3446.873779\n",
      "Training Batch: 2151 Loss: 3505.933105\n",
      "Training Batch: 2152 Loss: 3367.148438\n",
      "Training Batch: 2153 Loss: 3412.668457\n",
      "Training Batch: 2154 Loss: 3625.246826\n",
      "Training Batch: 2155 Loss: 3336.261230\n",
      "Training Batch: 2156 Loss: 3415.368652\n",
      "Training Batch: 2157 Loss: 3199.988770\n",
      "Training Batch: 2158 Loss: 3316.418457\n",
      "Training Batch: 2159 Loss: 3373.008057\n",
      "Training Batch: 2160 Loss: 3296.974365\n",
      "Training Batch: 2161 Loss: 3299.892090\n",
      "Training Batch: 2162 Loss: 3246.819824\n",
      "Training Batch: 2163 Loss: 3274.892090\n",
      "Training Batch: 2164 Loss: 3407.659180\n",
      "Training Batch: 2165 Loss: 3421.096924\n",
      "Training Batch: 2166 Loss: 3293.431641\n",
      "Training Batch: 2167 Loss: 3421.754395\n",
      "Training Batch: 2168 Loss: 3325.827881\n",
      "Training Batch: 2169 Loss: 3387.695312\n",
      "Training Batch: 2170 Loss: 3281.110352\n",
      "Training Batch: 2171 Loss: 3335.811035\n",
      "Training Batch: 2172 Loss: 3487.437500\n",
      "Training Batch: 2173 Loss: 3330.096924\n",
      "Training Batch: 2174 Loss: 3361.460449\n",
      "Training Batch: 2175 Loss: 3235.501953\n",
      "Training Batch: 2176 Loss: 3253.033203\n",
      "Training Batch: 2177 Loss: 3213.528809\n",
      "Training Batch: 2178 Loss: 3318.410156\n",
      "Training Batch: 2179 Loss: 3375.707764\n",
      "Training Batch: 2180 Loss: 3194.395020\n",
      "Training Batch: 2181 Loss: 3322.125488\n",
      "Training Batch: 2182 Loss: 3172.126465\n",
      "Training Batch: 2183 Loss: 3276.064453\n",
      "Training Batch: 2184 Loss: 3256.692383\n",
      "Training Batch: 2185 Loss: 3403.158203\n",
      "Training Batch: 2186 Loss: 3328.845703\n",
      "Training Batch: 2187 Loss: 3296.555420\n",
      "Training Batch: 2188 Loss: 3254.570801\n",
      "Training Batch: 2189 Loss: 3142.006104\n",
      "Training Batch: 2190 Loss: 3256.360352\n",
      "Training Batch: 2191 Loss: 3302.939453\n",
      "Training Batch: 2192 Loss: 3315.508301\n",
      "Training Batch: 2193 Loss: 3201.977539\n",
      "Training Batch: 2194 Loss: 3165.519287\n",
      "Training Batch: 2195 Loss: 3515.567871\n",
      "Training Batch: 2196 Loss: 3246.536133\n",
      "Training Batch: 2197 Loss: 3235.222656\n",
      "Training Batch: 2198 Loss: 3295.046387\n",
      "Training Batch: 2199 Loss: 3457.589844\n",
      "Training Batch: 2200 Loss: 3308.035889\n",
      "Training Batch: 2201 Loss: 3252.941406\n",
      "Training Batch: 2202 Loss: 3242.523438\n",
      "Training Batch: 2203 Loss: 3267.004639\n",
      "Training Batch: 2204 Loss: 3324.876465\n",
      "Training Batch: 2205 Loss: 3333.104004\n",
      "Training Batch: 2206 Loss: 3225.139160\n",
      "Training Batch: 2207 Loss: 3345.724121\n",
      "Training Batch: 2208 Loss: 3259.318604\n",
      "Training Batch: 2209 Loss: 3359.637207\n",
      "Training Batch: 2210 Loss: 3261.518066\n",
      "Training Batch: 2211 Loss: 3281.729736\n",
      "Training Batch: 2212 Loss: 3312.042969\n",
      "Training Batch: 2213 Loss: 3438.822266\n",
      "Training Batch: 2214 Loss: 3389.324707\n",
      "Training Batch: 2215 Loss: 3426.642090\n",
      "Training Batch: 2216 Loss: 3555.779541\n",
      "Training Batch: 2217 Loss: 3413.536621\n",
      "Training Batch: 2218 Loss: 3520.776367\n",
      "Training Batch: 2219 Loss: 3524.764648\n",
      "Training Batch: 2220 Loss: 3395.478027\n",
      "Training Batch: 2221 Loss: 3278.954590\n",
      "Training Batch: 2222 Loss: 3231.978516\n",
      "Training Batch: 2223 Loss: 3283.666992\n",
      "Training Batch: 2224 Loss: 3312.052734\n",
      "Training Batch: 2225 Loss: 3256.305664\n",
      "Training Batch: 2226 Loss: 3230.755371\n",
      "Training Batch: 2227 Loss: 3435.541504\n",
      "Training Batch: 2228 Loss: 3348.650879\n",
      "Training Batch: 2229 Loss: 3344.147705\n",
      "Training Batch: 2230 Loss: 3310.734375\n",
      "Training Batch: 2231 Loss: 3452.775635\n",
      "Training Batch: 2232 Loss: 3348.723633\n",
      "Training Batch: 2233 Loss: 3377.617676\n",
      "Training Batch: 2234 Loss: 3598.052002\n",
      "Training Batch: 2235 Loss: 3416.518311\n",
      "Training Batch: 2236 Loss: 3352.123047\n",
      "Training Batch: 2237 Loss: 3340.637207\n",
      "Training Batch: 2238 Loss: 3374.139893\n",
      "Training Batch: 2239 Loss: 3468.938965\n",
      "Training Batch: 2240 Loss: 3260.209473\n",
      "Training Batch: 2241 Loss: 3282.401611\n",
      "Training Batch: 2242 Loss: 3292.056396\n",
      "Training Batch: 2243 Loss: 3230.218506\n",
      "Training Batch: 2244 Loss: 3295.906738\n",
      "Training Batch: 2245 Loss: 3183.948242\n",
      "Training Batch: 2246 Loss: 3385.045898\n",
      "Training Batch: 2247 Loss: 3384.322510\n",
      "Training Batch: 2248 Loss: 3394.471191\n",
      "Training Batch: 2249 Loss: 3443.188965\n",
      "Training Batch: 2250 Loss: 3317.623535\n",
      "Training Batch: 2251 Loss: 3441.799316\n",
      "Training Batch: 2252 Loss: 3400.431641\n",
      "Training Batch: 2253 Loss: 3288.916016\n",
      "Training Batch: 2254 Loss: 3479.538574\n",
      "Training Batch: 2255 Loss: 3480.928223\n",
      "Training Batch: 2256 Loss: 3338.107666\n",
      "Training Batch: 2257 Loss: 3549.444336\n",
      "Training Batch: 2258 Loss: 3525.180908\n",
      "Training Batch: 2259 Loss: 3217.582275\n",
      "Training Batch: 2260 Loss: 3306.653564\n",
      "Training Batch: 2261 Loss: 3298.295898\n",
      "Training Batch: 2262 Loss: 3374.727051\n",
      "Training Batch: 2263 Loss: 3466.961426\n",
      "Training Batch: 2264 Loss: 3261.415771\n",
      "Training Batch: 2265 Loss: 3364.522217\n",
      "Training Batch: 2266 Loss: 3286.130859\n",
      "Training Batch: 2267 Loss: 3454.639648\n",
      "Training Batch: 2268 Loss: 3596.946289\n",
      "Training Batch: 2269 Loss: 3461.893066\n",
      "Training Batch: 2270 Loss: 3304.659424\n",
      "Training Batch: 2271 Loss: 3210.998535\n",
      "Training Batch: 2272 Loss: 3264.896484\n",
      "Training Batch: 2273 Loss: 3292.152344\n",
      "Training Batch: 2274 Loss: 3375.785889\n",
      "Training Batch: 2275 Loss: 3324.587158\n",
      "Training Batch: 2276 Loss: 3449.076172\n",
      "Training Batch: 2277 Loss: 3342.756836\n",
      "Training Batch: 2278 Loss: 3384.051270\n",
      "Training Batch: 2279 Loss: 3401.880859\n",
      "Training Batch: 2280 Loss: 3330.157715\n",
      "Training Batch: 2281 Loss: 3235.734863\n",
      "Training Batch: 2282 Loss: 3169.818359\n",
      "Training Batch: 2283 Loss: 3298.341309\n",
      "Training Batch: 2284 Loss: 3272.171631\n",
      "Training Batch: 2285 Loss: 3338.492188\n",
      "Training Batch: 2286 Loss: 3242.534180\n",
      "Training Batch: 2287 Loss: 3280.359375\n",
      "Training Batch: 2288 Loss: 3258.869873\n",
      "Training Batch: 2289 Loss: 3207.574219\n",
      "Training Batch: 2290 Loss: 3152.468750\n",
      "Training Batch: 2291 Loss: 3481.964600\n",
      "Training Batch: 2292 Loss: 3397.428955\n",
      "Training Batch: 2293 Loss: 3396.072754\n",
      "Training Batch: 2294 Loss: 3411.493896\n",
      "Training Batch: 2295 Loss: 3377.592041\n",
      "Training Batch: 2296 Loss: 3379.215088\n",
      "Training Batch: 2297 Loss: 3393.452148\n",
      "Training Batch: 2298 Loss: 3444.237793\n",
      "Training Batch: 2299 Loss: 3415.513672\n",
      "Training Batch: 2300 Loss: 3258.709473\n",
      "Training Batch: 2301 Loss: 3685.575684\n",
      "Training Batch: 2302 Loss: 3360.675781\n",
      "Training Batch: 2303 Loss: 3501.610840\n",
      "Training Batch: 2304 Loss: 3358.285156\n",
      "Training Batch: 2305 Loss: 3394.795898\n",
      "Training Batch: 2306 Loss: 3360.451172\n",
      "Training Batch: 2307 Loss: 3706.645508\n",
      "Training Batch: 2308 Loss: 3722.885254\n",
      "Training Batch: 2309 Loss: 3374.407715\n",
      "Training Batch: 2310 Loss: 3527.944336\n",
      "Training Batch: 2311 Loss: 3340.136230\n",
      "Training Batch: 2312 Loss: 3457.202393\n",
      "Training Batch: 2313 Loss: 3497.597168\n",
      "Training Batch: 2314 Loss: 3330.201904\n",
      "Training Batch: 2315 Loss: 3460.765869\n",
      "Training Batch: 2316 Loss: 3281.281250\n",
      "Training Batch: 2317 Loss: 3263.284912\n",
      "Training Batch: 2318 Loss: 3180.973633\n",
      "Training Batch: 2319 Loss: 3664.447021\n",
      "Training Batch: 2320 Loss: 3469.614014\n",
      "Training Batch: 2321 Loss: 3263.870117\n",
      "Training Batch: 2322 Loss: 3246.605469\n",
      "Training Batch: 2323 Loss: 3302.241455\n",
      "Training Batch: 2324 Loss: 3521.500244\n",
      "Training Batch: 2325 Loss: 3425.633789\n",
      "Training Batch: 2326 Loss: 3313.368164\n",
      "Training Batch: 2327 Loss: 3272.587646\n",
      "Training Batch: 2328 Loss: 3378.955811\n",
      "Training Batch: 2329 Loss: 3431.269043\n",
      "Training Batch: 2330 Loss: 3270.454102\n",
      "Training Batch: 2331 Loss: 3191.935059\n",
      "Training Batch: 2332 Loss: 3289.318848\n",
      "Training Batch: 2333 Loss: 3271.577881\n",
      "Training Batch: 2334 Loss: 3217.538574\n",
      "Training Batch: 2335 Loss: 3214.017822\n",
      "Training Batch: 2336 Loss: 3213.227051\n",
      "Training Batch: 2337 Loss: 3228.585205\n",
      "Training Batch: 2338 Loss: 3221.232422\n",
      "Training Batch: 2339 Loss: 3316.317871\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Batch: 2340 Loss: 3453.044434\n",
      "Training Batch: 2341 Loss: 3487.099609\n",
      "Training Batch: 2342 Loss: 3292.897461\n",
      "Training Batch: 2343 Loss: 3228.485352\n",
      "Training Batch: 2344 Loss: 3254.973633\n",
      "Training Batch: 2345 Loss: 3282.811523\n",
      "Training Batch: 2346 Loss: 3397.586426\n",
      "Training Batch: 2347 Loss: 3200.614746\n",
      "Training Batch: 2348 Loss: 3291.195312\n",
      "Training Batch: 2349 Loss: 3406.287598\n",
      "Training Batch: 2350 Loss: 3380.928955\n",
      "Training Batch: 2351 Loss: 3501.114502\n",
      "Training Batch: 2352 Loss: 3550.355469\n",
      "Training Batch: 2353 Loss: 3218.892334\n",
      "Training Batch: 2354 Loss: 3265.303711\n",
      "Training Batch: 2355 Loss: 3415.806641\n",
      "Training Batch: 2356 Loss: 3192.641113\n",
      "Training Batch: 2357 Loss: 3317.571777\n",
      "Training Batch: 2358 Loss: 3460.701172\n",
      "Training Batch: 2359 Loss: 3361.603027\n",
      "Training Batch: 2360 Loss: 3340.829590\n",
      "Training Batch: 2361 Loss: 3485.100342\n",
      "Training Batch: 2362 Loss: 3337.085938\n",
      "Training Batch: 2363 Loss: 3272.316406\n",
      "Training Batch: 2364 Loss: 3301.072510\n",
      "Training Batch: 2365 Loss: 3235.967773\n",
      "Training Batch: 2366 Loss: 3209.107422\n",
      "Training Batch: 2367 Loss: 3326.121338\n",
      "Training Batch: 2368 Loss: 3194.856445\n",
      "Training Batch: 2369 Loss: 3420.367920\n",
      "Training Batch: 2370 Loss: 3332.509277\n",
      "Training Batch: 2371 Loss: 3177.815186\n",
      "Training Batch: 2372 Loss: 3192.785156\n",
      "Training Batch: 2373 Loss: 3306.423340\n",
      "Training Batch: 2374 Loss: 3227.357178\n",
      "Training Batch: 2375 Loss: 3321.492676\n",
      "Training Batch: 2376 Loss: 3484.663330\n",
      "Training Batch: 2377 Loss: 3289.666504\n",
      "Training Batch: 2378 Loss: 3396.133301\n",
      "Training Batch: 2379 Loss: 3286.737793\n",
      "Training Batch: 2380 Loss: 3384.264160\n",
      "Training Batch: 2381 Loss: 3238.899414\n",
      "Training Batch: 2382 Loss: 3403.644287\n",
      "Training Batch: 2383 Loss: 3310.831543\n",
      "Training Batch: 2384 Loss: 3178.436523\n",
      "Training Batch: 2385 Loss: 3667.987793\n",
      "Training Batch: 2386 Loss: 3570.609375\n",
      "Training Batch: 2387 Loss: 3634.999023\n",
      "Training Batch: 2388 Loss: 3211.693359\n",
      "Training Batch: 2389 Loss: 3369.012207\n",
      "Training Batch: 2390 Loss: 3268.229980\n",
      "Training Batch: 2391 Loss: 3272.231934\n",
      "Training Batch: 2392 Loss: 3274.248535\n",
      "Training Batch: 2393 Loss: 3221.419922\n",
      "Training Batch: 2394 Loss: 3569.933105\n",
      "Training Batch: 2395 Loss: 3279.465820\n",
      "Training Batch: 2396 Loss: 3326.831299\n",
      "Training Batch: 2397 Loss: 3458.580078\n",
      "Training Batch: 2398 Loss: 3308.748047\n",
      "Training Batch: 2399 Loss: 3277.645752\n",
      "Training Batch: 2400 Loss: 3267.647461\n",
      "Training Batch: 2401 Loss: 3324.590820\n",
      "Training Batch: 2402 Loss: 3368.540771\n",
      "Training Batch: 2403 Loss: 3227.305420\n",
      "Training Batch: 2404 Loss: 3381.456299\n",
      "Training Batch: 2405 Loss: 3437.759033\n",
      "Training Batch: 2406 Loss: 3421.207031\n",
      "Training Batch: 2407 Loss: 3464.788086\n",
      "Training Batch: 2408 Loss: 3363.050293\n",
      "Training Batch: 2409 Loss: 3239.293945\n",
      "Training Batch: 2410 Loss: 3606.958496\n",
      "Training Batch: 2411 Loss: 3472.965820\n",
      "Training Batch: 2412 Loss: 3230.691895\n",
      "Training Batch: 2413 Loss: 3367.731445\n",
      "Training Batch: 2414 Loss: 3450.790039\n",
      "Training Batch: 2415 Loss: 3275.187988\n",
      "Training Batch: 2416 Loss: 3289.227051\n",
      "Training Batch: 2417 Loss: 3367.870605\n",
      "Training Batch: 2418 Loss: 3246.697754\n",
      "Training Batch: 2419 Loss: 3493.902100\n",
      "Training Batch: 2420 Loss: 3303.268066\n",
      "Training Batch: 2421 Loss: 3245.734863\n",
      "Training Batch: 2422 Loss: 3308.337891\n",
      "Training Batch: 2423 Loss: 3220.403320\n",
      "Training Batch: 2424 Loss: 3219.055420\n",
      "Training Batch: 2425 Loss: 3244.964600\n",
      "Training Batch: 2426 Loss: 3214.851562\n",
      "Training Batch: 2427 Loss: 3250.523438\n",
      "Training Batch: 2428 Loss: 3335.509033\n",
      "Training Batch: 2429 Loss: 3228.738770\n",
      "Training Batch: 2430 Loss: 3182.489502\n",
      "Training Batch: 2431 Loss: 3141.145996\n",
      "Training Batch: 2432 Loss: 3379.387939\n",
      "Training Batch: 2433 Loss: 3447.398438\n",
      "Training Batch: 2434 Loss: 3301.680664\n",
      "Training Batch: 2435 Loss: 3360.751221\n",
      "Training Batch: 2436 Loss: 3255.115723\n",
      "Training Batch: 2437 Loss: 3394.625244\n",
      "Training Batch: 2438 Loss: 3404.706055\n",
      "Training Batch: 2439 Loss: 3307.493652\n",
      "Training Batch: 2440 Loss: 4217.461914\n",
      "Training Batch: 2441 Loss: 3385.332031\n",
      "Training Batch: 2442 Loss: 3470.387695\n",
      "Training Batch: 2443 Loss: 3413.462891\n",
      "Training Batch: 2444 Loss: 3652.206299\n",
      "Training Batch: 2445 Loss: 3419.156738\n",
      "Training Batch: 2446 Loss: 3335.509766\n",
      "Training Batch: 2447 Loss: 3360.011230\n",
      "Training Batch: 2448 Loss: 3269.264160\n",
      "Training Batch: 2449 Loss: 3247.801758\n",
      "Training Batch: 2450 Loss: 3270.675781\n",
      "Training Batch: 2451 Loss: 3252.899414\n",
      "Training Batch: 2452 Loss: 3347.534668\n",
      "Training Batch: 2453 Loss: 3264.846924\n",
      "Training Batch: 2454 Loss: 3292.346680\n",
      "Training Batch: 2455 Loss: 3367.990234\n",
      "Training Batch: 2456 Loss: 3275.516602\n",
      "Training Batch: 2457 Loss: 3442.104980\n",
      "Training Batch: 2458 Loss: 3285.351074\n",
      "Training Batch: 2459 Loss: 3330.335449\n",
      "Training Batch: 2460 Loss: 3358.994629\n",
      "Training Batch: 2461 Loss: 3419.578369\n",
      "Training Batch: 2462 Loss: 3282.908203\n",
      "Training Batch: 2463 Loss: 3243.105225\n",
      "Training Batch: 2464 Loss: 3374.293457\n",
      "Training Batch: 2465 Loss: 3273.748047\n",
      "Training Batch: 2466 Loss: 3294.499756\n",
      "Training Batch: 2467 Loss: 3377.777100\n",
      "Training Batch: 2468 Loss: 3324.269775\n",
      "Training Batch: 2469 Loss: 3208.624023\n",
      "Training Batch: 2470 Loss: 3238.307861\n",
      "Training Batch: 2471 Loss: 3291.965576\n",
      "Training Batch: 2472 Loss: 3235.256592\n",
      "Training Batch: 2473 Loss: 3454.839355\n",
      "Training Batch: 2474 Loss: 3343.345215\n",
      "Training Batch: 2475 Loss: 3338.635254\n",
      "Training Batch: 2476 Loss: 3238.385986\n",
      "Training Batch: 2477 Loss: 3312.604980\n",
      "Training Batch: 2478 Loss: 3395.954590\n",
      "Training Batch: 2479 Loss: 3417.039551\n",
      "Training Batch: 2480 Loss: 3247.283691\n",
      "Training Batch: 2481 Loss: 3387.090332\n",
      "Training Batch: 2482 Loss: 3348.121338\n",
      "Training Batch: 2483 Loss: 3442.897461\n",
      "Training Batch: 2484 Loss: 3203.579102\n",
      "Training Batch: 2485 Loss: 3283.137695\n",
      "Training Batch: 2486 Loss: 3256.483398\n",
      "Training Batch: 2487 Loss: 3300.572266\n",
      "Training Batch: 2488 Loss: 3234.873535\n",
      "Training Batch: 2489 Loss: 3245.496338\n",
      "Training Batch: 2490 Loss: 3151.666260\n",
      "Training Batch: 2491 Loss: 3320.802490\n",
      "Training Batch: 2492 Loss: 3333.979004\n",
      "Training Batch: 2493 Loss: 3204.910889\n",
      "Training Batch: 2494 Loss: 3331.547363\n",
      "Training Batch: 2495 Loss: 3257.416992\n",
      "Training Batch: 2496 Loss: 3460.276367\n",
      "Training Batch: 2497 Loss: 3308.711914\n",
      "Training Batch: 2498 Loss: 3235.766113\n",
      "Training Batch: 2499 Loss: 3268.489258\n",
      "Training Batch: 2500 Loss: 3234.837891\n",
      "Training Batch: 2501 Loss: 3282.643066\n",
      "Training Batch: 2502 Loss: 3547.956543\n",
      "Training Batch: 2503 Loss: 3297.057129\n",
      "Training Batch: 2504 Loss: 3208.887207\n",
      "Training Batch: 2505 Loss: 3418.004150\n",
      "Training Batch: 2506 Loss: 3349.491699\n",
      "Training Batch: 2507 Loss: 3294.595459\n",
      "Training Batch: 2508 Loss: 3265.998535\n",
      "Training Batch: 2509 Loss: 3363.807617\n",
      "Training Batch: 2510 Loss: 3413.422607\n",
      "Training Batch: 2511 Loss: 3292.665527\n",
      "Training Batch: 2512 Loss: 3357.182129\n",
      "Training Batch: 2513 Loss: 3440.478271\n",
      "Training Batch: 2514 Loss: 3826.963379\n",
      "Training Batch: 2515 Loss: 3486.614746\n",
      "Training Batch: 2516 Loss: 3252.795898\n",
      "Training Batch: 2517 Loss: 3437.241943\n",
      "Training Batch: 2518 Loss: 3429.700684\n",
      "Training Batch: 2519 Loss: 3350.940430\n",
      "Training Batch: 2520 Loss: 3435.743652\n",
      "Training Batch: 2521 Loss: 3356.855957\n",
      "Training Batch: 2522 Loss: 3295.772949\n",
      "Training Batch: 2523 Loss: 3970.731445\n",
      "Training Batch: 2524 Loss: 3792.002441\n",
      "Training Batch: 2525 Loss: 3861.374512\n",
      "Training Batch: 2526 Loss: 3528.303223\n",
      "Training Batch: 2527 Loss: 3342.599121\n",
      "Training Batch: 2528 Loss: 3421.376465\n",
      "Training Batch: 2529 Loss: 3271.059814\n",
      "Training Batch: 2530 Loss: 3622.021729\n",
      "Training Batch: 2531 Loss: 3638.004395\n",
      "Training Batch: 2532 Loss: 3342.902344\n",
      "Training Batch: 2533 Loss: 3483.563477\n",
      "Training Batch: 2534 Loss: 3309.284180\n",
      "Training Batch: 2535 Loss: 3331.505615\n",
      "Training Batch: 2536 Loss: 3175.805664\n",
      "Training Batch: 2537 Loss: 3239.988770\n",
      "Training Batch: 2538 Loss: 3372.898926\n",
      "Training Batch: 2539 Loss: 3431.012207\n",
      "Training Batch: 2540 Loss: 3345.807129\n",
      "Training Batch: 2541 Loss: 3323.496582\n",
      "Training Batch: 2542 Loss: 3415.653320\n",
      "Training Batch: 2543 Loss: 3417.586182\n",
      "Training Batch: 2544 Loss: 3310.160156\n",
      "Training Batch: 2545 Loss: 3447.032715\n",
      "Training Batch: 2546 Loss: 3297.010010\n",
      "Training Batch: 2547 Loss: 3640.772461\n",
      "Training Batch: 2548 Loss: 3396.681641\n",
      "Training Batch: 2549 Loss: 3324.007324\n",
      "Training Batch: 2550 Loss: 3418.294434\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Batch: 2551 Loss: 3413.901855\n",
      "Training Batch: 2552 Loss: 3214.411133\n",
      "Training Batch: 2553 Loss: 3594.177734\n",
      "Training Batch: 2554 Loss: 3514.225830\n",
      "Training Batch: 2555 Loss: 3198.249512\n",
      "Training Batch: 2556 Loss: 3179.798828\n",
      "Training Batch: 2557 Loss: 3391.180664\n",
      "Training Batch: 2558 Loss: 3541.273926\n",
      "Training Batch: 2559 Loss: 3487.854736\n",
      "Training Batch: 2560 Loss: 3350.704834\n",
      "Training Batch: 2561 Loss: 3248.622803\n",
      "Training Batch: 2562 Loss: 3305.648682\n",
      "Training Batch: 2563 Loss: 3280.056396\n",
      "Training Batch: 2564 Loss: 3316.532715\n",
      "Training Batch: 2565 Loss: 3265.298340\n",
      "Training Batch: 2566 Loss: 3256.127441\n",
      "Training Batch: 2567 Loss: 3354.163086\n",
      "Training Batch: 2568 Loss: 3230.430176\n",
      "Training Batch: 2569 Loss: 3414.543945\n",
      "Training Batch: 2570 Loss: 3361.792480\n",
      "Training Batch: 2571 Loss: 3351.715820\n",
      "Training Batch: 2572 Loss: 3351.408691\n",
      "Training Batch: 2573 Loss: 3425.603516\n",
      "Training Batch: 2574 Loss: 3195.886230\n",
      "Training Batch: 2575 Loss: 3261.981934\n",
      "Training Batch: 2576 Loss: 3249.858643\n",
      "Training Batch: 2577 Loss: 3612.800781\n",
      "Training Batch: 2578 Loss: 3769.506836\n",
      "Training Batch: 2579 Loss: 3362.293945\n",
      "Training Batch: 2580 Loss: 3241.025879\n",
      "Training Batch: 2581 Loss: 3287.298340\n",
      "Training Batch: 2582 Loss: 3312.304443\n",
      "Training Batch: 2583 Loss: 3331.433594\n",
      "Training Batch: 2584 Loss: 3358.185303\n",
      "Training Batch: 2585 Loss: 3279.348389\n",
      "Training Batch: 2586 Loss: 3251.762207\n",
      "Training Batch: 2587 Loss: 3265.447266\n",
      "Training Batch: 2588 Loss: 3145.950195\n",
      "Training Batch: 2589 Loss: 3305.905762\n",
      "Training Batch: 2590 Loss: 3318.309326\n",
      "Training Batch: 2591 Loss: 3195.342773\n",
      "Training Batch: 2592 Loss: 3160.328857\n",
      "Training Batch: 2593 Loss: 3218.077637\n",
      "Training Batch: 2594 Loss: 3383.998535\n",
      "Training Batch: 2595 Loss: 3349.780273\n",
      "Training Batch: 2596 Loss: 3247.492188\n",
      "Training Batch: 2597 Loss: 3209.145264\n",
      "Training Batch: 2598 Loss: 3353.847168\n",
      "Training Batch: 2599 Loss: 3472.862793\n",
      "Training Batch: 2600 Loss: 3241.008789\n",
      "Training Batch: 2601 Loss: 3300.840088\n",
      "Training Batch: 2602 Loss: 3203.069336\n",
      "Training Batch: 2603 Loss: 3274.542480\n",
      "Training Batch: 2604 Loss: 3428.074219\n",
      "Training Batch: 2605 Loss: 3323.764648\n",
      "Training Batch: 2606 Loss: 3383.945801\n",
      "Training Batch: 2607 Loss: 3314.325195\n",
      "Training Batch: 2608 Loss: 3241.366211\n",
      "Training Batch: 2609 Loss: 3200.612549\n",
      "Training Batch: 2610 Loss: 3221.760742\n",
      "Training Batch: 2611 Loss: 3214.259277\n",
      "Training Batch: 2612 Loss: 3259.979004\n",
      "Training Batch: 2613 Loss: 3233.249023\n",
      "Training Batch: 2614 Loss: 3226.377441\n",
      "Training Batch: 2615 Loss: 3199.308105\n",
      "Training Batch: 2616 Loss: 3341.153809\n",
      "Training Batch: 2617 Loss: 3316.624512\n",
      "Training Batch: 2618 Loss: 3377.555664\n",
      "Training Batch: 2619 Loss: 3231.215332\n",
      "Training Batch: 2620 Loss: 3166.055664\n",
      "Training Batch: 2621 Loss: 3371.263428\n",
      "Training Batch: 2622 Loss: 3487.745361\n",
      "Training Batch: 2623 Loss: 3372.744629\n",
      "Training Batch: 2624 Loss: 3404.394531\n",
      "Training Batch: 2625 Loss: 3395.547852\n",
      "Training Batch: 2626 Loss: 3314.822021\n",
      "Training Batch: 2627 Loss: 3477.929199\n",
      "Training Batch: 2628 Loss: 3361.272949\n",
      "Training Batch: 2629 Loss: 3371.771973\n",
      "Training Batch: 2630 Loss: 3291.439697\n",
      "Training Batch: 2631 Loss: 3235.781738\n",
      "Training Batch: 2632 Loss: 3378.720459\n",
      "Training Batch: 2633 Loss: 3235.938477\n",
      "Training Batch: 2634 Loss: 3191.036621\n",
      "Training Batch: 2635 Loss: 3219.757812\n",
      "Training Batch: 2636 Loss: 3232.968750\n",
      "Training Batch: 2637 Loss: 3278.229492\n",
      "Training Batch: 2638 Loss: 3226.084961\n",
      "Training Batch: 2639 Loss: 3250.977051\n",
      "Training Batch: 2640 Loss: 3239.283203\n",
      "Training Batch: 2641 Loss: 3302.146973\n",
      "Training Batch: 2642 Loss: 3270.455078\n",
      "Training Batch: 2643 Loss: 3354.530273\n",
      "Training Batch: 2644 Loss: 3659.998779\n",
      "Training Batch: 2645 Loss: 3295.822998\n",
      "Training Batch: 2646 Loss: 3456.131348\n",
      "Training Batch: 2647 Loss: 3357.720459\n",
      "Training Batch: 2648 Loss: 3252.023438\n",
      "Training Batch: 2649 Loss: 3545.838867\n",
      "Training Batch: 2650 Loss: 3442.623535\n",
      "Training Batch: 2651 Loss: 3307.937988\n",
      "Training Batch: 2652 Loss: 3287.773438\n",
      "Training Batch: 2653 Loss: 3297.760498\n",
      "Training Batch: 2654 Loss: 3290.676514\n",
      "Training Batch: 2655 Loss: 3232.444092\n",
      "Training Batch: 2656 Loss: 3487.729004\n",
      "Training Batch: 2657 Loss: 3320.335205\n",
      "Training Batch: 2658 Loss: 3348.553711\n",
      "Training Batch: 2659 Loss: 3338.405762\n",
      "Training Batch: 2660 Loss: 3437.434814\n",
      "Training Batch: 2661 Loss: 3363.497559\n",
      "Training Batch: 2662 Loss: 3260.134766\n",
      "Training Batch: 2663 Loss: 3204.613281\n",
      "Training Batch: 2664 Loss: 3279.354980\n",
      "Training Batch: 2665 Loss: 3364.282715\n",
      "Training Batch: 2666 Loss: 3493.736328\n",
      "Training Batch: 2667 Loss: 3312.619141\n",
      "Training Batch: 2668 Loss: 3335.364502\n",
      "Training Batch: 2669 Loss: 3388.235840\n",
      "Training Batch: 2670 Loss: 3279.626953\n",
      "Training Batch: 2671 Loss: 3361.780273\n",
      "Training Batch: 2672 Loss: 3377.940186\n",
      "Training Batch: 2673 Loss: 3342.462646\n",
      "Training Batch: 2674 Loss: 3332.788086\n",
      "Training Batch: 2675 Loss: 3358.705566\n",
      "Training Batch: 2676 Loss: 3342.217773\n",
      "Training Batch: 2677 Loss: 3252.312500\n",
      "Training Batch: 2678 Loss: 3217.647949\n",
      "Training Batch: 2679 Loss: 3260.724121\n",
      "Training Batch: 2680 Loss: 3423.292969\n",
      "Training Batch: 2681 Loss: 3337.947998\n",
      "Training Batch: 2682 Loss: 3522.897949\n",
      "Training Batch: 2683 Loss: 3453.954102\n",
      "Training Batch: 2684 Loss: 3581.646973\n",
      "Training Batch: 2685 Loss: 3347.589844\n",
      "Training Batch: 2686 Loss: 3285.664551\n",
      "Training Batch: 2687 Loss: 3389.806885\n",
      "Training Batch: 2688 Loss: 3300.374512\n",
      "Training Batch: 2689 Loss: 3373.757812\n",
      "Training Batch: 2690 Loss: 3324.111328\n",
      "Training Batch: 2691 Loss: 3255.916016\n",
      "Training Batch: 2692 Loss: 3158.017578\n",
      "Training Batch: 2693 Loss: 3255.411133\n",
      "Training Batch: 2694 Loss: 3295.049561\n",
      "Training Batch: 2695 Loss: 3187.510498\n",
      "Training Batch: 2696 Loss: 3325.281250\n",
      "Training Batch: 2697 Loss: 3291.066406\n",
      "Training Batch: 2698 Loss: 3440.302246\n",
      "Training Batch: 2699 Loss: 3234.533447\n",
      "Training Batch: 2700 Loss: 3332.174805\n",
      "Training Batch: 2701 Loss: 3507.127197\n",
      "Training Batch: 2702 Loss: 3290.396240\n",
      "Training Batch: 2703 Loss: 3288.935547\n",
      "Training Batch: 2704 Loss: 3302.769287\n",
      "Training Batch: 2705 Loss: 3282.583252\n",
      "Training Batch: 2706 Loss: 3416.392090\n",
      "Training Batch: 2707 Loss: 3245.738770\n",
      "Training Batch: 2708 Loss: 3253.957275\n",
      "Training Batch: 2709 Loss: 3258.360352\n",
      "Training Batch: 2710 Loss: 3237.039551\n",
      "Training Batch: 2711 Loss: 3358.392090\n",
      "Training Batch: 2712 Loss: 3278.222656\n",
      "Training Batch: 2713 Loss: 3369.958496\n",
      "Training Batch: 2714 Loss: 3610.091553\n",
      "Training Batch: 2715 Loss: 3268.770020\n",
      "Training Batch: 2716 Loss: 3572.530518\n",
      "Training Batch: 2717 Loss: 3491.512695\n",
      "Training Batch: 2718 Loss: 3297.164551\n",
      "Training Batch: 2719 Loss: 3320.357910\n",
      "Training Batch: 2720 Loss: 3343.718506\n",
      "Training Batch: 2721 Loss: 3493.573486\n",
      "Training Batch: 2722 Loss: 3357.950195\n",
      "Training Batch: 2723 Loss: 3374.444824\n",
      "Training Batch: 2724 Loss: 3324.752197\n",
      "Training Batch: 2725 Loss: 3376.824463\n",
      "Training Batch: 2726 Loss: 3186.599854\n",
      "Training Batch: 2727 Loss: 3229.881592\n",
      "Training Batch: 2728 Loss: 3276.314209\n",
      "Training Batch: 2729 Loss: 3427.395752\n",
      "Training Batch: 2730 Loss: 3232.245361\n",
      "Training Batch: 2731 Loss: 3182.107666\n",
      "Training Batch: 2732 Loss: 3498.158203\n",
      "Training Batch: 2733 Loss: 3353.451660\n",
      "Training Batch: 2734 Loss: 3216.029053\n",
      "Training Batch: 2735 Loss: 3218.190430\n",
      "Training Batch: 2736 Loss: 3394.878906\n",
      "Training Batch: 2737 Loss: 3242.709473\n",
      "Training Batch: 2738 Loss: 3218.878906\n",
      "Training Batch: 2739 Loss: 3346.596680\n",
      "Training Batch: 2740 Loss: 3230.110596\n",
      "Training Batch: 2741 Loss: 3347.634033\n",
      "Training Batch: 2742 Loss: 3414.753174\n",
      "Training Batch: 2743 Loss: 3521.461182\n",
      "Training Batch: 2744 Loss: 3210.390381\n",
      "Training Batch: 2745 Loss: 3298.824951\n",
      "Training Batch: 2746 Loss: 3288.335449\n",
      "Training Batch: 2747 Loss: 3190.542480\n",
      "Training Batch: 2748 Loss: 3206.803223\n",
      "Training Batch: 2749 Loss: 3244.686035\n",
      "Training Batch: 2750 Loss: 3401.629395\n",
      "Training Batch: 2751 Loss: 3403.698486\n",
      "Training Batch: 2752 Loss: 3341.451660\n",
      "Training Batch: 2753 Loss: 3305.587646\n",
      "Training Batch: 2754 Loss: 3289.046631\n",
      "Training Batch: 2755 Loss: 3368.927246\n",
      "Training Batch: 2756 Loss: 3246.351074\n",
      "Training Batch: 2757 Loss: 3215.119629\n",
      "Training Batch: 2758 Loss: 3523.697021\n",
      "Training Batch: 2759 Loss: 3342.928955\n",
      "Training Batch: 2760 Loss: 3226.422119\n",
      "Training Batch: 2761 Loss: 3250.425293\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Batch: 2762 Loss: 3148.753906\n",
      "Training Batch: 2763 Loss: 3328.643555\n",
      "Training Batch: 2764 Loss: 3240.349854\n",
      "Training Batch: 2765 Loss: 3469.504395\n",
      "Training Batch: 2766 Loss: 3320.024902\n",
      "Training Batch: 2767 Loss: 3280.694824\n",
      "Training Batch: 2768 Loss: 3201.624023\n",
      "Training Batch: 2769 Loss: 3347.821289\n",
      "Training Batch: 2770 Loss: 3492.314941\n",
      "Training Batch: 2771 Loss: 3246.260742\n",
      "Training Batch: 2772 Loss: 3281.549316\n",
      "Training Batch: 2773 Loss: 3224.687500\n",
      "Training Batch: 2774 Loss: 3268.858643\n",
      "Training Batch: 2775 Loss: 3266.219727\n",
      "Training Batch: 2776 Loss: 3290.993164\n",
      "Training Batch: 2777 Loss: 3292.695801\n",
      "Training Batch: 2778 Loss: 3200.685547\n",
      "Training Batch: 2779 Loss: 3573.702637\n",
      "Training Batch: 2780 Loss: 3764.143311\n",
      "Training Batch: 2781 Loss: 3168.584229\n",
      "Training Batch: 2782 Loss: 3257.661621\n",
      "Training Batch: 2783 Loss: 3429.504639\n",
      "Training Batch: 2784 Loss: 3308.556152\n",
      "Training Batch: 2785 Loss: 3406.264648\n",
      "Training Batch: 2786 Loss: 3340.943604\n",
      "Training Batch: 2787 Loss: 3280.344727\n",
      "Training Batch: 2788 Loss: 3459.008789\n",
      "Training Batch: 2789 Loss: 3368.622559\n",
      "Training Batch: 2790 Loss: 3160.586914\n",
      "Training Batch: 2791 Loss: 3319.507812\n",
      "Training Batch: 2792 Loss: 3408.927979\n",
      "Training Batch: 2793 Loss: 3421.578125\n",
      "Training Batch: 2794 Loss: 3453.547363\n",
      "Training Batch: 2795 Loss: 3285.033936\n",
      "Training Batch: 2796 Loss: 3520.951172\n",
      "Training Batch: 2797 Loss: 3317.003174\n",
      "Training Batch: 2798 Loss: 3369.000977\n",
      "Training Batch: 2799 Loss: 3203.307129\n",
      "Training Batch: 2800 Loss: 3269.467285\n",
      "Training Batch: 2801 Loss: 3220.934570\n",
      "Training Batch: 2802 Loss: 3514.862793\n",
      "Training Batch: 2803 Loss: 3265.795166\n",
      "Training Batch: 2804 Loss: 3164.994141\n",
      "Training Batch: 2805 Loss: 3392.167969\n",
      "Training Batch: 2806 Loss: 3550.378906\n",
      "Training Batch: 2807 Loss: 3586.489258\n",
      "Training Batch: 2808 Loss: 3351.274902\n",
      "Training Batch: 2809 Loss: 3397.820312\n",
      "Training Batch: 2810 Loss: 3510.460938\n",
      "Training Batch: 2811 Loss: 3454.612305\n",
      "Training Batch: 2812 Loss: 3246.846191\n",
      "Training Batch: 2813 Loss: 3269.016357\n",
      "Training Batch: 2814 Loss: 3325.911133\n",
      "Training Batch: 2815 Loss: 3478.453369\n",
      "Training Batch: 2816 Loss: 3258.310059\n",
      "Training Batch: 2817 Loss: 3362.176758\n",
      "Training Batch: 2818 Loss: 3342.261719\n",
      "Training Batch: 2819 Loss: 3375.712402\n",
      "Training Batch: 2820 Loss: 3275.334473\n",
      "Training Batch: 2821 Loss: 3298.271484\n",
      "Training Batch: 2822 Loss: 3573.412598\n",
      "Training Batch: 2823 Loss: 3225.703613\n",
      "Training Batch: 2824 Loss: 3306.010742\n",
      "Training Batch: 2825 Loss: 3278.804199\n",
      "Training Batch: 2826 Loss: 3281.143799\n",
      "Training Batch: 2827 Loss: 3356.286621\n",
      "Training Batch: 2828 Loss: 3583.962402\n",
      "Training Batch: 2829 Loss: 3293.908203\n",
      "Training Batch: 2830 Loss: 3428.786621\n",
      "Training Batch: 2831 Loss: 3431.807373\n",
      "Training Batch: 2832 Loss: 3270.548828\n",
      "Training Batch: 2833 Loss: 3307.376953\n",
      "Training Batch: 2834 Loss: 3227.435547\n",
      "Training Batch: 2835 Loss: 3240.269043\n",
      "Training Batch: 2836 Loss: 3206.773926\n",
      "Training Batch: 2837 Loss: 3244.140137\n",
      "Training Batch: 2838 Loss: 3367.061523\n",
      "Training Batch: 2839 Loss: 3247.073730\n",
      "Training Batch: 2840 Loss: 3206.954590\n",
      "Training Batch: 2841 Loss: 3313.387207\n",
      "Training Batch: 2842 Loss: 3412.379395\n",
      "Training Batch: 2843 Loss: 3299.095703\n",
      "Training Batch: 2844 Loss: 3354.147949\n",
      "Training Batch: 2845 Loss: 3310.720703\n",
      "Training Batch: 2846 Loss: 3285.539062\n",
      "Training Batch: 2847 Loss: 3248.811768\n",
      "Training Batch: 2848 Loss: 3370.431641\n",
      "Training Batch: 2849 Loss: 3340.780518\n",
      "Training Batch: 2850 Loss: 3291.332520\n",
      "Training Batch: 2851 Loss: 3491.917969\n",
      "Training Batch: 2852 Loss: 3222.602051\n",
      "Training Batch: 2853 Loss: 3471.577148\n",
      "Training Batch: 2854 Loss: 3273.520752\n",
      "Training Batch: 2855 Loss: 3207.720215\n",
      "Training Batch: 2856 Loss: 3216.883545\n",
      "Training Batch: 2857 Loss: 3237.544434\n",
      "Training Batch: 2858 Loss: 3445.684326\n",
      "Training Batch: 2859 Loss: 3288.432617\n",
      "Training Batch: 2860 Loss: 3321.947754\n",
      "Training Batch: 2861 Loss: 3334.255127\n",
      "Training Batch: 2862 Loss: 3366.783203\n",
      "Training Batch: 2863 Loss: 3181.561768\n",
      "Training Batch: 2864 Loss: 3456.294922\n",
      "Training Batch: 2865 Loss: 3356.570068\n",
      "Training Batch: 2866 Loss: 3222.564941\n",
      "Training Batch: 2867 Loss: 3259.775391\n",
      "Training Batch: 2868 Loss: 3519.066895\n",
      "Training Batch: 2869 Loss: 3157.445557\n",
      "Training Batch: 2870 Loss: 3327.873535\n",
      "Training Batch: 2871 Loss: 3336.679199\n",
      "Training Batch: 2872 Loss: 3270.663086\n",
      "Training Batch: 2873 Loss: 3333.869141\n",
      "Training Batch: 2874 Loss: 3503.354980\n",
      "Training Batch: 2875 Loss: 3360.985840\n",
      "Training Batch: 2876 Loss: 3236.970703\n",
      "Training Batch: 2877 Loss: 3308.325684\n",
      "Training Batch: 2878 Loss: 3172.714600\n",
      "Training Batch: 2879 Loss: 3259.602051\n",
      "Training Batch: 2880 Loss: 3349.462158\n",
      "Training Batch: 2881 Loss: 3285.496338\n",
      "Training Batch: 2882 Loss: 3233.698730\n",
      "Training Batch: 2883 Loss: 3247.141602\n",
      "Training Batch: 2884 Loss: 3239.645752\n",
      "Training Batch: 2885 Loss: 3202.291016\n",
      "Training Batch: 2886 Loss: 3431.069336\n",
      "Training Batch: 2887 Loss: 3465.793945\n",
      "Training Batch: 2888 Loss: 3355.959961\n",
      "Training Batch: 2889 Loss: 3292.580078\n",
      "Training Batch: 2890 Loss: 3381.650146\n",
      "Training Batch: 2891 Loss: 3226.850098\n",
      "Training Batch: 2892 Loss: 3224.195801\n",
      "Training Batch: 2893 Loss: 3179.468994\n",
      "Training Batch: 2894 Loss: 3170.148926\n",
      "Training Batch: 2895 Loss: 3212.941406\n",
      "Training Batch: 2896 Loss: 3268.745117\n",
      "Training Batch: 2897 Loss: 3318.292969\n",
      "Training Batch: 2898 Loss: 3305.953369\n",
      "Training Batch: 2899 Loss: 3318.353027\n",
      "Training Batch: 2900 Loss: 3200.803223\n",
      "Training Batch: 2901 Loss: 3283.295166\n",
      "Training Batch: 2902 Loss: 3427.634277\n",
      "Training Batch: 2903 Loss: 3232.729004\n",
      "Training Batch: 2904 Loss: 3221.652344\n",
      "Training Batch: 2905 Loss: 3234.431641\n",
      "Training Batch: 2906 Loss: 3336.697754\n",
      "Training Batch: 2907 Loss: 3291.550293\n",
      "Training Batch: 2908 Loss: 3255.482910\n",
      "Training Batch: 2909 Loss: 3305.799805\n",
      "Training Batch: 2910 Loss: 3257.861816\n",
      "Training Batch: 2911 Loss: 3292.914062\n",
      "Training Batch: 2912 Loss: 3232.258545\n",
      "Training Batch: 2913 Loss: 3419.301025\n",
      "Training Batch: 2914 Loss: 3314.140625\n",
      "Training Batch: 2915 Loss: 3369.575684\n",
      "Training Batch: 2916 Loss: 3226.026367\n",
      "Training Batch: 2917 Loss: 3341.161621\n",
      "Training Batch: 2918 Loss: 3248.245605\n",
      "Training Batch: 2919 Loss: 3339.832031\n",
      "Training Batch: 2920 Loss: 3285.445801\n",
      "Training Batch: 2921 Loss: 3469.462402\n",
      "Training Batch: 2922 Loss: 3232.498291\n",
      "Training Batch: 2923 Loss: 3260.374023\n",
      "Training Batch: 2924 Loss: 3224.184082\n",
      "Training Batch: 2925 Loss: 3366.259277\n",
      "Training Batch: 2926 Loss: 3579.080811\n",
      "Training Batch: 2927 Loss: 3553.261719\n",
      "Training Batch: 2928 Loss: 3248.451172\n",
      "Training Batch: 2929 Loss: 3435.810303\n",
      "Training Batch: 2930 Loss: 3548.195068\n",
      "Training Batch: 2931 Loss: 3340.468750\n",
      "Training Batch: 2932 Loss: 3339.027344\n",
      "Training Batch: 2933 Loss: 3404.695312\n",
      "Training Batch: 2934 Loss: 3351.384766\n",
      "Training Batch: 2935 Loss: 3375.729492\n",
      "Training Batch: 2936 Loss: 3473.772461\n",
      "Training Batch: 2937 Loss: 3324.024414\n",
      "Training Batch: 2938 Loss: 3220.118164\n",
      "Training Batch: 2939 Loss: 3229.518066\n",
      "Training Batch: 2940 Loss: 3323.843750\n",
      "Training Batch: 2941 Loss: 3342.847168\n",
      "Training Batch: 2942 Loss: 3184.868652\n",
      "Training Batch: 2943 Loss: 3184.581543\n",
      "Training Batch: 2944 Loss: 3207.419434\n",
      "Training Batch: 2945 Loss: 3270.187500\n",
      "Training Batch: 2946 Loss: 3527.184082\n",
      "Training Batch: 2947 Loss: 3485.089844\n",
      "Training Batch: 2948 Loss: 3261.064453\n",
      "Training Batch: 2949 Loss: 3259.974121\n",
      "Training Batch: 2950 Loss: 3183.778320\n",
      "Training Batch: 2951 Loss: 3329.295654\n",
      "Training Batch: 2952 Loss: 3261.809082\n",
      "Training Batch: 2953 Loss: 3555.255127\n",
      "Training Batch: 2954 Loss: 3425.364746\n",
      "Training Batch: 2955 Loss: 3458.439209\n",
      "Training Batch: 2956 Loss: 3203.782227\n",
      "Training Batch: 2957 Loss: 3254.081787\n",
      "Training Batch: 2958 Loss: 3255.276367\n",
      "Training Batch: 2959 Loss: 3242.114014\n",
      "Training Batch: 2960 Loss: 3247.153809\n",
      "Training Batch: 2961 Loss: 3198.266602\n",
      "Training Batch: 2962 Loss: 3220.927246\n",
      "Training Batch: 2963 Loss: 3394.037109\n",
      "Training Batch: 2964 Loss: 3407.463867\n",
      "Training Batch: 2965 Loss: 3319.497314\n",
      "Training Batch: 2966 Loss: 3275.854492\n",
      "Training Batch: 2967 Loss: 3213.676025\n",
      "Training Batch: 2968 Loss: 3281.014160\n",
      "Training Batch: 2969 Loss: 3286.520996\n",
      "Training Batch: 2970 Loss: 3154.313232\n",
      "Training Batch: 2971 Loss: 3286.398438\n",
      "Training Batch: 2972 Loss: 3229.796387\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Batch: 2973 Loss: 3159.437012\n",
      "Training Batch: 2974 Loss: 3293.579834\n",
      "Training Batch: 2975 Loss: 3270.322510\n",
      "Training Batch: 2976 Loss: 3312.676758\n",
      "Training Batch: 2977 Loss: 3237.838135\n",
      "Training Batch: 2978 Loss: 3359.094238\n",
      "Training Batch: 2979 Loss: 3365.720459\n",
      "Training Batch: 2980 Loss: 3247.826416\n",
      "Training Batch: 2981 Loss: 3195.796875\n",
      "Training Batch: 2982 Loss: 3284.212891\n",
      "Training Batch: 2983 Loss: 3230.060547\n",
      "Training Batch: 2984 Loss: 3555.896484\n",
      "Training Batch: 2985 Loss: 3284.415771\n",
      "Training Batch: 2986 Loss: 3238.519531\n",
      "Training Batch: 2987 Loss: 3302.598877\n",
      "Training Batch: 2988 Loss: 3255.401855\n",
      "Training Batch: 2989 Loss: 3264.508789\n",
      "Training Batch: 2990 Loss: 3211.592285\n",
      "Training Batch: 2991 Loss: 3272.070312\n",
      "Training Batch: 2992 Loss: 3300.220703\n",
      "Training Batch: 2993 Loss: 3379.337402\n",
      "Training Batch: 2994 Loss: 3269.979004\n",
      "Training Batch: 2995 Loss: 3430.737549\n",
      "Training Batch: 2996 Loss: 3332.809082\n",
      "Training Batch: 2997 Loss: 3187.583984\n",
      "Training Batch: 2998 Loss: 3268.524170\n",
      "Training Batch: 2999 Loss: 3296.538086\n",
      "Training Batch: 3000 Loss: 3240.034912\n",
      "Training Batch: 3001 Loss: 3202.608887\n",
      "Training Batch: 3002 Loss: 3186.250732\n",
      "Training Batch: 3003 Loss: 3221.056152\n",
      "Training Batch: 3004 Loss: 3446.686768\n",
      "Training Batch: 3005 Loss: 3576.364746\n",
      "Training Batch: 3006 Loss: 3217.554199\n",
      "Training Batch: 3007 Loss: 3404.976074\n",
      "Training Batch: 3008 Loss: 3285.720703\n",
      "Training Batch: 3009 Loss: 3253.254395\n",
      "Training Batch: 3010 Loss: 3522.683105\n",
      "Training Batch: 3011 Loss: 3232.210205\n",
      "Training Batch: 3012 Loss: 3330.807129\n",
      "Training Batch: 3013 Loss: 3336.562012\n",
      "Training Batch: 3014 Loss: 3204.836914\n",
      "Training Batch: 3015 Loss: 3325.989258\n",
      "Training Batch: 3016 Loss: 3196.849121\n",
      "Training Batch: 3017 Loss: 3200.407959\n",
      "Training Batch: 3018 Loss: 3418.636475\n",
      "Training Batch: 3019 Loss: 3398.665283\n",
      "Training Batch: 3020 Loss: 3398.117676\n",
      "Training Batch: 3021 Loss: 3290.450684\n",
      "Training Batch: 3022 Loss: 3345.572266\n",
      "Training Batch: 3023 Loss: 3186.922363\n",
      "Training Batch: 3024 Loss: 3319.632812\n",
      "Training Batch: 3025 Loss: 3386.790283\n",
      "Training Batch: 3026 Loss: 3303.962891\n",
      "Training Batch: 3027 Loss: 3264.357910\n",
      "Training Batch: 3028 Loss: 3207.175293\n",
      "Training Batch: 3029 Loss: 3259.493164\n",
      "Training Batch: 3030 Loss: 3210.023926\n",
      "Training Batch: 3031 Loss: 3177.267578\n",
      "Training Batch: 3032 Loss: 3267.907471\n",
      "Training Batch: 3033 Loss: 3245.536133\n",
      "Training Batch: 3034 Loss: 3211.565918\n",
      "Training Batch: 3035 Loss: 3366.000244\n",
      "Training Batch: 3036 Loss: 3349.913086\n",
      "Training Batch: 3037 Loss: 3242.361816\n",
      "Training Batch: 3038 Loss: 3380.998047\n",
      "Training Batch: 3039 Loss: 3521.914062\n",
      "Training Batch: 3040 Loss: 3282.878906\n",
      "Training Batch: 3041 Loss: 3240.821777\n",
      "Training Batch: 3042 Loss: 3172.842285\n",
      "Training Batch: 3043 Loss: 3289.692871\n",
      "Training Batch: 3044 Loss: 3377.505859\n",
      "Training Batch: 3045 Loss: 3389.491211\n",
      "Training Batch: 3046 Loss: 3243.665039\n",
      "Training Batch: 3047 Loss: 3570.428223\n",
      "Training Batch: 3048 Loss: 3536.659180\n",
      "Training Batch: 3049 Loss: 3282.252930\n",
      "Training Batch: 3050 Loss: 3479.408203\n",
      "Training Batch: 3051 Loss: 3637.643066\n",
      "Training Batch: 3052 Loss: 3508.700684\n",
      "Training Batch: 3053 Loss: 3371.887207\n",
      "Training Batch: 3054 Loss: 3360.347656\n",
      "Training Batch: 3055 Loss: 3538.781494\n",
      "Training Batch: 3056 Loss: 3267.680664\n",
      "Training Batch: 3057 Loss: 3231.203857\n",
      "Training Batch: 3058 Loss: 3193.513184\n",
      "Training Batch: 3059 Loss: 3180.179688\n",
      "Training Batch: 3060 Loss: 3193.580078\n",
      "Training Batch: 3061 Loss: 3215.079590\n",
      "Training Batch: 3062 Loss: 3351.719727\n",
      "Training Batch: 3063 Loss: 3264.096191\n",
      "Training Batch: 3064 Loss: 3347.092773\n",
      "Training Batch: 3065 Loss: 3218.973633\n",
      "Training Batch: 3066 Loss: 3253.553955\n",
      "Training Batch: 3067 Loss: 3196.055664\n",
      "Training Batch: 3068 Loss: 3290.874023\n",
      "Training Batch: 3069 Loss: 3371.814453\n",
      "Training Batch: 3070 Loss: 3281.524902\n",
      "Training Batch: 3071 Loss: 3310.473633\n",
      "Training Batch: 3072 Loss: 3423.262695\n",
      "Training Batch: 3073 Loss: 3603.544434\n",
      "Training Batch: 3074 Loss: 3282.336182\n",
      "Training Batch: 3075 Loss: 3189.425049\n",
      "Training Batch: 3076 Loss: 3291.127686\n",
      "Training Batch: 3077 Loss: 3236.316895\n",
      "Training Batch: 3078 Loss: 3339.796631\n",
      "Training Batch: 3079 Loss: 3404.458496\n",
      "Training Batch: 3080 Loss: 3306.953613\n",
      "Training Batch: 3081 Loss: 3232.415527\n",
      "Training Batch: 3082 Loss: 3175.089844\n",
      "Training Batch: 3083 Loss: 3270.810059\n",
      "Training Batch: 3084 Loss: 3251.575684\n",
      "Training Batch: 3085 Loss: 3273.727539\n",
      "Training Batch: 3086 Loss: 3242.282715\n",
      "Training Batch: 3087 Loss: 3399.233154\n",
      "Training Batch: 3088 Loss: 3304.551758\n",
      "Training Batch: 3089 Loss: 3186.563477\n",
      "Training Batch: 3090 Loss: 3289.886719\n",
      "Training Batch: 3091 Loss: 3338.089844\n",
      "Training Batch: 3092 Loss: 3170.903320\n",
      "Training Batch: 3093 Loss: 3329.672363\n",
      "Training Batch: 3094 Loss: 3209.708008\n",
      "Training Batch: 3095 Loss: 3344.094238\n",
      "Training Batch: 3096 Loss: 3512.739746\n",
      "Training Batch: 3097 Loss: 3368.607910\n",
      "Training Batch: 3098 Loss: 3396.858154\n",
      "Training Batch: 3099 Loss: 3396.141113\n",
      "Training Batch: 3100 Loss: 3645.160645\n",
      "Training Batch: 3101 Loss: 3225.851318\n",
      "Training Batch: 3102 Loss: 3527.985352\n",
      "Training Batch: 3103 Loss: 3376.264160\n",
      "Training Batch: 3104 Loss: 3337.318848\n",
      "Training Batch: 3105 Loss: 3325.762939\n",
      "Training Batch: 3106 Loss: 3314.145020\n",
      "Training Batch: 3107 Loss: 3312.638672\n",
      "Training Batch: 3108 Loss: 3128.691162\n",
      "Training Batch: 3109 Loss: 3150.522217\n",
      "Training Batch: 3110 Loss: 3207.430176\n",
      "Training Batch: 3111 Loss: 3451.705322\n",
      "Training Batch: 3112 Loss: 3230.171631\n",
      "Training Batch: 3113 Loss: 3286.018311\n",
      "Training Batch: 3114 Loss: 3310.504395\n",
      "Training Batch: 3115 Loss: 3221.750732\n",
      "Training Batch: 3116 Loss: 3254.442383\n",
      "Training Batch: 3117 Loss: 3269.233398\n",
      "Training Batch: 3118 Loss: 3224.528809\n",
      "Training Batch: 3119 Loss: 3348.535156\n",
      "Training Batch: 3120 Loss: 3387.524902\n",
      "Training Batch: 3121 Loss: 3312.575195\n",
      "Training Batch: 3122 Loss: 3357.031006\n",
      "Training Batch: 3123 Loss: 3179.119629\n",
      "Training Batch: 3124 Loss: 3272.384033\n",
      "Training Batch: 3125 Loss: 3155.616211\n",
      "Training Batch: 3126 Loss: 3420.671875\n",
      "Training Batch: 3127 Loss: 3291.375977\n",
      "Training Batch: 3128 Loss: 3245.869629\n",
      "Training Batch: 3129 Loss: 3363.083252\n",
      "Training Batch: 3130 Loss: 3330.518066\n",
      "Training Batch: 3131 Loss: 3283.625977\n",
      "Training Batch: 3132 Loss: 3316.089844\n",
      "Training Batch: 3133 Loss: 3368.106201\n",
      "Training Batch: 3134 Loss: 3272.582520\n",
      "Training Batch: 3135 Loss: 3190.296631\n",
      "Training Batch: 3136 Loss: 3329.011719\n",
      "Training Batch: 3137 Loss: 3485.209473\n",
      "Training Batch: 3138 Loss: 3412.014404\n",
      "Training Batch: 3139 Loss: 3324.463867\n",
      "Training Batch: 3140 Loss: 3329.180176\n",
      "Training Batch: 3141 Loss: 3287.742676\n",
      "Training Batch: 3142 Loss: 3235.812744\n",
      "Training Batch: 3143 Loss: 3196.284668\n",
      "Training Batch: 3144 Loss: 3258.711914\n",
      "Training Batch: 3145 Loss: 3371.673828\n",
      "Training Batch: 3146 Loss: 3519.479980\n",
      "Training Batch: 3147 Loss: 3383.237305\n",
      "Training Batch: 3148 Loss: 3265.522461\n",
      "Training Batch: 3149 Loss: 3302.230957\n",
      "Training Batch: 3150 Loss: 3197.260254\n",
      "Training Batch: 3151 Loss: 3542.383301\n",
      "Training Batch: 3152 Loss: 3498.624023\n",
      "Training Batch: 3153 Loss: 3566.186279\n",
      "Training Batch: 3154 Loss: 3274.960938\n",
      "Training Batch: 3155 Loss: 3223.631348\n",
      "Training Batch: 3156 Loss: 3385.614014\n",
      "Training Batch: 3157 Loss: 3387.177246\n",
      "Training Batch: 3158 Loss: 3252.581543\n",
      "Training Batch: 3159 Loss: 3367.714844\n",
      "Training Batch: 3160 Loss: 3308.501465\n",
      "Training Batch: 3161 Loss: 3354.010254\n",
      "Training Batch: 3162 Loss: 3277.071289\n",
      "Training Batch: 3163 Loss: 3177.300293\n",
      "Training Batch: 3164 Loss: 3240.938965\n",
      "Training Batch: 3165 Loss: 3346.002930\n",
      "Training Batch: 3166 Loss: 3280.719238\n",
      "Training Batch: 3167 Loss: 3291.229248\n",
      "Training Batch: 3168 Loss: 3302.392090\n",
      "Training Batch: 3169 Loss: 3391.041016\n",
      "Training Batch: 3170 Loss: 3549.344482\n",
      "Training Batch: 3171 Loss: 3381.802734\n",
      "Training Batch: 3172 Loss: 3308.155762\n",
      "Training Batch: 3173 Loss: 3353.567871\n",
      "Training Batch: 3174 Loss: 3293.347900\n",
      "Training Batch: 3175 Loss: 3540.204102\n",
      "Training Batch: 3176 Loss: 3302.467041\n",
      "Training Batch: 3177 Loss: 3388.533447\n",
      "Training Batch: 3178 Loss: 3527.875000\n",
      "Training Batch: 3179 Loss: 3422.566406\n",
      "Training Batch: 3180 Loss: 3344.205078\n",
      "Training Batch: 3181 Loss: 3225.506592\n",
      "Training Batch: 3182 Loss: 3738.868652\n",
      "Training Batch: 3183 Loss: 3200.723145\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Batch: 3184 Loss: 3348.324219\n",
      "Training Batch: 3185 Loss: 3287.261719\n",
      "Training Batch: 3186 Loss: 3259.495605\n",
      "Training Batch: 3187 Loss: 3207.851562\n",
      "Training Batch: 3188 Loss: 3182.084961\n",
      "Training Batch: 3189 Loss: 3401.603027\n",
      "Training Batch: 3190 Loss: 3346.550781\n",
      "Training Batch: 3191 Loss: 3227.341797\n",
      "Training Batch: 3192 Loss: 3250.851318\n",
      "Training Batch: 3193 Loss: 3292.102295\n",
      "Training Batch: 3194 Loss: 3402.640625\n",
      "Training Batch: 3195 Loss: 3297.520996\n",
      "Training Batch: 3196 Loss: 3444.435059\n",
      "Training Batch: 3197 Loss: 3437.666992\n",
      "Training Batch: 3198 Loss: 3254.205078\n",
      "Training Batch: 3199 Loss: 3139.526855\n",
      "Training Batch: 3200 Loss: 3143.132812\n",
      "Training Batch: 3201 Loss: 3165.884766\n",
      "Training Batch: 3202 Loss: 3381.339111\n",
      "Training Batch: 3203 Loss: 3158.676514\n",
      "Training Batch: 3204 Loss: 3507.720703\n",
      "Training Batch: 3205 Loss: 3340.774902\n",
      "Training Batch: 3206 Loss: 3338.247070\n",
      "Training Batch: 3207 Loss: 3208.897949\n",
      "Training Batch: 3208 Loss: 3280.372070\n",
      "Training Batch: 3209 Loss: 3655.943604\n",
      "Training Batch: 3210 Loss: 3270.045410\n",
      "Training Batch: 3211 Loss: 3177.005371\n",
      "Training Batch: 3212 Loss: 3232.777832\n",
      "Training Batch: 3213 Loss: 3355.131348\n",
      "Training Batch: 3214 Loss: 3388.825195\n",
      "Training Batch: 3215 Loss: 3275.089600\n",
      "Training Batch: 3216 Loss: 3295.509766\n",
      "Training Batch: 3217 Loss: 3526.798340\n",
      "Training Batch: 3218 Loss: 3374.314941\n",
      "Training Batch: 3219 Loss: 3437.492188\n",
      "Training Batch: 3220 Loss: 3362.085938\n",
      "Training Batch: 3221 Loss: 3243.233398\n",
      "Training Batch: 3222 Loss: 3308.916992\n",
      "Training Batch: 3223 Loss: 3365.170410\n",
      "Training Batch: 3224 Loss: 3270.512207\n",
      "Training Batch: 3225 Loss: 3329.331543\n",
      "Training Batch: 3226 Loss: 3150.667480\n",
      "Training Batch: 3227 Loss: 3228.273438\n",
      "Training Batch: 3228 Loss: 3239.538574\n",
      "Training Batch: 3229 Loss: 3339.135742\n",
      "Training Batch: 3230 Loss: 3284.751465\n",
      "Training Batch: 3231 Loss: 3271.619629\n",
      "Training Batch: 3232 Loss: 3306.943848\n",
      "Training Batch: 3233 Loss: 3225.662842\n",
      "Training Batch: 3234 Loss: 3263.231201\n",
      "Training Batch: 3235 Loss: 3343.984375\n",
      "Training Batch: 3236 Loss: 3352.898438\n",
      "Training Batch: 3237 Loss: 3273.606689\n",
      "Training Batch: 3238 Loss: 3312.189453\n",
      "Training Batch: 3239 Loss: 3583.433594\n",
      "Training Batch: 3240 Loss: 3660.421387\n",
      "Training Batch: 3241 Loss: 3427.867188\n",
      "Training Batch: 3242 Loss: 3260.459229\n",
      "Training Batch: 3243 Loss: 3403.023926\n",
      "Training Batch: 3244 Loss: 3417.984619\n",
      "Training Batch: 3245 Loss: 3253.861328\n",
      "Training Batch: 3246 Loss: 3278.855957\n",
      "Training Batch: 3247 Loss: 3242.392090\n",
      "Training Batch: 3248 Loss: 3226.561523\n",
      "Training Batch: 3249 Loss: 3294.885254\n",
      "Training Batch: 3250 Loss: 3463.689453\n",
      "Training Batch: 3251 Loss: 3285.681641\n",
      "Training Batch: 3252 Loss: 3227.668945\n",
      "Training Batch: 3253 Loss: 3219.235352\n",
      "Training Batch: 3254 Loss: 3283.226562\n",
      "Training Batch: 3255 Loss: 3404.099121\n",
      "Training Batch: 3256 Loss: 3595.208496\n",
      "Training Batch: 3257 Loss: 3545.958984\n",
      "Training Batch: 3258 Loss: 3280.576172\n",
      "Training Batch: 3259 Loss: 3279.887207\n",
      "Training Batch: 3260 Loss: 3217.060547\n",
      "Training Batch: 3261 Loss: 3263.904297\n",
      "Training Batch: 3262 Loss: 3226.060547\n",
      "Training Batch: 3263 Loss: 3234.032227\n",
      "Training Batch: 3264 Loss: 3288.523193\n",
      "Training Batch: 3265 Loss: 3339.557129\n",
      "Training Batch: 3266 Loss: 3228.196777\n",
      "Training Batch: 3267 Loss: 3209.900879\n",
      "Training Batch: 3268 Loss: 3303.272949\n",
      "Training Batch: 3269 Loss: 3217.687988\n",
      "Training Batch: 3270 Loss: 3273.713867\n",
      "Training Batch: 3271 Loss: 3301.828613\n",
      "Training Batch: 3272 Loss: 3247.973877\n",
      "Training Batch: 3273 Loss: 3292.083984\n",
      "Training Batch: 3274 Loss: 3290.192871\n",
      "Training Batch: 3275 Loss: 3222.431641\n",
      "Training Batch: 3276 Loss: 3262.682129\n",
      "Training Batch: 3277 Loss: 3339.801270\n",
      "Training Batch: 3278 Loss: 3328.348145\n",
      "Training Batch: 3279 Loss: 3341.192383\n",
      "Training Batch: 3280 Loss: 3257.468018\n",
      "Training Batch: 3281 Loss: 3363.680908\n",
      "Training Batch: 3282 Loss: 3118.271484\n",
      "Training Batch: 3283 Loss: 3187.944092\n",
      "Training Batch: 3284 Loss: 3281.796875\n",
      "Training Batch: 3285 Loss: 3292.501953\n",
      "Training Batch: 3286 Loss: 3277.402588\n",
      "Training Batch: 3287 Loss: 3344.494385\n",
      "Training Batch: 3288 Loss: 3214.536133\n",
      "Training Batch: 3289 Loss: 3236.405273\n",
      "Training Batch: 3290 Loss: 3310.941895\n",
      "Training Batch: 3291 Loss: 3316.076172\n",
      "Training Batch: 3292 Loss: 3231.987061\n",
      "Training Batch: 3293 Loss: 3215.029297\n",
      "Training Batch: 3294 Loss: 3354.663574\n",
      "Training Batch: 3295 Loss: 3214.017578\n",
      "Training Batch: 3296 Loss: 3288.322510\n",
      "Training Batch: 3297 Loss: 3306.181641\n",
      "Training Batch: 3298 Loss: 3206.975586\n",
      "Training Batch: 3299 Loss: 3207.775391\n",
      "Training Batch: 3300 Loss: 3312.708496\n",
      "Training Batch: 3301 Loss: 3294.354492\n",
      "Training Batch: 3302 Loss: 3316.451904\n",
      "Training Batch: 3303 Loss: 3234.358398\n",
      "Training Batch: 3304 Loss: 3354.617188\n",
      "Training Batch: 3305 Loss: 3313.517334\n",
      "Training Batch: 3306 Loss: 3221.010742\n",
      "Training Batch: 3307 Loss: 3212.076172\n",
      "Training Batch: 3308 Loss: 3193.554688\n",
      "Training Batch: 3309 Loss: 3218.466553\n",
      "Training Batch: 3310 Loss: 3272.857422\n",
      "Training Batch: 3311 Loss: 3331.389893\n",
      "Training Batch: 3312 Loss: 3407.714111\n",
      "Training Batch: 3313 Loss: 3379.161133\n",
      "Training Batch: 3314 Loss: 3464.076172\n",
      "Training Batch: 3315 Loss: 3547.593262\n",
      "Training Batch: 3316 Loss: 3614.649414\n",
      "Training Batch: 3317 Loss: 3467.325195\n",
      "Training Batch: 3318 Loss: 3415.305176\n",
      "Training Batch: 3319 Loss: 3258.770264\n",
      "Training Batch: 3320 Loss: 3262.155273\n",
      "Training Batch: 3321 Loss: 3330.097656\n",
      "Training Batch: 3322 Loss: 3307.979492\n",
      "Training Batch: 3323 Loss: 3126.424316\n",
      "Training Batch: 3324 Loss: 3314.213379\n",
      "Training Batch: 3325 Loss: 3201.101074\n",
      "Training Batch: 3326 Loss: 3383.158203\n",
      "Training Batch: 3327 Loss: 3195.849609\n",
      "Training Batch: 3328 Loss: 3155.714111\n",
      "Training Batch: 3329 Loss: 3203.133057\n",
      "Training Batch: 3330 Loss: 3518.359375\n",
      "Training Batch: 3331 Loss: 3408.786621\n",
      "Training Batch: 3332 Loss: 3214.674805\n",
      "Training Batch: 3333 Loss: 3441.601562\n",
      "Training Batch: 3334 Loss: 3239.944824\n",
      "Training Batch: 3335 Loss: 3280.941895\n",
      "Training Batch: 3336 Loss: 3180.573730\n",
      "Training Batch: 3337 Loss: 3303.795898\n",
      "Training Batch: 3338 Loss: 3383.419434\n",
      "Training Batch: 3339 Loss: 3318.738525\n",
      "Training Batch: 3340 Loss: 3276.853027\n",
      "Training Batch: 3341 Loss: 3496.024414\n",
      "Training Batch: 3342 Loss: 3548.645996\n",
      "Training Batch: 3343 Loss: 3450.545166\n",
      "Training Batch: 3344 Loss: 3371.914307\n",
      "Training Batch: 3345 Loss: 3298.096680\n",
      "Training Batch: 3346 Loss: 3485.656494\n",
      "Training Batch: 3347 Loss: 3287.201904\n",
      "Training Batch: 3348 Loss: 3328.893555\n",
      "Training Batch: 3349 Loss: 3224.482422\n",
      "Training Batch: 3350 Loss: 3215.441406\n",
      "Training Batch: 3351 Loss: 3346.704346\n",
      "Training Batch: 3352 Loss: 3106.675049\n",
      "Training Batch: 3353 Loss: 3263.709961\n",
      "Training Batch: 3354 Loss: 3346.097656\n",
      "Training Batch: 3355 Loss: 3338.623535\n",
      "Training Batch: 3356 Loss: 3344.961426\n",
      "Training Batch: 3357 Loss: 3295.914307\n",
      "Training Batch: 3358 Loss: 3284.754395\n",
      "Training Batch: 3359 Loss: 3426.909912\n",
      "Training Batch: 3360 Loss: 3578.503906\n",
      "Training Batch: 3361 Loss: 3256.785156\n",
      "Training Batch: 3362 Loss: 3826.194824\n",
      "Training Batch: 3363 Loss: 3236.314209\n",
      "Training Batch: 3364 Loss: 3401.374512\n",
      "Training Batch: 3365 Loss: 3245.932617\n",
      "Training Batch: 3366 Loss: 3341.070801\n",
      "Training Batch: 3367 Loss: 3239.151123\n",
      "Training Batch: 3368 Loss: 3271.107910\n",
      "Training Batch: 3369 Loss: 3245.016602\n",
      "Training Batch: 3370 Loss: 3528.892578\n",
      "Training Batch: 3371 Loss: 3518.134277\n",
      "Training Batch: 3372 Loss: 3369.244873\n",
      "Training Batch: 3373 Loss: 3381.321533\n",
      "Training Batch: 3374 Loss: 3342.291504\n",
      "Training Batch: 3375 Loss: 3490.476318\n",
      "Training Batch: 3376 Loss: 3290.416992\n",
      "Training Batch: 3377 Loss: 3386.739746\n",
      "Training Batch: 3378 Loss: 3296.785645\n",
      "Training Batch: 3379 Loss: 3473.411377\n",
      "Training Batch: 3380 Loss: 3316.028320\n",
      "Training Batch: 3381 Loss: 3293.915527\n",
      "Training Batch: 3382 Loss: 3186.562012\n",
      "Training Batch: 3383 Loss: 3243.757324\n",
      "Training Batch: 3384 Loss: 3308.845703\n",
      "Training Batch: 3385 Loss: 3363.164795\n",
      "Training Batch: 3386 Loss: 3343.619873\n",
      "Training Batch: 3387 Loss: 3183.693359\n",
      "Training Batch: 3388 Loss: 3296.343750\n",
      "Training Batch: 3389 Loss: 3251.281494\n",
      "Training Batch: 3390 Loss: 3249.866211\n",
      "Training Batch: 3391 Loss: 3357.981445\n",
      "Training Batch: 3392 Loss: 3167.119629\n",
      "Training Batch: 3393 Loss: 3208.984619\n",
      "Training Batch: 3394 Loss: 3323.382812\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Batch: 3395 Loss: 3303.310547\n",
      "Training Batch: 3396 Loss: 3228.144531\n",
      "Training Batch: 3397 Loss: 3352.202637\n",
      "Training Batch: 3398 Loss: 3485.055664\n",
      "Training Batch: 3399 Loss: 3383.752441\n",
      "Training Batch: 3400 Loss: 3238.792236\n",
      "Training Batch: 3401 Loss: 3151.300781\n",
      "Training Batch: 3402 Loss: 3227.053955\n",
      "Training Batch: 3403 Loss: 3289.007324\n",
      "Training Batch: 3404 Loss: 3220.570801\n",
      "Training Batch: 3405 Loss: 3226.119629\n",
      "Training Batch: 3406 Loss: 3579.468018\n",
      "Training Batch: 3407 Loss: 3362.100586\n",
      "Training Batch: 3408 Loss: 3366.355469\n",
      "Training Batch: 3409 Loss: 3357.751465\n",
      "Training Batch: 3410 Loss: 3328.240234\n",
      "Training Batch: 3411 Loss: 3260.766602\n",
      "Training Batch: 3412 Loss: 3305.624512\n",
      "Training Batch: 3413 Loss: 3231.872559\n",
      "Training Batch: 3414 Loss: 3320.615479\n",
      "Training Batch: 3415 Loss: 3321.884766\n",
      "Training Batch: 3416 Loss: 3281.593750\n",
      "Training Batch: 3417 Loss: 3297.887695\n",
      "Training Batch: 3418 Loss: 3280.950195\n",
      "Training Batch: 3419 Loss: 3326.416504\n",
      "Training Batch: 3420 Loss: 3227.929199\n",
      "Training Batch: 3421 Loss: 3208.104004\n",
      "Training Batch: 3422 Loss: 3201.530029\n",
      "Training Batch: 3423 Loss: 3303.058838\n",
      "Training Batch: 3424 Loss: 3276.594482\n",
      "Training Batch: 3425 Loss: 3366.585938\n",
      "Training Batch: 3426 Loss: 3281.188232\n",
      "Training Batch: 3427 Loss: 3446.631104\n",
      "Training Batch: 3428 Loss: 3369.265625\n",
      "Training Batch: 3429 Loss: 3223.553711\n",
      "Training Batch: 3430 Loss: 3565.248779\n",
      "Training Batch: 3431 Loss: 3405.264404\n",
      "Training Batch: 3432 Loss: 3315.532715\n",
      "Training Batch: 3433 Loss: 3394.463867\n",
      "Training Batch: 3434 Loss: 3247.329102\n",
      "Training Batch: 3435 Loss: 3179.264648\n",
      "Training Batch: 3436 Loss: 3386.622314\n",
      "Training Batch: 3437 Loss: 3372.959961\n",
      "Training Batch: 3438 Loss: 3405.553223\n",
      "Training Batch: 3439 Loss: 3505.538086\n",
      "Training Batch: 3440 Loss: 3437.557861\n",
      "Training Batch: 3441 Loss: 3534.104492\n",
      "Training Batch: 3442 Loss: 3271.798828\n",
      "Training Batch: 3443 Loss: 3375.984863\n",
      "Training Batch: 3444 Loss: 3285.383789\n",
      "Training Batch: 3445 Loss: 3177.163574\n",
      "Training Batch: 3446 Loss: 3284.748047\n",
      "Training Batch: 3447 Loss: 3474.760742\n",
      "Training Batch: 3448 Loss: 3324.133301\n",
      "Training Batch: 3449 Loss: 3270.575439\n",
      "Training Batch: 3450 Loss: 3205.923828\n",
      "Training Batch: 3451 Loss: 3237.448242\n",
      "Training Batch: 3452 Loss: 3315.167480\n",
      "Training Batch: 3453 Loss: 3406.024902\n",
      "Training Batch: 3454 Loss: 3358.574707\n",
      "Training Batch: 3455 Loss: 3393.970215\n",
      "Training Batch: 3456 Loss: 3285.002441\n",
      "Training Batch: 3457 Loss: 3229.958008\n",
      "Training Batch: 3458 Loss: 3275.631592\n",
      "Training Batch: 3459 Loss: 3456.384766\n",
      "Training Batch: 3460 Loss: 3282.794434\n",
      "Training Batch: 3461 Loss: 3485.216309\n",
      "Training Batch: 3462 Loss: 3550.560547\n",
      "Training Batch: 3463 Loss: 3299.052246\n",
      "Training Batch: 3464 Loss: 3381.667480\n",
      "Training Batch: 3465 Loss: 3454.441895\n",
      "Training Batch: 3466 Loss: 3383.504395\n",
      "Training Batch: 3467 Loss: 3270.460938\n",
      "Training Batch: 3468 Loss: 3481.820801\n",
      "Training Batch: 3469 Loss: 3397.521973\n",
      "Training Batch: 3470 Loss: 3288.797363\n",
      "Training Batch: 3471 Loss: 3136.208252\n",
      "Training Batch: 3472 Loss: 3406.587891\n",
      "Training Batch: 3473 Loss: 3256.554199\n",
      "Training Batch: 3474 Loss: 3310.083252\n",
      "Training Batch: 3475 Loss: 3188.667236\n",
      "Training Batch: 3476 Loss: 3289.565918\n",
      "Training Batch: 3477 Loss: 3418.630371\n",
      "Training Batch: 3478 Loss: 3199.032715\n",
      "Training Batch: 3479 Loss: 3297.914062\n",
      "Training Batch: 3480 Loss: 3275.459961\n",
      "Training Batch: 3481 Loss: 3295.323242\n",
      "Training Batch: 3482 Loss: 3305.804688\n",
      "Training Batch: 3483 Loss: 3330.381104\n",
      "Training Batch: 3484 Loss: 3336.213379\n",
      "Training Batch: 3485 Loss: 3233.027832\n",
      "Training Batch: 3486 Loss: 3275.704590\n",
      "Training Batch: 3487 Loss: 3329.161133\n",
      "Training Batch: 3488 Loss: 3176.419189\n",
      "Training Batch: 3489 Loss: 3267.633789\n",
      "Training Batch: 3490 Loss: 3225.951660\n",
      "Training Batch: 3491 Loss: 3340.891113\n",
      "Training Batch: 3492 Loss: 3738.513184\n",
      "Training Batch: 3493 Loss: 3300.184570\n",
      "Training Batch: 3494 Loss: 3445.302246\n",
      "Training Batch: 3495 Loss: 3349.640625\n",
      "Training Batch: 3496 Loss: 3436.589600\n",
      "Training Batch: 3497 Loss: 3446.717529\n",
      "Training Batch: 3498 Loss: 3518.515381\n",
      "Training Batch: 3499 Loss: 3497.482910\n",
      "Training Batch: 3500 Loss: 3248.644531\n",
      "Training Batch: 3501 Loss: 3339.714844\n",
      "Training Batch: 3502 Loss: 3290.642822\n",
      "Training Batch: 3503 Loss: 3226.802246\n",
      "Training Batch: 3504 Loss: 3332.689453\n",
      "Training Batch: 3505 Loss: 3208.776123\n",
      "Training Batch: 3506 Loss: 3535.172363\n",
      "Training Batch: 3507 Loss: 3258.796387\n",
      "Training Batch: 3508 Loss: 3303.742920\n",
      "Training Batch: 3509 Loss: 3190.717773\n",
      "Training Batch: 3510 Loss: 3356.467529\n",
      "Training Batch: 3511 Loss: 3480.969238\n",
      "Training Batch: 3512 Loss: 3301.469482\n",
      "Training Batch: 3513 Loss: 3285.292480\n",
      "Training Batch: 3514 Loss: 3488.264893\n",
      "Training Batch: 3515 Loss: 3311.669922\n",
      "Training Batch: 3516 Loss: 3398.715576\n",
      "Training Batch: 3517 Loss: 3312.526855\n",
      "Training Batch: 3518 Loss: 3268.811035\n",
      "Training Batch: 3519 Loss: 3290.099121\n",
      "Training Batch: 3520 Loss: 3189.796875\n",
      "Training Batch: 3521 Loss: 3307.151855\n",
      "Training Batch: 3522 Loss: 3233.892578\n",
      "Training Batch: 3523 Loss: 3244.210449\n",
      "Training Batch: 3524 Loss: 3140.196045\n",
      "Training Batch: 3525 Loss: 3506.416260\n",
      "Training Batch: 3526 Loss: 3222.229004\n",
      "Training Batch: 3527 Loss: 3305.853027\n",
      "Training Batch: 3528 Loss: 3394.521484\n",
      "Training Batch: 3529 Loss: 3317.504639\n",
      "Training Batch: 3530 Loss: 3208.520508\n",
      "Training Batch: 3531 Loss: 3271.058594\n",
      "Training Batch: 3532 Loss: 3297.640625\n",
      "Training Batch: 3533 Loss: 3167.248535\n",
      "Training Batch: 3534 Loss: 3304.850586\n",
      "Training Batch: 3535 Loss: 3222.539062\n",
      "Training Batch: 3536 Loss: 3167.472656\n",
      "Training Batch: 3537 Loss: 3223.897705\n",
      "Training Batch: 3538 Loss: 3171.157715\n",
      "Training Batch: 3539 Loss: 3217.925049\n",
      "Training Batch: 3540 Loss: 3281.003174\n",
      "Training Batch: 3541 Loss: 3508.420410\n",
      "Training Batch: 3542 Loss: 3364.388672\n",
      "Training Batch: 3543 Loss: 3236.237305\n",
      "Training Batch: 3544 Loss: 3327.100830\n",
      "Training Batch: 3545 Loss: 3324.799316\n",
      "Training Batch: 3546 Loss: 3291.192139\n",
      "Training Batch: 3547 Loss: 3321.139404\n",
      "Training Batch: 3548 Loss: 3344.356445\n",
      "Training Batch: 3549 Loss: 3271.111816\n",
      "Training Batch: 3550 Loss: 3250.413330\n",
      "Training Batch: 3551 Loss: 3431.399414\n",
      "Training Batch: 3552 Loss: 3264.927734\n",
      "Training Batch: 3553 Loss: 3526.416992\n",
      "Training Batch: 3554 Loss: 3250.799316\n",
      "Training Batch: 3555 Loss: 3599.580322\n",
      "Training Batch: 3556 Loss: 3358.558594\n",
      "Training Batch: 3557 Loss: 3253.254395\n",
      "Training Batch: 3558 Loss: 3190.562988\n",
      "Training Batch: 3559 Loss: 3492.205078\n",
      "Training Batch: 3560 Loss: 3518.939941\n",
      "Training Batch: 3561 Loss: 3287.401367\n",
      "Training Batch: 3562 Loss: 3277.720215\n",
      "Training Batch: 3563 Loss: 3469.486572\n",
      "Training Batch: 3564 Loss: 3386.722656\n",
      "Training Batch: 3565 Loss: 3203.296875\n",
      "Training Batch: 3566 Loss: 3395.559082\n",
      "Training Batch: 3567 Loss: 3369.675049\n",
      "Training Batch: 3568 Loss: 3313.252686\n",
      "Training Batch: 3569 Loss: 3404.537598\n",
      "Training Batch: 3570 Loss: 3326.782227\n",
      "Training Batch: 3571 Loss: 3365.286621\n",
      "Training Batch: 3572 Loss: 3184.710449\n",
      "Training Batch: 3573 Loss: 3320.831543\n",
      "Training Batch: 3574 Loss: 3151.905273\n",
      "Training Batch: 3575 Loss: 3192.083984\n",
      "Training Batch: 3576 Loss: 3291.465820\n",
      "Training Batch: 3577 Loss: 3747.223877\n",
      "Training Batch: 3578 Loss: 3146.154785\n",
      "Training Batch: 3579 Loss: 3271.112305\n",
      "Training Batch: 3580 Loss: 3184.365234\n",
      "Training Batch: 3581 Loss: 3547.845947\n",
      "Training Batch: 3582 Loss: 3357.066895\n",
      "Training Batch: 3583 Loss: 3306.220703\n",
      "Training Batch: 3584 Loss: 3411.369141\n",
      "Training Batch: 3585 Loss: 3229.608154\n",
      "Training Batch: 3586 Loss: 3256.173828\n",
      "Training Batch: 3587 Loss: 3314.594482\n",
      "Training Batch: 3588 Loss: 3175.976318\n",
      "Training Batch: 3589 Loss: 3299.875488\n",
      "Training Batch: 3590 Loss: 3472.384277\n",
      "Training Batch: 3591 Loss: 3292.211914\n",
      "Training Batch: 3592 Loss: 3460.208740\n",
      "Training Batch: 3593 Loss: 3376.145752\n",
      "Training Batch: 3594 Loss: 3260.385254\n",
      "Training Batch: 3595 Loss: 3470.838867\n",
      "Training Batch: 3596 Loss: 3267.822021\n",
      "Training Batch: 3597 Loss: 3349.028320\n",
      "Training Batch: 3598 Loss: 3205.424805\n",
      "Training Batch: 3599 Loss: 3348.178223\n",
      "Training Batch: 3600 Loss: 3284.058350\n",
      "Training Batch: 3601 Loss: 3201.326904\n",
      "Training Batch: 3602 Loss: 3243.926514\n",
      "Training Batch: 3603 Loss: 3275.622559\n",
      "Training Batch: 3604 Loss: 3237.192871\n",
      "Training Batch: 3605 Loss: 3618.518555\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Batch: 3606 Loss: 3370.429443\n",
      "Training Batch: 3607 Loss: 3258.251953\n",
      "Training Batch: 3608 Loss: 3338.533447\n",
      "Training Batch: 3609 Loss: 3318.926758\n",
      "Training Batch: 3610 Loss: 3188.738770\n",
      "Training Batch: 3611 Loss: 3144.643555\n",
      "Training Batch: 3612 Loss: 3203.317383\n",
      "Training Batch: 3613 Loss: 3295.141113\n",
      "Training Batch: 3614 Loss: 3154.350342\n",
      "Training Batch: 3615 Loss: 3513.605469\n",
      "Training Batch: 3616 Loss: 3260.525391\n",
      "Training Batch: 3617 Loss: 3269.958984\n",
      "Training Batch: 3618 Loss: 3181.307617\n",
      "Training Batch: 3619 Loss: 3281.985352\n",
      "Training Batch: 3620 Loss: 3301.138916\n",
      "Training Batch: 3621 Loss: 3271.243408\n",
      "Training Batch: 3622 Loss: 3250.480225\n",
      "Training Batch: 3623 Loss: 3189.384766\n",
      "Training Batch: 3624 Loss: 3203.785889\n",
      "Training Batch: 3625 Loss: 3251.459717\n",
      "Training Batch: 3626 Loss: 3286.318848\n",
      "Training Batch: 3627 Loss: 3145.542236\n",
      "Training Batch: 3628 Loss: 3252.676025\n",
      "Training Batch: 3629 Loss: 3183.643555\n",
      "Training Batch: 3630 Loss: 3234.866943\n",
      "Training Batch: 3631 Loss: 3249.139648\n",
      "Training Batch: 3632 Loss: 3167.527832\n",
      "Training Batch: 3633 Loss: 3286.341309\n",
      "Training Batch: 3634 Loss: 3136.738281\n",
      "Training Batch: 3635 Loss: 3250.589844\n",
      "Training Batch: 3636 Loss: 3248.626221\n",
      "Training Batch: 3637 Loss: 3181.019043\n",
      "Training Batch: 3638 Loss: 3358.737549\n",
      "Training Batch: 3639 Loss: 3408.449707\n",
      "Training Batch: 3640 Loss: 3243.135742\n",
      "Training Batch: 3641 Loss: 3288.775879\n",
      "Training Batch: 3642 Loss: 3218.136230\n",
      "Training Batch: 3643 Loss: 3154.421631\n",
      "Training Batch: 3644 Loss: 3420.304688\n",
      "Training Batch: 3645 Loss: 3324.115234\n",
      "Training Batch: 3646 Loss: 3433.782471\n",
      "Training Batch: 3647 Loss: 3233.519287\n",
      "Training Batch: 3648 Loss: 3262.509766\n",
      "Training Batch: 3649 Loss: 3193.094482\n",
      "Training Batch: 3650 Loss: 3330.655762\n",
      "Training Batch: 3651 Loss: 3236.981934\n",
      "Training Batch: 3652 Loss: 3430.914062\n",
      "Training Batch: 3653 Loss: 3235.958740\n",
      "Training Batch: 3654 Loss: 3319.823242\n",
      "Training Batch: 3655 Loss: 3320.959229\n",
      "Training Batch: 3656 Loss: 3538.178223\n",
      "Training Batch: 3657 Loss: 3424.346191\n",
      "Training Batch: 3658 Loss: 3253.909912\n",
      "Training Batch: 3659 Loss: 3294.831543\n",
      "Training Batch: 3660 Loss: 3283.388672\n",
      "Training Batch: 3661 Loss: 3255.812012\n",
      "Training Batch: 3662 Loss: 3328.849121\n",
      "Training Batch: 3663 Loss: 3350.446289\n",
      "Training Batch: 3664 Loss: 3265.832031\n",
      "Training Batch: 3665 Loss: 3227.273926\n",
      "Training Batch: 3666 Loss: 3338.338379\n",
      "Training Batch: 3667 Loss: 3321.074219\n",
      "Training Batch: 3668 Loss: 3251.115234\n",
      "Training Batch: 3669 Loss: 3355.754150\n",
      "Training Batch: 3670 Loss: 3481.615723\n",
      "Training Batch: 3671 Loss: 3200.988770\n",
      "Training Batch: 3672 Loss: 3336.539307\n",
      "Training Batch: 3673 Loss: 3373.919434\n",
      "Training Batch: 3674 Loss: 3280.211426\n",
      "Training Batch: 3675 Loss: 3410.586426\n",
      "Training Batch: 3676 Loss: 3235.402344\n",
      "Training Batch: 3677 Loss: 3241.074219\n",
      "Training Batch: 3678 Loss: 3340.448730\n",
      "Training Batch: 3679 Loss: 3256.753906\n",
      "Training Batch: 3680 Loss: 3250.372070\n",
      "Training Batch: 3681 Loss: 3207.842285\n",
      "Training Batch: 3682 Loss: 3382.318848\n",
      "Training Batch: 3683 Loss: 3292.457031\n",
      "Training Batch: 3684 Loss: 3290.508057\n",
      "Training Batch: 3685 Loss: 3285.693604\n",
      "Training Batch: 3686 Loss: 3270.914307\n",
      "Training Batch: 3687 Loss: 3528.586426\n",
      "Training Batch: 3688 Loss: 3524.251221\n",
      "Training Batch: 3689 Loss: 3397.295410\n",
      "Training Batch: 3690 Loss: 3317.166992\n",
      "Training Batch: 3691 Loss: 3508.442871\n",
      "Training Batch: 3692 Loss: 3339.338379\n",
      "Training Batch: 3693 Loss: 3330.841553\n",
      "Training Batch: 3694 Loss: 3203.218750\n",
      "Training Batch: 3695 Loss: 3214.934570\n",
      "Training Batch: 3696 Loss: 3203.396240\n",
      "Training Batch: 3697 Loss: 3231.059570\n",
      "Training Batch: 3698 Loss: 3249.680176\n",
      "Training Batch: 3699 Loss: 3264.546387\n",
      "Training Batch: 3700 Loss: 3416.161621\n",
      "Training Batch: 3701 Loss: 3402.569824\n",
      "Training Batch: 3702 Loss: 3357.393311\n",
      "Training Batch: 3703 Loss: 3295.233398\n",
      "Training Batch: 3704 Loss: 3227.810547\n",
      "Training Batch: 3705 Loss: 3383.747070\n",
      "Training Batch: 3706 Loss: 3284.007324\n",
      "Training Batch: 3707 Loss: 3247.447510\n",
      "Training Batch: 3708 Loss: 3610.019043\n",
      "Training Batch: 3709 Loss: 3374.085205\n",
      "Training Batch: 3710 Loss: 3264.465576\n",
      "Training Batch: 3711 Loss: 3323.076172\n",
      "Training Batch: 3712 Loss: 3556.523438\n",
      "Training Batch: 3713 Loss: 3368.834717\n",
      "Training Batch: 3714 Loss: 3920.148193\n",
      "Training Batch: 3715 Loss: 3762.915039\n",
      "Training Batch: 3716 Loss: 3237.236328\n",
      "Training Batch: 3717 Loss: 3346.735352\n",
      "Training Batch: 3718 Loss: 3232.727539\n",
      "Training Batch: 3719 Loss: 3342.131348\n",
      "Training Batch: 3720 Loss: 3308.794678\n",
      "Training Batch: 3721 Loss: 3388.661621\n",
      "Training Batch: 3722 Loss: 3312.436279\n",
      "Training Batch: 3723 Loss: 3223.213623\n",
      "Training Batch: 3724 Loss: 3297.500977\n",
      "Training Batch: 3725 Loss: 3200.252930\n",
      "Training Batch: 3726 Loss: 3418.647705\n",
      "Training Batch: 3727 Loss: 3305.688965\n",
      "Training Batch: 3728 Loss: 3288.783691\n",
      "Training Batch: 3729 Loss: 3270.946777\n",
      "Training Batch: 3730 Loss: 3319.983887\n",
      "Training Batch: 3731 Loss: 3193.407959\n",
      "Training Batch: 3732 Loss: 3354.563721\n",
      "Training Batch: 3733 Loss: 3222.802979\n",
      "Training Batch: 3734 Loss: 3258.140869\n",
      "Training Batch: 3735 Loss: 3240.158691\n",
      "Training Batch: 3736 Loss: 3243.031738\n",
      "Training Batch: 3737 Loss: 3295.446289\n",
      "Training Batch: 3738 Loss: 3221.820557\n",
      "Training Batch: 3739 Loss: 3201.187500\n",
      "Training Batch: 3740 Loss: 3209.582520\n",
      "Training Batch: 3741 Loss: 3244.965088\n",
      "Training Batch: 3742 Loss: 3292.541504\n",
      "Training Batch: 3743 Loss: 3289.474609\n",
      "Training Batch: 3744 Loss: 3254.770996\n",
      "Training Batch: 3745 Loss: 3242.969727\n",
      "Training Batch: 3746 Loss: 3350.540527\n",
      "Training Batch: 3747 Loss: 3250.211426\n",
      "Training Batch: 3748 Loss: 3216.149658\n",
      "Training Batch: 3749 Loss: 3209.965820\n",
      "Training Batch: 3750 Loss: 3232.912598\n",
      "Training Batch: 3751 Loss: 3189.797607\n",
      "Training Batch: 3752 Loss: 3316.322754\n",
      "Training Batch: 3753 Loss: 3224.082764\n",
      "Training Batch: 3754 Loss: 3085.728027\n",
      "Training Batch: 3755 Loss: 3152.721436\n",
      "Training Batch: 3756 Loss: 3293.113281\n",
      "Training Batch: 3757 Loss: 3486.497070\n",
      "Training Batch: 3758 Loss: 3251.059082\n",
      "Training Batch: 3759 Loss: 3251.176758\n",
      "Training Batch: 3760 Loss: 3140.834717\n",
      "Training Batch: 3761 Loss: 3388.926514\n",
      "Training Batch: 3762 Loss: 3184.397461\n",
      "Training Batch: 3763 Loss: 3188.700195\n",
      "Training Batch: 3764 Loss: 3261.460938\n",
      "Training Batch: 3765 Loss: 3348.818604\n",
      "Training Batch: 3766 Loss: 3548.994141\n",
      "Training Batch: 3767 Loss: 5198.782715\n",
      "Training Batch: 3768 Loss: 3382.355469\n",
      "Training Batch: 3769 Loss: 3336.274902\n",
      "Training Batch: 3770 Loss: 3321.415527\n",
      "Training Batch: 3771 Loss: 3253.184570\n",
      "Training Batch: 3772 Loss: 3201.379883\n",
      "Training Batch: 3773 Loss: 3266.678223\n",
      "Training Batch: 3774 Loss: 3234.199463\n",
      "Training Batch: 3775 Loss: 3193.628418\n",
      "Training Batch: 3776 Loss: 3287.818848\n",
      "Training Batch: 3777 Loss: 3379.668701\n",
      "Training Batch: 3778 Loss: 3237.496094\n",
      "Training Batch: 3779 Loss: 3322.122559\n",
      "Training Batch: 3780 Loss: 3246.609863\n",
      "Training Batch: 3781 Loss: 3299.212891\n",
      "Training Batch: 3782 Loss: 3206.595215\n",
      "Training Batch: 3783 Loss: 3405.033936\n",
      "Training Batch: 3784 Loss: 3239.417480\n",
      "Training Batch: 3785 Loss: 3322.943359\n",
      "Training Batch: 3786 Loss: 3234.404785\n",
      "Training Batch: 3787 Loss: 3224.988281\n",
      "Training Batch: 3788 Loss: 3317.362793\n",
      "Training Batch: 3789 Loss: 3165.565430\n",
      "Training Batch: 3790 Loss: 3537.795410\n",
      "Training Batch: 3791 Loss: 3612.145508\n",
      "Training Batch: 3792 Loss: 3317.787598\n",
      "Training Batch: 3793 Loss: 3511.067871\n",
      "Training Batch: 3794 Loss: 3419.419922\n",
      "Training Batch: 3795 Loss: 3338.371582\n",
      "Training Batch: 3796 Loss: 3366.856689\n",
      "Training Batch: 3797 Loss: 3327.182129\n",
      "Training Batch: 3798 Loss: 3275.570557\n",
      "Training Batch: 3799 Loss: 3206.528809\n",
      "Training Batch: 3800 Loss: 3356.592773\n",
      "Training Batch: 3801 Loss: 3599.053223\n",
      "Training Batch: 3802 Loss: 3452.205078\n",
      "Training Batch: 3803 Loss: 3345.229980\n",
      "Training Batch: 3804 Loss: 3368.217285\n",
      "Training Batch: 3805 Loss: 3317.072021\n",
      "Training Batch: 3806 Loss: 3181.150879\n",
      "Training Batch: 3807 Loss: 3325.945312\n",
      "Training Batch: 3808 Loss: 3234.512207\n",
      "Training Batch: 3809 Loss: 3210.438965\n",
      "Training Batch: 3810 Loss: 3238.853516\n",
      "Training Batch: 3811 Loss: 3389.258789\n",
      "Training Batch: 3812 Loss: 3175.349609\n",
      "Training Batch: 3813 Loss: 3512.860840\n",
      "Training Batch: 3814 Loss: 3197.519043\n",
      "Training Batch: 3815 Loss: 3156.786133\n",
      "Training Batch: 3816 Loss: 3275.707031\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Batch: 3817 Loss: 3272.356445\n",
      "Training Batch: 3818 Loss: 3330.202148\n",
      "Training Batch: 3819 Loss: 3409.650635\n",
      "Training Batch: 3820 Loss: 3313.763428\n",
      "Training Batch: 3821 Loss: 3331.493652\n",
      "Training Batch: 3822 Loss: 3190.343262\n",
      "Training Batch: 3823 Loss: 3359.364746\n",
      "Training Batch: 3824 Loss: 3343.797363\n",
      "Training Batch: 3825 Loss: 3289.948486\n",
      "Training Batch: 3826 Loss: 3362.517090\n",
      "Training Batch: 3827 Loss: 3338.884277\n",
      "Training Batch: 3828 Loss: 3161.723389\n",
      "Training Batch: 3829 Loss: 3215.601807\n",
      "Training Batch: 3830 Loss: 3290.769043\n",
      "Training Batch: 3831 Loss: 3367.405762\n",
      "Training Batch: 3832 Loss: 3279.124023\n",
      "Training Batch: 3833 Loss: 3136.889404\n",
      "Training Batch: 3834 Loss: 3229.454102\n",
      "Training Batch: 3835 Loss: 3229.611816\n",
      "Training Batch: 3836 Loss: 3268.699707\n",
      "Training Batch: 3837 Loss: 3241.265869\n",
      "Training Batch: 3838 Loss: 3208.773926\n",
      "Training Batch: 3839 Loss: 3385.689453\n",
      "Training Batch: 3840 Loss: 3206.791260\n",
      "Training Batch: 3841 Loss: 3224.952637\n",
      "Training Batch: 3842 Loss: 3176.639648\n",
      "Training Batch: 3843 Loss: 3162.195801\n",
      "Training Batch: 3844 Loss: 3439.786133\n",
      "Training Batch: 3845 Loss: 3435.615723\n",
      "Training Batch: 3846 Loss: 3220.579346\n",
      "Training Batch: 3847 Loss: 3105.118164\n",
      "Training Batch: 3848 Loss: 3476.764160\n",
      "Training Batch: 3849 Loss: 3381.865479\n",
      "Training Batch: 3850 Loss: 3265.207031\n",
      "Training Batch: 3851 Loss: 3188.067871\n",
      "Training Batch: 3852 Loss: 3339.619141\n",
      "Training Batch: 3853 Loss: 3380.871826\n",
      "Training Batch: 3854 Loss: 3254.416504\n",
      "Training Batch: 3855 Loss: 3335.864014\n",
      "Training Batch: 3856 Loss: 3260.168945\n",
      "Training Batch: 3857 Loss: 3149.658203\n",
      "Training Batch: 3858 Loss: 3399.186523\n",
      "Training Batch: 3859 Loss: 3278.695801\n",
      "Training Batch: 3860 Loss: 3252.366211\n",
      "Training Batch: 3861 Loss: 3357.241211\n",
      "Training Batch: 3862 Loss: 3255.825195\n",
      "Training Batch: 3863 Loss: 3199.148926\n",
      "Training Batch: 3864 Loss: 3281.956055\n",
      "Training Batch: 3865 Loss: 3147.262695\n",
      "Training Batch: 3866 Loss: 3336.243896\n",
      "Training Batch: 3867 Loss: 3183.920410\n",
      "Training Batch: 3868 Loss: 3243.728027\n",
      "Training Batch: 3869 Loss: 3255.385254\n",
      "Training Batch: 3870 Loss: 3286.384277\n",
      "Training Batch: 3871 Loss: 3234.273438\n",
      "Training Batch: 3872 Loss: 3270.565430\n",
      "Training Batch: 3873 Loss: 3355.154297\n",
      "Training Batch: 3874 Loss: 3183.485840\n",
      "Training Batch: 3875 Loss: 3155.544434\n",
      "Training Batch: 3876 Loss: 3309.459473\n",
      "Training Batch: 3877 Loss: 3305.672852\n",
      "Training Batch: 3878 Loss: 3307.488037\n",
      "Training Batch: 3879 Loss: 3206.028320\n",
      "Training Batch: 3880 Loss: 3281.197266\n",
      "Training Batch: 3881 Loss: 3240.251953\n",
      "Training Batch: 3882 Loss: 3189.479248\n",
      "Training Batch: 3883 Loss: 3152.355957\n",
      "Training Batch: 3884 Loss: 3215.539551\n",
      "Training Batch: 3885 Loss: 3158.099121\n",
      "Training Batch: 3886 Loss: 3191.581055\n",
      "Training Batch: 3887 Loss: 3470.186279\n",
      "Training Batch: 3888 Loss: 3182.033691\n",
      "Training Batch: 3889 Loss: 3430.143799\n",
      "Training Batch: 3890 Loss: 3187.863770\n",
      "Training Batch: 3891 Loss: 3589.903564\n",
      "Training Batch: 3892 Loss: 3592.363770\n",
      "Training Batch: 3893 Loss: 3251.604736\n",
      "Training Batch: 3894 Loss: 3204.198975\n",
      "Training Batch: 3895 Loss: 3197.099854\n",
      "Training Batch: 3896 Loss: 3192.320801\n",
      "Training Batch: 3897 Loss: 3508.713135\n",
      "Training Batch: 3898 Loss: 3501.081299\n",
      "Training Batch: 3899 Loss: 3250.780273\n",
      "Training Batch: 3900 Loss: 3304.734375\n",
      "Training Batch: 3901 Loss: 3306.756348\n",
      "Training Batch: 3902 Loss: 3363.060059\n",
      "Training Batch: 3903 Loss: 3309.583740\n",
      "Training Batch: 3904 Loss: 3444.976807\n",
      "Training Batch: 3905 Loss: 3367.707275\n",
      "Training Batch: 3906 Loss: 3205.063965\n",
      "Training Batch: 3907 Loss: 3229.913086\n",
      "Training Batch: 3908 Loss: 3253.497070\n",
      "Training Batch: 3909 Loss: 3586.520508\n",
      "Training Batch: 3910 Loss: 3284.160889\n",
      "Training Batch: 3911 Loss: 3270.307617\n",
      "Training Batch: 3912 Loss: 3237.926758\n",
      "Training Batch: 3913 Loss: 3311.996582\n",
      "Training Batch: 3914 Loss: 3220.286133\n",
      "Training Batch: 3915 Loss: 3406.642578\n",
      "Training Batch: 3916 Loss: 3160.192383\n",
      "Training Batch: 3917 Loss: 3343.736328\n",
      "Training Batch: 3918 Loss: 3294.785156\n",
      "Training Batch: 3919 Loss: 3237.271973\n",
      "Training Batch: 3920 Loss: 3187.048584\n",
      "Training Batch: 3921 Loss: 3299.770264\n",
      "Training Batch: 3922 Loss: 3444.056885\n",
      "Training Batch: 3923 Loss: 3234.267334\n",
      "Training Batch: 3924 Loss: 3260.710938\n",
      "Training Batch: 3925 Loss: 3265.890137\n",
      "Training Batch: 3926 Loss: 3340.809570\n",
      "Training Batch: 3927 Loss: 3185.350586\n",
      "Training Batch: 3928 Loss: 3380.943604\n",
      "Training Batch: 3929 Loss: 3332.501465\n",
      "Training Batch: 3930 Loss: 3184.424316\n",
      "Training Batch: 3931 Loss: 3283.557617\n",
      "Training Batch: 3932 Loss: 3415.430420\n",
      "Training Batch: 3933 Loss: 3251.757812\n",
      "Training Batch: 3934 Loss: 3209.445801\n",
      "Training Batch: 3935 Loss: 3205.827148\n",
      "Training Batch: 3936 Loss: 3109.677490\n",
      "Training Batch: 3937 Loss: 3400.941895\n",
      "Training Batch: 3938 Loss: 3286.078125\n",
      "Training Batch: 3939 Loss: 3251.819580\n",
      "Training Batch: 3940 Loss: 3250.146973\n",
      "Training Batch: 3941 Loss: 3342.681152\n",
      "Training Batch: 3942 Loss: 3227.429688\n",
      "Training Batch: 3943 Loss: 3172.969238\n",
      "Training Batch: 3944 Loss: 3256.357910\n",
      "Training Batch: 3945 Loss: 3283.199219\n",
      "Training Batch: 3946 Loss: 3209.169922\n",
      "Training Batch: 3947 Loss: 3262.218262\n",
      "Training Batch: 3948 Loss: 3166.704590\n",
      "Training Batch: 3949 Loss: 3224.312256\n",
      "Training Batch: 3950 Loss: 3295.903076\n",
      "Training Batch: 3951 Loss: 3366.051758\n",
      "Training Batch: 3952 Loss: 3213.647461\n",
      "Training Batch: 3953 Loss: 3367.730225\n",
      "Training Batch: 3954 Loss: 3350.735352\n",
      "Training Batch: 3955 Loss: 3185.849609\n",
      "Training Batch: 3956 Loss: 3313.232910\n",
      "Training Batch: 3957 Loss: 3178.177490\n",
      "Training Batch: 3958 Loss: 3322.639160\n",
      "Training Batch: 3959 Loss: 3297.815430\n",
      "Training Batch: 3960 Loss: 3257.109619\n",
      "Training Batch: 3961 Loss: 3433.452637\n",
      "Training Batch: 3962 Loss: 3354.842773\n",
      "Training Batch: 3963 Loss: 3244.836426\n",
      "Training Batch: 3964 Loss: 3182.998535\n",
      "Training Batch: 3965 Loss: 3310.785400\n",
      "Training Batch: 3966 Loss: 3227.535400\n",
      "Training Batch: 3967 Loss: 3213.760742\n",
      "Training Batch: 3968 Loss: 3234.045898\n",
      "Training Batch: 3969 Loss: 3330.736328\n",
      "Training Batch: 3970 Loss: 3230.478027\n",
      "Training Batch: 3971 Loss: 3166.350098\n",
      "Training Batch: 3972 Loss: 3261.730957\n",
      "Training Batch: 3973 Loss: 3242.730469\n",
      "Training Batch: 3974 Loss: 3304.719482\n",
      "Training Batch: 3975 Loss: 3288.993896\n",
      "Training Batch: 3976 Loss: 3376.184814\n",
      "Training Batch: 3977 Loss: 3256.060059\n",
      "Training Batch: 3978 Loss: 3428.129395\n",
      "Training Batch: 3979 Loss: 3367.314941\n",
      "Training Batch: 3980 Loss: 3319.377930\n",
      "Training Batch: 3981 Loss: 3245.229492\n",
      "Training Batch: 3982 Loss: 3192.627441\n",
      "Training Batch: 3983 Loss: 3203.216064\n",
      "Training Batch: 3984 Loss: 3519.145508\n",
      "Training Batch: 3985 Loss: 3247.007568\n",
      "Training Batch: 3986 Loss: 3185.708984\n",
      "Training Batch: 3987 Loss: 3276.415771\n",
      "Training Batch: 3988 Loss: 3527.408691\n",
      "Training Batch: 3989 Loss: 3295.475098\n",
      "Training Batch: 3990 Loss: 3295.515137\n",
      "Training Batch: 3991 Loss: 3388.269531\n",
      "Training Batch: 3992 Loss: 3271.938477\n",
      "Training Batch: 3993 Loss: 3190.787598\n",
      "Training Batch: 3994 Loss: 3307.314697\n",
      "Training Batch: 3995 Loss: 3349.614014\n",
      "Training Batch: 3996 Loss: 3287.358887\n",
      "Training Batch: 3997 Loss: 3294.443604\n",
      "Training Batch: 3998 Loss: 3312.840332\n",
      "Training Batch: 3999 Loss: 3262.476074\n",
      "Training Batch: 4000 Loss: 3180.968262\n",
      "Training Batch: 4001 Loss: 3361.317871\n",
      "Training Batch: 4002 Loss: 3246.243652\n",
      "Training Batch: 4003 Loss: 3222.216309\n",
      "Training Batch: 4004 Loss: 3373.127930\n",
      "Training Batch: 4005 Loss: 3618.270508\n",
      "Training Batch: 4006 Loss: 3342.647949\n",
      "Training Batch: 4007 Loss: 3358.158203\n",
      "Training Batch: 4008 Loss: 3233.630859\n",
      "Training Batch: 4009 Loss: 3376.322021\n",
      "Training Batch: 4010 Loss: 3453.200684\n",
      "Training Batch: 4011 Loss: 3255.356201\n",
      "Training Batch: 4012 Loss: 3216.454346\n",
      "Training Batch: 4013 Loss: 3475.179199\n",
      "Training Batch: 4014 Loss: 3309.449707\n",
      "Training Batch: 4015 Loss: 3246.739990\n",
      "Training Batch: 4016 Loss: 3283.959473\n",
      "Training Batch: 4017 Loss: 3354.527344\n",
      "Training Batch: 4018 Loss: 3213.494141\n",
      "Training Batch: 4019 Loss: 3218.235596\n",
      "Training Batch: 4020 Loss: 3235.427246\n",
      "Training Batch: 4021 Loss: 3198.087158\n",
      "Training Batch: 4022 Loss: 3255.152832\n",
      "Training Batch: 4023 Loss: 3426.306641\n",
      "Training Batch: 4024 Loss: 3361.784180\n",
      "Training Batch: 4025 Loss: 3232.171631\n",
      "Training Batch: 4026 Loss: 3190.523926\n",
      "Training Batch: 4027 Loss: 3201.767822\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Batch: 4028 Loss: 3313.171875\n",
      "Training Batch: 4029 Loss: 3500.295898\n",
      "Training Batch: 4030 Loss: 3154.922119\n",
      "Training Batch: 4031 Loss: 3238.342773\n",
      "Training Batch: 4032 Loss: 3414.901855\n",
      "Training Batch: 4033 Loss: 3334.675781\n",
      "Training Batch: 4034 Loss: 3271.090820\n",
      "Training Batch: 4035 Loss: 3486.990967\n",
      "Training Batch: 4036 Loss: 3309.040771\n",
      "Training Batch: 4037 Loss: 3326.416504\n",
      "Training Batch: 4038 Loss: 3358.025391\n",
      "Training Batch: 4039 Loss: 3285.821289\n",
      "Training Batch: 4040 Loss: 3304.093994\n",
      "Training Batch: 4041 Loss: 3283.189941\n",
      "Training Batch: 4042 Loss: 3320.339355\n",
      "Training Batch: 4043 Loss: 3302.666992\n",
      "Training Batch: 4044 Loss: 3270.626709\n",
      "Training Batch: 4045 Loss: 3242.180908\n",
      "Training Batch: 4046 Loss: 3258.486816\n",
      "Training Batch: 4047 Loss: 3334.293945\n",
      "Training Batch: 4048 Loss: 3290.340332\n",
      "Training Batch: 4049 Loss: 3286.697754\n",
      "Training Batch: 4050 Loss: 3336.839355\n",
      "Training Batch: 4051 Loss: 3360.857422\n",
      "Training Batch: 4052 Loss: 3454.684570\n",
      "Training Batch: 4053 Loss: 3557.852295\n",
      "Training Batch: 4054 Loss: 3441.824219\n",
      "Training Batch: 4055 Loss: 3420.836426\n",
      "Training Batch: 4056 Loss: 3524.360596\n",
      "Training Batch: 4057 Loss: 3479.480957\n",
      "Training Batch: 4058 Loss: 3387.857422\n",
      "Training Batch: 4059 Loss: 3330.679199\n",
      "Training Batch: 4060 Loss: 3492.260742\n",
      "Training Batch: 4061 Loss: 3387.166260\n",
      "Training Batch: 4062 Loss: 3359.109619\n",
      "Training Batch: 4063 Loss: 3170.221680\n",
      "Training Batch: 4064 Loss: 3229.451660\n",
      "Training Batch: 4065 Loss: 3125.165527\n",
      "Training Batch: 4066 Loss: 3354.169922\n",
      "Training Batch: 4067 Loss: 3282.368896\n",
      "Training Batch: 4068 Loss: 3263.506348\n",
      "Training Batch: 4069 Loss: 3267.161377\n",
      "Training Batch: 4070 Loss: 3239.897949\n",
      "Training Batch: 4071 Loss: 3237.581543\n",
      "Training Batch: 4072 Loss: 3297.193848\n",
      "Training Batch: 4073 Loss: 3523.349609\n",
      "Training Batch: 4074 Loss: 3280.728027\n",
      "Training Batch: 4075 Loss: 3151.337158\n",
      "Training Batch: 4076 Loss: 3298.928955\n",
      "Training Batch: 4077 Loss: 3301.306396\n",
      "Training Batch: 4078 Loss: 3639.096191\n",
      "Training Batch: 4079 Loss: 3707.882812\n",
      "Training Batch: 4080 Loss: 3338.817383\n",
      "Training Batch: 4081 Loss: 3374.455078\n",
      "Training Batch: 4082 Loss: 3291.109131\n",
      "Training Batch: 4083 Loss: 3215.349609\n",
      "Training Batch: 4084 Loss: 3149.694824\n",
      "Training Batch: 4085 Loss: 3272.095703\n",
      "Training Batch: 4086 Loss: 3441.313965\n",
      "Training Batch: 4087 Loss: 3339.793213\n",
      "Training Batch: 4088 Loss: 3192.836182\n",
      "Training Batch: 4089 Loss: 3159.959229\n",
      "Training Batch: 4090 Loss: 3128.967529\n",
      "Training Batch: 4091 Loss: 3234.807617\n",
      "Training Batch: 4092 Loss: 3290.389648\n",
      "Training Batch: 4093 Loss: 3374.817139\n",
      "Training Batch: 4094 Loss: 3341.317383\n",
      "Training Batch: 4095 Loss: 3310.795898\n",
      "Training Batch: 4096 Loss: 3310.849365\n",
      "Training Batch: 4097 Loss: 3383.932861\n",
      "Training Batch: 4098 Loss: 3120.471680\n",
      "Training Batch: 4099 Loss: 3249.177734\n",
      "Training Batch: 4100 Loss: 3378.382812\n",
      "Training Batch: 4101 Loss: 3193.785156\n",
      "Training Batch: 4102 Loss: 3391.083252\n",
      "Training Batch: 4103 Loss: 3268.738281\n",
      "Training Batch: 4104 Loss: 3159.886230\n",
      "Training Batch: 4105 Loss: 3280.700684\n",
      "Training Batch: 4106 Loss: 3177.838379\n",
      "Training Batch: 4107 Loss: 3246.199219\n",
      "Training Batch: 4108 Loss: 3248.374512\n",
      "Training Batch: 4109 Loss: 3507.002930\n",
      "Training Batch: 4110 Loss: 3421.503906\n",
      "Training Batch: 4111 Loss: 3211.683838\n",
      "Training Batch: 4112 Loss: 3334.699463\n",
      "Training Batch: 4113 Loss: 3349.114746\n",
      "Training Batch: 4114 Loss: 3170.138184\n",
      "Training Batch: 4115 Loss: 3403.020996\n",
      "Training Batch: 4116 Loss: 3274.118652\n",
      "Training Batch: 4117 Loss: 3405.373047\n",
      "Training Batch: 4118 Loss: 3296.106689\n",
      "Training Batch: 4119 Loss: 3261.122803\n",
      "Training Batch: 4120 Loss: 3441.415039\n",
      "Training Batch: 4121 Loss: 3295.575195\n",
      "Training Batch: 4122 Loss: 3303.622559\n",
      "Training Batch: 4123 Loss: 3303.597900\n",
      "Training Batch: 4124 Loss: 3239.808594\n",
      "Training Batch: 4125 Loss: 3300.862793\n",
      "Training Batch: 4126 Loss: 3310.214355\n",
      "Training Batch: 4127 Loss: 3380.075684\n",
      "Training Batch: 4128 Loss: 3811.080078\n",
      "Training Batch: 4129 Loss: 3345.837402\n",
      "Training Batch: 4130 Loss: 3261.291260\n",
      "Training Batch: 4131 Loss: 3220.276855\n",
      "Training Batch: 4132 Loss: 3375.691406\n",
      "Training Batch: 4133 Loss: 3304.425293\n",
      "Training Batch: 4134 Loss: 3196.324219\n",
      "Training Batch: 4135 Loss: 3423.523193\n",
      "Training Batch: 4136 Loss: 3191.991211\n",
      "Training Batch: 4137 Loss: 3221.345215\n",
      "Training Batch: 4138 Loss: 3194.296631\n",
      "Training Batch: 4139 Loss: 3196.410645\n",
      "Training Batch: 4140 Loss: 3345.701172\n",
      "Training Batch: 4141 Loss: 3306.584717\n",
      "Training Batch: 4142 Loss: 3224.967773\n",
      "Training Batch: 4143 Loss: 3219.089355\n",
      "Training Batch: 4144 Loss: 3231.382812\n",
      "Training Batch: 4145 Loss: 3177.438965\n",
      "Training Batch: 4146 Loss: 3294.730469\n",
      "Training Batch: 4147 Loss: 3293.201904\n",
      "Training Batch: 4148 Loss: 3390.325195\n",
      "Training Batch: 4149 Loss: 3172.490723\n",
      "Training Batch: 4150 Loss: 3344.565430\n",
      "Training Batch: 4151 Loss: 3193.958008\n",
      "Training Batch: 4152 Loss: 3124.861328\n",
      "Training Batch: 4153 Loss: 3280.618164\n",
      "Training Batch: 4154 Loss: 3213.469238\n",
      "Training Batch: 4155 Loss: 3334.737793\n",
      "Training Batch: 4156 Loss: 3251.633789\n",
      "Training Batch: 4157 Loss: 3381.519531\n",
      "Training Batch: 4158 Loss: 3319.496338\n",
      "Training Batch: 4159 Loss: 3450.642578\n",
      "Training Batch: 4160 Loss: 3242.053711\n",
      "Training Batch: 4161 Loss: 3331.231445\n",
      "Training Batch: 4162 Loss: 3222.776611\n",
      "Training Batch: 4163 Loss: 3306.389648\n",
      "Training Batch: 4164 Loss: 3266.493408\n",
      "Training Batch: 4165 Loss: 3290.669434\n",
      "Training Batch: 4166 Loss: 3328.611328\n",
      "Training Batch: 4167 Loss: 3374.143066\n",
      "Training Batch: 4168 Loss: 3287.074707\n",
      "Training Batch: 4169 Loss: 3236.261719\n",
      "Training Batch: 4170 Loss: 3186.623047\n",
      "Training Batch: 4171 Loss: 3289.398193\n",
      "Training Batch: 4172 Loss: 3299.186279\n",
      "Training Batch: 4173 Loss: 3371.818359\n",
      "Training Batch: 4174 Loss: 3465.027344\n",
      "Training Batch: 4175 Loss: 3466.248779\n",
      "Training Batch: 4176 Loss: 3249.623291\n",
      "Training Batch: 4177 Loss: 3414.787109\n",
      "Training Batch: 4178 Loss: 3329.027344\n",
      "Training Batch: 4179 Loss: 3226.093262\n",
      "Training Batch: 4180 Loss: 3451.812012\n",
      "Training Batch: 4181 Loss: 3579.822021\n",
      "Training Batch: 4182 Loss: 3522.356201\n",
      "Training Batch: 4183 Loss: 3366.512451\n",
      "Training Batch: 4184 Loss: 3308.955811\n",
      "Training Batch: 4185 Loss: 3323.436035\n",
      "Training Batch: 4186 Loss: 3669.995117\n",
      "Training Batch: 4187 Loss: 3305.276367\n",
      "Training Batch: 4188 Loss: 3311.941895\n",
      "Training Batch: 4189 Loss: 3303.951904\n",
      "Training Batch: 4190 Loss: 3301.832031\n",
      "Training Batch: 4191 Loss: 3255.599609\n",
      "Training Batch: 4192 Loss: 3213.329590\n",
      "Training Batch: 4193 Loss: 3255.761719\n",
      "Training Batch: 4194 Loss: 3217.703125\n",
      "Training Batch: 4195 Loss: 3135.698242\n",
      "Training Batch: 4196 Loss: 3198.720215\n",
      "Training Batch: 4197 Loss: 3246.246094\n",
      "Training Batch: 4198 Loss: 3232.552979\n",
      "Training Batch: 4199 Loss: 3219.260498\n",
      "Training Batch: 4200 Loss: 3219.608887\n",
      "Training Batch: 4201 Loss: 3307.732910\n",
      "Training Batch: 4202 Loss: 3465.554199\n",
      "Training Batch: 4203 Loss: 3059.528809\n",
      "Training Batch: 4204 Loss: 3346.801758\n",
      "Training Batch: 4205 Loss: 3333.979248\n",
      "Training Batch: 4206 Loss: 3590.744141\n",
      "Training Batch: 4207 Loss: 3775.395508\n",
      "Training Batch: 4208 Loss: 3415.420410\n",
      "Training Batch: 4209 Loss: 3309.831543\n",
      "Training Batch: 4210 Loss: 3360.353027\n",
      "Training Batch: 4211 Loss: 3263.077148\n",
      "Training Batch: 4212 Loss: 3241.215088\n",
      "Training Batch: 4213 Loss: 3179.103516\n",
      "Training Batch: 4214 Loss: 3234.077148\n",
      "Training Batch: 4215 Loss: 3187.812744\n",
      "Training Batch: 4216 Loss: 3252.458496\n",
      "Training Batch: 4217 Loss: 3281.452148\n",
      "Training Batch: 4218 Loss: 3172.285645\n",
      "Training Batch: 4219 Loss: 3139.504639\n",
      "Training Batch: 4220 Loss: 3286.601562\n",
      "Training Batch: 4221 Loss: 3330.049561\n",
      "Training Batch: 4222 Loss: 3294.317139\n",
      "Training Batch: 4223 Loss: 3280.941406\n",
      "Training Batch: 4224 Loss: 3124.217041\n",
      "Training Batch: 4225 Loss: 3369.341309\n",
      "Training Batch: 4226 Loss: 3377.134277\n",
      "Training Batch: 4227 Loss: 3257.057617\n",
      "Training Batch: 4228 Loss: 3544.380371\n",
      "Training Batch: 4229 Loss: 3241.711426\n",
      "Training Batch: 4230 Loss: 3858.195312\n",
      "Training Batch: 4231 Loss: 3849.109375\n",
      "Training Batch: 4232 Loss: 3439.218750\n",
      "Training Batch: 4233 Loss: 3360.126465\n",
      "Training Batch: 4234 Loss: 3368.176758\n",
      "Training Batch: 4235 Loss: 3189.392090\n",
      "Training Batch: 4236 Loss: 3317.261475\n",
      "Training Batch: 4237 Loss: 3185.594727\n",
      "Training Batch: 4238 Loss: 3303.058594\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Batch: 4239 Loss: 3245.917969\n",
      "Training Batch: 4240 Loss: 3262.992432\n",
      "Training Batch: 4241 Loss: 3379.902344\n",
      "Training Batch: 4242 Loss: 3335.984863\n",
      "Training Batch: 4243 Loss: 3139.518311\n",
      "Training Batch: 4244 Loss: 3411.217529\n",
      "Training Batch: 4245 Loss: 3189.300049\n",
      "Training Batch: 4246 Loss: 3280.092041\n",
      "Training Batch: 4247 Loss: 3258.980957\n",
      "Training Batch: 4248 Loss: 3265.095215\n",
      "Training Batch: 4249 Loss: 3464.191406\n",
      "Training Batch: 4250 Loss: 3198.307617\n",
      "Training Batch: 4251 Loss: 3308.206299\n",
      "Training Batch: 4252 Loss: 3238.414551\n",
      "Training Batch: 4253 Loss: 3442.379395\n",
      "Training Batch: 4254 Loss: 3326.354980\n",
      "Training Batch: 4255 Loss: 3199.997070\n",
      "Training Batch: 4256 Loss: 3300.233154\n",
      "Training Batch: 4257 Loss: 3313.229980\n",
      "Training Batch: 4258 Loss: 3381.754395\n",
      "Training Batch: 4259 Loss: 3179.690674\n",
      "Training Batch: 4260 Loss: 3251.423340\n",
      "Training Batch: 4261 Loss: 3210.436279\n",
      "Training Batch: 4262 Loss: 3127.898682\n",
      "Training Batch: 4263 Loss: 3216.543457\n",
      "Training Batch: 4264 Loss: 3248.574219\n",
      "Training Batch: 4265 Loss: 3289.042480\n",
      "Training Batch: 4266 Loss: 3275.781982\n",
      "Training Batch: 4267 Loss: 3233.671143\n",
      "Training Batch: 4268 Loss: 3278.020508\n",
      "Training Batch: 4269 Loss: 3196.218750\n",
      "Training Batch: 4270 Loss: 3249.298096\n",
      "Training Batch: 4271 Loss: 3150.337891\n",
      "Training Batch: 4272 Loss: 3217.853271\n",
      "Training Batch: 4273 Loss: 3316.863281\n",
      "Training Batch: 4274 Loss: 3275.573730\n",
      "Training Batch: 4275 Loss: 3334.078125\n",
      "Training Batch: 4276 Loss: 3106.003906\n",
      "Training Batch: 4277 Loss: 3163.429688\n",
      "Training Batch: 4278 Loss: 3198.250977\n",
      "Training Batch: 4279 Loss: 3222.458984\n",
      "Training Batch: 4280 Loss: 3185.644531\n",
      "Training Batch: 4281 Loss: 3251.122070\n",
      "Training Batch: 4282 Loss: 3149.557617\n",
      "Training Batch: 4283 Loss: 3279.081787\n",
      "Training Batch: 4284 Loss: 3175.084473\n",
      "Training Batch: 4285 Loss: 3176.218506\n",
      "Training Batch: 4286 Loss: 3318.717773\n",
      "Training Batch: 4287 Loss: 3333.188232\n",
      "Training Batch: 4288 Loss: 3234.977051\n",
      "Training Batch: 4289 Loss: 3241.815430\n",
      "Training Batch: 4290 Loss: 3296.933838\n",
      "Training Batch: 4291 Loss: 3226.868164\n",
      "Training Batch: 4292 Loss: 3321.522705\n",
      "Training Batch: 4293 Loss: 3202.295410\n",
      "Training Batch: 4294 Loss: 3558.200195\n",
      "Training Batch: 4295 Loss: 3257.498535\n",
      "Training Batch: 4296 Loss: 3248.411377\n",
      "Training Batch: 4297 Loss: 3083.169434\n",
      "Training Batch: 4298 Loss: 3201.390625\n",
      "Training Batch: 4299 Loss: 3231.352539\n",
      "Training Batch: 4300 Loss: 3249.459473\n",
      "Training Batch: 4301 Loss: 3303.985840\n",
      "Training Batch: 4302 Loss: 3428.161133\n",
      "Training Batch: 4303 Loss: 3206.119629\n",
      "Training Batch: 4304 Loss: 3247.411621\n",
      "Training Batch: 4305 Loss: 3288.712402\n",
      "Training Batch: 4306 Loss: 3287.214355\n",
      "Training Batch: 4307 Loss: 3238.867188\n",
      "Training Batch: 4308 Loss: 3548.925293\n",
      "Training Batch: 4309 Loss: 3255.375000\n",
      "Training Batch: 4310 Loss: 3141.706543\n",
      "Training Batch: 4311 Loss: 3244.052734\n",
      "Training Batch: 4312 Loss: 3188.606201\n",
      "Training Batch: 4313 Loss: 3367.707031\n",
      "Training Batch: 4314 Loss: 3295.843018\n",
      "Training Batch: 4315 Loss: 3175.965332\n",
      "Training Batch: 4316 Loss: 3268.956055\n",
      "Training Batch: 4317 Loss: 3177.744141\n",
      "Training Batch: 4318 Loss: 3423.734375\n",
      "Training Batch: 4319 Loss: 3232.539062\n",
      "Training Batch: 4320 Loss: 3329.137695\n",
      "Training Batch: 4321 Loss: 3395.673828\n",
      "Training Batch: 4322 Loss: 3199.140137\n",
      "Training Batch: 4323 Loss: 3281.322266\n",
      "Training Batch: 4324 Loss: 3227.832764\n",
      "Training Batch: 4325 Loss: 3417.893066\n",
      "Training Batch: 4326 Loss: 3204.450684\n",
      "Training Batch: 4327 Loss: 3281.743408\n",
      "Training Batch: 4328 Loss: 3256.262695\n",
      "Training Batch: 4329 Loss: 3211.333984\n",
      "Training Batch: 4330 Loss: 3231.867676\n",
      "Training Batch: 4331 Loss: 3310.020508\n",
      "Training Batch: 4332 Loss: 3186.343994\n",
      "Training Batch: 4333 Loss: 3321.999512\n",
      "Training Batch: 4334 Loss: 3488.031250\n",
      "Training Batch: 4335 Loss: 3314.693848\n",
      "Training Batch: 4336 Loss: 3209.773438\n",
      "Training Batch: 4337 Loss: 3197.013184\n",
      "Training Batch: 4338 Loss: 3304.806396\n",
      "Training Batch: 4339 Loss: 3171.389648\n",
      "Training Batch: 4340 Loss: 3177.001953\n",
      "Training Batch: 4341 Loss: 3229.647949\n",
      "Training Batch: 4342 Loss: 3169.227051\n",
      "Training Batch: 4343 Loss: 3197.874268\n",
      "Training Batch: 4344 Loss: 3275.057617\n",
      "Training Batch: 4345 Loss: 3303.907715\n",
      "Training Batch: 4346 Loss: 3393.480225\n",
      "Training Batch: 4347 Loss: 3202.649170\n",
      "Training Batch: 4348 Loss: 3163.702637\n",
      "Training Batch: 4349 Loss: 3211.491943\n",
      "Training Batch: 4350 Loss: 3132.164062\n",
      "Training Batch: 4351 Loss: 3193.680664\n",
      "Training Batch: 4352 Loss: 3269.587891\n",
      "Training Batch: 4353 Loss: 3365.213623\n",
      "Training Batch: 4354 Loss: 3180.970703\n",
      "Training Batch: 4355 Loss: 3311.336426\n",
      "Training Batch: 4356 Loss: 3399.295410\n",
      "Training Batch: 4357 Loss: 3408.907715\n",
      "Training Batch: 4358 Loss: 3545.753662\n",
      "Training Batch: 4359 Loss: 3213.830078\n",
      "Training Batch: 4360 Loss: 3164.400146\n",
      "Training Batch: 4361 Loss: 3281.623779\n",
      "Training Batch: 4362 Loss: 3236.179199\n",
      "Training Batch: 4363 Loss: 3253.502930\n",
      "Training Batch: 4364 Loss: 3160.679199\n",
      "Training Batch: 4365 Loss: 3305.938965\n",
      "Training Batch: 4366 Loss: 3213.613770\n",
      "Training Batch: 4367 Loss: 3322.458496\n",
      "Training Batch: 4368 Loss: 3310.741699\n",
      "Training Batch: 4369 Loss: 3282.403809\n",
      "Training Batch: 4370 Loss: 3236.853271\n",
      "Training Batch: 4371 Loss: 3169.110352\n",
      "Training Batch: 4372 Loss: 3294.207031\n",
      "Training Batch: 4373 Loss: 3235.699219\n",
      "Training Batch: 4374 Loss: 3194.094727\n",
      "Training Batch: 4375 Loss: 3455.559814\n",
      "Training Batch: 4376 Loss: 3271.841797\n",
      "Training Batch: 4377 Loss: 3128.536133\n",
      "Training Batch: 4378 Loss: 3209.115234\n",
      "Training Batch: 4379 Loss: 3120.609375\n",
      "Training Batch: 4380 Loss: 3188.237061\n",
      "Training Batch: 4381 Loss: 3372.709473\n",
      "Training Batch: 4382 Loss: 3193.334961\n",
      "Training Batch: 4383 Loss: 3267.008057\n",
      "Training Batch: 4384 Loss: 3267.852051\n",
      "Training Batch: 4385 Loss: 3344.836426\n",
      "Training Batch: 4386 Loss: 3208.056641\n",
      "Training Batch: 4387 Loss: 3314.431152\n",
      "Training Batch: 4388 Loss: 3261.021240\n",
      "Training Batch: 4389 Loss: 3270.174805\n",
      "Training Batch: 4390 Loss: 3588.054199\n",
      "Training Batch: 4391 Loss: 3242.286133\n",
      "Training Batch: 4392 Loss: 3483.753174\n",
      "Training Batch: 4393 Loss: 3271.170898\n",
      "Training Batch: 4394 Loss: 3330.497070\n",
      "Training Batch: 4395 Loss: 3201.683350\n",
      "Training Batch: 4396 Loss: 3207.758301\n",
      "Training Batch: 4397 Loss: 3180.493408\n",
      "Training Batch: 4398 Loss: 3218.564209\n",
      "Training Batch: 4399 Loss: 3204.864990\n",
      "Training Batch: 4400 Loss: 3269.506836\n",
      "Training Batch: 4401 Loss: 3247.051758\n",
      "Training Batch: 4402 Loss: 3312.741211\n",
      "Training Batch: 4403 Loss: 3215.178467\n",
      "Training Batch: 4404 Loss: 3217.112305\n",
      "Training Batch: 4405 Loss: 3184.304688\n",
      "Training Batch: 4406 Loss: 3226.436768\n",
      "Training Batch: 4407 Loss: 3228.312744\n",
      "Training Batch: 4408 Loss: 3181.895020\n",
      "Training Batch: 4409 Loss: 3570.284180\n",
      "Training Batch: 4410 Loss: 3417.924561\n",
      "Training Batch: 4411 Loss: 3329.017090\n",
      "Training Batch: 4412 Loss: 3674.301270\n",
      "Training Batch: 4413 Loss: 3379.503174\n",
      "Training Batch: 4414 Loss: 3445.168457\n",
      "Training Batch: 4415 Loss: 3239.079346\n",
      "Training Batch: 4416 Loss: 3299.181152\n",
      "Training Batch: 4417 Loss: 3342.700684\n",
      "Training Batch: 4418 Loss: 3343.997070\n",
      "Training Batch: 4419 Loss: 3258.718750\n",
      "Training Batch: 4420 Loss: 3192.527344\n",
      "Training Batch: 4421 Loss: 3207.508545\n",
      "Training Batch: 4422 Loss: 3289.613037\n",
      "Training Batch: 4423 Loss: 3201.419434\n",
      "Training Batch: 4424 Loss: 3246.074219\n",
      "Training Batch: 4425 Loss: 3436.921387\n",
      "Training Batch: 4426 Loss: 3302.726562\n",
      "Training Batch: 4427 Loss: 3250.068359\n",
      "Training Batch: 4428 Loss: 3207.294678\n",
      "Training Batch: 4429 Loss: 3240.008057\n",
      "Training Batch: 4430 Loss: 3182.057373\n",
      "Training Batch: 4431 Loss: 3275.806152\n",
      "Training Batch: 4432 Loss: 3292.739258\n",
      "Training Batch: 4433 Loss: 3225.360352\n",
      "Training Batch: 4434 Loss: 3158.446289\n",
      "Training Batch: 4435 Loss: 3211.425049\n",
      "Training Batch: 4436 Loss: 3274.226318\n",
      "Training Batch: 4437 Loss: 3184.999512\n",
      "Training Batch: 4438 Loss: 3248.745117\n",
      "Training Batch: 4439 Loss: 3315.049561\n",
      "Training Batch: 4440 Loss: 3163.891846\n",
      "Training Batch: 4441 Loss: 3212.682617\n",
      "Training Batch: 4442 Loss: 3314.811035\n",
      "Training Batch: 4443 Loss: 3206.497559\n",
      "Training Batch: 4444 Loss: 3357.603516\n",
      "Training Batch: 4445 Loss: 3317.896973\n",
      "Training Batch: 4446 Loss: 3326.979004\n",
      "Training Batch: 4447 Loss: 3362.110107\n",
      "Training Batch: 4448 Loss: 3274.441406\n",
      "Training Batch: 4449 Loss: 3155.022949\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Batch: 4450 Loss: 3372.634277\n",
      "Training Batch: 4451 Loss: 3307.694336\n",
      "Training Batch: 4452 Loss: 3269.566406\n",
      "Training Batch: 4453 Loss: 3210.944824\n",
      "Training Batch: 4454 Loss: 3424.251221\n",
      "Training Batch: 4455 Loss: 3158.374023\n",
      "Training Batch: 4456 Loss: 3271.431152\n",
      "Training Batch: 4457 Loss: 3355.738281\n",
      "Training Batch: 4458 Loss: 3168.697510\n",
      "Training Batch: 4459 Loss: 3284.313477\n",
      "Training Batch: 4460 Loss: 3217.395020\n",
      "Training Batch: 4461 Loss: 3285.869629\n",
      "Training Batch: 4462 Loss: 3267.908447\n",
      "Training Batch: 4463 Loss: 3197.714844\n",
      "Training Batch: 4464 Loss: 3251.671387\n",
      "Training Batch: 4465 Loss: 3254.439453\n",
      "Training Batch: 4466 Loss: 3270.624023\n",
      "Training Batch: 4467 Loss: 3137.462158\n",
      "Training Batch: 4468 Loss: 3192.867188\n",
      "Training Batch: 4469 Loss: 3201.490723\n",
      "Training Batch: 4470 Loss: 3164.259766\n",
      "Training Batch: 4471 Loss: 3362.297119\n",
      "Training Batch: 4472 Loss: 3331.910400\n",
      "Training Batch: 4473 Loss: 3219.510986\n",
      "Training Batch: 4474 Loss: 3233.218262\n",
      "Training Batch: 4475 Loss: 3207.846680\n",
      "Training Batch: 4476 Loss: 3410.091064\n",
      "Training Batch: 4477 Loss: 3152.937744\n",
      "Training Batch: 4478 Loss: 3251.298828\n",
      "Training Batch: 4479 Loss: 3420.025879\n",
      "Training Batch: 4480 Loss: 3190.498779\n",
      "Training Batch: 4481 Loss: 3314.835449\n",
      "Training Batch: 4482 Loss: 3244.330078\n",
      "Training Batch: 4483 Loss: 3407.133301\n",
      "Training Batch: 4484 Loss: 3264.176758\n",
      "Training Batch: 4485 Loss: 3237.392090\n",
      "Training Batch: 4486 Loss: 3196.736328\n",
      "Training Batch: 4487 Loss: 3181.790527\n",
      "Training Batch: 4488 Loss: 3223.861328\n",
      "Training Batch: 4489 Loss: 3365.593262\n",
      "Training Batch: 4490 Loss: 3291.875732\n",
      "Training Batch: 4491 Loss: 3228.346680\n",
      "Training Batch: 4492 Loss: 3213.646484\n",
      "Training Batch: 4493 Loss: 3284.351074\n",
      "Training Batch: 4494 Loss: 3238.898926\n",
      "Training Batch: 4495 Loss: 3218.385254\n",
      "Training Batch: 4496 Loss: 3228.549072\n",
      "Training Batch: 4497 Loss: 3336.106445\n",
      "Training Batch: 4498 Loss: 3160.506836\n",
      "Training Batch: 4499 Loss: 3227.166504\n",
      "Training Batch: 4500 Loss: 3148.211914\n",
      "Training Batch: 4501 Loss: 3233.072510\n",
      "Training Batch: 4502 Loss: 3241.864258\n",
      "Training Batch: 4503 Loss: 3303.497070\n",
      "Training Batch: 4504 Loss: 3384.495605\n",
      "Training Batch: 4505 Loss: 3348.410400\n",
      "Training Batch: 4506 Loss: 3304.386719\n",
      "Training Batch: 4507 Loss: 3332.977051\n",
      "Training Batch: 4508 Loss: 3250.757324\n",
      "Training Batch: 4509 Loss: 3412.541260\n",
      "Training Batch: 4510 Loss: 3254.303711\n",
      "Training Batch: 4511 Loss: 3299.944824\n",
      "Training Batch: 4512 Loss: 3287.151123\n",
      "Training Batch: 4513 Loss: 3230.656250\n",
      "Training Batch: 4514 Loss: 3400.722900\n",
      "Training Batch: 4515 Loss: 3349.552734\n",
      "Training Batch: 4516 Loss: 3265.480225\n",
      "Training Batch: 4517 Loss: 3251.581543\n",
      "Training Batch: 4518 Loss: 3431.066406\n",
      "Training Batch: 4519 Loss: 3375.049316\n",
      "Training Batch: 4520 Loss: 3380.464355\n",
      "Training Batch: 4521 Loss: 3422.332764\n",
      "Training Batch: 4522 Loss: 3200.268555\n",
      "Training Batch: 4523 Loss: 3353.942383\n",
      "Training Batch: 4524 Loss: 3307.492920\n",
      "Training Batch: 4525 Loss: 3706.398926\n",
      "Training Batch: 4526 Loss: 3296.461182\n",
      "Training Batch: 4527 Loss: 3319.570801\n",
      "Training Batch: 4528 Loss: 3560.601562\n",
      "Training Batch: 4529 Loss: 3525.069824\n",
      "Training Batch: 4530 Loss: 3221.658203\n",
      "Training Batch: 4531 Loss: 3281.474365\n",
      "Training Batch: 4532 Loss: 3396.463135\n",
      "Training Batch: 4533 Loss: 3264.544189\n",
      "Training Batch: 4534 Loss: 3305.581055\n",
      "Training Batch: 4535 Loss: 3395.154785\n",
      "Training Batch: 4536 Loss: 3655.514648\n",
      "Training Batch: 4537 Loss: 3547.388672\n",
      "Training Batch: 4538 Loss: 3302.109863\n",
      "Training Batch: 4539 Loss: 3307.294678\n",
      "Training Batch: 4540 Loss: 3238.654785\n",
      "Training Batch: 4541 Loss: 3209.997070\n",
      "Training Batch: 4542 Loss: 3347.207520\n",
      "Training Batch: 4543 Loss: 3183.463623\n",
      "Training Batch: 4544 Loss: 3257.014648\n",
      "Training Batch: 4545 Loss: 3397.323242\n",
      "Training Batch: 4546 Loss: 3211.411621\n",
      "Training Batch: 4547 Loss: 3248.932617\n",
      "Training Batch: 4548 Loss: 3171.983398\n",
      "Training Batch: 4549 Loss: 3153.898682\n",
      "Training Batch: 4550 Loss: 3215.746094\n",
      "Training Batch: 4551 Loss: 3229.320312\n",
      "Training Batch: 4552 Loss: 3215.854492\n",
      "Training Batch: 4553 Loss: 3274.166992\n",
      "Training Batch: 4554 Loss: 3332.085449\n",
      "Training Batch: 4555 Loss: 3310.807861\n",
      "Training Batch: 4556 Loss: 3097.416504\n",
      "Training Batch: 4557 Loss: 3145.644531\n",
      "Training Batch: 4558 Loss: 3117.909668\n",
      "Training Batch: 4559 Loss: 3238.734619\n",
      "Training Batch: 4560 Loss: 3207.938232\n",
      "Training Batch: 4561 Loss: 3327.578125\n",
      "Training Batch: 4562 Loss: 3249.116211\n",
      "Training Batch: 4563 Loss: 3437.461426\n",
      "Training Batch: 4564 Loss: 3332.116211\n",
      "Training Batch: 4565 Loss: 3639.353027\n",
      "Training Batch: 4566 Loss: 3210.617188\n",
      "Training Batch: 4567 Loss: 3253.712891\n",
      "Training Batch: 4568 Loss: 3266.395020\n",
      "Training Batch: 4569 Loss: 3182.486328\n",
      "Training Batch: 4570 Loss: 3193.106934\n",
      "Training Batch: 4571 Loss: 3279.337891\n",
      "Training Batch: 4572 Loss: 3261.018066\n",
      "Training Batch: 4573 Loss: 3342.834473\n",
      "Training Batch: 4574 Loss: 3272.704834\n",
      "Training Batch: 4575 Loss: 3170.988281\n",
      "Training Batch: 4576 Loss: 3365.406982\n",
      "Training Batch: 4577 Loss: 3179.906494\n",
      "Training Batch: 4578 Loss: 3268.757812\n",
      "Training Batch: 4579 Loss: 3374.420166\n",
      "Training Batch: 4580 Loss: 3271.772705\n",
      "Training Batch: 4581 Loss: 3317.520020\n",
      "Training Batch: 4582 Loss: 3193.957764\n",
      "Training Batch: 4583 Loss: 3192.972168\n",
      "Training Batch: 4584 Loss: 3245.950684\n",
      "Training Batch: 4585 Loss: 3322.378906\n",
      "Training Batch: 4586 Loss: 3162.982910\n",
      "Training Batch: 4587 Loss: 3293.896729\n",
      "Training Batch: 4588 Loss: 3366.250000\n",
      "Training Batch: 4589 Loss: 3117.283203\n",
      "Training Batch: 4590 Loss: 3132.543945\n",
      "Training Batch: 4591 Loss: 3217.413086\n",
      "Training Batch: 4592 Loss: 3228.555908\n",
      "Training Batch: 4593 Loss: 3202.694824\n",
      "Training Batch: 4594 Loss: 3178.205566\n",
      "Training Batch: 4595 Loss: 3228.349121\n",
      "Training Batch: 4596 Loss: 3235.327148\n",
      "Training Batch: 4597 Loss: 3148.700195\n",
      "Training Batch: 4598 Loss: 3237.225830\n",
      "Training Batch: 4599 Loss: 3421.684570\n",
      "Training Batch: 4600 Loss: 3172.566650\n",
      "Training Batch: 4601 Loss: 3282.642578\n",
      "Training Batch: 4602 Loss: 3276.339355\n",
      "Training Batch: 4603 Loss: 3309.779053\n",
      "Training Batch: 4604 Loss: 3284.174805\n",
      "Training Batch: 4605 Loss: 3347.342285\n",
      "Training Batch: 4606 Loss: 3284.217285\n",
      "Training Batch: 4607 Loss: 3246.989746\n",
      "Training Batch: 4608 Loss: 3433.638184\n",
      "Training Batch: 4609 Loss: 3383.091797\n",
      "Training Batch: 4610 Loss: 3219.385742\n",
      "Training Batch: 4611 Loss: 3233.353027\n",
      "Training Batch: 4612 Loss: 3197.272217\n",
      "Training Batch: 4613 Loss: 3221.875977\n",
      "Training Batch: 4614 Loss: 3241.916504\n",
      "Training Batch: 4615 Loss: 3104.869385\n",
      "Training Batch: 4616 Loss: 3173.180176\n",
      "Training Batch: 4617 Loss: 3310.399658\n",
      "Training Batch: 4618 Loss: 3230.980957\n",
      "Training Batch: 4619 Loss: 3287.484863\n",
      "Training Batch: 4620 Loss: 3269.126953\n",
      "Training Batch: 4621 Loss: 3421.694336\n",
      "Training Batch: 4622 Loss: 3164.853027\n",
      "Training Batch: 4623 Loss: 3416.107666\n",
      "Training Batch: 4624 Loss: 3414.895020\n",
      "Training Batch: 4625 Loss: 3326.152344\n",
      "Training Batch: 4626 Loss: 3316.963379\n",
      "Training Batch: 4627 Loss: 3448.034668\n",
      "Training Batch: 4628 Loss: 3436.290039\n",
      "Training Batch: 4629 Loss: 3434.498535\n",
      "Training Batch: 4630 Loss: 3397.236328\n",
      "Training Batch: 4631 Loss: 3300.604004\n",
      "Training Batch: 4632 Loss: 3236.586426\n",
      "Training Batch: 4633 Loss: 3175.720215\n",
      "Training Batch: 4634 Loss: 3295.615723\n",
      "Training Batch: 4635 Loss: 3209.836914\n",
      "Training Batch: 4636 Loss: 3308.908691\n",
      "Training Batch: 4637 Loss: 3211.581787\n",
      "Training Batch: 4638 Loss: 3180.886230\n",
      "Training Batch: 4639 Loss: 3148.207031\n",
      "Training Batch: 4640 Loss: 3119.735352\n",
      "Training Batch: 4641 Loss: 3373.656494\n",
      "Training Batch: 4642 Loss: 3140.051025\n",
      "Training Batch: 4643 Loss: 3261.810059\n",
      "Training Batch: 4644 Loss: 3220.612549\n",
      "Training Batch: 4645 Loss: 3122.796631\n",
      "Training Batch: 4646 Loss: 3232.343262\n",
      "Training Batch: 4647 Loss: 3200.500000\n",
      "Training Batch: 4648 Loss: 3203.466309\n",
      "Training Batch: 4649 Loss: 3293.231934\n",
      "Training Batch: 4650 Loss: 3254.410645\n",
      "Training Batch: 4651 Loss: 3201.411377\n",
      "Training Batch: 4652 Loss: 3544.153809\n",
      "Training Batch: 4653 Loss: 3444.147461\n",
      "Training Batch: 4654 Loss: 3528.008301\n",
      "Training Batch: 4655 Loss: 3551.604004\n",
      "Training Batch: 4656 Loss: 3441.339355\n",
      "Training Batch: 4657 Loss: 3198.959473\n",
      "Training Batch: 4658 Loss: 3337.304199\n",
      "Training Batch: 4659 Loss: 3196.614258\n",
      "Training Batch: 4660 Loss: 3198.537598\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Batch: 4661 Loss: 3391.501221\n",
      "Training Batch: 4662 Loss: 3220.996094\n",
      "Training Batch: 4663 Loss: 3176.509277\n",
      "Training Batch: 4664 Loss: 3090.482178\n",
      "Training Batch: 4665 Loss: 3190.720703\n",
      "Training Batch: 4666 Loss: 3216.056152\n",
      "Training Batch: 4667 Loss: 3289.330078\n",
      "Training Batch: 4668 Loss: 3219.012207\n",
      "Training Batch: 4669 Loss: 3202.208740\n",
      "Training Batch: 4670 Loss: 3228.128174\n",
      "Training Batch: 4671 Loss: 3199.624023\n",
      "Training Batch: 4672 Loss: 3161.861328\n",
      "Training Batch: 4673 Loss: 3402.353516\n",
      "Training Batch: 4674 Loss: 3217.517822\n",
      "Training Batch: 4675 Loss: 3261.887695\n",
      "Training Batch: 4676 Loss: 3340.588623\n",
      "Training Batch: 4677 Loss: 3363.226562\n",
      "Training Batch: 4678 Loss: 3437.215820\n",
      "Training Batch: 4679 Loss: 3203.359375\n",
      "Training Batch: 4680 Loss: 3192.188477\n",
      "Training Batch: 4681 Loss: 3458.878906\n",
      "Training Batch: 4682 Loss: 3272.065430\n",
      "Training Batch: 4683 Loss: 3283.790771\n",
      "Training Batch: 4684 Loss: 3325.271484\n",
      "Training Batch: 4685 Loss: 3256.242676\n",
      "Training Batch: 4686 Loss: 3244.563232\n",
      "Training Batch: 4687 Loss: 3211.099121\n",
      "Training Batch: 4688 Loss: 3397.038818\n",
      "Training Batch: 4689 Loss: 3206.953125\n",
      "Training Batch: 4690 Loss: 3278.391357\n",
      "Training Batch: 4691 Loss: 3199.370117\n",
      "Training Batch: 4692 Loss: 3158.255859\n",
      "Training Batch: 4693 Loss: 3356.559570\n",
      "Training Batch: 4694 Loss: 3290.240723\n",
      "Training Batch: 4695 Loss: 3336.733398\n",
      "Training Batch: 4696 Loss: 3287.049561\n",
      "Training Batch: 4697 Loss: 3257.698730\n",
      "Training Batch: 4698 Loss: 3353.346191\n",
      "Training Batch: 4699 Loss: 3274.965332\n",
      "Training Batch: 4700 Loss: 3414.516602\n",
      "Training Batch: 4701 Loss: 3347.859863\n",
      "Training Batch: 4702 Loss: 3263.314941\n",
      "Training Batch: 4703 Loss: 3218.262939\n",
      "Training Batch: 4704 Loss: 3215.264160\n",
      "Training Batch: 4705 Loss: 3340.907715\n",
      "Training Batch: 4706 Loss: 3355.726562\n",
      "Training Batch: 4707 Loss: 3206.690430\n",
      "Training Batch: 4708 Loss: 3237.050293\n",
      "Training Batch: 4709 Loss: 3197.231201\n",
      "Training Batch: 4710 Loss: 3063.003174\n",
      "Training Batch: 4711 Loss: 3212.950439\n",
      "Training Batch: 4712 Loss: 3196.677979\n",
      "Training Batch: 4713 Loss: 3119.691406\n",
      "Training Batch: 4714 Loss: 3335.317627\n",
      "Training Batch: 4715 Loss: 3384.102783\n",
      "Training Batch: 4716 Loss: 3212.590332\n",
      "Training Batch: 4717 Loss: 3503.512207\n",
      "Training Batch: 4718 Loss: 3205.910645\n",
      "Training Batch: 4719 Loss: 3266.522217\n",
      "Training Batch: 4720 Loss: 3307.196533\n",
      "Training Batch: 4721 Loss: 3287.956055\n",
      "Training Batch: 4722 Loss: 3337.656250\n",
      "Training Batch: 4723 Loss: 3263.388184\n",
      "Training Batch: 4724 Loss: 3217.327637\n",
      "Training Batch: 4725 Loss: 3242.895020\n",
      "Training Batch: 4726 Loss: 3345.919678\n",
      "Training Batch: 4727 Loss: 3203.884277\n",
      "Training Batch: 4728 Loss: 3347.271973\n",
      "Training Batch: 4729 Loss: 3472.151855\n",
      "Training Batch: 4730 Loss: 3288.496338\n",
      "Training Batch: 4731 Loss: 3550.644043\n",
      "Training Batch: 4732 Loss: 3390.448975\n",
      "Training Batch: 4733 Loss: 3548.791016\n",
      "Training Batch: 4734 Loss: 3168.644531\n",
      "Training Batch: 4735 Loss: 3569.794189\n",
      "Training Batch: 4736 Loss: 3197.404785\n",
      "Training Batch: 4737 Loss: 3206.586182\n",
      "Training Batch: 4738 Loss: 3195.777344\n",
      "Training Batch: 4739 Loss: 3304.288574\n",
      "Training Batch: 4740 Loss: 3135.603027\n",
      "Training Batch: 4741 Loss: 3211.086426\n",
      "Training Batch: 4742 Loss: 3332.560303\n",
      "Training Batch: 4743 Loss: 3225.076172\n",
      "Training Batch: 4744 Loss: 3301.773438\n",
      "Training Batch: 4745 Loss: 3298.119629\n",
      "Training Batch: 4746 Loss: 3431.846191\n",
      "Training Batch: 4747 Loss: 3249.804443\n",
      "Training Batch: 4748 Loss: 3305.947266\n",
      "Training Batch: 4749 Loss: 3299.939453\n",
      "Training Batch: 4750 Loss: 3319.014160\n",
      "Training Batch: 4751 Loss: 3825.652344\n",
      "Training Batch: 4752 Loss: 3868.744141\n",
      "Training Batch: 4753 Loss: 3385.118164\n",
      "Training Batch: 4754 Loss: 3300.954102\n",
      "Training Batch: 4755 Loss: 3199.145508\n",
      "Training Batch: 4756 Loss: 3463.406250\n",
      "Training Batch: 4757 Loss: 3404.976074\n",
      "Training Batch: 4758 Loss: 3212.528809\n",
      "Training Batch: 4759 Loss: 3187.946777\n",
      "Training Batch: 4760 Loss: 3390.294922\n",
      "Training Batch: 4761 Loss: 3132.238281\n",
      "Training Batch: 4762 Loss: 3172.484375\n",
      "Training Batch: 4763 Loss: 3163.168701\n",
      "Training Batch: 4764 Loss: 3247.485596\n",
      "Training Batch: 4765 Loss: 3304.473633\n",
      "Training Batch: 4766 Loss: 3300.949219\n",
      "Training Batch: 4767 Loss: 3237.212646\n",
      "Training Batch: 4768 Loss: 3247.313477\n",
      "Training Batch: 4769 Loss: 3289.399902\n",
      "Training Batch: 4770 Loss: 3319.234619\n",
      "Training Batch: 4771 Loss: 3396.268555\n",
      "Training Batch: 4772 Loss: 3200.231934\n",
      "Training Batch: 4773 Loss: 3573.860840\n",
      "Training Batch: 4774 Loss: 3402.299316\n",
      "Training Batch: 4775 Loss: 3311.969238\n",
      "Training Batch: 4776 Loss: 3232.062500\n",
      "Training Batch: 4777 Loss: 3335.338867\n",
      "Training Batch: 4778 Loss: 3342.159668\n",
      "Training Batch: 4779 Loss: 3208.951660\n",
      "Training Batch: 4780 Loss: 3197.412354\n",
      "Training Batch: 4781 Loss: 3177.912109\n",
      "Training Batch: 4782 Loss: 3159.944824\n",
      "Training Batch: 4783 Loss: 3146.328613\n",
      "Training Batch: 4784 Loss: 3144.666016\n",
      "Training Batch: 4785 Loss: 3271.494873\n",
      "Training Batch: 4786 Loss: 3253.210449\n",
      "Training Batch: 4787 Loss: 3262.970215\n",
      "Training Batch: 4788 Loss: 3229.902832\n",
      "Training Batch: 4789 Loss: 3207.981445\n",
      "Training Batch: 4790 Loss: 3387.079834\n",
      "Training Batch: 4791 Loss: 3205.253906\n",
      "Training Batch: 4792 Loss: 3314.237793\n",
      "Training Batch: 4793 Loss: 3210.863770\n",
      "Training Batch: 4794 Loss: 3209.169922\n",
      "Training Batch: 4795 Loss: 3239.870117\n",
      "Training Batch: 4796 Loss: 3270.869629\n",
      "Training Batch: 4797 Loss: 3232.005859\n",
      "Training Batch: 4798 Loss: 3196.453857\n",
      "Training Batch: 4799 Loss: 3186.102051\n",
      "Training Batch: 4800 Loss: 3086.951660\n",
      "Training Batch: 4801 Loss: 3193.103760\n",
      "Training Batch: 4802 Loss: 3319.833008\n",
      "Training Batch: 4803 Loss: 3213.561523\n",
      "Training Batch: 4804 Loss: 3130.290039\n",
      "Training Batch: 4805 Loss: 3196.232910\n",
      "Training Batch: 4806 Loss: 3166.044434\n",
      "Training Batch: 4807 Loss: 3238.611328\n",
      "Training Batch: 4808 Loss: 3223.821777\n",
      "Training Batch: 4809 Loss: 3156.887207\n",
      "Training Batch: 4810 Loss: 3193.145020\n",
      "Training Batch: 4811 Loss: 3312.430420\n",
      "Training Batch: 4812 Loss: 3200.083008\n",
      "Training Batch: 4813 Loss: 3318.358887\n",
      "Training Batch: 4814 Loss: 3275.637451\n",
      "Training Batch: 4815 Loss: 3230.750488\n",
      "Training Batch: 4816 Loss: 3368.657959\n",
      "Training Batch: 4817 Loss: 3220.709473\n",
      "Training Batch: 4818 Loss: 3224.939453\n",
      "Training Batch: 4819 Loss: 3231.482178\n",
      "Training Batch: 4820 Loss: 3264.215088\n",
      "Training Batch: 4821 Loss: 3371.246582\n",
      "Training Batch: 4822 Loss: 3292.907227\n",
      "Training Batch: 4823 Loss: 3217.317871\n",
      "Training Batch: 4824 Loss: 3217.948242\n",
      "Training Batch: 4825 Loss: 3192.694092\n",
      "Training Batch: 4826 Loss: 3343.712402\n",
      "Training Batch: 4827 Loss: 3368.691406\n",
      "Training Batch: 4828 Loss: 3225.168457\n",
      "Training Batch: 4829 Loss: 3292.822754\n",
      "Training Batch: 4830 Loss: 3340.457031\n",
      "Training Batch: 4831 Loss: 3452.135254\n",
      "Training Batch: 4832 Loss: 3602.563232\n",
      "Training Batch: 4833 Loss: 3383.834961\n",
      "Training Batch: 4834 Loss: 3315.869873\n",
      "Training Batch: 4835 Loss: 3273.334473\n",
      "Training Batch: 4836 Loss: 3210.095703\n",
      "Training Batch: 4837 Loss: 3297.983887\n",
      "Training Batch: 4838 Loss: 3236.908203\n",
      "Training Batch: 4839 Loss: 3323.850586\n",
      "Training Batch: 4840 Loss: 3177.670410\n",
      "Training Batch: 4841 Loss: 3247.896484\n",
      "Training Batch: 4842 Loss: 3182.560059\n",
      "Training Batch: 4843 Loss: 3233.163574\n",
      "Training Batch: 4844 Loss: 3281.219727\n",
      "Training Batch: 4845 Loss: 3342.977051\n",
      "Training Batch: 4846 Loss: 3494.233643\n",
      "Training Batch: 4847 Loss: 3508.091797\n",
      "Training Batch: 4848 Loss: 3233.198486\n",
      "Training Batch: 4849 Loss: 3173.749023\n",
      "Training Batch: 4850 Loss: 3276.811768\n",
      "Training Batch: 4851 Loss: 3198.747559\n",
      "Training Batch: 4852 Loss: 3170.293945\n",
      "Training Batch: 4853 Loss: 3140.785889\n",
      "Training Batch: 4854 Loss: 3126.066406\n",
      "Training Batch: 4855 Loss: 3188.682129\n",
      "Training Batch: 4856 Loss: 3251.797852\n",
      "Training Batch: 4857 Loss: 3968.439941\n",
      "Training Batch: 4858 Loss: 3866.237305\n",
      "Training Batch: 4859 Loss: 3403.483154\n",
      "Training Batch: 4860 Loss: 3501.621582\n",
      "Training Batch: 4861 Loss: 3279.093750\n",
      "Training Batch: 4862 Loss: 3299.225586\n",
      "Training Batch: 4863 Loss: 3415.043457\n",
      "Training Batch: 4864 Loss: 3384.115234\n",
      "Training Batch: 4865 Loss: 3305.973145\n",
      "Training Batch: 4866 Loss: 3376.599609\n",
      "Training Batch: 4867 Loss: 3260.130371\n",
      "Training Batch: 4868 Loss: 3305.254883\n",
      "Training Batch: 4869 Loss: 3331.149902\n",
      "Training Batch: 4870 Loss: 3136.562500\n",
      "Training Batch: 4871 Loss: 3224.396484\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Batch: 4872 Loss: 3141.745605\n",
      "Training Batch: 4873 Loss: 3202.812988\n",
      "Training Batch: 4874 Loss: 3369.637939\n",
      "Training Batch: 4875 Loss: 3266.072510\n",
      "Training Batch: 4876 Loss: 3249.191406\n",
      "Training Batch: 4877 Loss: 3235.763184\n",
      "Training Batch: 4878 Loss: 3424.365723\n",
      "Training Batch: 4879 Loss: 3313.078857\n",
      "Training Batch: 4880 Loss: 3255.047119\n",
      "Training Batch: 4881 Loss: 3233.920410\n",
      "Training Batch: 4882 Loss: 3309.609863\n",
      "Training Batch: 4883 Loss: 3288.203369\n",
      "Training Batch: 4884 Loss: 3279.808105\n",
      "Training Batch: 4885 Loss: 3247.677979\n",
      "Training Batch: 4886 Loss: 3178.509277\n",
      "Training Batch: 4887 Loss: 3442.548340\n",
      "Training Batch: 4888 Loss: 3361.735107\n",
      "Training Batch: 4889 Loss: 3300.730469\n",
      "Training Batch: 4890 Loss: 3381.264160\n",
      "Training Batch: 4891 Loss: 3247.992188\n",
      "Training Batch: 4892 Loss: 3343.265869\n",
      "Training Batch: 4893 Loss: 3214.783203\n",
      "Training Batch: 4894 Loss: 3296.893066\n",
      "Training Batch: 4895 Loss: 3375.522949\n",
      "Training Batch: 4896 Loss: 3221.373779\n",
      "Training Batch: 4897 Loss: 3185.692383\n",
      "Training Batch: 4898 Loss: 3232.958984\n",
      "Training Batch: 4899 Loss: 3228.012695\n",
      "Training Batch: 4900 Loss: 3583.097412\n",
      "Training Batch: 4901 Loss: 3203.026855\n",
      "Training Batch: 4902 Loss: 3137.859375\n",
      "Training Batch: 4903 Loss: 3131.835449\n",
      "Training Batch: 4904 Loss: 3492.022461\n",
      "Training Batch: 4905 Loss: 3219.474609\n",
      "Training Batch: 4906 Loss: 3247.364258\n",
      "Training Batch: 4907 Loss: 3119.417969\n",
      "Training Batch: 4908 Loss: 3233.331543\n",
      "Training Batch: 4909 Loss: 3203.666016\n",
      "Training Batch: 4910 Loss: 3274.912598\n",
      "Training Batch: 4911 Loss: 3242.536621\n",
      "Training Batch: 4912 Loss: 3252.370850\n",
      "Training Batch: 4913 Loss: 3223.813232\n",
      "Training Batch: 4914 Loss: 3274.622070\n",
      "Training Batch: 4915 Loss: 3331.496826\n",
      "Training Batch: 4916 Loss: 3188.843262\n",
      "Training Batch: 4917 Loss: 3235.637451\n",
      "Training Batch: 4918 Loss: 3152.034912\n",
      "Training Batch: 4919 Loss: 3132.255859\n",
      "Training Batch: 4920 Loss: 3191.163086\n",
      "Training Batch: 4921 Loss: 3294.115234\n",
      "Training Batch: 4922 Loss: 3271.724854\n",
      "Training Batch: 4923 Loss: 3206.536133\n",
      "Training Batch: 4924 Loss: 3171.893066\n",
      "Training Batch: 4925 Loss: 3284.453613\n",
      "Training Batch: 4926 Loss: 3247.133301\n",
      "Training Batch: 4927 Loss: 3176.046631\n",
      "Training Batch: 4928 Loss: 3162.632812\n",
      "Training Batch: 4929 Loss: 3193.816406\n",
      "Training Batch: 4930 Loss: 3323.663818\n",
      "Training Batch: 4931 Loss: 3348.983887\n",
      "Training Batch: 4932 Loss: 3392.060303\n",
      "Training Batch: 4933 Loss: 3303.618164\n",
      "Training Batch: 4934 Loss: 3134.039551\n",
      "Training Batch: 4935 Loss: 3400.522461\n",
      "Training Batch: 4936 Loss: 3391.778320\n",
      "Training Batch: 4937 Loss: 3122.149414\n",
      "Training Batch: 4938 Loss: 3356.035400\n",
      "Training Batch: 4939 Loss: 3249.772705\n",
      "Training Batch: 4940 Loss: 3229.651123\n",
      "Training Batch: 4941 Loss: 3217.148193\n",
      "Training Batch: 4942 Loss: 3176.719727\n",
      "Training Batch: 4943 Loss: 3252.489502\n",
      "Training Batch: 4944 Loss: 3220.617188\n",
      "Training Batch: 4945 Loss: 3635.482910\n",
      "Training Batch: 4946 Loss: 3993.432129\n",
      "Training Batch: 4947 Loss: 3491.645508\n",
      "Training Batch: 4948 Loss: 3421.232910\n",
      "Training Batch: 4949 Loss: 3505.171875\n",
      "Training Batch: 4950 Loss: 3263.092285\n",
      "Training Batch: 4951 Loss: 3282.668457\n",
      "Training Batch: 4952 Loss: 3266.029541\n",
      "Training Batch: 4953 Loss: 3218.936035\n",
      "Training Batch: 4954 Loss: 3237.872803\n",
      "Training Batch: 4955 Loss: 3211.341309\n",
      "Training Batch: 4956 Loss: 3309.180420\n",
      "Training Batch: 4957 Loss: 3277.935791\n",
      "Training Batch: 4958 Loss: 3227.142578\n",
      "Training Batch: 4959 Loss: 3272.037109\n",
      "Training Batch: 4960 Loss: 3257.690430\n",
      "Training Batch: 4961 Loss: 3136.132812\n",
      "Training Batch: 4962 Loss: 3111.684570\n",
      "Training Batch: 4963 Loss: 3459.725830\n",
      "Training Batch: 4964 Loss: 3315.979492\n",
      "Training Batch: 4965 Loss: 3332.679688\n",
      "Training Batch: 4966 Loss: 3150.915039\n",
      "Training Batch: 4967 Loss: 3145.635742\n",
      "Training Batch: 4968 Loss: 3205.802246\n",
      "Training Batch: 4969 Loss: 3149.609375\n",
      "Training Batch: 4970 Loss: 3233.528320\n",
      "Training Batch: 4971 Loss: 3150.365723\n",
      "Training Batch: 4972 Loss: 3037.350586\n",
      "Training Batch: 4973 Loss: 3157.214355\n",
      "Training Batch: 4974 Loss: 3148.926758\n",
      "Training Batch: 4975 Loss: 3271.424805\n",
      "Training Batch: 4976 Loss: 3234.001465\n",
      "Training Batch: 4977 Loss: 3214.914062\n",
      "Training Batch: 4978 Loss: 3186.649902\n",
      "Training Batch: 4979 Loss: 3292.412598\n",
      "Training Batch: 4980 Loss: 3244.201660\n",
      "Training Batch: 4981 Loss: 3337.543945\n",
      "Training Batch: 4982 Loss: 3186.818848\n",
      "Training Batch: 4983 Loss: 3183.815430\n",
      "Training Batch: 4984 Loss: 3171.919922\n",
      "Training Batch: 4985 Loss: 3541.070068\n",
      "Training Batch: 4986 Loss: 3174.187500\n",
      "Training Batch: 4987 Loss: 3531.798340\n",
      "Training Batch: 4988 Loss: 3207.480225\n",
      "Training Batch: 4989 Loss: 3284.583496\n",
      "Training Batch: 4990 Loss: 3246.131348\n",
      "Training Batch: 4991 Loss: 3289.287354\n",
      "Training Batch: 4992 Loss: 3410.954834\n",
      "Training Batch: 4993 Loss: 3460.846436\n",
      "Training Batch: 4994 Loss: 3167.732178\n",
      "Training Batch: 4995 Loss: 3334.211670\n",
      "Training Batch: 4996 Loss: 3340.649414\n",
      "Training Batch: 4997 Loss: 3190.470459\n",
      "Training Batch: 4998 Loss: 3287.822754\n",
      "Training Batch: 4999 Loss: 3327.202637\n",
      "Training Batch: 5000 Loss: 3564.327148\n",
      "Training Batch: 5001 Loss: 3493.471191\n",
      "Training Batch: 5002 Loss: 3367.746338\n",
      "Training Batch: 5003 Loss: 3127.196777\n",
      "Training Batch: 5004 Loss: 3196.841797\n",
      "Training Batch: 5005 Loss: 3333.566162\n",
      "Training Batch: 5006 Loss: 3319.921387\n",
      "Training Batch: 5007 Loss: 3336.631836\n",
      "Training Batch: 5008 Loss: 3614.728760\n",
      "Training Batch: 5009 Loss: 3526.343750\n",
      "Training Batch: 5010 Loss: 3287.238281\n",
      "Training Batch: 5011 Loss: 3204.403809\n",
      "Training Batch: 5012 Loss: 3159.925781\n",
      "Training Batch: 5013 Loss: 3392.391113\n",
      "Training Batch: 5014 Loss: 3230.318848\n",
      "Training Batch: 5015 Loss: 3214.902344\n",
      "Training Batch: 5016 Loss: 3220.271484\n",
      "Training Batch: 5017 Loss: 3317.211426\n",
      "Training Batch: 5018 Loss: 3139.565430\n",
      "Training Batch: 5019 Loss: 3231.289551\n",
      "Training Batch: 5020 Loss: 3163.399658\n",
      "Training Batch: 5021 Loss: 3364.821777\n",
      "Training Batch: 5022 Loss: 3400.562744\n",
      "Training Batch: 5023 Loss: 3136.458984\n",
      "Training Batch: 5024 Loss: 3230.024902\n",
      "Training Batch: 5025 Loss: 3272.836182\n",
      "Training Batch: 5026 Loss: 3239.410889\n",
      "Training Batch: 5027 Loss: 3216.524902\n",
      "Training Batch: 5028 Loss: 3153.901855\n",
      "Training Batch: 5029 Loss: 3261.791260\n",
      "Training Batch: 5030 Loss: 3175.711914\n",
      "Training Batch: 5031 Loss: 3216.674072\n",
      "Training Batch: 5032 Loss: 3195.650879\n",
      "Training Batch: 5033 Loss: 3143.195801\n",
      "Training Batch: 5034 Loss: 3148.448730\n",
      "Training Batch: 5035 Loss: 3261.766113\n",
      "Training Batch: 5036 Loss: 3279.083984\n",
      "Training Batch: 5037 Loss: 3153.189941\n",
      "Training Batch: 5038 Loss: 3191.353271\n",
      "Training Batch: 5039 Loss: 3202.362793\n",
      "Training Batch: 5040 Loss: 3180.448486\n",
      "Training Batch: 5041 Loss: 3215.049805\n",
      "Training Batch: 5042 Loss: 3240.625977\n",
      "Training Batch: 5043 Loss: 3436.551270\n",
      "Training Batch: 5044 Loss: 3260.427734\n",
      "Training Batch: 5045 Loss: 3258.410645\n",
      "Training Batch: 5046 Loss: 3215.451172\n",
      "Training Batch: 5047 Loss: 3280.278076\n",
      "Training Batch: 5048 Loss: 3212.574219\n",
      "Training Batch: 5049 Loss: 3200.148438\n",
      "Training Batch: 5050 Loss: 3884.481934\n",
      "Training Batch: 5051 Loss: 3248.396973\n",
      "Training Batch: 5052 Loss: 3199.951172\n",
      "Training Batch: 5053 Loss: 3213.919434\n",
      "Training Batch: 5054 Loss: 3148.433594\n",
      "Training Batch: 5055 Loss: 3187.149414\n",
      "Training Batch: 5056 Loss: 3182.790283\n",
      "Training Batch: 5057 Loss: 3345.878418\n",
      "Training Batch: 5058 Loss: 3458.243164\n",
      "Training Batch: 5059 Loss: 3275.188965\n",
      "Training Batch: 5060 Loss: 3250.723145\n",
      "Training Batch: 5061 Loss: 3143.333984\n",
      "Training Batch: 5062 Loss: 3375.781982\n",
      "Training Batch: 5063 Loss: 3250.979980\n",
      "Training Batch: 5064 Loss: 3352.717041\n",
      "Training Batch: 5065 Loss: 3134.673828\n",
      "Training Batch: 5066 Loss: 3169.450684\n",
      "Training Batch: 5067 Loss: 3129.229004\n",
      "Training Batch: 5068 Loss: 3182.586426\n",
      "Training Batch: 5069 Loss: 3155.313477\n",
      "Training Batch: 5070 Loss: 3272.181152\n",
      "Training Batch: 5071 Loss: 3282.098145\n",
      "Training Batch: 5072 Loss: 3195.065918\n",
      "Training Batch: 5073 Loss: 3219.006836\n",
      "Training Batch: 5074 Loss: 3560.314941\n",
      "Training Batch: 5075 Loss: 3431.797363\n",
      "Training Batch: 5076 Loss: 3327.780273\n",
      "Training Batch: 5077 Loss: 3301.010742\n",
      "Training Batch: 5078 Loss: 3316.955078\n",
      "Training Batch: 5079 Loss: 3184.660645\n",
      "Training Batch: 5080 Loss: 3347.402832\n",
      "Training Batch: 5081 Loss: 3228.346191\n",
      "Training Batch: 5082 Loss: 3282.857422\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Batch: 5083 Loss: 3233.147461\n",
      "Training Batch: 5084 Loss: 3273.433105\n",
      "Training Batch: 5085 Loss: 3305.007324\n",
      "Training Batch: 5086 Loss: 3437.511963\n",
      "Training Batch: 5087 Loss: 3246.830078\n",
      "Training Batch: 5088 Loss: 3337.160400\n",
      "Training Batch: 5089 Loss: 3242.109863\n",
      "Training Batch: 5090 Loss: 3263.979980\n",
      "Training Batch: 5091 Loss: 3259.963379\n",
      "Training Batch: 5092 Loss: 3266.388672\n",
      "Training Batch: 5093 Loss: 3258.247803\n",
      "Training Batch: 5094 Loss: 3115.494629\n",
      "Training Batch: 5095 Loss: 3246.837646\n",
      "Training Batch: 5096 Loss: 3412.322754\n",
      "Training Batch: 5097 Loss: 3390.056396\n",
      "Training Batch: 5098 Loss: 3362.853027\n",
      "Training Batch: 5099 Loss: 3259.757812\n",
      "Training Batch: 5100 Loss: 3190.897461\n",
      "Training Batch: 5101 Loss: 3298.971680\n",
      "Training Batch: 5102 Loss: 3214.907715\n",
      "Training Batch: 5103 Loss: 3271.643555\n",
      "Training Batch: 5104 Loss: 3085.045898\n",
      "Training Batch: 5105 Loss: 3307.953613\n",
      "Training Batch: 5106 Loss: 3155.449707\n",
      "Training Batch: 5107 Loss: 3121.919922\n",
      "Training Batch: 5108 Loss: 3168.965332\n",
      "Training Batch: 5109 Loss: 3118.572998\n",
      "Training Batch: 5110 Loss: 3276.058838\n",
      "Training Batch: 5111 Loss: 3474.622559\n",
      "Training Batch: 5112 Loss: 3159.125488\n",
      "Training Batch: 5113 Loss: 3292.010498\n",
      "Training Batch: 5114 Loss: 3259.627441\n",
      "Training Batch: 5115 Loss: 3374.020996\n",
      "Training Batch: 5116 Loss: 3541.539795\n",
      "Training Batch: 5117 Loss: 3382.053955\n",
      "Training Batch: 5118 Loss: 3106.124512\n",
      "Training Batch: 5119 Loss: 3493.845703\n",
      "Training Batch: 5120 Loss: 3348.234863\n",
      "Training Batch: 5121 Loss: 3193.607910\n",
      "Training Batch: 5122 Loss: 3360.760742\n",
      "Training Batch: 5123 Loss: 3198.144531\n",
      "Training Batch: 5124 Loss: 3165.146484\n",
      "Training Batch: 5125 Loss: 3184.135742\n",
      "Training Batch: 5126 Loss: 3288.929932\n",
      "Training Batch: 5127 Loss: 3250.048096\n",
      "Training Batch: 5128 Loss: 3192.077637\n",
      "Training Batch: 5129 Loss: 3165.014648\n",
      "Training Batch: 5130 Loss: 3210.376465\n",
      "Training Batch: 5131 Loss: 3262.661133\n",
      "Training Batch: 5132 Loss: 3293.322998\n",
      "Training Batch: 5133 Loss: 3566.728516\n",
      "Training Batch: 5134 Loss: 3305.062012\n",
      "Training Batch: 5135 Loss: 3316.469238\n",
      "Training Batch: 5136 Loss: 3193.022949\n",
      "Training Batch: 5137 Loss: 3196.445557\n",
      "Training Batch: 5138 Loss: 3133.402832\n",
      "Training Batch: 5139 Loss: 3200.806152\n",
      "Training Batch: 5140 Loss: 3219.642578\n",
      "Training Batch: 5141 Loss: 3380.510986\n",
      "Training Batch: 5142 Loss: 3201.368652\n",
      "Training Batch: 5143 Loss: 3229.867676\n",
      "Training Batch: 5144 Loss: 3457.967773\n",
      "Training Batch: 5145 Loss: 3531.830566\n",
      "Training Batch: 5146 Loss: 3407.330566\n",
      "Training Batch: 5147 Loss: 3275.279297\n",
      "Training Batch: 5148 Loss: 3358.111816\n",
      "Training Batch: 5149 Loss: 3121.078613\n",
      "Training Batch: 5150 Loss: 3200.958984\n",
      "Training Batch: 5151 Loss: 3344.871338\n",
      "Training Batch: 5152 Loss: 3406.820801\n",
      "Training Batch: 5153 Loss: 3398.409180\n",
      "Training Batch: 5154 Loss: 3247.701660\n",
      "Training Batch: 5155 Loss: 3243.822998\n",
      "Training Batch: 5156 Loss: 3570.189697\n",
      "Training Batch: 5157 Loss: 3165.345703\n",
      "Training Batch: 5158 Loss: 3171.143066\n",
      "Training Batch: 5159 Loss: 3379.464355\n",
      "Training Batch: 5160 Loss: 3294.476318\n",
      "Training Batch: 5161 Loss: 3278.842529\n",
      "Training Batch: 5162 Loss: 3335.295654\n",
      "Training Batch: 5163 Loss: 3216.280273\n",
      "Training Batch: 5164 Loss: 3310.603271\n",
      "Training Batch: 5165 Loss: 3198.114258\n",
      "Training Batch: 5166 Loss: 3168.266357\n",
      "Training Batch: 5167 Loss: 3309.274658\n",
      "Training Batch: 5168 Loss: 3291.214355\n",
      "Training Batch: 5169 Loss: 3488.602539\n",
      "Training Batch: 5170 Loss: 3404.064209\n",
      "Training Batch: 5171 Loss: 3364.375488\n",
      "Training Batch: 5172 Loss: 3224.458008\n",
      "Training Batch: 5173 Loss: 3189.234863\n",
      "Training Batch: 5174 Loss: 3162.054199\n",
      "Training Batch: 5175 Loss: 3299.552246\n",
      "Training Batch: 5176 Loss: 3172.020752\n",
      "Training Batch: 5177 Loss: 3173.613281\n",
      "Training Batch: 5178 Loss: 3337.421143\n",
      "Training Batch: 5179 Loss: 3242.659912\n",
      "Training Batch: 5180 Loss: 3221.401611\n",
      "Training Batch: 5181 Loss: 3284.465820\n",
      "Training Batch: 5182 Loss: 3266.710938\n",
      "Training Batch: 5183 Loss: 3389.166992\n",
      "Training Batch: 5184 Loss: 3220.999512\n",
      "Training Batch: 5185 Loss: 3176.711426\n",
      "Training Batch: 5186 Loss: 3155.990234\n",
      "Training Batch: 5187 Loss: 3243.240234\n",
      "Training Batch: 5188 Loss: 3155.915527\n",
      "Training Batch: 5189 Loss: 3140.915527\n",
      "Training Batch: 5190 Loss: 3336.369629\n",
      "Training Batch: 5191 Loss: 3470.420898\n",
      "Training Batch: 5192 Loss: 3259.230957\n",
      "Training Batch: 5193 Loss: 3273.373291\n",
      "Training Batch: 5194 Loss: 3415.055176\n",
      "Training Batch: 5195 Loss: 3409.770508\n",
      "Training Batch: 5196 Loss: 3258.941406\n",
      "Training Batch: 5197 Loss: 3195.154785\n",
      "Training Batch: 5198 Loss: 3312.642090\n",
      "Training Batch: 5199 Loss: 3270.901855\n",
      "Training Batch: 5200 Loss: 3490.664062\n",
      "Training Batch: 5201 Loss: 3352.452148\n",
      "Training Batch: 5202 Loss: 3375.211670\n",
      "Training Batch: 5203 Loss: 3325.341797\n",
      "Training Batch: 5204 Loss: 3382.328613\n",
      "Training Batch: 5205 Loss: 3449.558838\n",
      "Training Batch: 5206 Loss: 3404.537109\n",
      "Training Batch: 5207 Loss: 3277.641113\n",
      "Training Batch: 5208 Loss: 3137.767334\n",
      "Training Batch: 5209 Loss: 3337.856934\n",
      "Training Batch: 5210 Loss: 3383.383057\n",
      "Training Batch: 5211 Loss: 3211.119141\n",
      "Training Batch: 5212 Loss: 3200.631836\n",
      "Training Batch: 5213 Loss: 3114.529297\n",
      "Training Batch: 5214 Loss: 3212.078369\n",
      "Training Batch: 5215 Loss: 3214.835449\n",
      "Training Batch: 5216 Loss: 3149.599121\n",
      "Training Batch: 5217 Loss: 3431.218262\n",
      "Training Batch: 5218 Loss: 3207.729004\n",
      "Training Batch: 5219 Loss: 3151.398438\n",
      "Training Batch: 5220 Loss: 3328.922852\n",
      "Training Batch: 5221 Loss: 3197.599609\n",
      "Training Batch: 5222 Loss: 3152.850342\n",
      "Training Batch: 5223 Loss: 3122.645508\n",
      "Training Batch: 5224 Loss: 3103.222168\n",
      "Training Batch: 5225 Loss: 3006.925049\n",
      "Training Batch: 5226 Loss: 3514.715820\n",
      "Training Batch: 5227 Loss: 3251.047607\n",
      "Training Batch: 5228 Loss: 3248.129395\n",
      "Training Batch: 5229 Loss: 3202.770508\n",
      "Training Batch: 5230 Loss: 3212.642822\n",
      "Training Batch: 5231 Loss: 3290.187988\n",
      "Training Batch: 5232 Loss: 3227.483398\n",
      "Training Batch: 5233 Loss: 3192.041504\n",
      "Training Batch: 5234 Loss: 3315.416504\n",
      "Training Batch: 5235 Loss: 3311.791016\n",
      "Training Batch: 5236 Loss: 3339.962891\n",
      "Training Batch: 5237 Loss: 3411.037598\n",
      "Training Batch: 5238 Loss: 3277.015381\n",
      "Training Batch: 5239 Loss: 3291.306396\n",
      "Training Batch: 5240 Loss: 3222.658691\n",
      "Training Batch: 5241 Loss: 3424.100586\n",
      "Training Batch: 5242 Loss: 3367.582520\n",
      "Training Batch: 5243 Loss: 3387.276367\n",
      "Training Batch: 5244 Loss: 3500.186523\n",
      "Training Batch: 5245 Loss: 3357.571777\n",
      "Training Batch: 5246 Loss: 3291.877930\n",
      "Training Batch: 5247 Loss: 3315.156982\n",
      "Training Batch: 5248 Loss: 3232.397217\n",
      "Training Batch: 5249 Loss: 3205.639648\n",
      "Training Batch: 5250 Loss: 3211.843750\n",
      "Training Batch: 5251 Loss: 3155.473145\n",
      "Training Batch: 5252 Loss: 3181.565186\n",
      "Training Batch: 5253 Loss: 3116.142822\n",
      "Training Batch: 5254 Loss: 3189.767090\n",
      "Training Batch: 5255 Loss: 3289.987305\n",
      "Training Batch: 5256 Loss: 3275.577393\n",
      "Training Batch: 5257 Loss: 4042.102051\n",
      "Training Batch: 5258 Loss: 3508.952148\n",
      "Training Batch: 5259 Loss: 3180.354492\n",
      "Training Batch: 5260 Loss: 3269.200195\n",
      "Training Batch: 5261 Loss: 3274.200928\n",
      "Training Batch: 5262 Loss: 3321.903076\n",
      "Training Batch: 5263 Loss: 3257.297852\n",
      "Training Batch: 5264 Loss: 3090.079590\n",
      "Training Batch: 5265 Loss: 3277.701172\n",
      "Training Batch: 5266 Loss: 3343.979980\n",
      "Training Batch: 5267 Loss: 3373.229492\n",
      "Training Batch: 5268 Loss: 3073.478760\n",
      "Training Batch: 5269 Loss: 3170.610352\n",
      "Training Batch: 5270 Loss: 3204.622070\n",
      "Training Batch: 5271 Loss: 3201.925293\n",
      "Training Batch: 5272 Loss: 3115.494629\n",
      "Training Batch: 5273 Loss: 3317.030762\n",
      "Training Batch: 5274 Loss: 3230.110840\n",
      "Training Batch: 5275 Loss: 3161.275391\n",
      "Training Batch: 5276 Loss: 3098.837891\n",
      "Training Batch: 5277 Loss: 3387.635254\n",
      "Training Batch: 5278 Loss: 3231.646240\n",
      "Training Batch: 5279 Loss: 3322.016113\n",
      "Training Batch: 5280 Loss: 3465.947021\n",
      "Training Batch: 5281 Loss: 3123.885010\n",
      "Training Batch: 5282 Loss: 3253.183594\n",
      "Training Batch: 5283 Loss: 3414.996582\n",
      "Training Batch: 5284 Loss: 3262.472656\n",
      "Training Batch: 5285 Loss: 3303.914307\n",
      "Training Batch: 5286 Loss: 3255.778320\n",
      "Training Batch: 5287 Loss: 3148.075439\n",
      "Training Batch: 5288 Loss: 3244.905762\n",
      "Training Batch: 5289 Loss: 3241.398926\n",
      "Training Batch: 5290 Loss: 3318.916504\n",
      "Training Batch: 5291 Loss: 3272.239990\n",
      "Training Batch: 5292 Loss: 3357.594727\n",
      "Training Batch: 5293 Loss: 3389.561523\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Batch: 5294 Loss: 3556.309082\n",
      "Training Batch: 5295 Loss: 3303.630127\n",
      "Training Batch: 5296 Loss: 3287.716309\n",
      "Training Batch: 5297 Loss: 3184.479492\n",
      "Training Batch: 5298 Loss: 3487.084961\n",
      "Training Batch: 5299 Loss: 3525.345459\n",
      "Training Batch: 5300 Loss: 3356.384277\n",
      "Training Batch: 5301 Loss: 3371.810303\n",
      "Training Batch: 5302 Loss: 3359.633789\n",
      "Training Batch: 5303 Loss: 3234.240723\n",
      "Training Batch: 5304 Loss: 3128.660889\n",
      "Training Batch: 5305 Loss: 3349.177734\n",
      "Training Batch: 5306 Loss: 3437.168457\n",
      "Training Batch: 5307 Loss: 3229.394775\n",
      "Training Batch: 5308 Loss: 3141.984863\n",
      "Training Batch: 5309 Loss: 3211.804199\n",
      "Training Batch: 5310 Loss: 3220.183350\n",
      "Training Batch: 5311 Loss: 3260.218262\n",
      "Training Batch: 5312 Loss: 3280.517090\n",
      "Training Batch: 5313 Loss: 3368.153809\n",
      "Training Batch: 5314 Loss: 3315.259277\n",
      "Training Batch: 5315 Loss: 3360.845703\n",
      "Training Batch: 5316 Loss: 3288.723145\n",
      "Training Batch: 5317 Loss: 3280.776855\n",
      "Training Batch: 5318 Loss: 3126.593262\n",
      "Training Batch: 5319 Loss: 3235.970947\n",
      "Training Batch: 5320 Loss: 3277.313477\n",
      "Training Batch: 5321 Loss: 3267.147949\n",
      "Training Batch: 5322 Loss: 3183.976562\n",
      "Training Batch: 5323 Loss: 3350.793701\n",
      "Training Batch: 5324 Loss: 3132.575684\n",
      "Training Batch: 5325 Loss: 3206.969482\n",
      "Training Batch: 5326 Loss: 3183.798340\n",
      "Training Batch: 5327 Loss: 3221.587402\n",
      "Training Batch: 5328 Loss: 3189.005127\n",
      "Training Batch: 5329 Loss: 3228.534180\n",
      "Training Batch: 5330 Loss: 3167.469727\n",
      "Training Batch: 5331 Loss: 3213.542969\n",
      "Training Batch: 5332 Loss: 3174.897705\n",
      "Training Batch: 5333 Loss: 3264.905273\n",
      "Training Batch: 5334 Loss: 3395.427734\n",
      "Training Batch: 5335 Loss: 3190.503906\n",
      "Training Batch: 5336 Loss: 3271.153809\n",
      "Training Batch: 5337 Loss: 3419.648926\n",
      "Training Batch: 5338 Loss: 3238.280762\n",
      "Training Batch: 5339 Loss: 3382.616699\n",
      "Training Batch: 5340 Loss: 3512.866211\n",
      "Training Batch: 5341 Loss: 3315.386230\n",
      "Training Batch: 5342 Loss: 3293.282227\n",
      "Training Batch: 5343 Loss: 3184.778320\n",
      "Training Batch: 5344 Loss: 3266.317383\n",
      "Training Batch: 5345 Loss: 3269.831055\n",
      "Training Batch: 5346 Loss: 3426.751465\n",
      "Training Batch: 5347 Loss: 3281.268066\n",
      "Training Batch: 5348 Loss: 3279.886719\n",
      "Training Batch: 5349 Loss: 3303.645264\n",
      "Training Batch: 5350 Loss: 3466.305664\n",
      "Training Batch: 5351 Loss: 3366.874512\n",
      "Training Batch: 5352 Loss: 3306.609863\n",
      "Training Batch: 5353 Loss: 3229.302734\n",
      "Training Batch: 5354 Loss: 3321.251953\n",
      "Training Batch: 5355 Loss: 3357.202148\n",
      "Training Batch: 5356 Loss: 3315.167480\n",
      "Training Batch: 5357 Loss: 3419.924072\n",
      "Training Batch: 5358 Loss: 3316.567871\n",
      "Training Batch: 5359 Loss: 3354.907227\n",
      "Training Batch: 5360 Loss: 3200.751709\n",
      "Training Batch: 5361 Loss: 3518.398926\n",
      "Training Batch: 5362 Loss: 3201.349121\n",
      "Training Batch: 5363 Loss: 3441.558105\n",
      "Training Batch: 5364 Loss: 3201.725586\n",
      "Training Batch: 5365 Loss: 3311.481934\n",
      "Training Batch: 5366 Loss: 3149.081055\n",
      "Training Batch: 5367 Loss: 3251.301270\n",
      "Training Batch: 5368 Loss: 3316.512451\n",
      "Training Batch: 5369 Loss: 3222.695312\n",
      "Training Batch: 5370 Loss: 3234.729492\n",
      "Training Batch: 5371 Loss: 3089.977539\n",
      "Training Batch: 5372 Loss: 3254.265625\n",
      "Training Batch: 5373 Loss: 3418.890869\n",
      "Training Batch: 5374 Loss: 3344.419922\n",
      "Training Batch: 5375 Loss: 3389.537109\n",
      "Training Batch: 5376 Loss: 3107.133301\n",
      "Training Batch: 5377 Loss: 3211.800293\n",
      "Training Batch: 5378 Loss: 3584.438232\n",
      "Training Batch: 5379 Loss: 3351.805420\n",
      "Training Batch: 5380 Loss: 3187.035156\n",
      "Training Batch: 5381 Loss: 3191.745361\n",
      "Training Batch: 5382 Loss: 3148.631348\n",
      "Training Batch: 5383 Loss: 3551.088623\n",
      "Training Batch: 5384 Loss: 3253.140625\n",
      "Training Batch: 5385 Loss: 3310.139648\n",
      "Training Batch: 5386 Loss: 3324.573242\n",
      "Training Batch: 5387 Loss: 3220.373535\n",
      "Training Batch: 5388 Loss: 3294.662598\n",
      "Training Batch: 5389 Loss: 3289.983398\n",
      "Training Batch: 5390 Loss: 3267.867676\n",
      "Training Batch: 5391 Loss: 3207.826172\n",
      "Training Batch: 5392 Loss: 3234.944824\n",
      "Training Batch: 5393 Loss: 3214.392578\n",
      "Training Batch: 5394 Loss: 3511.789795\n",
      "Training Batch: 5395 Loss: 3190.805420\n",
      "Training Batch: 5396 Loss: 3315.833496\n",
      "Training Batch: 5397 Loss: 3506.132812\n",
      "Training Batch: 5398 Loss: 3484.897949\n",
      "Training Batch: 5399 Loss: 3245.875488\n",
      "Training Batch: 5400 Loss: 3231.667969\n",
      "Training Batch: 5401 Loss: 3259.970215\n",
      "Training Batch: 5402 Loss: 3261.771973\n",
      "Training Batch: 5403 Loss: 3269.375732\n",
      "Training Batch: 5404 Loss: 3212.949707\n",
      "Training Batch: 5405 Loss: 3343.601562\n",
      "Training Batch: 5406 Loss: 3289.988770\n",
      "Training Batch: 5407 Loss: 3221.960449\n",
      "Training Batch: 5408 Loss: 3259.043213\n",
      "Training Batch: 5409 Loss: 3314.939453\n",
      "Training Batch: 5410 Loss: 3192.139160\n",
      "Training Batch: 5411 Loss: 3276.843262\n",
      "Training Batch: 5412 Loss: 3148.228760\n",
      "Training Batch: 5413 Loss: 3179.812988\n",
      "Training Batch: 5414 Loss: 3200.604980\n",
      "Training Batch: 5415 Loss: 3200.548584\n",
      "Training Batch: 5416 Loss: 3433.432617\n",
      "Training Batch: 5417 Loss: 3225.153564\n",
      "Training Batch: 5418 Loss: 3235.651123\n",
      "Training Batch: 5419 Loss: 3267.029297\n",
      "Training Batch: 5420 Loss: 3156.893555\n",
      "Training Batch: 5421 Loss: 3254.238770\n",
      "Training Batch: 5422 Loss: 3205.528809\n",
      "Training Batch: 5423 Loss: 3227.183105\n",
      "Training Batch: 5424 Loss: 3292.670410\n",
      "Training Batch: 5425 Loss: 3188.009766\n",
      "Training Batch: 5426 Loss: 3173.385742\n",
      "Training Batch: 5427 Loss: 3109.086670\n",
      "Training Batch: 5428 Loss: 3131.096680\n",
      "Training Batch: 5429 Loss: 3267.750488\n",
      "Training Batch: 5430 Loss: 3206.590332\n",
      "Training Batch: 5431 Loss: 3161.890625\n",
      "Training Batch: 5432 Loss: 3374.252441\n",
      "Training Batch: 5433 Loss: 3168.224121\n",
      "Training Batch: 5434 Loss: 3208.787598\n",
      "Training Batch: 5435 Loss: 3267.876465\n",
      "Training Batch: 5436 Loss: 3257.682129\n",
      "Training Batch: 5437 Loss: 3170.526855\n",
      "Training Batch: 5438 Loss: 3463.652832\n",
      "Training Batch: 5439 Loss: 3303.246826\n",
      "Training Batch: 5440 Loss: 3475.710449\n",
      "Training Batch: 5441 Loss: 3253.687744\n",
      "Training Batch: 5442 Loss: 3780.904297\n",
      "Training Batch: 5443 Loss: 3564.963379\n",
      "Training Batch: 5444 Loss: 3121.674316\n",
      "Training Batch: 5445 Loss: 3518.760254\n",
      "Training Batch: 5446 Loss: 3237.935547\n",
      "Training Batch: 5447 Loss: 3201.806152\n",
      "Training Batch: 5448 Loss: 3200.026855\n",
      "Training Batch: 5449 Loss: 3125.706787\n",
      "Training Batch: 5450 Loss: 3098.521973\n",
      "Training Batch: 5451 Loss: 3232.607910\n",
      "Training Batch: 5452 Loss: 3226.771729\n",
      "Training Batch: 5453 Loss: 3268.283691\n",
      "Training Batch: 5454 Loss: 3252.589111\n",
      "Training Batch: 5455 Loss: 3222.006348\n",
      "Training Batch: 5456 Loss: 3203.259277\n",
      "Training Batch: 5457 Loss: 3120.057617\n",
      "Training Batch: 5458 Loss: 3579.098145\n",
      "Training Batch: 5459 Loss: 3399.191650\n",
      "Training Batch: 5460 Loss: 3269.636230\n",
      "Training Batch: 5461 Loss: 3142.312988\n",
      "Training Batch: 5462 Loss: 3382.371582\n",
      "Training Batch: 5463 Loss: 3254.078125\n",
      "Training Batch: 5464 Loss: 3317.876221\n",
      "Training Batch: 5465 Loss: 3434.819336\n",
      "Training Batch: 5466 Loss: 3191.736572\n",
      "Training Batch: 5467 Loss: 3112.737793\n",
      "Training Batch: 5468 Loss: 3222.536377\n",
      "Training Batch: 5469 Loss: 3256.382324\n",
      "Training Batch: 5470 Loss: 3400.056152\n",
      "Training Batch: 5471 Loss: 3590.692871\n",
      "Training Batch: 5472 Loss: 3247.575195\n",
      "Training Batch: 5473 Loss: 3200.485352\n",
      "Training Batch: 5474 Loss: 3469.245605\n",
      "Training Batch: 5475 Loss: 3251.332764\n",
      "Training Batch: 5476 Loss: 3195.237793\n",
      "Training Batch: 5477 Loss: 3229.060547\n",
      "Training Batch: 5478 Loss: 3152.347900\n",
      "Training Batch: 5479 Loss: 3178.991455\n",
      "Training Batch: 5480 Loss: 3132.384277\n",
      "Training Batch: 5481 Loss: 3385.066406\n",
      "Training Batch: 5482 Loss: 3394.414062\n",
      "Training Batch: 5483 Loss: 3308.182373\n",
      "Training Batch: 5484 Loss: 3191.971191\n",
      "Training Batch: 5485 Loss: 3337.546387\n",
      "Training Batch: 5486 Loss: 3214.016113\n",
      "Training Batch: 5487 Loss: 3280.326172\n",
      "Training Batch: 5488 Loss: 3161.842285\n",
      "Training Batch: 5489 Loss: 3197.262695\n",
      "Training Batch: 5490 Loss: 3230.101807\n",
      "Training Batch: 5491 Loss: 3132.881348\n",
      "Training Batch: 5492 Loss: 3270.095703\n",
      "Training Batch: 5493 Loss: 3571.579102\n",
      "Training Batch: 5494 Loss: 3329.418701\n",
      "Training Batch: 5495 Loss: 3189.220703\n",
      "Training Batch: 5496 Loss: 3195.349121\n",
      "Training Batch: 5497 Loss: 3205.489502\n",
      "Training Batch: 5498 Loss: 3234.360352\n",
      "Training Batch: 5499 Loss: 3214.776855\n",
      "Training Batch: 5500 Loss: 3128.810547\n",
      "Training Batch: 5501 Loss: 3477.245605\n",
      "Training Batch: 5502 Loss: 3216.094238\n",
      "Training Batch: 5503 Loss: 3214.535645\n",
      "Training Batch: 5504 Loss: 3256.285400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Batch: 5505 Loss: 3180.425781\n",
      "Training Batch: 5506 Loss: 3398.311768\n",
      "Training Batch: 5507 Loss: 3140.535645\n",
      "Training Batch: 5508 Loss: 3251.755859\n",
      "Training Batch: 5509 Loss: 3195.093262\n",
      "Training Batch: 5510 Loss: 3280.697266\n",
      "Training Batch: 5511 Loss: 3199.694336\n",
      "Training Batch: 5512 Loss: 3315.847900\n",
      "Training Batch: 5513 Loss: 3215.340820\n",
      "Training Batch: 5514 Loss: 3164.939941\n",
      "Training Batch: 5515 Loss: 3302.423584\n",
      "Training Batch: 5516 Loss: 3357.272461\n",
      "Training Batch: 5517 Loss: 3138.255371\n",
      "Training Batch: 5518 Loss: 3203.935547\n",
      "Training Batch: 5519 Loss: 3397.856445\n",
      "Training Batch: 5520 Loss: 3249.990479\n",
      "Training Batch: 5521 Loss: 3436.439453\n",
      "Training Batch: 5522 Loss: 3376.535400\n",
      "Training Batch: 5523 Loss: 3268.057373\n",
      "Training Batch: 5524 Loss: 3244.483887\n",
      "Training Batch: 5525 Loss: 3241.492676\n",
      "Training Batch: 5526 Loss: 3571.474854\n",
      "Training Batch: 5527 Loss: 3264.519043\n",
      "Training Batch: 5528 Loss: 3243.977539\n",
      "Training Batch: 5529 Loss: 3694.020020\n",
      "Training Batch: 5530 Loss: 3573.457031\n",
      "Training Batch: 5531 Loss: 3528.122070\n",
      "Training Batch: 5532 Loss: 3572.329102\n",
      "Training Batch: 5533 Loss: 3439.716309\n",
      "Training Batch: 5534 Loss: 3235.421387\n",
      "Training Batch: 5535 Loss: 3254.392822\n",
      "Training Batch: 5536 Loss: 3164.213623\n",
      "Training Batch: 5537 Loss: 3285.085449\n",
      "Training Batch: 5538 Loss: 3277.875000\n",
      "Training Batch: 5539 Loss: 3367.465820\n",
      "Training Batch: 5540 Loss: 3328.337891\n",
      "Training Batch: 5541 Loss: 3308.560547\n",
      "Training Batch: 5542 Loss: 3370.930176\n",
      "Training Batch: 5543 Loss: 3296.229980\n",
      "Training Batch: 5544 Loss: 3269.517822\n",
      "Training Batch: 5545 Loss: 3293.177979\n",
      "Training Batch: 5546 Loss: 3288.953613\n",
      "Training Batch: 5547 Loss: 3393.480957\n",
      "Training Batch: 5548 Loss: 3537.016602\n",
      "Training Batch: 5549 Loss: 3333.143555\n",
      "Training Batch: 5550 Loss: 3288.131592\n",
      "Training Batch: 5551 Loss: 3066.528076\n",
      "Training Batch: 5552 Loss: 3391.282227\n",
      "Training Batch: 5553 Loss: 3339.834961\n",
      "Training Batch: 5554 Loss: 3287.033691\n",
      "Training Batch: 5555 Loss: 3257.365723\n",
      "Training Batch: 5556 Loss: 3240.350586\n",
      "Training Batch: 5557 Loss: 3125.125977\n",
      "Training Batch: 5558 Loss: 3251.277344\n",
      "Training Batch: 5559 Loss: 3241.218262\n",
      "Training Batch: 5560 Loss: 3145.095703\n",
      "Training Batch: 5561 Loss: 3189.220703\n",
      "Training Batch: 5562 Loss: 3122.773193\n",
      "Training Batch: 5563 Loss: 3087.804199\n",
      "Training Batch: 5564 Loss: 3293.956787\n",
      "Training Batch: 5565 Loss: 3353.070557\n",
      "Training Batch: 5566 Loss: 3501.810059\n",
      "Training Batch: 5567 Loss: 3444.851562\n",
      "Training Batch: 5568 Loss: 3300.654541\n",
      "Training Batch: 5569 Loss: 3230.386719\n",
      "Training Batch: 5570 Loss: 3210.979248\n",
      "Training Batch: 5571 Loss: 3230.771973\n",
      "Training Batch: 5572 Loss: 3203.555664\n",
      "Training Batch: 5573 Loss: 3134.963623\n",
      "Training Batch: 5574 Loss: 3291.331055\n",
      "Training Batch: 5575 Loss: 3240.647949\n",
      "Training Batch: 5576 Loss: 3271.643311\n",
      "Training Batch: 5577 Loss: 3182.287109\n",
      "Training Batch: 5578 Loss: 3154.058350\n",
      "Training Batch: 5579 Loss: 3353.116455\n",
      "Training Batch: 5580 Loss: 3187.434570\n",
      "Training Batch: 5581 Loss: 3250.041992\n",
      "Training Batch: 5582 Loss: 3192.086426\n",
      "Training Batch: 5583 Loss: 3445.363770\n",
      "Training Batch: 5584 Loss: 3212.648438\n",
      "Training Batch: 5585 Loss: 3237.304688\n",
      "Training Batch: 5586 Loss: 3229.160645\n",
      "Training Batch: 5587 Loss: 3245.969482\n",
      "Training Batch: 5588 Loss: 3239.646973\n",
      "Training Batch: 5589 Loss: 3200.784180\n",
      "Training Batch: 5590 Loss: 3225.264160\n",
      "Training Batch: 5591 Loss: 3213.211426\n",
      "Training Batch: 5592 Loss: 3253.999023\n",
      "Training Batch: 5593 Loss: 3296.857422\n",
      "Training Batch: 5594 Loss: 3360.758789\n",
      "Training Batch: 5595 Loss: 3094.670654\n",
      "Training Batch: 5596 Loss: 3180.094971\n",
      "Training Batch: 5597 Loss: 3119.532227\n",
      "Training Batch: 5598 Loss: 3182.989258\n",
      "Training Batch: 5599 Loss: 3112.019531\n",
      "Training Batch: 5600 Loss: 3219.183105\n",
      "Training Batch: 5601 Loss: 3233.852051\n",
      "Training Batch: 5602 Loss: 3215.601562\n",
      "Training Batch: 5603 Loss: 3197.184326\n",
      "Training Batch: 5604 Loss: 3224.792480\n",
      "Training Batch: 5605 Loss: 3079.521484\n",
      "Training Batch: 5606 Loss: 3238.601807\n",
      "Training Batch: 5607 Loss: 3353.926514\n",
      "Training Batch: 5608 Loss: 3244.204102\n",
      "Training Batch: 5609 Loss: 3465.522949\n",
      "Training Batch: 5610 Loss: 3198.196777\n",
      "Training Batch: 5611 Loss: 3229.798828\n",
      "Training Batch: 5612 Loss: 3241.919434\n",
      "Training Batch: 5613 Loss: 3291.570312\n",
      "Training Batch: 5614 Loss: 3249.210693\n",
      "Training Batch: 5615 Loss: 3306.112793\n",
      "Training Batch: 5616 Loss: 3311.100098\n",
      "Training Batch: 5617 Loss: 3250.265625\n",
      "Training Batch: 5618 Loss: 3185.191406\n",
      "Training Batch: 5619 Loss: 3358.744141\n",
      "Training Batch: 5620 Loss: 3438.786621\n",
      "Training Batch: 5621 Loss: 3158.293945\n",
      "Training Batch: 5622 Loss: 3279.328613\n",
      "Training Batch: 5623 Loss: 3205.336914\n",
      "Training Batch: 5624 Loss: 3220.373047\n",
      "Training Batch: 5625 Loss: 3355.286133\n",
      "Training Batch: 5626 Loss: 3185.252930\n",
      "Training Batch: 5627 Loss: 3434.502197\n",
      "Training Batch: 5628 Loss: 3123.888672\n",
      "Training Batch: 5629 Loss: 3203.377441\n",
      "Training Batch: 5630 Loss: 3133.767578\n",
      "Training Batch: 5631 Loss: 3154.431885\n",
      "Training Batch: 5632 Loss: 3385.398926\n",
      "Training Batch: 5633 Loss: 3254.380859\n",
      "Training Batch: 5634 Loss: 3255.079102\n",
      "Training Batch: 5635 Loss: 3333.362793\n",
      "Training Batch: 5636 Loss: 3273.215088\n",
      "Training Batch: 5637 Loss: 3277.082031\n",
      "Training Batch: 5638 Loss: 3353.769531\n",
      "Training Batch: 5639 Loss: 3351.598633\n",
      "Training Batch: 5640 Loss: 4819.439453\n",
      "Training Batch: 5641 Loss: 3346.828857\n",
      "Training Batch: 5642 Loss: 3227.325195\n",
      "Training Batch: 5643 Loss: 3238.060547\n",
      "Training Batch: 5644 Loss: 3198.717285\n",
      "Training Batch: 5645 Loss: 3213.860352\n",
      "Training Batch: 5646 Loss: 3185.122314\n",
      "Training Batch: 5647 Loss: 3132.227539\n",
      "Training Batch: 5648 Loss: 3302.305664\n",
      "Training Batch: 5649 Loss: 3269.458008\n",
      "Training Batch: 5650 Loss: 3248.773438\n",
      "Training Batch: 5651 Loss: 3228.503418\n",
      "Training Batch: 5652 Loss: 3247.283691\n",
      "Training Batch: 5653 Loss: 3271.478271\n",
      "Training Batch: 5654 Loss: 3251.549805\n",
      "Training Batch: 5655 Loss: 3268.881836\n",
      "Training Batch: 5656 Loss: 3166.663086\n",
      "Training Batch: 5657 Loss: 3201.168457\n",
      "Training Batch: 5658 Loss: 3323.326904\n",
      "Training Batch: 5659 Loss: 3252.349609\n",
      "Training Batch: 5660 Loss: 3382.855469\n",
      "Training Batch: 5661 Loss: 3105.286621\n",
      "Training Batch: 5662 Loss: 3282.724365\n",
      "Training Batch: 5663 Loss: 3574.579590\n",
      "Training Batch: 5664 Loss: 3243.653320\n",
      "Training Batch: 5665 Loss: 3255.188477\n",
      "Training Batch: 5666 Loss: 3276.173828\n",
      "Training Batch: 5667 Loss: 3269.600098\n",
      "Training Batch: 5668 Loss: 3497.624512\n",
      "Training Batch: 5669 Loss: 3220.608154\n",
      "Training Batch: 5670 Loss: 3211.057617\n",
      "Training Batch: 5671 Loss: 3234.929688\n",
      "Training Batch: 5672 Loss: 3324.714844\n",
      "Training Batch: 5673 Loss: 3185.342285\n",
      "Training Batch: 5674 Loss: 3137.544434\n",
      "Training Batch: 5675 Loss: 3295.186523\n",
      "Training Batch: 5676 Loss: 3243.893311\n",
      "Training Batch: 5677 Loss: 3323.759766\n",
      "Training Batch: 5678 Loss: 3378.539551\n",
      "Training Batch: 5679 Loss: 3304.683838\n",
      "Training Batch: 5680 Loss: 3434.720215\n",
      "Training Batch: 5681 Loss: 3462.193848\n",
      "Training Batch: 5682 Loss: 3198.188232\n",
      "Training Batch: 5683 Loss: 3314.695801\n",
      "Training Batch: 5684 Loss: 3266.690918\n",
      "Training Batch: 5685 Loss: 3092.626953\n",
      "Training Batch: 5686 Loss: 3149.610352\n",
      "Training Batch: 5687 Loss: 3185.614258\n",
      "Training Batch: 5688 Loss: 3374.071289\n",
      "Training Batch: 5689 Loss: 3199.055176\n",
      "Training Batch: 5690 Loss: 3047.078369\n",
      "Training Batch: 5691 Loss: 3086.671387\n",
      "Training Batch: 5692 Loss: 3159.259277\n",
      "Training Batch: 5693 Loss: 3148.917725\n",
      "Training Batch: 5694 Loss: 3204.766602\n",
      "Training Batch: 5695 Loss: 3161.505371\n",
      "Training Batch: 5696 Loss: 3373.782959\n",
      "Training Batch: 5697 Loss: 3523.636230\n",
      "Training Batch: 5698 Loss: 3247.854492\n",
      "Training Batch: 5699 Loss: 3250.680664\n",
      "Training Batch: 5700 Loss: 3186.935059\n",
      "Training Batch: 5701 Loss: 3288.119141\n",
      "Training Batch: 5702 Loss: 3219.149902\n",
      "Training Batch: 5703 Loss: 3218.726074\n",
      "Training Batch: 5704 Loss: 3268.198730\n",
      "Training Batch: 5705 Loss: 3298.955811\n",
      "Training Batch: 5706 Loss: 3259.675293\n",
      "Training Batch: 5707 Loss: 3459.143066\n",
      "Training Batch: 5708 Loss: 3165.837402\n",
      "Training Batch: 5709 Loss: 3150.346924\n",
      "Training Batch: 5710 Loss: 3131.751709\n",
      "Training Batch: 5711 Loss: 3083.875000\n",
      "Training Batch: 5712 Loss: 3245.181396\n",
      "Training Batch: 5713 Loss: 3243.249268\n",
      "Training Batch: 5714 Loss: 3519.326660\n",
      "Training Batch: 5715 Loss: 3225.388184\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Batch: 5716 Loss: 3213.222656\n",
      "Training Batch: 5717 Loss: 3394.022461\n",
      "Training Batch: 5718 Loss: 3299.030762\n",
      "Training Batch: 5719 Loss: 3286.347412\n",
      "Training Batch: 5720 Loss: 3134.259277\n",
      "Training Batch: 5721 Loss: 3423.197021\n",
      "Training Batch: 5722 Loss: 3188.325684\n",
      "Training Batch: 5723 Loss: 3155.090820\n",
      "Training Batch: 5724 Loss: 3114.809082\n",
      "Training Batch: 5725 Loss: 3154.604492\n",
      "Training Batch: 5726 Loss: 3193.105713\n",
      "Training Batch: 5727 Loss: 3381.736328\n",
      "Training Batch: 5728 Loss: 3250.874268\n",
      "Training Batch: 5729 Loss: 3095.562744\n",
      "Training Batch: 5730 Loss: 3127.117920\n",
      "Training Batch: 5731 Loss: 3198.165283\n",
      "Training Batch: 5732 Loss: 3154.286133\n",
      "Training Batch: 5733 Loss: 3189.830078\n",
      "Training Batch: 5734 Loss: 3274.090820\n",
      "Training Batch: 5735 Loss: 3213.625977\n",
      "Training Batch: 5736 Loss: 3309.337402\n",
      "Training Batch: 5737 Loss: 3143.475098\n",
      "Training Batch: 5738 Loss: 3298.681641\n",
      "Training Batch: 5739 Loss: 3300.362061\n",
      "Training Batch: 5740 Loss: 3286.372803\n",
      "Training Batch: 5741 Loss: 3394.355957\n",
      "Training Batch: 5742 Loss: 3171.515625\n",
      "Training Batch: 5743 Loss: 3170.382324\n",
      "Training Batch: 5744 Loss: 3463.625000\n",
      "Training Batch: 5745 Loss: 3409.286133\n",
      "Training Batch: 5746 Loss: 3504.119629\n",
      "Training Batch: 5747 Loss: 3590.069092\n",
      "Training Batch: 5748 Loss: 3272.347900\n",
      "Training Batch: 5749 Loss: 3211.498047\n",
      "Training Batch: 5750 Loss: 3229.217285\n",
      "Training Batch: 5751 Loss: 3235.205566\n",
      "Training Batch: 5752 Loss: 3119.313477\n",
      "Training Batch: 5753 Loss: 3109.662842\n",
      "Training Batch: 5754 Loss: 3204.947754\n",
      "Training Batch: 5755 Loss: 3145.183105\n",
      "Training Batch: 5756 Loss: 3177.150146\n",
      "Training Batch: 5757 Loss: 3335.900879\n",
      "Training Batch: 5758 Loss: 3138.848389\n",
      "Training Batch: 5759 Loss: 3337.028320\n",
      "Training Batch: 5760 Loss: 3193.555176\n",
      "Training Batch: 5761 Loss: 3148.697266\n",
      "Training Batch: 5762 Loss: 3287.567627\n",
      "Training Batch: 5763 Loss: 3543.711182\n",
      "Training Batch: 5764 Loss: 3307.678711\n",
      "Training Batch: 5765 Loss: 3222.399902\n",
      "Training Batch: 5766 Loss: 3169.115479\n",
      "Training Batch: 5767 Loss: 3294.603760\n",
      "Training Batch: 5768 Loss: 3250.437500\n",
      "Training Batch: 5769 Loss: 3433.166748\n",
      "Training Batch: 5770 Loss: 3477.645508\n",
      "Training Batch: 5771 Loss: 3175.455078\n",
      "Training Batch: 5772 Loss: 3241.786621\n",
      "Training Batch: 5773 Loss: 3592.497559\n",
      "Training Batch: 5774 Loss: 3165.002441\n",
      "Training Batch: 5775 Loss: 3265.058105\n",
      "Training Batch: 5776 Loss: 3343.637939\n",
      "Training Batch: 5777 Loss: 3225.923828\n",
      "Training Batch: 5778 Loss: 3280.654541\n",
      "Training Batch: 5779 Loss: 3191.421387\n",
      "Training Batch: 5780 Loss: 3194.797363\n",
      "Training Batch: 5781 Loss: 3121.705078\n",
      "Training Batch: 5782 Loss: 3361.829834\n",
      "Training Batch: 5783 Loss: 3334.729248\n",
      "Training Batch: 5784 Loss: 3125.985840\n",
      "Training Batch: 5785 Loss: 3315.771973\n",
      "Training Batch: 5786 Loss: 3237.715820\n",
      "Training Batch: 5787 Loss: 3209.197754\n",
      "Training Batch: 5788 Loss: 3225.844238\n",
      "Training Batch: 5789 Loss: 3246.629395\n",
      "Training Batch: 5790 Loss: 3234.573486\n",
      "Training Batch: 5791 Loss: 3207.748047\n",
      "Training Batch: 5792 Loss: 3181.618652\n",
      "Training Batch: 5793 Loss: 3309.432861\n",
      "Training Batch: 5794 Loss: 3265.430908\n",
      "Training Batch: 5795 Loss: 3297.580322\n",
      "Training Batch: 5796 Loss: 3220.020508\n",
      "Training Batch: 5797 Loss: 3284.942383\n",
      "Training Batch: 5798 Loss: 3281.232910\n",
      "Training Batch: 5799 Loss: 3200.746582\n",
      "Training Batch: 5800 Loss: 3337.021484\n",
      "Training Batch: 5801 Loss: 3186.223633\n",
      "Training Batch: 5802 Loss: 3108.732910\n",
      "Training Batch: 5803 Loss: 3424.791260\n",
      "Training Batch: 5804 Loss: 3252.643799\n",
      "Training Batch: 5805 Loss: 3128.010742\n",
      "Training Batch: 5806 Loss: 3415.917236\n",
      "Training Batch: 5807 Loss: 3307.450684\n",
      "Training Batch: 5808 Loss: 3092.463379\n",
      "Training Batch: 5809 Loss: 3345.775391\n",
      "Training Batch: 5810 Loss: 3334.483643\n",
      "Training Batch: 5811 Loss: 3351.565430\n",
      "Training Batch: 5812 Loss: 3195.975098\n",
      "Training Batch: 5813 Loss: 3250.001953\n",
      "Training Batch: 5814 Loss: 3183.698242\n",
      "Training Batch: 5815 Loss: 3177.713623\n",
      "Training Batch: 5816 Loss: 3064.226562\n",
      "Training Batch: 5817 Loss: 3358.988281\n",
      "Training Batch: 5818 Loss: 3124.222656\n",
      "Training Batch: 5819 Loss: 3137.397949\n",
      "Training Batch: 5820 Loss: 3149.628906\n",
      "Training Batch: 5821 Loss: 3136.427979\n",
      "Training Batch: 5822 Loss: 3188.776367\n",
      "Training Batch: 5823 Loss: 3648.299072\n",
      "Training Batch: 5824 Loss: 3575.429688\n",
      "Training Batch: 5825 Loss: 3406.169434\n",
      "Training Batch: 5826 Loss: 3192.600586\n",
      "Training Batch: 5827 Loss: 3214.816406\n",
      "Training Batch: 5828 Loss: 3371.374512\n",
      "Training Batch: 5829 Loss: 3376.795898\n",
      "Training Batch: 5830 Loss: 3149.372803\n",
      "Training Batch: 5831 Loss: 3119.085449\n",
      "Training Batch: 5832 Loss: 3327.410156\n",
      "Training Batch: 5833 Loss: 3387.399170\n",
      "Training Batch: 5834 Loss: 3384.780273\n",
      "Training Batch: 5835 Loss: 3275.954590\n",
      "Training Batch: 5836 Loss: 3149.672607\n",
      "Training Batch: 5837 Loss: 3218.644531\n",
      "Training Batch: 5838 Loss: 3398.073486\n",
      "Training Batch: 5839 Loss: 3253.806641\n",
      "Training Batch: 5840 Loss: 3277.227051\n",
      "Training Batch: 5841 Loss: 3187.530762\n",
      "Training Batch: 5842 Loss: 3146.006592\n",
      "Training Batch: 5843 Loss: 3113.416992\n",
      "Training Batch: 5844 Loss: 3316.922363\n",
      "Training Batch: 5845 Loss: 3134.028320\n",
      "Training Batch: 5846 Loss: 3102.915771\n",
      "Training Batch: 5847 Loss: 3367.837646\n",
      "Training Batch: 5848 Loss: 3236.410156\n",
      "Training Batch: 5849 Loss: 3179.320557\n",
      "Training Batch: 5850 Loss: 3298.859131\n",
      "Training Batch: 5851 Loss: 3290.395508\n",
      "Training Batch: 5852 Loss: 3447.866943\n",
      "Training Batch: 5853 Loss: 3348.268799\n",
      "Training Batch: 5854 Loss: 3125.724365\n",
      "Training Batch: 5855 Loss: 3256.383789\n",
      "Training Batch: 5856 Loss: 3233.098145\n",
      "Training Batch: 5857 Loss: 3270.015625\n",
      "Training Batch: 5858 Loss: 3355.153320\n",
      "Training Batch: 5859 Loss: 3523.645020\n",
      "Training Batch: 5860 Loss: 3230.934570\n",
      "Training Batch: 5861 Loss: 3406.243652\n",
      "Training Batch: 5862 Loss: 3582.694336\n",
      "Training Batch: 5863 Loss: 3402.948242\n",
      "Training Batch: 5864 Loss: 3377.525879\n",
      "Training Batch: 5865 Loss: 3323.019531\n",
      "Training Batch: 5866 Loss: 3175.491699\n",
      "Training Batch: 5867 Loss: 3194.485840\n",
      "Training Batch: 5868 Loss: 3310.044434\n",
      "Training Batch: 5869 Loss: 3362.149658\n",
      "Training Batch: 5870 Loss: 3275.446289\n",
      "Training Batch: 5871 Loss: 3230.875000\n",
      "Training Batch: 5872 Loss: 3248.406738\n",
      "Training Batch: 5873 Loss: 3250.894043\n",
      "Training Batch: 5874 Loss: 3384.059570\n",
      "Training Batch: 5875 Loss: 3344.122559\n",
      "Training Batch: 5876 Loss: 3212.094727\n",
      "Training Batch: 5877 Loss: 3236.668945\n",
      "Training Batch: 5878 Loss: 3173.296631\n",
      "Training Batch: 5879 Loss: 3181.102539\n",
      "Training Batch: 5880 Loss: 3086.535156\n",
      "Training Batch: 5881 Loss: 3197.826904\n",
      "Training Batch: 5882 Loss: 3323.105713\n",
      "Training Batch: 5883 Loss: 3412.002930\n",
      "Training Batch: 5884 Loss: 3605.075195\n",
      "Training Batch: 5885 Loss: 3517.736328\n",
      "Training Batch: 5886 Loss: 3384.964844\n",
      "Training Batch: 5887 Loss: 3425.443848\n",
      "Training Batch: 5888 Loss: 3261.072266\n",
      "Training Batch: 5889 Loss: 3351.306152\n",
      "Training Batch: 5890 Loss: 3262.258545\n",
      "Training Batch: 5891 Loss: 3288.782715\n",
      "Training Batch: 5892 Loss: 3193.179688\n",
      "Training Batch: 5893 Loss: 3275.579590\n",
      "Training Batch: 5894 Loss: 3158.679688\n",
      "Training Batch: 5895 Loss: 3238.152832\n",
      "Training Batch: 5896 Loss: 3112.795898\n",
      "Training Batch: 5897 Loss: 3166.907959\n",
      "Training Batch: 5898 Loss: 3156.010254\n",
      "Training Batch: 5899 Loss: 3227.508789\n",
      "Training Batch: 5900 Loss: 3285.597168\n",
      "Training Batch: 5901 Loss: 3340.958984\n",
      "Training Batch: 5902 Loss: 3304.370850\n",
      "Training Batch: 5903 Loss: 3113.269043\n",
      "Training Batch: 5904 Loss: 3354.724121\n",
      "Training Batch: 5905 Loss: 3324.804199\n",
      "Training Batch: 5906 Loss: 3341.449219\n",
      "Training Batch: 5907 Loss: 3475.861816\n",
      "Training Batch: 5908 Loss: 3205.019531\n",
      "Training Batch: 5909 Loss: 3145.119385\n",
      "Training Batch: 5910 Loss: 3259.690918\n",
      "Training Batch: 5911 Loss: 3369.428711\n",
      "Training Batch: 5912 Loss: 3180.637207\n",
      "Training Batch: 5913 Loss: 3197.611328\n",
      "Training Batch: 5914 Loss: 3281.546631\n",
      "Training Batch: 5915 Loss: 3124.567383\n",
      "Training Batch: 5916 Loss: 3142.590820\n",
      "Training Batch: 5917 Loss: 3349.626709\n",
      "Training Batch: 5918 Loss: 3171.078613\n",
      "Training Batch: 5919 Loss: 3219.213379\n",
      "Training Batch: 5920 Loss: 3091.833740\n",
      "Training Batch: 5921 Loss: 3179.727051\n",
      "Training Batch: 5922 Loss: 3169.863770\n",
      "Training Batch: 5923 Loss: 3236.095703\n",
      "Training Batch: 5924 Loss: 3277.484863\n",
      "Training Batch: 5925 Loss: 3271.372314\n",
      "Training Batch: 5926 Loss: 3266.576904\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Batch: 5927 Loss: 3195.227051\n",
      "Training Batch: 5928 Loss: 3097.086182\n",
      "Training Batch: 5929 Loss: 3158.707275\n",
      "Training Batch: 5930 Loss: 3136.422852\n",
      "Training Batch: 5931 Loss: 3142.434082\n",
      "Training Batch: 5932 Loss: 3126.067627\n",
      "Training Batch: 5933 Loss: 3269.722656\n",
      "Training Batch: 5934 Loss: 3154.134766\n",
      "Training Batch: 5935 Loss: 3392.446777\n",
      "Training Batch: 5936 Loss: 3157.647217\n",
      "Training Batch: 5937 Loss: 3212.152344\n",
      "Training Batch: 5938 Loss: 3161.741699\n",
      "Training Batch: 5939 Loss: 3166.383301\n",
      "Training Batch: 5940 Loss: 3172.148926\n",
      "Training Batch: 5941 Loss: 3256.070312\n",
      "Training Batch: 5942 Loss: 3228.632324\n",
      "Training Batch: 5943 Loss: 3216.608887\n",
      "Training Batch: 5944 Loss: 3230.607422\n",
      "Training Batch: 5945 Loss: 3437.149414\n",
      "Training Batch: 5946 Loss: 3141.365234\n",
      "Training Batch: 5947 Loss: 3205.405762\n",
      "Training Batch: 5948 Loss: 3284.760254\n",
      "Training Batch: 5949 Loss: 3270.236816\n",
      "Training Batch: 5950 Loss: 3147.415771\n",
      "Training Batch: 5951 Loss: 3126.109863\n",
      "Training Batch: 5952 Loss: 3290.075684\n",
      "Training Batch: 5953 Loss: 3194.191406\n",
      "Training Batch: 5954 Loss: 3265.979492\n",
      "Training Batch: 5955 Loss: 3192.539551\n",
      "Training Batch: 5956 Loss: 3228.470459\n",
      "Training Batch: 5957 Loss: 3250.593262\n",
      "Training Batch: 5958 Loss: 3500.548828\n",
      "Training Batch: 5959 Loss: 3385.794678\n",
      "Training Batch: 5960 Loss: 3219.009277\n",
      "Training Batch: 5961 Loss: 3304.617676\n",
      "Training Batch: 5962 Loss: 3202.778809\n",
      "Training Batch: 5963 Loss: 3266.109863\n",
      "Training Batch: 5964 Loss: 3348.925049\n",
      "Training Batch: 5965 Loss: 3179.266113\n",
      "Training Batch: 5966 Loss: 3169.527344\n",
      "Training Batch: 5967 Loss: 3171.973389\n",
      "Training Batch: 5968 Loss: 3187.478027\n",
      "Training Batch: 5969 Loss: 3344.875488\n",
      "Training Batch: 5970 Loss: 3227.829346\n",
      "Training Batch: 5971 Loss: 3151.741211\n",
      "Training Batch: 5972 Loss: 3212.368652\n",
      "Training Batch: 5973 Loss: 3213.685059\n",
      "Training Batch: 5974 Loss: 3204.984131\n",
      "Training Batch: 5975 Loss: 3227.734375\n",
      "Training Batch: 5976 Loss: 3519.425781\n",
      "Training Batch: 5977 Loss: 3624.620117\n",
      "Training Batch: 5978 Loss: 3241.875000\n",
      "Training Batch: 5979 Loss: 3315.238770\n",
      "Training Batch: 5980 Loss: 3333.903809\n",
      "Training Batch: 5981 Loss: 3444.920898\n",
      "Training Batch: 5982 Loss: 3348.744629\n",
      "Training Batch: 5983 Loss: 3235.833984\n",
      "Training Batch: 5984 Loss: 3307.436523\n",
      "Training Batch: 5985 Loss: 3368.289551\n",
      "Training Batch: 5986 Loss: 3322.604736\n",
      "Training Batch: 5987 Loss: 3353.754639\n",
      "Training Batch: 5988 Loss: 3197.254395\n",
      "Training Batch: 5989 Loss: 3172.421387\n",
      "Training Batch: 5990 Loss: 3290.561035\n",
      "Training Batch: 5991 Loss: 3319.294189\n",
      "Training Batch: 5992 Loss: 3215.448730\n",
      "Training Batch: 5993 Loss: 3215.310059\n",
      "Training Batch: 5994 Loss: 3199.845703\n",
      "Training Batch: 5995 Loss: 3156.223145\n",
      "Training Batch: 5996 Loss: 3204.762695\n",
      "Training Batch: 5997 Loss: 3176.150879\n",
      "Training Batch: 5998 Loss: 3116.008301\n",
      "Training Batch: 5999 Loss: 3087.481934\n",
      "Training Batch: 6000 Loss: 3312.920410\n",
      "Training Batch: 6001 Loss: 3317.410889\n",
      "Training Batch: 6002 Loss: 3199.648438\n",
      "Training Batch: 6003 Loss: 3277.545410\n",
      "Training Batch: 6004 Loss: 3159.369385\n",
      "Training Batch: 6005 Loss: 3342.402832\n",
      "Training Batch: 6006 Loss: 3219.124512\n",
      "Training Batch: 6007 Loss: 3381.291504\n",
      "Training Batch: 6008 Loss: 3381.565430\n",
      "Training Batch: 6009 Loss: 3193.048340\n",
      "Training Batch: 6010 Loss: 3195.610352\n",
      "Training Batch: 6011 Loss: 3226.305664\n",
      "Training Batch: 6012 Loss: 3151.407715\n",
      "Training Batch: 6013 Loss: 3195.731445\n",
      "Training Batch: 6014 Loss: 3131.227295\n",
      "Training Batch: 6015 Loss: 3173.382812\n",
      "Training Batch: 6016 Loss: 3214.185303\n",
      "Training Batch: 6017 Loss: 3197.712891\n",
      "Training Batch: 6018 Loss: 3248.305176\n",
      "Training Batch: 6019 Loss: 3288.750000\n",
      "Training Batch: 6020 Loss: 3330.223633\n",
      "Training Batch: 6021 Loss: 3293.499023\n",
      "Training Batch: 6022 Loss: 3277.169922\n",
      "Training Batch: 6023 Loss: 3198.559082\n",
      "Training Batch: 6024 Loss: 3272.853271\n",
      "Training Batch: 6025 Loss: 3361.743164\n",
      "Training Batch: 6026 Loss: 3408.996338\n",
      "Training Batch: 6027 Loss: 3225.447754\n",
      "Training Batch: 6028 Loss: 3274.576172\n",
      "Training Batch: 6029 Loss: 3179.400391\n",
      "Training Batch: 6030 Loss: 3535.608154\n",
      "Training Batch: 6031 Loss: 3243.471680\n",
      "Training Batch: 6032 Loss: 3643.073730\n",
      "Training Batch: 6033 Loss: 3168.077148\n",
      "Training Batch: 6034 Loss: 3252.439453\n",
      "Training Batch: 6035 Loss: 3209.623291\n",
      "Training Batch: 6036 Loss: 3309.513428\n",
      "Training Batch: 6037 Loss: 3693.795898\n",
      "Training Batch: 6038 Loss: 3292.265625\n",
      "Training Batch: 6039 Loss: 3282.251465\n",
      "Training Batch: 6040 Loss: 3365.621582\n",
      "Training Batch: 6041 Loss: 3232.002930\n",
      "Training Batch: 6042 Loss: 3224.714844\n",
      "Training Batch: 6043 Loss: 3232.150879\n",
      "Training Batch: 6044 Loss: 3346.094238\n",
      "Training Batch: 6045 Loss: 3414.733154\n",
      "Training Batch: 6046 Loss: 3530.000977\n",
      "Training Batch: 6047 Loss: 3224.408691\n",
      "Training Batch: 6048 Loss: 3380.366455\n",
      "Training Batch: 6049 Loss: 3352.091797\n",
      "Training Batch: 6050 Loss: 3125.718018\n",
      "Training Batch: 6051 Loss: 3159.769043\n",
      "Training Batch: 6052 Loss: 3232.383301\n",
      "Training Batch: 6053 Loss: 3197.070312\n",
      "Training Batch: 6054 Loss: 3315.109863\n",
      "Training Batch: 6055 Loss: 3330.358887\n",
      "Training Batch: 6056 Loss: 3266.951660\n",
      "Training Batch: 6057 Loss: 3241.284180\n",
      "Training Batch: 6058 Loss: 3165.249023\n",
      "Training Batch: 6059 Loss: 3146.293945\n",
      "Training Batch: 6060 Loss: 3337.255859\n",
      "Training Batch: 6061 Loss: 3221.411133\n",
      "Training Batch: 6062 Loss: 3223.803223\n",
      "Training Batch: 6063 Loss: 3158.803223\n",
      "Training Batch: 6064 Loss: 3110.531738\n",
      "Training Batch: 6065 Loss: 3200.563965\n",
      "Training Batch: 6066 Loss: 3299.622070\n",
      "Training Batch: 6067 Loss: 3242.588867\n",
      "Training Batch: 6068 Loss: 3579.608887\n",
      "Training Batch: 6069 Loss: 3278.643555\n",
      "Training Batch: 6070 Loss: 3234.388428\n",
      "Training Batch: 6071 Loss: 3257.391113\n",
      "Training Batch: 6072 Loss: 3283.881836\n",
      "Training Batch: 6073 Loss: 3310.288818\n",
      "Training Batch: 6074 Loss: 3308.593750\n",
      "Training Batch: 6075 Loss: 3378.126465\n",
      "Training Batch: 6076 Loss: 3298.113770\n",
      "Training Batch: 6077 Loss: 3221.932129\n",
      "Training Batch: 6078 Loss: 3198.354736\n",
      "Training Batch: 6079 Loss: 3179.938477\n",
      "Training Batch: 6080 Loss: 3225.073730\n",
      "Training Batch: 6081 Loss: 3472.302734\n",
      "Training Batch: 6082 Loss: 3222.886475\n",
      "Training Batch: 6083 Loss: 3227.719238\n",
      "Training Batch: 6084 Loss: 3127.323730\n",
      "Training Batch: 6085 Loss: 3222.888672\n",
      "Training Batch: 6086 Loss: 3271.483887\n",
      "Training Batch: 6087 Loss: 3177.355957\n",
      "Training Batch: 6088 Loss: 3119.639648\n",
      "Training Batch: 6089 Loss: 3125.266357\n",
      "Training Batch: 6090 Loss: 3290.616211\n",
      "Training Batch: 6091 Loss: 3315.041504\n",
      "Training Batch: 6092 Loss: 3237.773438\n",
      "Training Batch: 6093 Loss: 3290.049561\n",
      "Training Batch: 6094 Loss: 3209.287109\n",
      "Training Batch: 6095 Loss: 3178.516602\n",
      "Training Batch: 6096 Loss: 3244.190430\n",
      "Training Batch: 6097 Loss: 3152.678467\n",
      "Training Batch: 6098 Loss: 3217.976074\n",
      "Training Batch: 6099 Loss: 3239.521240\n",
      "Training Batch: 6100 Loss: 3782.862549\n",
      "Training Batch: 6101 Loss: 3659.211914\n",
      "Training Batch: 6102 Loss: 3320.806885\n",
      "Training Batch: 6103 Loss: 3180.001465\n",
      "Training Batch: 6104 Loss: 3202.987549\n",
      "Training Batch: 6105 Loss: 3238.711426\n",
      "Training Batch: 6106 Loss: 3110.457764\n",
      "Training Batch: 6107 Loss: 3201.083496\n",
      "Training Batch: 6108 Loss: 3281.908691\n",
      "Training Batch: 6109 Loss: 3568.256104\n",
      "Training Batch: 6110 Loss: 3191.601562\n",
      "Training Batch: 6111 Loss: 3419.377686\n",
      "Training Batch: 6112 Loss: 3282.372070\n",
      "Training Batch: 6113 Loss: 3234.156738\n",
      "Training Batch: 6114 Loss: 3253.233398\n",
      "Training Batch: 6115 Loss: 3229.009766\n",
      "Training Batch: 6116 Loss: 3220.128906\n",
      "Training Batch: 6117 Loss: 3717.585938\n",
      "Training Batch: 6118 Loss: 3383.455566\n",
      "Training Batch: 6119 Loss: 3237.351807\n",
      "Training Batch: 6120 Loss: 3254.068848\n",
      "Training Batch: 6121 Loss: 3334.611816\n",
      "Training Batch: 6122 Loss: 3225.217285\n",
      "Training Batch: 6123 Loss: 3455.478027\n",
      "Training Batch: 6124 Loss: 3260.595703\n",
      "Training Batch: 6125 Loss: 3388.977539\n",
      "Training Batch: 6126 Loss: 3121.270020\n",
      "Training Batch: 6127 Loss: 3256.572754\n",
      "Training Batch: 6128 Loss: 3381.213379\n",
      "Training Batch: 6129 Loss: 3245.323975\n",
      "Training Batch: 6130 Loss: 3301.750977\n",
      "Training Batch: 6131 Loss: 3354.627441\n",
      "Training Batch: 6132 Loss: 3234.383057\n",
      "Training Batch: 6133 Loss: 3212.424072\n",
      "Training Batch: 6134 Loss: 3232.265381\n",
      "Training Batch: 6135 Loss: 3119.252930\n",
      "Training Batch: 6136 Loss: 3156.158691\n",
      "Training Batch: 6137 Loss: 3169.282227\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Batch: 6138 Loss: 3354.904785\n",
      "Training Batch: 6139 Loss: 3158.887207\n",
      "Training Batch: 6140 Loss: 3211.300537\n",
      "Training Batch: 6141 Loss: 3169.354492\n",
      "Training Batch: 6142 Loss: 3285.172119\n",
      "Training Batch: 6143 Loss: 3205.860107\n",
      "Training Batch: 6144 Loss: 3118.187988\n",
      "Training Batch: 6145 Loss: 3301.146973\n",
      "Training Batch: 6146 Loss: 3147.054199\n",
      "Training Batch: 6147 Loss: 3125.374023\n",
      "Training Batch: 6148 Loss: 3193.911377\n",
      "Training Batch: 6149 Loss: 3310.403809\n",
      "Training Batch: 6150 Loss: 3295.677002\n",
      "Training Batch: 6151 Loss: 3185.275879\n",
      "Training Batch: 6152 Loss: 3235.363281\n",
      "Training Batch: 6153 Loss: 3162.693359\n",
      "Training Batch: 6154 Loss: 3161.577637\n",
      "Training Batch: 6155 Loss: 3322.581543\n",
      "Training Batch: 6156 Loss: 3201.479004\n",
      "Training Batch: 6157 Loss: 3256.531250\n",
      "Training Batch: 6158 Loss: 3276.988037\n",
      "Training Batch: 6159 Loss: 3216.689697\n",
      "Training Batch: 6160 Loss: 3205.957764\n",
      "Training Batch: 6161 Loss: 3242.679688\n",
      "Training Batch: 6162 Loss: 3209.883301\n",
      "Training Batch: 6163 Loss: 3142.419922\n",
      "Training Batch: 6164 Loss: 3367.020020\n",
      "Training Batch: 6165 Loss: 3369.386719\n",
      "Training Batch: 6166 Loss: 3151.076660\n",
      "Training Batch: 6167 Loss: 3482.266357\n",
      "Training Batch: 6168 Loss: 3413.094727\n",
      "Training Batch: 6169 Loss: 3478.325684\n",
      "Training Batch: 6170 Loss: 3188.797852\n",
      "Training Batch: 6171 Loss: 3214.502197\n",
      "Training Batch: 6172 Loss: 3489.334229\n",
      "Training Batch: 6173 Loss: 3188.055908\n",
      "Training Batch: 6174 Loss: 3287.607910\n",
      "Training Batch: 6175 Loss: 3209.549805\n",
      "Training Batch: 6176 Loss: 3382.744141\n",
      "Training Batch: 6177 Loss: 3034.918457\n",
      "Training Batch: 6178 Loss: 3134.457520\n",
      "Training Batch: 6179 Loss: 3235.819824\n",
      "Training Batch: 6180 Loss: 3123.486816\n",
      "Training Batch: 6181 Loss: 3170.947266\n",
      "Training Batch: 6182 Loss: 3316.881836\n",
      "Training Batch: 6183 Loss: 3257.150146\n",
      "Training Batch: 6184 Loss: 3151.603516\n",
      "Training Batch: 6185 Loss: 3290.322266\n",
      "Training Batch: 6186 Loss: 3190.768311\n",
      "Training Batch: 6187 Loss: 3280.047852\n",
      "Training Batch: 6188 Loss: 3232.556641\n",
      "Training Batch: 6189 Loss: 3201.491211\n",
      "Training Batch: 6190 Loss: 3392.270508\n",
      "Training Batch: 6191 Loss: 3391.039795\n",
      "Training Batch: 6192 Loss: 3143.606689\n",
      "Training Batch: 6193 Loss: 3155.327148\n",
      "Training Batch: 6194 Loss: 3109.991211\n",
      "Training Batch: 6195 Loss: 3238.320801\n",
      "Training Batch: 6196 Loss: 3186.010742\n",
      "Training Batch: 6197 Loss: 3332.009766\n",
      "Training Batch: 6198 Loss: 3348.523438\n",
      "Training Batch: 6199 Loss: 3136.797852\n",
      "Training Batch: 6200 Loss: 3557.456055\n",
      "Training Batch: 6201 Loss: 3159.797607\n",
      "Training Batch: 6202 Loss: 3514.963379\n",
      "Training Batch: 6203 Loss: 3087.515625\n",
      "Training Batch: 6204 Loss: 3351.635498\n",
      "Training Batch: 6205 Loss: 3269.291504\n",
      "Training Batch: 6206 Loss: 3215.096191\n",
      "Training Batch: 6207 Loss: 3361.037109\n",
      "Training Batch: 6208 Loss: 3152.048828\n",
      "Training Batch: 6209 Loss: 3235.493652\n",
      "Training Batch: 6210 Loss: 3365.140869\n",
      "Training Batch: 6211 Loss: 3209.957520\n",
      "Training Batch: 6212 Loss: 3200.282227\n",
      "Training Batch: 6213 Loss: 3235.417480\n",
      "Training Batch: 6214 Loss: 3148.136719\n",
      "Training Batch: 6215 Loss: 3443.193848\n",
      "Training Batch: 6216 Loss: 3322.877441\n",
      "Training Batch: 6217 Loss: 3186.041016\n",
      "Training Batch: 6218 Loss: 3199.005371\n",
      "Training Batch: 6219 Loss: 3503.244141\n",
      "Training Batch: 6220 Loss: 3213.108398\n",
      "Training Batch: 6221 Loss: 3178.001465\n",
      "Training Batch: 6222 Loss: 3312.747559\n",
      "Training Batch: 6223 Loss: 3359.187988\n",
      "Training Batch: 6224 Loss: 3149.983398\n",
      "Training Batch: 6225 Loss: 3329.513184\n",
      "Training Batch: 6226 Loss: 3202.089111\n",
      "Training Batch: 6227 Loss: 3554.081543\n",
      "Training Batch: 6228 Loss: 3289.014160\n",
      "Training Batch: 6229 Loss: 3289.255859\n",
      "Training Batch: 6230 Loss: 3197.188477\n",
      "Training Batch: 6231 Loss: 3345.910645\n",
      "Training Batch: 6232 Loss: 3198.318848\n",
      "Training Batch: 6233 Loss: 3168.282959\n",
      "Training Batch: 6234 Loss: 3216.180664\n",
      "Training Batch: 6235 Loss: 3272.368896\n",
      "Training Batch: 6236 Loss: 3399.036621\n",
      "Training Batch: 6237 Loss: 3160.259033\n",
      "Training Batch: 6238 Loss: 3510.247314\n",
      "Training Batch: 6239 Loss: 3172.138672\n",
      "Training Batch: 6240 Loss: 3197.360840\n",
      "Training Batch: 6241 Loss: 3357.763184\n",
      "Training Batch: 6242 Loss: 3330.546875\n",
      "Training Batch: 6243 Loss: 3309.799805\n",
      "Training Batch: 6244 Loss: 3149.373047\n",
      "Training Batch: 6245 Loss: 3150.396484\n",
      "Training Batch: 6246 Loss: 3488.659180\n",
      "Training Batch: 6247 Loss: 3379.195801\n",
      "Training Batch: 6248 Loss: 3176.730469\n",
      "Training Batch: 6249 Loss: 3231.060547\n",
      "Training Batch: 6250 Loss: 3270.307129\n",
      "Training Batch: 6251 Loss: 3234.079102\n",
      "Training Batch: 6252 Loss: 3283.454590\n",
      "Training Batch: 6253 Loss: 3317.305908\n",
      "Training Batch: 6254 Loss: 3204.885742\n",
      "Training Batch: 6255 Loss: 3139.361816\n",
      "Training Batch: 6256 Loss: 3172.046143\n",
      "Training Batch: 6257 Loss: 3240.641602\n",
      "Training Batch: 6258 Loss: 3180.027832\n",
      "Training Batch: 6259 Loss: 3053.852051\n",
      "Training Batch: 6260 Loss: 3230.998535\n",
      "Training Batch: 6261 Loss: 3349.455566\n",
      "Training Batch: 6262 Loss: 3164.525879\n",
      "Training Batch: 6263 Loss: 3202.461426\n",
      "Training Batch: 6264 Loss: 3159.588379\n",
      "Training Batch: 6265 Loss: 3472.204590\n",
      "Training Batch: 6266 Loss: 3137.955566\n",
      "Training Batch: 6267 Loss: 3373.088623\n",
      "Training Batch: 6268 Loss: 3248.627930\n",
      "Training Batch: 6269 Loss: 3126.383789\n",
      "Training Batch: 6270 Loss: 3218.379883\n",
      "Training Batch: 6271 Loss: 3106.045898\n",
      "Training Batch: 6272 Loss: 3280.717041\n",
      "Training Batch: 6273 Loss: 3390.725586\n",
      "Training Batch: 6274 Loss: 3238.552246\n",
      "Training Batch: 6275 Loss: 3507.768066\n",
      "Training Batch: 6276 Loss: 3251.520020\n",
      "Training Batch: 6277 Loss: 3109.881836\n",
      "Training Batch: 6278 Loss: 3187.747314\n",
      "Training Batch: 6279 Loss: 3175.945801\n",
      "Training Batch: 6280 Loss: 3183.071777\n",
      "Training Batch: 6281 Loss: 3272.693359\n",
      "Training Batch: 6282 Loss: 3481.033691\n",
      "Training Batch: 6283 Loss: 3281.697266\n",
      "Training Batch: 6284 Loss: 3343.500488\n",
      "Training Batch: 6285 Loss: 3300.792969\n",
      "Training Batch: 6286 Loss: 3291.909424\n",
      "Training Batch: 6287 Loss: 3218.268799\n",
      "Training Batch: 6288 Loss: 3168.282227\n",
      "Training Batch: 6289 Loss: 3303.718750\n",
      "Training Batch: 6290 Loss: 3366.392090\n",
      "Training Batch: 6291 Loss: 3292.941895\n",
      "Training Batch: 6292 Loss: 3217.818848\n",
      "Training Batch: 6293 Loss: 3119.801270\n",
      "Training Batch: 6294 Loss: 3229.170410\n",
      "Training Batch: 6295 Loss: 3420.252930\n",
      "Training Batch: 6296 Loss: 3220.932617\n",
      "Training Batch: 6297 Loss: 3199.110352\n",
      "Training Batch: 6298 Loss: 3237.453125\n",
      "Training Batch: 6299 Loss: 3281.471191\n",
      "Training Batch: 6300 Loss: 3250.410156\n",
      "Training Batch: 6301 Loss: 3169.448730\n",
      "Training Batch: 6302 Loss: 3332.881348\n",
      "Training Batch: 6303 Loss: 3344.980469\n",
      "Training Batch: 6304 Loss: 3166.757812\n",
      "Training Batch: 6305 Loss: 3301.230957\n",
      "Training Batch: 6306 Loss: 3264.124756\n",
      "Training Batch: 6307 Loss: 3192.622559\n",
      "Training Batch: 6308 Loss: 3570.039795\n",
      "Training Batch: 6309 Loss: 3338.227051\n",
      "Training Batch: 6310 Loss: 3253.588135\n",
      "Training Batch: 6311 Loss: 3291.020996\n",
      "Training Batch: 6312 Loss: 3294.980957\n",
      "Training Batch: 6313 Loss: 3319.357178\n",
      "Training Batch: 6314 Loss: 3331.709473\n",
      "Training Batch: 6315 Loss: 3289.439453\n",
      "Training Batch: 6316 Loss: 3251.630371\n",
      "Training Batch: 6317 Loss: 3450.406250\n",
      "Training Batch: 6318 Loss: 3179.348877\n",
      "Training Batch: 6319 Loss: 3336.531250\n",
      "Training Batch: 6320 Loss: 3565.128418\n",
      "Training Batch: 6321 Loss: 3153.027344\n",
      "Training Batch: 6322 Loss: 3145.255371\n",
      "Training Batch: 6323 Loss: 3160.481445\n",
      "Training Batch: 6324 Loss: 3234.952148\n",
      "Training Batch: 6325 Loss: 3223.301270\n",
      "Training Batch: 6326 Loss: 3117.277100\n",
      "Training Batch: 6327 Loss: 3222.383301\n",
      "Training Batch: 6328 Loss: 3103.494629\n",
      "Training Batch: 6329 Loss: 3185.013184\n",
      "Training Batch: 6330 Loss: 3166.592285\n",
      "Training Batch: 6331 Loss: 3137.660156\n",
      "Training Batch: 6332 Loss: 3330.402832\n",
      "Training Batch: 6333 Loss: 3304.826660\n",
      "Training Batch: 6334 Loss: 3252.966797\n",
      "Training Batch: 6335 Loss: 3141.952637\n",
      "Training Batch: 6336 Loss: 3117.217041\n",
      "Training Batch: 6337 Loss: 3205.817383\n",
      "Training Batch: 6338 Loss: 3112.688477\n",
      "Training Batch: 6339 Loss: 3277.657715\n",
      "Training Batch: 6340 Loss: 3111.502441\n",
      "Training Batch: 6341 Loss: 3517.128174\n",
      "Training Batch: 6342 Loss: 3352.157715\n",
      "Training Batch: 6343 Loss: 3261.909424\n",
      "Training Batch: 6344 Loss: 3097.582764\n",
      "Training Batch: 6345 Loss: 3408.587402\n",
      "Training Batch: 6346 Loss: 3115.761963\n",
      "Training Batch: 6347 Loss: 3345.604980\n",
      "Training Batch: 6348 Loss: 3314.742920\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Batch: 6349 Loss: 3441.474609\n",
      "Training Batch: 6350 Loss: 3326.084961\n",
      "Training Batch: 6351 Loss: 3311.208496\n",
      "Training Batch: 6352 Loss: 3155.985596\n",
      "Training Batch: 6353 Loss: 3144.822754\n",
      "Training Batch: 6354 Loss: 3204.224121\n",
      "Training Batch: 6355 Loss: 3236.915527\n",
      "Training Batch: 6356 Loss: 3259.478027\n",
      "Training Batch: 6357 Loss: 3177.346680\n",
      "Training Batch: 6358 Loss: 3346.439453\n",
      "Training Batch: 6359 Loss: 3166.944092\n",
      "Training Batch: 6360 Loss: 3569.405762\n",
      "Training Batch: 6361 Loss: 3565.903809\n",
      "Training Batch: 6362 Loss: 3331.719238\n",
      "Training Batch: 6363 Loss: 3343.479248\n",
      "Training Batch: 6364 Loss: 3394.879883\n",
      "Training Batch: 6365 Loss: 3180.041016\n",
      "Training Batch: 6366 Loss: 3206.766602\n",
      "Training Batch: 6367 Loss: 3115.049805\n",
      "Training Batch: 6368 Loss: 3170.885742\n",
      "Training Batch: 6369 Loss: 3309.134766\n",
      "Training Batch: 6370 Loss: 3194.899414\n",
      "Training Batch: 6371 Loss: 3381.300781\n",
      "Training Batch: 6372 Loss: 3419.531250\n",
      "Training Batch: 6373 Loss: 3310.906250\n",
      "Training Batch: 6374 Loss: 3138.380615\n",
      "Training Batch: 6375 Loss: 3382.044434\n",
      "Training Batch: 6376 Loss: 3282.105957\n",
      "Training Batch: 6377 Loss: 3179.724609\n",
      "Training Batch: 6378 Loss: 3317.794434\n",
      "Training Batch: 6379 Loss: 3320.772949\n",
      "Training Batch: 6380 Loss: 3153.810303\n",
      "Training Batch: 6381 Loss: 3326.578125\n",
      "Training Batch: 6382 Loss: 3842.091797\n",
      "Training Batch: 6383 Loss: 3427.554688\n",
      "Training Batch: 6384 Loss: 3403.846191\n",
      "Training Batch: 6385 Loss: 3171.041504\n",
      "Training Batch: 6386 Loss: 3455.354980\n",
      "Training Batch: 6387 Loss: 3249.740723\n",
      "Training Batch: 6388 Loss: 3188.514648\n",
      "Training Batch: 6389 Loss: 3229.230469\n",
      "Training Batch: 6390 Loss: 3112.740234\n",
      "Training Batch: 6391 Loss: 3093.515137\n",
      "Training Batch: 6392 Loss: 3273.994629\n",
      "Training Batch: 6393 Loss: 3403.339355\n",
      "Training Batch: 6394 Loss: 3322.584961\n",
      "Training Batch: 6395 Loss: 3144.232422\n",
      "Training Batch: 6396 Loss: 3296.996094\n",
      "Training Batch: 6397 Loss: 3240.634766\n",
      "Training Batch: 6398 Loss: 3234.281982\n",
      "Training Batch: 6399 Loss: 3170.732910\n",
      "Training Batch: 6400 Loss: 3183.035156\n",
      "Training Batch: 6401 Loss: 3487.043945\n",
      "Training Batch: 6402 Loss: 3174.201660\n",
      "Training Batch: 6403 Loss: 3202.708740\n",
      "Training Batch: 6404 Loss: 3126.979736\n",
      "Training Batch: 6405 Loss: 3258.508301\n",
      "Training Batch: 6406 Loss: 3372.030762\n",
      "Training Batch: 6407 Loss: 3069.325439\n",
      "Training Batch: 6408 Loss: 3339.776855\n",
      "Training Batch: 6409 Loss: 3199.819824\n",
      "Training Batch: 6410 Loss: 3281.607422\n",
      "Training Batch: 6411 Loss: 3107.779297\n",
      "Training Batch: 6412 Loss: 3214.858398\n",
      "Training Batch: 6413 Loss: 3212.862305\n",
      "Training Batch: 6414 Loss: 3209.317627\n",
      "Training Batch: 6415 Loss: 3023.624023\n",
      "Training Batch: 6416 Loss: 3195.467285\n",
      "Training Batch: 6417 Loss: 3248.977539\n",
      "Training Batch: 6418 Loss: 3162.603760\n",
      "Training Batch: 6419 Loss: 3075.739990\n",
      "Training Batch: 6420 Loss: 3123.158447\n",
      "Training Batch: 6421 Loss: 3376.864258\n",
      "Training Batch: 6422 Loss: 3135.215332\n",
      "Training Batch: 6423 Loss: 3105.266113\n",
      "Training Batch: 6424 Loss: 3188.129883\n",
      "Training Batch: 6425 Loss: 3042.892334\n",
      "Training Batch: 6426 Loss: 3183.365479\n",
      "Training Batch: 6427 Loss: 3204.006348\n",
      "Training Batch: 6428 Loss: 3217.635986\n",
      "Training Batch: 6429 Loss: 3483.191650\n",
      "Training Batch: 6430 Loss: 3208.376465\n",
      "Training Batch: 6431 Loss: 3161.916504\n",
      "Training Batch: 6432 Loss: 3469.904785\n",
      "Training Batch: 6433 Loss: 3201.187012\n",
      "Training Batch: 6434 Loss: 3142.188965\n",
      "Training Batch: 6435 Loss: 3232.030762\n",
      "Training Batch: 6436 Loss: 3237.420410\n",
      "Training Batch: 6437 Loss: 3210.688477\n",
      "Training Batch: 6438 Loss: 3462.994629\n",
      "Training Batch: 6439 Loss: 3148.728760\n",
      "Training Batch: 6440 Loss: 3367.191406\n",
      "Training Batch: 6441 Loss: 3526.953369\n",
      "Training Batch: 6442 Loss: 3117.262695\n",
      "Training Batch: 6443 Loss: 3164.309082\n",
      "Training Batch: 6444 Loss: 3438.151855\n",
      "Training Batch: 6445 Loss: 3313.399414\n",
      "Training Batch: 6446 Loss: 3161.584961\n",
      "Training Batch: 6447 Loss: 3429.812500\n",
      "Training Batch: 6448 Loss: 3109.427246\n",
      "Training Batch: 6449 Loss: 3390.402344\n",
      "Training Batch: 6450 Loss: 3124.413574\n",
      "Training Batch: 6451 Loss: 3252.083008\n",
      "Training Batch: 6452 Loss: 3198.640625\n",
      "Training Batch: 6453 Loss: 3171.965332\n",
      "Training Batch: 6454 Loss: 3175.240479\n",
      "Training Batch: 6455 Loss: 3138.264648\n",
      "Training Batch: 6456 Loss: 3156.718750\n",
      "Training Batch: 6457 Loss: 3337.876221\n",
      "Training Batch: 6458 Loss: 3211.175781\n",
      "Training Batch: 6459 Loss: 3409.920898\n",
      "Training Batch: 6460 Loss: 3167.725830\n",
      "Training Batch: 6461 Loss: 3219.529053\n",
      "Training Batch: 6462 Loss: 3212.517090\n",
      "Training Batch: 6463 Loss: 3331.785645\n",
      "Training Batch: 6464 Loss: 3246.614014\n",
      "Training Batch: 6465 Loss: 3114.720703\n",
      "Training Batch: 6466 Loss: 3140.121582\n",
      "Training Batch: 6467 Loss: 3125.859375\n",
      "Training Batch: 6468 Loss: 3347.096924\n",
      "Training Batch: 6469 Loss: 3406.491455\n",
      "Training Batch: 6470 Loss: 3063.000977\n",
      "Training Batch: 6471 Loss: 3140.790527\n",
      "Training Batch: 6472 Loss: 3145.635742\n",
      "Training Batch: 6473 Loss: 3549.571045\n",
      "Training Batch: 6474 Loss: 3502.388672\n",
      "Training Batch: 6475 Loss: 3248.894531\n",
      "Training Batch: 6476 Loss: 3217.536621\n",
      "Training Batch: 6477 Loss: 3243.475586\n",
      "Training Batch: 6478 Loss: 3257.406250\n",
      "Training Batch: 6479 Loss: 3371.068848\n",
      "Training Batch: 6480 Loss: 3434.017822\n",
      "Training Batch: 6481 Loss: 3350.353027\n",
      "Training Batch: 6482 Loss: 3250.682373\n",
      "Training Batch: 6483 Loss: 3523.415039\n",
      "Training Batch: 6484 Loss: 3170.416016\n",
      "Training Batch: 6485 Loss: 3328.231934\n",
      "Training Batch: 6486 Loss: 3303.024902\n",
      "Training Batch: 6487 Loss: 3224.029297\n",
      "Training Batch: 6488 Loss: 3145.106445\n",
      "Training Batch: 6489 Loss: 3199.482178\n",
      "Training Batch: 6490 Loss: 3394.127930\n",
      "Training Batch: 6491 Loss: 3115.579590\n",
      "Training Batch: 6492 Loss: 3112.111328\n",
      "Training Batch: 6493 Loss: 3289.081787\n",
      "Training Batch: 6494 Loss: 3257.364014\n",
      "Training Batch: 6495 Loss: 3368.461182\n",
      "Training Batch: 6496 Loss: 3598.855957\n",
      "Training Batch: 6497 Loss: 3421.840332\n",
      "Training Batch: 6498 Loss: 3360.076172\n",
      "Training Batch: 6499 Loss: 3598.549316\n",
      "Training Batch: 6500 Loss: 3439.937500\n",
      "Training Batch: 6501 Loss: 3240.178711\n",
      "Training Batch: 6502 Loss: 3280.034668\n",
      "Training Batch: 6503 Loss: 3215.059082\n",
      "Training Batch: 6504 Loss: 3312.088379\n",
      "Training Batch: 6505 Loss: 3496.232910\n",
      "Training Batch: 6506 Loss: 3181.930664\n",
      "Training Batch: 6507 Loss: 3292.112793\n",
      "Training Batch: 6508 Loss: 3118.121338\n",
      "Training Batch: 6509 Loss: 3200.310547\n",
      "Training Batch: 6510 Loss: 3188.014404\n",
      "Training Batch: 6511 Loss: 3114.148193\n",
      "Training Batch: 6512 Loss: 3205.168701\n",
      "Training Batch: 6513 Loss: 3213.617188\n",
      "Training Batch: 6514 Loss: 3221.567139\n",
      "Training Batch: 6515 Loss: 3289.014160\n",
      "Training Batch: 6516 Loss: 3268.730957\n",
      "Training Batch: 6517 Loss: 3238.910156\n",
      "Training Batch: 6518 Loss: 3115.843750\n",
      "Training Batch: 6519 Loss: 3169.707520\n",
      "Training Batch: 6520 Loss: 3215.395508\n",
      "Training Batch: 6521 Loss: 3093.668213\n",
      "Training Batch: 6522 Loss: 3355.863770\n",
      "Training Batch: 6523 Loss: 3472.880371\n",
      "Training Batch: 6524 Loss: 3158.558105\n",
      "Training Batch: 6525 Loss: 3139.747070\n",
      "Training Batch: 6526 Loss: 3204.082520\n",
      "Training Batch: 6527 Loss: 3241.060547\n",
      "Training Batch: 6528 Loss: 3277.093506\n",
      "Training Batch: 6529 Loss: 3160.845703\n",
      "Training Batch: 6530 Loss: 3303.168945\n",
      "Training Batch: 6531 Loss: 3303.270508\n",
      "Training Batch: 6532 Loss: 3088.084473\n",
      "Training Batch: 6533 Loss: 3138.050293\n",
      "Training Batch: 6534 Loss: 3343.092285\n",
      "Training Batch: 6535 Loss: 3173.140625\n",
      "Training Batch: 6536 Loss: 3447.350586\n",
      "Training Batch: 6537 Loss: 3243.062500\n",
      "Training Batch: 6538 Loss: 3358.968994\n",
      "Training Batch: 6539 Loss: 3428.132568\n",
      "Training Batch: 6540 Loss: 3408.763916\n",
      "Training Batch: 6541 Loss: 3181.810547\n",
      "Training Batch: 6542 Loss: 3284.485596\n",
      "Training Batch: 6543 Loss: 3410.506836\n",
      "Training Batch: 6544 Loss: 3261.598389\n",
      "Training Batch: 6545 Loss: 3282.317871\n",
      "Training Batch: 6546 Loss: 3167.523438\n",
      "Training Batch: 6547 Loss: 3128.058105\n",
      "Training Batch: 6548 Loss: 3178.504883\n",
      "Training Batch: 6549 Loss: 3118.099121\n",
      "Training Batch: 6550 Loss: 3181.947998\n",
      "Training Batch: 6551 Loss: 3287.827881\n",
      "Training Batch: 6552 Loss: 3214.994629\n",
      "Training Batch: 6553 Loss: 3144.843506\n",
      "Training Batch: 6554 Loss: 3183.822754\n",
      "Training Batch: 6555 Loss: 3174.906250\n",
      "Training Batch: 6556 Loss: 3367.422119\n",
      "Training Batch: 6557 Loss: 3509.916992\n",
      "Training Batch: 6558 Loss: 3442.496582\n",
      "Training Batch: 6559 Loss: 3331.304932\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Batch: 6560 Loss: 3213.353760\n",
      "Training Batch: 6561 Loss: 3317.334717\n",
      "Training Batch: 6562 Loss: 3287.422119\n",
      "Training Batch: 6563 Loss: 3095.162598\n",
      "Training Batch: 6564 Loss: 3252.577393\n",
      "Training Batch: 6565 Loss: 3133.991699\n",
      "Training Batch: 6566 Loss: 3127.815430\n",
      "Training Batch: 6567 Loss: 3233.862793\n",
      "Training Batch: 6568 Loss: 3396.733398\n",
      "Training Batch: 6569 Loss: 3246.619385\n",
      "Training Batch: 6570 Loss: 3306.879883\n",
      "Training Batch: 6571 Loss: 3288.741699\n",
      "Training Batch: 6572 Loss: 3257.521484\n",
      "Training Batch: 6573 Loss: 3286.369385\n",
      "Training Batch: 6574 Loss: 3271.253906\n",
      "Training Batch: 6575 Loss: 3250.216064\n",
      "Training Batch: 6576 Loss: 3133.290527\n",
      "Training Batch: 6577 Loss: 3210.482910\n",
      "Training Batch: 6578 Loss: 3226.005859\n",
      "Training Batch: 6579 Loss: 3247.633789\n",
      "Training Batch: 6580 Loss: 3549.910645\n",
      "Training Batch: 6581 Loss: 3311.098633\n",
      "Training Batch: 6582 Loss: 3365.185547\n",
      "Training Batch: 6583 Loss: 3336.624512\n",
      "Training Batch: 6584 Loss: 3178.860840\n",
      "Training Batch: 6585 Loss: 3190.639404\n",
      "Training Batch: 6586 Loss: 3331.521973\n",
      "Training Batch: 6587 Loss: 3434.520996\n",
      "Training Batch: 6588 Loss: 3284.596191\n",
      "Training Batch: 6589 Loss: 3178.985840\n",
      "Training Batch: 6590 Loss: 3311.865234\n",
      "Training Batch: 6591 Loss: 3229.540039\n",
      "Training Batch: 6592 Loss: 3088.698486\n",
      "Training Batch: 6593 Loss: 3280.553711\n",
      "Training Batch: 6594 Loss: 3610.736328\n",
      "Training Batch: 6595 Loss: 3269.212891\n",
      "Training Batch: 6596 Loss: 3282.442871\n",
      "Training Batch: 6597 Loss: 3363.613281\n",
      "Training Batch: 6598 Loss: 3597.329590\n",
      "Training Batch: 6599 Loss: 3236.053711\n",
      "Training Batch: 6600 Loss: 3440.007568\n",
      "Training Batch: 6601 Loss: 3468.510254\n",
      "Training Batch: 6602 Loss: 3427.483887\n",
      "Training Batch: 6603 Loss: 3272.039062\n",
      "Training Batch: 6604 Loss: 3674.337158\n",
      "Training Batch: 6605 Loss: 3488.812988\n",
      "Training Batch: 6606 Loss: 3120.570312\n",
      "Training Batch: 6607 Loss: 3235.658203\n",
      "Training Batch: 6608 Loss: 3299.076172\n",
      "Training Batch: 6609 Loss: 3280.558838\n",
      "Training Batch: 6610 Loss: 3163.556641\n",
      "Training Batch: 6611 Loss: 3266.721191\n",
      "Training Batch: 6612 Loss: 3309.667236\n",
      "Training Batch: 6613 Loss: 3144.228760\n",
      "Training Batch: 6614 Loss: 3577.365234\n",
      "Training Batch: 6615 Loss: 3290.959473\n",
      "Training Batch: 6616 Loss: 3378.220703\n",
      "Training Batch: 6617 Loss: 3296.521973\n",
      "Training Batch: 6618 Loss: 3236.796631\n",
      "Training Batch: 6619 Loss: 3237.763184\n",
      "Training Batch: 6620 Loss: 3243.076904\n",
      "Training Batch: 6621 Loss: 3166.507812\n",
      "Training Batch: 6622 Loss: 3241.535645\n",
      "Training Batch: 6623 Loss: 3269.253418\n",
      "Training Batch: 6624 Loss: 3230.551758\n",
      "Training Batch: 6625 Loss: 3209.508789\n",
      "Training Batch: 6626 Loss: 3242.943848\n",
      "Training Batch: 6627 Loss: 3149.692871\n",
      "Training Batch: 6628 Loss: 3276.048828\n",
      "Training Batch: 6629 Loss: 3291.497559\n",
      "Training Batch: 6630 Loss: 3122.243652\n",
      "Training Batch: 6631 Loss: 3285.802734\n",
      "Training Batch: 6632 Loss: 3422.286621\n",
      "Training Batch: 6633 Loss: 3183.595703\n",
      "Training Batch: 6634 Loss: 3359.426758\n",
      "Training Batch: 6635 Loss: 3378.690186\n",
      "Training Batch: 6636 Loss: 3188.333984\n",
      "Training Batch: 6637 Loss: 3200.841797\n",
      "Training Batch: 6638 Loss: 3197.675781\n",
      "Training Batch: 6639 Loss: 3174.764160\n",
      "Training Batch: 6640 Loss: 3284.327637\n",
      "Training Batch: 6641 Loss: 3235.741699\n",
      "Training Batch: 6642 Loss: 3360.146240\n",
      "Training Batch: 6643 Loss: 3569.017334\n",
      "Training Batch: 6644 Loss: 3604.158691\n",
      "Training Batch: 6645 Loss: 3580.790039\n",
      "Training Batch: 6646 Loss: 3321.554688\n",
      "Training Batch: 6647 Loss: 3352.937012\n",
      "Training Batch: 6648 Loss: 3490.562744\n",
      "Training Batch: 6649 Loss: 3156.869629\n",
      "Training Batch: 6650 Loss: 3206.015869\n",
      "Training Batch: 6651 Loss: 3418.848633\n",
      "Training Batch: 6652 Loss: 3166.315674\n",
      "Training Batch: 6653 Loss: 3121.221191\n",
      "Training Batch: 6654 Loss: 3325.236328\n",
      "Training Batch: 6655 Loss: 3195.982422\n",
      "Training Batch: 6656 Loss: 3276.813965\n",
      "Training Batch: 6657 Loss: 3462.782227\n",
      "Training Batch: 6658 Loss: 3204.591553\n",
      "Training Batch: 6659 Loss: 3275.301270\n",
      "Training Batch: 6660 Loss: 3093.526367\n",
      "Training Batch: 6661 Loss: 3156.367676\n",
      "Training Batch: 6662 Loss: 3156.899902\n",
      "Training Batch: 6663 Loss: 3433.481689\n",
      "Training Batch: 6664 Loss: 3201.609375\n",
      "Training Batch: 6665 Loss: 3285.058105\n",
      "Training Batch: 6666 Loss: 3321.976562\n",
      "Training Batch: 6667 Loss: 3248.796875\n",
      "Training Batch: 6668 Loss: 3145.683105\n",
      "Training Batch: 6669 Loss: 3188.755371\n",
      "Training Batch: 6670 Loss: 3318.718262\n",
      "Training Batch: 6671 Loss: 3117.662109\n",
      "Training Batch: 6672 Loss: 3209.631104\n",
      "Training Batch: 6673 Loss: 3122.737793\n",
      "Training Batch: 6674 Loss: 3065.925293\n",
      "Training Batch: 6675 Loss: 3237.677734\n",
      "Training Batch: 6676 Loss: 3169.429199\n",
      "Training Batch: 6677 Loss: 3105.666504\n",
      "Training Batch: 6678 Loss: 6030.702148\n",
      "Training Batch: 6679 Loss: 3258.281738\n",
      "Training Batch: 6680 Loss: 3250.341309\n",
      "Training Batch: 6681 Loss: 3261.629395\n",
      "Training Batch: 6682 Loss: 3244.931396\n",
      "Training Batch: 6683 Loss: 3310.006104\n",
      "Training Batch: 6684 Loss: 3191.547363\n",
      "Training Batch: 6685 Loss: 3357.364746\n",
      "Training Batch: 6686 Loss: 3321.817871\n",
      "Training Batch: 6687 Loss: 3309.668945\n",
      "Training Batch: 6688 Loss: 3179.504395\n",
      "Training Batch: 6689 Loss: 3108.287109\n",
      "Training Batch: 6690 Loss: 3386.742676\n",
      "Training Batch: 6691 Loss: 3111.131348\n",
      "Training Batch: 6692 Loss: 3256.850098\n",
      "Training Batch: 6693 Loss: 3042.429443\n",
      "Training Batch: 6694 Loss: 3269.601318\n",
      "Training Batch: 6695 Loss: 3203.408691\n",
      "Training Batch: 6696 Loss: 3288.938721\n",
      "Training Batch: 6697 Loss: 3262.948730\n",
      "Training Batch: 6698 Loss: 3245.591309\n",
      "Training Batch: 6699 Loss: 3218.532227\n",
      "Training Batch: 6700 Loss: 3171.223145\n",
      "Training Batch: 6701 Loss: 3316.329346\n",
      "Training Batch: 6702 Loss: 3221.266602\n",
      "Training Batch: 6703 Loss: 3140.573486\n",
      "Training Batch: 6704 Loss: 3144.225830\n",
      "Training Batch: 6705 Loss: 3119.869629\n",
      "Training Batch: 6706 Loss: 3176.795654\n",
      "Training Batch: 6707 Loss: 3334.439453\n",
      "Training Batch: 6708 Loss: 3157.298096\n",
      "Training Batch: 6709 Loss: 3217.113525\n",
      "Training Batch: 6710 Loss: 3205.044189\n",
      "Training Batch: 6711 Loss: 3347.476318\n",
      "Training Batch: 6712 Loss: 3144.783203\n",
      "Training Batch: 6713 Loss: 3223.956543\n",
      "Training Batch: 6714 Loss: 3191.209473\n",
      "Training Batch: 6715 Loss: 3315.875488\n",
      "Training Batch: 6716 Loss: 3210.890137\n",
      "Training Batch: 6717 Loss: 3238.465332\n",
      "Training Batch: 6718 Loss: 3205.699951\n",
      "Training Batch: 6719 Loss: 3188.824219\n",
      "Training Batch: 6720 Loss: 3234.137207\n",
      "Training Batch: 6721 Loss: 3413.630859\n",
      "Training Batch: 6722 Loss: 3226.026367\n",
      "Training Batch: 6723 Loss: 3328.240234\n",
      "Training Batch: 6724 Loss: 3432.783691\n",
      "Training Batch: 6725 Loss: 3214.421875\n",
      "Training Batch: 6726 Loss: 3195.525879\n",
      "Training Batch: 6727 Loss: 3319.007568\n",
      "Training Batch: 6728 Loss: 3170.008789\n",
      "Training Batch: 6729 Loss: 3316.572754\n",
      "Training Batch: 6730 Loss: 3194.512207\n",
      "Training Batch: 6731 Loss: 3109.918457\n",
      "Training Batch: 6732 Loss: 3077.317871\n",
      "Training Batch: 6733 Loss: 3099.510254\n",
      "Training Batch: 6734 Loss: 3295.812012\n",
      "Training Batch: 6735 Loss: 3230.704590\n",
      "Training Batch: 6736 Loss: 3200.924805\n",
      "Training Batch: 6737 Loss: 3281.939453\n",
      "Training Batch: 6738 Loss: 3498.139893\n",
      "Training Batch: 6739 Loss: 3387.494873\n",
      "Training Batch: 6740 Loss: 3348.759766\n",
      "Training Batch: 6741 Loss: 3443.440430\n",
      "Training Batch: 6742 Loss: 3351.898926\n",
      "Training Batch: 6743 Loss: 3148.841309\n",
      "Training Batch: 6744 Loss: 3436.836914\n",
      "Training Batch: 6745 Loss: 3410.840332\n",
      "Training Batch: 6746 Loss: 3520.554688\n",
      "Training Batch: 6747 Loss: 3327.257080\n",
      "Training Batch: 6748 Loss: 3326.006104\n",
      "Training Batch: 6749 Loss: 3308.689453\n",
      "Training Batch: 6750 Loss: 3221.944580\n",
      "Training Batch: 6751 Loss: 3126.270508\n",
      "Training Batch: 6752 Loss: 3711.931641\n",
      "Training Batch: 6753 Loss: 3622.089600\n",
      "Training Batch: 6754 Loss: 3481.065430\n",
      "Training Batch: 6755 Loss: 3433.043457\n",
      "Training Batch: 6756 Loss: 3564.564941\n",
      "Training Batch: 6757 Loss: 3813.526367\n",
      "Training Batch: 6758 Loss: 4413.362305\n",
      "Training Batch: 6759 Loss: 4636.142578\n",
      "Training Batch: 6760 Loss: 3642.670654\n",
      "Training Batch: 6761 Loss: 3769.378418\n",
      "Training Batch: 6762 Loss: 3579.572266\n",
      "Training Batch: 6763 Loss: 3354.763428\n",
      "Training Batch: 6764 Loss: 3549.118652\n",
      "Training Batch: 6765 Loss: 3431.931152\n",
      "Training Batch: 6766 Loss: 3113.825439\n",
      "Training Batch: 6767 Loss: 3180.378662\n",
      "Training Batch: 6768 Loss: 3188.329102\n",
      "Training Batch: 6769 Loss: 3368.354492\n",
      "Training Batch: 6770 Loss: 3208.504883\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Batch: 6771 Loss: 3276.010254\n",
      "Training Batch: 6772 Loss: 3179.729248\n",
      "Training Batch: 6773 Loss: 3614.512695\n",
      "Training Batch: 6774 Loss: 3514.501709\n",
      "Training Batch: 6775 Loss: 3227.933838\n",
      "Training Batch: 6776 Loss: 3258.394531\n",
      "Training Batch: 6777 Loss: 3183.733887\n",
      "Training Batch: 6778 Loss: 3348.983887\n",
      "Training Batch: 6779 Loss: 3249.067871\n",
      "Training Batch: 6780 Loss: 3226.356201\n",
      "Training Batch: 6781 Loss: 3220.528076\n",
      "Training Batch: 6782 Loss: 3215.276367\n",
      "Training Batch: 6783 Loss: 3284.932129\n",
      "Training Batch: 6784 Loss: 3201.754883\n",
      "Training Batch: 6785 Loss: 3417.625977\n",
      "Training Batch: 6786 Loss: 3229.788574\n",
      "Training Batch: 6787 Loss: 3394.694824\n",
      "Training Batch: 6788 Loss: 3377.047852\n",
      "Training Batch: 6789 Loss: 3156.612793\n",
      "Training Batch: 6790 Loss: 3114.320312\n",
      "Training Batch: 6791 Loss: 3143.836914\n",
      "Training Batch: 6792 Loss: 3331.862793\n",
      "Training Batch: 6793 Loss: 3205.753662\n",
      "Training Batch: 6794 Loss: 3248.209961\n",
      "Training Batch: 6795 Loss: 3226.445068\n",
      "Training Batch: 6796 Loss: 3113.397461\n",
      "Training Batch: 6797 Loss: 3198.725586\n",
      "Training Batch: 6798 Loss: 3168.278809\n",
      "Training Batch: 6799 Loss: 3076.124023\n",
      "Training Batch: 6800 Loss: 3173.331543\n",
      "Training Batch: 6801 Loss: 3111.524170\n",
      "Training Batch: 6802 Loss: 3391.152832\n",
      "Training Batch: 6803 Loss: 3220.484375\n",
      "Training Batch: 6804 Loss: 3200.641113\n",
      "Training Batch: 6805 Loss: 3272.543945\n",
      "Training Batch: 6806 Loss: 3239.154297\n",
      "Training Batch: 6807 Loss: 3318.226562\n",
      "Training Batch: 6808 Loss: 3113.108887\n",
      "Training Batch: 6809 Loss: 3308.715820\n",
      "Training Batch: 6810 Loss: 3153.128418\n",
      "Training Batch: 6811 Loss: 3108.106445\n",
      "Training Batch: 6812 Loss: 3252.573730\n",
      "Training Batch: 6813 Loss: 3351.787109\n",
      "Training Batch: 6814 Loss: 3245.485352\n",
      "Training Batch: 6815 Loss: 3393.509277\n",
      "Training Batch: 6816 Loss: 3316.521484\n",
      "Training Batch: 6817 Loss: 3366.065430\n",
      "Training Batch: 6818 Loss: 3361.067383\n",
      "Training Batch: 6819 Loss: 3135.597656\n",
      "Training Batch: 6820 Loss: 3221.574707\n",
      "Training Batch: 6821 Loss: 3248.485596\n",
      "Training Batch: 6822 Loss: 3251.193359\n",
      "Training Batch: 6823 Loss: 3274.150391\n",
      "Training Batch: 6824 Loss: 3288.263916\n",
      "Training Batch: 6825 Loss: 3172.574219\n",
      "Training Batch: 6826 Loss: 3177.660156\n",
      "Training Batch: 6827 Loss: 3254.961182\n",
      "Training Batch: 6828 Loss: 3460.529297\n",
      "Training Batch: 6829 Loss: 3404.052246\n",
      "Training Batch: 6830 Loss: 3204.213379\n",
      "Training Batch: 6831 Loss: 3220.657715\n",
      "Training Batch: 6832 Loss: 3184.788818\n",
      "Training Batch: 6833 Loss: 3245.541992\n",
      "Training Batch: 6834 Loss: 3239.326172\n",
      "Training Batch: 6835 Loss: 3277.280762\n",
      "Training Batch: 6836 Loss: 3138.339844\n",
      "Training Batch: 6837 Loss: 3210.385254\n",
      "Training Batch: 6838 Loss: 3164.233398\n",
      "Training Batch: 6839 Loss: 3325.327393\n",
      "Training Batch: 6840 Loss: 3291.967773\n",
      "Training Batch: 6841 Loss: 3304.269043\n",
      "Training Batch: 6842 Loss: 3297.678955\n",
      "Training Batch: 6843 Loss: 3362.262207\n",
      "Training Batch: 6844 Loss: 3169.048096\n",
      "Training Batch: 6845 Loss: 3310.920898\n",
      "Training Batch: 6846 Loss: 3209.849121\n",
      "Training Batch: 6847 Loss: 3247.447754\n",
      "Training Batch: 6848 Loss: 3196.128906\n",
      "Training Batch: 6849 Loss: 3123.508789\n",
      "Training Batch: 6850 Loss: 3093.201172\n",
      "Training Batch: 6851 Loss: 3137.029541\n",
      "Training Batch: 6852 Loss: 3225.454102\n",
      "Training Batch: 6853 Loss: 3166.350098\n",
      "Training Batch: 6854 Loss: 3068.639160\n",
      "Training Batch: 6855 Loss: 3208.149902\n",
      "Training Batch: 6856 Loss: 3184.711426\n",
      "Training Batch: 6857 Loss: 3568.511475\n",
      "Training Batch: 6858 Loss: 3226.912354\n",
      "Training Batch: 6859 Loss: 3231.225586\n",
      "Training Batch: 6860 Loss: 3111.083740\n",
      "Training Batch: 6861 Loss: 3275.249023\n",
      "Training Batch: 6862 Loss: 3162.059326\n",
      "Training Batch: 6863 Loss: 3284.435059\n",
      "Training Batch: 6864 Loss: 3222.252441\n",
      "Training Batch: 6865 Loss: 3218.700684\n",
      "Training Batch: 6866 Loss: 3154.337402\n",
      "Training Batch: 6867 Loss: 3270.267090\n",
      "Training Batch: 6868 Loss: 3210.009766\n",
      "Training Batch: 6869 Loss: 3445.240234\n",
      "Training Batch: 6870 Loss: 3169.194580\n",
      "Training Batch: 6871 Loss: 3074.757080\n",
      "Training Batch: 6872 Loss: 3111.693848\n",
      "Training Batch: 6873 Loss: 3212.031982\n",
      "Training Batch: 6874 Loss: 3520.392578\n",
      "Training Batch: 6875 Loss: 3244.821289\n",
      "Training Batch: 6876 Loss: 3414.410156\n",
      "Training Batch: 6877 Loss: 3136.002686\n",
      "Training Batch: 6878 Loss: 3249.155273\n",
      "Training Batch: 6879 Loss: 3514.992676\n",
      "Training Batch: 6880 Loss: 3259.281738\n",
      "Training Batch: 6881 Loss: 3181.582764\n",
      "Training Batch: 6882 Loss: 3157.115723\n",
      "Training Batch: 6883 Loss: 3331.391602\n",
      "Training Batch: 6884 Loss: 3291.291016\n",
      "Training Batch: 6885 Loss: 3125.285400\n",
      "Training Batch: 6886 Loss: 3271.421875\n",
      "Training Batch: 6887 Loss: 3216.586426\n",
      "Training Batch: 6888 Loss: 3189.127930\n",
      "Training Batch: 6889 Loss: 3297.314209\n",
      "Training Batch: 6890 Loss: 3181.999512\n",
      "Training Batch: 6891 Loss: 3242.983398\n",
      "Training Batch: 6892 Loss: 3175.054688\n",
      "Training Batch: 6893 Loss: 3152.205811\n",
      "Training Batch: 6894 Loss: 3119.149170\n",
      "Training Batch: 6895 Loss: 3324.544434\n",
      "Training Batch: 6896 Loss: 3264.538574\n",
      "Training Batch: 6897 Loss: 3488.341797\n",
      "Training Batch: 6898 Loss: 3413.242188\n",
      "Training Batch: 6899 Loss: 3494.224121\n",
      "Training Batch: 6900 Loss: 3294.930664\n",
      "Training Batch: 6901 Loss: 3255.887207\n",
      "Training Batch: 6902 Loss: 3176.484863\n",
      "Training Batch: 6903 Loss: 3613.529297\n",
      "Training Batch: 6904 Loss: 3551.062012\n",
      "Training Batch: 6905 Loss: 3250.000000\n",
      "Training Batch: 6906 Loss: 3214.863281\n",
      "Training Batch: 6907 Loss: 3283.681152\n",
      "Training Batch: 6908 Loss: 3120.455322\n",
      "Training Batch: 6909 Loss: 3298.222656\n",
      "Training Batch: 6910 Loss: 3116.464355\n",
      "Training Batch: 6911 Loss: 3311.184570\n",
      "Training Batch: 6912 Loss: 3230.768555\n",
      "Training Batch: 6913 Loss: 3315.555664\n",
      "Training Batch: 6914 Loss: 3212.270264\n",
      "Training Batch: 6915 Loss: 3331.191406\n",
      "Training Batch: 6916 Loss: 3383.905762\n",
      "Training Batch: 6917 Loss: 3174.849609\n",
      "Training Batch: 6918 Loss: 3294.044434\n",
      "Training Batch: 6919 Loss: 3222.152344\n",
      "Training Batch: 6920 Loss: 3484.245117\n",
      "Training Batch: 6921 Loss: 3193.366699\n",
      "Training Batch: 6922 Loss: 3141.053223\n",
      "Training Batch: 6923 Loss: 3174.945312\n",
      "Training Batch: 6924 Loss: 3281.425049\n",
      "Training Batch: 6925 Loss: 3317.537354\n",
      "Training Batch: 6926 Loss: 3354.593262\n",
      "Training Batch: 6927 Loss: 3274.053711\n",
      "Training Batch: 6928 Loss: 3310.249023\n",
      "Training Batch: 6929 Loss: 3180.734619\n",
      "Training Batch: 6930 Loss: 3364.560547\n",
      "Training Batch: 6931 Loss: 3364.276855\n",
      "Training Batch: 6932 Loss: 3392.198730\n",
      "Training Batch: 6933 Loss: 3243.699707\n",
      "Training Batch: 6934 Loss: 3291.742676\n",
      "Training Batch: 6935 Loss: 3323.943359\n",
      "Training Batch: 6936 Loss: 3369.158691\n",
      "Training Batch: 6937 Loss: 3146.866943\n",
      "Training Batch: 6938 Loss: 3160.551025\n",
      "Training Batch: 6939 Loss: 3293.884766\n",
      "Training Batch: 6940 Loss: 3152.738770\n",
      "Training Batch: 6941 Loss: 3180.760742\n",
      "Training Batch: 6942 Loss: 3100.331055\n",
      "Training Batch: 6943 Loss: 3152.246338\n",
      "Training Batch: 6944 Loss: 3110.753906\n",
      "Training Batch: 6945 Loss: 3163.704590\n",
      "Training Batch: 6946 Loss: 3166.246094\n",
      "Training Batch: 6947 Loss: 3466.142578\n",
      "Training Batch: 6948 Loss: 3447.224609\n",
      "Training Batch: 6949 Loss: 3169.619629\n",
      "Training Batch: 6950 Loss: 3256.174316\n",
      "Training Batch: 6951 Loss: 3373.735352\n",
      "Training Batch: 6952 Loss: 3468.723633\n",
      "Training Batch: 6953 Loss: 3106.288818\n",
      "Training Batch: 6954 Loss: 3251.237793\n",
      "Training Batch: 6955 Loss: 3171.168701\n",
      "Training Batch: 6956 Loss: 3330.025391\n",
      "Training Batch: 6957 Loss: 3148.713867\n",
      "Training Batch: 6958 Loss: 3392.047363\n",
      "Training Batch: 6959 Loss: 3153.210938\n",
      "Training Batch: 6960 Loss: 3251.786377\n",
      "Training Batch: 6961 Loss: 3224.826904\n",
      "Training Batch: 6962 Loss: 3558.088379\n",
      "Training Batch: 6963 Loss: 3221.040771\n",
      "Training Batch: 6964 Loss: 3268.981445\n",
      "Training Batch: 6965 Loss: 3162.859863\n",
      "Training Batch: 6966 Loss: 3175.974609\n",
      "Training Batch: 6967 Loss: 3158.331543\n",
      "Training Batch: 6968 Loss: 3149.687012\n",
      "Training Batch: 6969 Loss: 3374.144043\n",
      "Training Batch: 6970 Loss: 3171.006836\n",
      "Training Batch: 6971 Loss: 3378.146973\n",
      "Training Batch: 6972 Loss: 3225.235352\n",
      "Training Batch: 6973 Loss: 3312.463623\n",
      "Training Batch: 6974 Loss: 3257.971680\n",
      "Training Batch: 6975 Loss: 3299.048828\n",
      "Training Batch: 6976 Loss: 3178.825195\n",
      "Training Batch: 6977 Loss: 3185.542480\n",
      "Training Batch: 6978 Loss: 3177.813232\n",
      "Training Batch: 6979 Loss: 3163.236328\n",
      "Training Batch: 6980 Loss: 3069.292480\n",
      "Training Batch: 6981 Loss: 3212.645020\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Batch: 6982 Loss: 3292.327393\n",
      "Training Batch: 6983 Loss: 3241.426270\n",
      "Training Batch: 6984 Loss: 3073.345703\n",
      "Training Batch: 6985 Loss: 3169.483154\n",
      "Training Batch: 6986 Loss: 3094.927246\n",
      "Training Batch: 6987 Loss: 3278.487793\n",
      "Training Batch: 6988 Loss: 3197.759521\n",
      "Training Batch: 6989 Loss: 3141.641602\n",
      "Training Batch: 6990 Loss: 3127.057861\n",
      "Training Batch: 6991 Loss: 3260.072510\n",
      "Training Batch: 6992 Loss: 3220.749023\n",
      "Training Batch: 6993 Loss: 3374.066162\n",
      "Training Batch: 6994 Loss: 3168.567139\n",
      "Training Batch: 6995 Loss: 3232.168945\n",
      "Training Batch: 6996 Loss: 3238.646973\n",
      "Training Batch: 6997 Loss: 3080.976562\n",
      "Training Batch: 6998 Loss: 3274.223389\n",
      "Training Batch: 6999 Loss: 3168.201660\n",
      "Training Batch: 7000 Loss: 3207.800049\n",
      "Training Batch: 7001 Loss: 3132.887695\n",
      "Training Batch: 7002 Loss: 3142.121582\n",
      "Training Batch: 7003 Loss: 3313.010742\n",
      "Training Batch: 7004 Loss: 3155.231201\n",
      "Training Batch: 7005 Loss: 3214.929199\n",
      "Training Batch: 7006 Loss: 3723.967285\n",
      "Training Batch: 7007 Loss: 3229.945801\n",
      "Training Batch: 7008 Loss: 3426.110840\n",
      "Training Batch: 7009 Loss: 3299.608887\n",
      "Training Batch: 7010 Loss: 3106.081543\n",
      "Training Batch: 7011 Loss: 3100.129395\n",
      "Training Batch: 7012 Loss: 3122.649170\n",
      "Training Batch: 7013 Loss: 3107.627197\n",
      "Training Batch: 7014 Loss: 3203.301270\n",
      "Training Batch: 7015 Loss: 3103.477051\n",
      "Training Batch: 7016 Loss: 3258.268555\n",
      "Training Batch: 7017 Loss: 3205.834961\n",
      "Training Batch: 7018 Loss: 3159.666504\n",
      "Training Batch: 7019 Loss: 3209.582520\n",
      "Training Batch: 7020 Loss: 3148.370605\n",
      "Training Batch: 7021 Loss: 3176.377441\n",
      "Training Batch: 7022 Loss: 3245.463379\n",
      "Training Batch: 7023 Loss: 3164.076172\n",
      "Training Batch: 7024 Loss: 3516.376465\n",
      "Training Batch: 7025 Loss: 3387.417480\n",
      "Training Batch: 7026 Loss: 3318.488281\n",
      "Training Batch: 7027 Loss: 3288.551758\n",
      "Training Batch: 7028 Loss: 3325.001221\n",
      "Training Batch: 7029 Loss: 3136.596191\n",
      "Training Batch: 7030 Loss: 3132.102539\n",
      "Training Batch: 7031 Loss: 3023.234863\n",
      "Training Batch: 7032 Loss: 3147.374268\n",
      "Training Batch: 7033 Loss: 3146.260010\n",
      "Training Batch: 7034 Loss: 3282.177246\n",
      "Training Batch: 7035 Loss: 3410.296387\n",
      "Training Batch: 7036 Loss: 3410.803711\n",
      "Training Batch: 7037 Loss: 3229.460938\n",
      "Training Batch: 7038 Loss: 3147.667480\n",
      "Training Batch: 7039 Loss: 3237.953613\n",
      "Training Batch: 7040 Loss: 3323.237793\n",
      "Training Batch: 7041 Loss: 3169.133789\n",
      "Training Batch: 7042 Loss: 3182.845215\n",
      "Training Batch: 7043 Loss: 3136.340820\n",
      "Training Batch: 7044 Loss: 3206.734375\n",
      "Training Batch: 7045 Loss: 3221.102051\n",
      "Training Batch: 7046 Loss: 3397.130859\n",
      "Training Batch: 7047 Loss: 3378.776855\n",
      "Training Batch: 7048 Loss: 3314.718750\n",
      "Training Batch: 7049 Loss: 3187.552734\n",
      "Training Batch: 7050 Loss: 3312.335938\n",
      "Training Batch: 7051 Loss: 3195.294189\n",
      "Training Batch: 7052 Loss: 3248.829102\n",
      "Training Batch: 7053 Loss: 3284.637695\n",
      "Training Batch: 7054 Loss: 3117.098389\n",
      "Training Batch: 7055 Loss: 3072.341797\n",
      "Training Batch: 7056 Loss: 3255.404785\n",
      "Training Batch: 7057 Loss: 3346.586670\n",
      "Training Batch: 7058 Loss: 3304.530762\n",
      "Training Batch: 7059 Loss: 3411.715332\n",
      "Training Batch: 7060 Loss: 3666.712158\n",
      "Training Batch: 7061 Loss: 3241.600586\n",
      "Training Batch: 7062 Loss: 3238.215820\n",
      "Training Batch: 7063 Loss: 3359.085693\n",
      "Training Batch: 7064 Loss: 3166.918945\n",
      "Training Batch: 7065 Loss: 3110.781250\n",
      "Training Batch: 7066 Loss: 3268.335205\n",
      "Training Batch: 7067 Loss: 3703.887207\n",
      "Training Batch: 7068 Loss: 3308.897461\n",
      "Training Batch: 7069 Loss: 3217.147461\n",
      "Training Batch: 7070 Loss: 3262.865723\n",
      "Training Batch: 7071 Loss: 3256.541748\n",
      "Training Batch: 7072 Loss: 3212.594727\n",
      "Training Batch: 7073 Loss: 3155.196289\n",
      "Training Batch: 7074 Loss: 3277.464355\n",
      "Training Batch: 7075 Loss: 3212.104492\n",
      "Training Batch: 7076 Loss: 3310.750977\n",
      "Training Batch: 7077 Loss: 3356.920410\n",
      "Training Batch: 7078 Loss: 3152.133057\n",
      "Training Batch: 7079 Loss: 3219.002686\n",
      "Training Batch: 7080 Loss: 3174.398682\n",
      "Training Batch: 7081 Loss: 3424.673340\n",
      "Training Batch: 7082 Loss: 3191.273926\n",
      "Training Batch: 7083 Loss: 3170.819824\n",
      "Training Batch: 7084 Loss: 3210.514893\n",
      "Training Batch: 7085 Loss: 3191.666992\n",
      "Training Batch: 7086 Loss: 3241.294434\n",
      "Training Batch: 7087 Loss: 3237.476074\n",
      "Training Batch: 7088 Loss: 3092.145996\n",
      "Training Batch: 7089 Loss: 3257.236816\n",
      "Training Batch: 7090 Loss: 3256.313721\n",
      "Training Batch: 7091 Loss: 3245.138428\n",
      "Training Batch: 7092 Loss: 3355.759033\n",
      "Training Batch: 7093 Loss: 3138.883301\n",
      "Training Batch: 7094 Loss: 3293.687988\n",
      "Training Batch: 7095 Loss: 3158.233398\n",
      "Training Batch: 7096 Loss: 3248.949707\n",
      "Training Batch: 7097 Loss: 3169.918457\n",
      "Training Batch: 7098 Loss: 3234.039062\n",
      "Training Batch: 7099 Loss: 3211.341309\n",
      "Training Batch: 7100 Loss: 3180.391113\n",
      "Training Batch: 7101 Loss: 3334.200684\n",
      "Training Batch: 7102 Loss: 3237.790039\n",
      "Training Batch: 7103 Loss: 3270.502441\n",
      "Training Batch: 7104 Loss: 3279.584717\n",
      "Training Batch: 7105 Loss: 3203.582031\n",
      "Training Batch: 7106 Loss: 3238.663574\n",
      "Training Batch: 7107 Loss: 3498.767090\n",
      "Training Batch: 7108 Loss: 3283.609131\n",
      "Training Batch: 7109 Loss: 3334.997559\n",
      "Training Batch: 7110 Loss: 3365.567383\n",
      "Training Batch: 7111 Loss: 3310.330566\n",
      "Training Batch: 7112 Loss: 3446.905762\n",
      "Training Batch: 7113 Loss: 3340.127930\n",
      "Training Batch: 7114 Loss: 3223.580078\n",
      "Training Batch: 7115 Loss: 3204.110352\n",
      "Training Batch: 7116 Loss: 3249.627441\n",
      "Training Batch: 7117 Loss: 3296.467285\n",
      "Training Batch: 7118 Loss: 3227.816406\n",
      "Training Batch: 7119 Loss: 3239.488037\n",
      "Training Batch: 7120 Loss: 3325.062500\n",
      "Training Batch: 7121 Loss: 3198.385986\n",
      "Training Batch: 7122 Loss: 3129.262695\n",
      "Training Batch: 7123 Loss: 3146.396484\n",
      "Training Batch: 7124 Loss: 3280.845215\n",
      "Training Batch: 7125 Loss: 3299.971680\n",
      "Training Batch: 7126 Loss: 3144.055176\n",
      "Training Batch: 7127 Loss: 3413.272705\n",
      "Training Batch: 7128 Loss: 3168.401855\n",
      "Training Batch: 7129 Loss: 3147.996582\n",
      "Training Batch: 7130 Loss: 3390.031738\n",
      "Training Batch: 7131 Loss: 3229.544434\n",
      "Training Batch: 7132 Loss: 3224.479492\n",
      "Training Batch: 7133 Loss: 3254.456543\n",
      "Training Batch: 7134 Loss: 3302.996094\n",
      "Training Batch: 7135 Loss: 3247.330811\n",
      "Training Batch: 7136 Loss: 3132.831299\n",
      "Training Batch: 7137 Loss: 3145.723633\n",
      "Training Batch: 7138 Loss: 3111.777832\n",
      "Training Batch: 7139 Loss: 3192.054688\n",
      "Training Batch: 7140 Loss: 3334.976318\n",
      "Training Batch: 7141 Loss: 3370.374023\n",
      "Training Batch: 7142 Loss: 3291.751465\n",
      "Training Batch: 7143 Loss: 3197.710205\n",
      "Training Batch: 7144 Loss: 3304.816895\n",
      "Training Batch: 7145 Loss: 3455.394043\n",
      "Training Batch: 7146 Loss: 3182.460449\n",
      "Training Batch: 7147 Loss: 3305.813965\n",
      "Training Batch: 7148 Loss: 3287.567871\n",
      "Training Batch: 7149 Loss: 3180.231445\n",
      "Training Batch: 7150 Loss: 3141.291748\n",
      "Training Batch: 7151 Loss: 3153.283691\n",
      "Training Batch: 7152 Loss: 3223.005371\n",
      "Training Batch: 7153 Loss: 3235.774902\n",
      "Training Batch: 7154 Loss: 3165.396729\n",
      "Training Batch: 7155 Loss: 3301.628418\n",
      "Training Batch: 7156 Loss: 3122.952148\n",
      "Training Batch: 7157 Loss: 3122.424561\n",
      "Training Batch: 7158 Loss: 3242.709961\n",
      "Training Batch: 7159 Loss: 3186.220703\n",
      "Training Batch: 7160 Loss: 3072.058594\n",
      "Training Batch: 7161 Loss: 3247.592285\n",
      "Training Batch: 7162 Loss: 3105.503418\n",
      "Training Batch: 7163 Loss: 3127.976074\n",
      "Training Batch: 7164 Loss: 3242.852539\n",
      "Training Batch: 7165 Loss: 3235.627930\n",
      "Training Batch: 7166 Loss: 3149.358398\n",
      "Training Batch: 7167 Loss: 3201.685059\n",
      "Training Batch: 7168 Loss: 3297.666992\n",
      "Training Batch: 7169 Loss: 3263.833008\n",
      "Training Batch: 7170 Loss: 3278.956543\n",
      "Training Batch: 7171 Loss: 3181.572998\n",
      "Training Batch: 7172 Loss: 3146.101562\n",
      "Training Batch: 7173 Loss: 3104.681641\n",
      "Training Batch: 7174 Loss: 3120.877930\n",
      "Training Batch: 7175 Loss: 3080.923340\n",
      "Training Batch: 7176 Loss: 3352.000000\n",
      "Training Batch: 7177 Loss: 3218.564453\n",
      "Training Batch: 7178 Loss: 3190.289307\n",
      "Training Batch: 7179 Loss: 3247.904785\n",
      "Training Batch: 7180 Loss: 3179.477051\n",
      "Training Batch: 7181 Loss: 3087.065430\n",
      "Training Batch: 7182 Loss: 3143.598633\n",
      "Training Batch: 7183 Loss: 3204.730469\n",
      "Training Batch: 7184 Loss: 3329.939453\n",
      "Training Batch: 7185 Loss: 3254.359375\n",
      "Training Batch: 7186 Loss: 3165.081543\n",
      "Training Batch: 7187 Loss: 3268.098633\n",
      "Training Batch: 7188 Loss: 3249.185303\n",
      "Training Batch: 7189 Loss: 3197.344238\n",
      "Training Batch: 7190 Loss: 3268.947266\n",
      "Training Batch: 7191 Loss: 3107.118164\n",
      "Training Batch: 7192 Loss: 3150.518066\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Batch: 7193 Loss: 3210.979980\n",
      "Training Batch: 7194 Loss: 3218.166016\n",
      "Training Batch: 7195 Loss: 3332.957031\n",
      "Training Batch: 7196 Loss: 3359.831543\n",
      "Training Batch: 7197 Loss: 3556.723145\n",
      "Training Batch: 7198 Loss: 3166.592285\n",
      "Training Batch: 7199 Loss: 3246.455811\n",
      "Training Batch: 7200 Loss: 3260.348877\n",
      "Training Batch: 7201 Loss: 3194.931641\n",
      "Training Batch: 7202 Loss: 3201.429688\n",
      "Training Batch: 7203 Loss: 3120.544922\n",
      "Training Batch: 7204 Loss: 3427.397461\n",
      "Training Batch: 7205 Loss: 3420.225586\n",
      "Training Batch: 7206 Loss: 3194.600098\n",
      "Training Batch: 7207 Loss: 3224.382324\n",
      "Training Batch: 7208 Loss: 3237.063965\n",
      "Training Batch: 7209 Loss: 3110.162598\n",
      "Training Batch: 7210 Loss: 3161.561523\n",
      "Training Batch: 7211 Loss: 3226.269531\n",
      "Training Batch: 7212 Loss: 3248.241943\n",
      "Training Batch: 7213 Loss: 3296.914551\n",
      "Training Batch: 7214 Loss: 3527.633789\n",
      "Training Batch: 7215 Loss: 3228.502930\n",
      "Training Batch: 7216 Loss: 3246.174561\n",
      "Training Batch: 7217 Loss: 3282.733643\n",
      "Training Batch: 7218 Loss: 3251.933105\n",
      "Training Batch: 7219 Loss: 3122.020508\n",
      "Training Batch: 7220 Loss: 3130.106201\n",
      "Training Batch: 7221 Loss: 3158.629883\n",
      "Training Batch: 7222 Loss: 3285.586670\n",
      "Training Batch: 7223 Loss: 3148.893066\n",
      "Training Batch: 7224 Loss: 3197.827637\n",
      "Training Batch: 7225 Loss: 3239.982422\n",
      "Training Batch: 7226 Loss: 3151.434570\n",
      "Training Batch: 7227 Loss: 3432.736816\n",
      "Training Batch: 7228 Loss: 3238.971680\n",
      "Training Batch: 7229 Loss: 3340.184570\n",
      "Training Batch: 7230 Loss: 3459.200928\n",
      "Training Batch: 7231 Loss: 3308.404297\n",
      "Training Batch: 7232 Loss: 3235.175293\n",
      "Training Batch: 7233 Loss: 3229.311523\n",
      "Training Batch: 7234 Loss: 3385.449707\n",
      "Training Batch: 7235 Loss: 3254.424316\n",
      "Training Batch: 7236 Loss: 3203.507812\n",
      "Training Batch: 7237 Loss: 3748.707520\n",
      "Training Batch: 7238 Loss: 3274.612793\n",
      "Training Batch: 7239 Loss: 3485.213135\n",
      "Training Batch: 7240 Loss: 3229.498535\n",
      "Training Batch: 7241 Loss: 3224.009766\n",
      "Training Batch: 7242 Loss: 3138.416748\n",
      "Training Batch: 7243 Loss: 3060.317383\n",
      "Training Batch: 7244 Loss: 3304.864014\n",
      "Training Batch: 7245 Loss: 3665.992676\n",
      "Training Batch: 7246 Loss: 3415.935547\n",
      "Training Batch: 7247 Loss: 3128.188477\n",
      "Training Batch: 7248 Loss: 3108.452637\n",
      "Training Batch: 7249 Loss: 3224.991699\n",
      "Training Batch: 7250 Loss: 3295.264648\n",
      "Training Batch: 7251 Loss: 3504.251953\n",
      "Training Batch: 7252 Loss: 3624.853760\n",
      "Training Batch: 7253 Loss: 3274.054199\n",
      "Training Batch: 7254 Loss: 3248.544922\n",
      "Training Batch: 7255 Loss: 3267.692383\n",
      "Training Batch: 7256 Loss: 3334.633545\n",
      "Training Batch: 7257 Loss: 3233.075195\n",
      "Training Batch: 7258 Loss: 3390.147461\n",
      "Training Batch: 7259 Loss: 3379.776611\n",
      "Training Batch: 7260 Loss: 3295.252930\n",
      "Training Batch: 7261 Loss: 3287.539551\n",
      "Training Batch: 7262 Loss: 3501.388428\n",
      "Training Batch: 7263 Loss: 3282.858887\n",
      "Training Batch: 7264 Loss: 3266.362061\n",
      "Training Batch: 7265 Loss: 3330.429199\n",
      "Training Batch: 7266 Loss: 3106.014404\n",
      "Training Batch: 7267 Loss: 3208.586182\n",
      "Training Batch: 7268 Loss: 3383.364258\n",
      "Training Batch: 7269 Loss: 3452.333008\n",
      "Training Batch: 7270 Loss: 3283.776855\n",
      "Training Batch: 7271 Loss: 3421.335938\n",
      "Training Batch: 7272 Loss: 3283.645508\n",
      "Training Batch: 7273 Loss: 3669.790283\n",
      "Training Batch: 7274 Loss: 3844.270020\n",
      "Training Batch: 7275 Loss: 3435.036377\n",
      "Training Batch: 7276 Loss: 3144.865234\n",
      "Training Batch: 7277 Loss: 3410.203613\n",
      "Training Batch: 7278 Loss: 3221.846436\n",
      "Training Batch: 7279 Loss: 3095.198242\n",
      "Training Batch: 7280 Loss: 3389.206543\n",
      "Training Batch: 7281 Loss: 3137.267822\n",
      "Training Batch: 7282 Loss: 3272.490723\n",
      "Training Batch: 7283 Loss: 3149.090820\n",
      "Training Batch: 7284 Loss: 3446.365723\n",
      "Training Batch: 7285 Loss: 3313.854492\n",
      "Training Batch: 7286 Loss: 3233.790283\n",
      "Training Batch: 7287 Loss: 3083.788818\n",
      "Training Batch: 7288 Loss: 3151.414062\n",
      "Training Batch: 7289 Loss: 3242.895996\n",
      "Training Batch: 7290 Loss: 3121.904541\n",
      "Training Batch: 7291 Loss: 3299.732910\n",
      "Training Batch: 7292 Loss: 3154.729492\n",
      "Training Batch: 7293 Loss: 3241.840820\n",
      "Training Batch: 7294 Loss: 3349.724609\n",
      "Training Batch: 7295 Loss: 3283.645020\n",
      "Training Batch: 7296 Loss: 3255.478271\n",
      "Training Batch: 7297 Loss: 3233.918945\n",
      "Training Batch: 7298 Loss: 3160.385498\n",
      "Training Batch: 7299 Loss: 3149.420654\n",
      "Training Batch: 7300 Loss: 3109.287109\n",
      "Training Batch: 7301 Loss: 3258.296387\n",
      "Training Batch: 7302 Loss: 3112.875244\n",
      "Training Batch: 7303 Loss: 3105.336914\n",
      "Training Batch: 7304 Loss: 3349.212402\n",
      "Training Batch: 7305 Loss: 3143.108887\n",
      "Training Batch: 7306 Loss: 3147.794434\n",
      "Training Batch: 7307 Loss: 3111.190430\n",
      "Training Batch: 7308 Loss: 3110.285156\n",
      "Training Batch: 7309 Loss: 3056.394531\n",
      "Training Batch: 7310 Loss: 3067.533691\n",
      "Training Batch: 7311 Loss: 3150.258301\n",
      "Training Batch: 7312 Loss: 3170.434326\n",
      "Training Batch: 7313 Loss: 3133.426758\n",
      "Training Batch: 7314 Loss: 3279.201904\n",
      "Training Batch: 7315 Loss: 3110.326904\n",
      "Training Batch: 7316 Loss: 3254.842285\n",
      "Training Batch: 7317 Loss: 3232.510986\n",
      "Training Batch: 7318 Loss: 3201.130371\n",
      "Training Batch: 7319 Loss: 3192.767090\n",
      "Training Batch: 7320 Loss: 3083.640625\n",
      "Training Batch: 7321 Loss: 3361.992188\n",
      "Training Batch: 7322 Loss: 3211.974609\n",
      "Training Batch: 7323 Loss: 3150.632568\n",
      "Training Batch: 7324 Loss: 3011.631104\n",
      "Training Batch: 7325 Loss: 3175.561768\n",
      "Training Batch: 7326 Loss: 3210.824951\n",
      "Training Batch: 7327 Loss: 3270.643066\n",
      "Training Batch: 7328 Loss: 3140.160645\n",
      "Training Batch: 7329 Loss: 3166.973877\n",
      "Training Batch: 7330 Loss: 3276.912109\n",
      "Training Batch: 7331 Loss: 3118.129639\n",
      "Training Batch: 7332 Loss: 3077.350098\n",
      "Training Batch: 7333 Loss: 3343.406738\n",
      "Training Batch: 7334 Loss: 3267.221191\n",
      "Training Batch: 7335 Loss: 3281.066406\n",
      "Training Batch: 7336 Loss: 3524.529297\n",
      "Training Batch: 7337 Loss: 3294.257812\n",
      "Training Batch: 7338 Loss: 3250.950684\n",
      "Training Batch: 7339 Loss: 3109.414062\n",
      "Training Batch: 7340 Loss: 3131.830566\n",
      "Training Batch: 7341 Loss: 3351.516357\n",
      "Training Batch: 7342 Loss: 3265.013672\n",
      "Training Batch: 7343 Loss: 3209.721191\n",
      "Training Batch: 7344 Loss: 3560.356934\n",
      "Training Batch: 7345 Loss: 3334.291992\n",
      "Training Batch: 7346 Loss: 3244.830078\n",
      "Training Batch: 7347 Loss: 3312.795898\n",
      "Training Batch: 7348 Loss: 3219.198242\n",
      "Training Batch: 7349 Loss: 3181.742676\n",
      "Training Batch: 7350 Loss: 3445.125977\n",
      "Training Batch: 7351 Loss: 3128.526855\n",
      "Training Batch: 7352 Loss: 3124.163086\n",
      "Training Batch: 7353 Loss: 3170.228271\n",
      "Training Batch: 7354 Loss: 3113.966797\n",
      "Training Batch: 7355 Loss: 3114.072754\n",
      "Training Batch: 7356 Loss: 3098.602051\n",
      "Training Batch: 7357 Loss: 3232.995117\n",
      "Training Batch: 7358 Loss: 3169.787109\n",
      "Training Batch: 7359 Loss: 3373.817383\n",
      "Training Batch: 7360 Loss: 3271.212891\n",
      "Training Batch: 7361 Loss: 3273.736084\n",
      "Training Batch: 7362 Loss: 3236.094727\n",
      "Training Batch: 7363 Loss: 3215.935547\n",
      "Training Batch: 7364 Loss: 3193.645264\n",
      "Training Batch: 7365 Loss: 3128.704590\n",
      "Training Batch: 7366 Loss: 3198.973145\n",
      "Training Batch: 7367 Loss: 3292.759277\n",
      "Training Batch: 7368 Loss: 3291.235840\n",
      "Training Batch: 7369 Loss: 3548.048828\n",
      "Training Batch: 7370 Loss: 3253.624512\n",
      "Training Batch: 7371 Loss: 3285.688721\n",
      "Training Batch: 7372 Loss: 3503.618652\n",
      "Training Batch: 7373 Loss: 3141.194092\n",
      "Training Batch: 7374 Loss: 3165.708496\n",
      "Training Batch: 7375 Loss: 3222.712402\n",
      "Training Batch: 7376 Loss: 3387.847656\n",
      "Training Batch: 7377 Loss: 3213.288574\n",
      "Training Batch: 7378 Loss: 3201.144287\n",
      "Training Batch: 7379 Loss: 3228.563721\n",
      "Training Batch: 7380 Loss: 3268.508789\n",
      "Training Batch: 7381 Loss: 3246.729004\n",
      "Training Batch: 7382 Loss: 3344.883789\n",
      "Training Batch: 7383 Loss: 3498.825195\n",
      "Training Batch: 7384 Loss: 3228.029785\n",
      "Training Batch: 7385 Loss: 3242.937012\n",
      "Training Batch: 7386 Loss: 3221.873291\n",
      "Training Batch: 7387 Loss: 3268.251953\n",
      "Training Batch: 7388 Loss: 3218.238281\n",
      "Training Batch: 7389 Loss: 3407.299072\n",
      "Training Batch: 7390 Loss: 3178.425781\n",
      "Training Batch: 7391 Loss: 3451.530762\n",
      "Training Batch: 7392 Loss: 3343.456055\n",
      "Training Batch: 7393 Loss: 3242.146973\n",
      "Training Batch: 7394 Loss: 3166.015137\n",
      "Training Batch: 7395 Loss: 3461.442627\n",
      "Training Batch: 7396 Loss: 3448.382324\n",
      "Training Batch: 7397 Loss: 3274.958984\n",
      "Training Batch: 7398 Loss: 3101.172852\n",
      "Training Batch: 7399 Loss: 3262.040771\n",
      "Training Batch: 7400 Loss: 3162.977539\n",
      "Training Batch: 7401 Loss: 3294.774170\n",
      "Training Batch: 7402 Loss: 3196.066895\n",
      "Training Batch: 7403 Loss: 3107.214111\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Batch: 7404 Loss: 3280.252197\n",
      "Training Batch: 7405 Loss: 3143.701660\n",
      "Training Batch: 7406 Loss: 3268.532227\n",
      "Training Batch: 7407 Loss: 3170.514648\n",
      "Training Batch: 7408 Loss: 3251.382324\n",
      "Training Batch: 7409 Loss: 3338.433838\n",
      "Training Batch: 7410 Loss: 3202.535400\n",
      "Training Batch: 7411 Loss: 3269.578369\n",
      "Training Batch: 7412 Loss: 3353.807129\n",
      "Training Batch: 7413 Loss: 3158.341797\n",
      "Training Batch: 7414 Loss: 3143.744629\n",
      "Training Batch: 7415 Loss: 3339.766846\n",
      "Training Batch: 7416 Loss: 3226.580322\n",
      "Training Batch: 7417 Loss: 3144.107422\n",
      "Training Batch: 7418 Loss: 3183.371094\n",
      "Training Batch: 7419 Loss: 3266.916504\n",
      "Training Batch: 7420 Loss: 3210.026367\n",
      "Training Batch: 7421 Loss: 3123.235840\n",
      "Training Batch: 7422 Loss: 3276.324463\n",
      "Training Batch: 7423 Loss: 3114.131836\n",
      "Training Batch: 7424 Loss: 3322.420654\n",
      "Training Batch: 7425 Loss: 3145.494141\n",
      "Training Batch: 7426 Loss: 3165.197754\n",
      "Training Batch: 7427 Loss: 3176.834229\n",
      "Training Batch: 7428 Loss: 3151.309814\n",
      "Training Batch: 7429 Loss: 3170.739746\n",
      "Training Batch: 7430 Loss: 3231.368408\n",
      "Training Batch: 7431 Loss: 3175.430176\n",
      "Training Batch: 7432 Loss: 3184.362793\n",
      "Training Batch: 7433 Loss: 3174.495361\n",
      "Training Batch: 7434 Loss: 3199.417480\n",
      "Training Batch: 7435 Loss: 3241.975586\n",
      "Training Batch: 7436 Loss: 3132.822754\n",
      "Training Batch: 7437 Loss: 3387.790039\n",
      "Training Batch: 7438 Loss: 3282.851562\n",
      "Training Batch: 7439 Loss: 3297.171143\n",
      "Training Batch: 7440 Loss: 3118.077881\n",
      "Training Batch: 7441 Loss: 3150.923340\n",
      "Training Batch: 7442 Loss: 3427.978271\n",
      "Training Batch: 7443 Loss: 3192.401855\n",
      "Training Batch: 7444 Loss: 3232.925293\n",
      "Training Batch: 7445 Loss: 3463.215088\n",
      "Training Batch: 7446 Loss: 3250.686035\n",
      "Training Batch: 7447 Loss: 3335.766113\n",
      "Training Batch: 7448 Loss: 3218.399902\n",
      "Training Batch: 7449 Loss: 3365.038574\n",
      "Training Batch: 7450 Loss: 3123.527832\n",
      "Training Batch: 7451 Loss: 3204.979980\n",
      "Training Batch: 7452 Loss: 3144.459473\n",
      "Training Batch: 7453 Loss: 3298.609863\n",
      "Training Batch: 7454 Loss: 3320.066162\n",
      "Training Batch: 7455 Loss: 3119.865234\n",
      "Training Batch: 7456 Loss: 3194.583252\n",
      "Training Batch: 7457 Loss: 3389.766602\n",
      "Training Batch: 7458 Loss: 3249.249023\n",
      "Training Batch: 7459 Loss: 3195.200684\n",
      "Training Batch: 7460 Loss: 3252.795410\n",
      "Training Batch: 7461 Loss: 3237.198730\n",
      "Training Batch: 7462 Loss: 3160.043701\n",
      "Training Batch: 7463 Loss: 3193.766602\n",
      "Training Batch: 7464 Loss: 3203.986816\n",
      "Training Batch: 7465 Loss: 3230.322510\n",
      "Training Batch: 7466 Loss: 3152.755859\n",
      "Training Batch: 7467 Loss: 3116.637451\n",
      "Training Batch: 7468 Loss: 3254.300537\n",
      "Training Batch: 7469 Loss: 3137.890625\n",
      "Training Batch: 7470 Loss: 3232.729004\n",
      "Training Batch: 7471 Loss: 3128.509766\n",
      "Training Batch: 7472 Loss: 3194.421875\n",
      "Training Batch: 7473 Loss: 3199.256592\n",
      "Training Batch: 7474 Loss: 3414.637207\n",
      "Training Batch: 7475 Loss: 3184.159668\n",
      "Training Batch: 7476 Loss: 3261.300293\n",
      "Training Batch: 7477 Loss: 3178.868652\n",
      "Training Batch: 7478 Loss: 3214.933838\n",
      "Training Batch: 7479 Loss: 3230.436279\n",
      "Training Batch: 7480 Loss: 3180.372070\n",
      "Training Batch: 7481 Loss: 3231.618164\n",
      "Training Batch: 7482 Loss: 3219.769287\n",
      "Training Batch: 7483 Loss: 3306.562744\n",
      "Training Batch: 7484 Loss: 3368.563477\n",
      "Training Batch: 7485 Loss: 3174.136719\n",
      "Training Batch: 7486 Loss: 3083.673340\n",
      "Training Batch: 7487 Loss: 3092.012207\n",
      "Training Batch: 7488 Loss: 3259.603027\n",
      "Training Batch: 7489 Loss: 3365.042969\n",
      "Training Batch: 7490 Loss: 3308.744141\n",
      "Training Batch: 7491 Loss: 3172.109863\n",
      "Training Batch: 7492 Loss: 3172.717773\n",
      "Training Batch: 7493 Loss: 3257.503174\n",
      "Training Batch: 7494 Loss: 3277.565186\n",
      "Training Batch: 7495 Loss: 3158.870117\n",
      "Training Batch: 7496 Loss: 3464.532715\n",
      "Training Batch: 7497 Loss: 3273.480469\n",
      "Training Batch: 7498 Loss: 3164.248535\n",
      "Training Batch: 7499 Loss: 3078.362793\n",
      "Training Batch: 7500 Loss: 3218.941406\n",
      "Training Batch: 7501 Loss: 3205.655762\n",
      "Training Batch: 7502 Loss: 3221.687988\n",
      "Training Batch: 7503 Loss: 3097.941895\n",
      "Training Batch: 7504 Loss: 3126.199463\n",
      "Training Batch: 7505 Loss: 3143.260254\n",
      "Training Batch: 7506 Loss: 3128.332031\n",
      "Training Batch: 7507 Loss: 3134.600342\n",
      "Training Batch: 7508 Loss: 3332.766113\n",
      "Training Batch: 7509 Loss: 3229.270020\n",
      "Training Batch: 7510 Loss: 3072.928223\n",
      "Training Batch: 7511 Loss: 3308.421387\n",
      "Training Batch: 7512 Loss: 3161.003906\n",
      "Training Batch: 7513 Loss: 3146.510498\n",
      "Training Batch: 7514 Loss: 3183.344238\n",
      "Training Batch: 7515 Loss: 3254.468506\n",
      "Training Batch: 7516 Loss: 3390.675293\n",
      "Training Batch: 7517 Loss: 3215.293457\n",
      "Training Batch: 7518 Loss: 3391.009521\n",
      "Training Batch: 7519 Loss: 3294.000000\n",
      "Training Batch: 7520 Loss: 3175.684326\n",
      "Training Batch: 7521 Loss: 3236.639648\n",
      "Training Batch: 7522 Loss: 3069.885254\n",
      "Training Batch: 7523 Loss: 3127.113770\n",
      "Training Batch: 7524 Loss: 3194.635742\n",
      "Training Batch: 7525 Loss: 3270.803955\n",
      "Training Batch: 7526 Loss: 3209.282471\n",
      "Training Batch: 7527 Loss: 3193.255615\n",
      "Training Batch: 7528 Loss: 3142.472656\n",
      "Training Batch: 7529 Loss: 3112.245117\n",
      "Training Batch: 7530 Loss: 3481.232422\n",
      "Training Batch: 7531 Loss: 3232.344238\n",
      "Training Batch: 7532 Loss: 3160.211914\n",
      "Training Batch: 7533 Loss: 3190.427246\n",
      "Training Batch: 7534 Loss: 3317.070557\n",
      "Training Batch: 7535 Loss: 3230.981689\n",
      "Training Batch: 7536 Loss: 3344.519775\n",
      "Training Batch: 7537 Loss: 3170.763672\n",
      "Training Batch: 7538 Loss: 3184.577637\n",
      "Training Batch: 7539 Loss: 3178.054688\n",
      "Training Batch: 7540 Loss: 3170.836670\n",
      "Training Batch: 7541 Loss: 3354.656250\n",
      "Training Batch: 7542 Loss: 3274.727051\n",
      "Training Batch: 7543 Loss: 3390.245117\n",
      "Training Batch: 7544 Loss: 3240.707520\n",
      "Training Batch: 7545 Loss: 3316.130615\n",
      "Training Batch: 7546 Loss: 3491.017822\n",
      "Training Batch: 7547 Loss: 3318.082031\n",
      "Training Batch: 7548 Loss: 3146.622559\n",
      "Training Batch: 7549 Loss: 3218.320068\n",
      "Training Batch: 7550 Loss: 3408.744629\n",
      "Training Batch: 7551 Loss: 3185.621094\n",
      "Training Batch: 7552 Loss: 3147.479004\n",
      "Training Batch: 7553 Loss: 3176.551514\n",
      "Training Batch: 7554 Loss: 3163.548340\n",
      "Training Batch: 7555 Loss: 3239.568848\n",
      "Training Batch: 7556 Loss: 3257.863770\n",
      "Training Batch: 7557 Loss: 3461.898438\n",
      "Training Batch: 7558 Loss: 3259.466553\n",
      "Training Batch: 7559 Loss: 3291.211426\n",
      "Training Batch: 7560 Loss: 3240.127930\n",
      "Training Batch: 7561 Loss: 3173.748047\n",
      "Training Batch: 7562 Loss: 3250.721680\n",
      "Training Batch: 7563 Loss: 3305.887939\n",
      "Training Batch: 7564 Loss: 3353.032227\n",
      "Training Batch: 7565 Loss: 3275.677246\n",
      "Training Batch: 7566 Loss: 3708.087646\n",
      "Training Batch: 7567 Loss: 3807.392578\n",
      "Training Batch: 7568 Loss: 3274.340576\n",
      "Training Batch: 7569 Loss: 3295.651855\n",
      "Training Batch: 7570 Loss: 3227.338379\n",
      "Training Batch: 7571 Loss: 3346.394043\n",
      "Training Batch: 7572 Loss: 3644.520508\n",
      "Training Batch: 7573 Loss: 3343.011719\n",
      "Training Batch: 7574 Loss: 3158.422363\n",
      "Training Batch: 7575 Loss: 3226.234863\n",
      "Training Batch: 7576 Loss: 3282.221680\n",
      "Training Batch: 7577 Loss: 3210.593506\n",
      "Training Batch: 7578 Loss: 3162.555664\n",
      "Training Batch: 7579 Loss: 3335.233398\n",
      "Training Batch: 7580 Loss: 3245.893555\n",
      "Training Batch: 7581 Loss: 3283.799805\n",
      "Training Batch: 7582 Loss: 3566.567871\n",
      "Training Batch: 7583 Loss: 3213.690430\n",
      "Training Batch: 7584 Loss: 3171.101318\n",
      "Training Batch: 7585 Loss: 3285.951416\n",
      "Training Batch: 7586 Loss: 3245.127930\n",
      "Training Batch: 7587 Loss: 3162.864258\n",
      "Training Batch: 7588 Loss: 3747.539551\n",
      "Training Batch: 7589 Loss: 3798.986084\n",
      "Training Batch: 7590 Loss: 3289.898926\n",
      "Training Batch: 7591 Loss: 3663.696777\n",
      "Training Batch: 7592 Loss: 3431.044434\n",
      "Training Batch: 7593 Loss: 3319.945801\n",
      "Training Batch: 7594 Loss: 3122.146484\n",
      "Training Batch: 7595 Loss: 3210.393311\n",
      "Training Batch: 7596 Loss: 3316.792480\n",
      "Training Batch: 7597 Loss: 3289.635254\n",
      "Training Batch: 7598 Loss: 3376.545654\n",
      "Training Batch: 7599 Loss: 3111.034668\n",
      "Training Batch: 7600 Loss: 3209.916504\n",
      "Training Batch: 7601 Loss: 3225.934082\n",
      "Training Batch: 7602 Loss: 3238.251465\n",
      "Training Batch: 7603 Loss: 3186.461914\n",
      "Training Batch: 7604 Loss: 3276.444580\n",
      "Training Batch: 7605 Loss: 3145.191895\n",
      "Training Batch: 7606 Loss: 3187.733887\n",
      "Training Batch: 7607 Loss: 3149.138184\n",
      "Training Batch: 7608 Loss: 3248.775879\n",
      "Training Batch: 7609 Loss: 3293.167969\n",
      "Training Batch: 7610 Loss: 3133.239258\n",
      "Training Batch: 7611 Loss: 3166.730469\n",
      "Training Batch: 7612 Loss: 3283.540527\n",
      "Training Batch: 7613 Loss: 3265.897461\n",
      "Training Batch: 7614 Loss: 3169.664062\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Batch: 7615 Loss: 3169.600342\n",
      "Training Batch: 7616 Loss: 3171.913574\n",
      "Training Batch: 7617 Loss: 3113.056641\n",
      "Training Batch: 7618 Loss: 3233.245117\n",
      "Training Batch: 7619 Loss: 3166.444336\n",
      "Training Batch: 7620 Loss: 3130.857910\n",
      "Training Batch: 7621 Loss: 3176.207275\n",
      "Training Batch: 7622 Loss: 3123.824219\n",
      "Training Batch: 7623 Loss: 3438.601562\n",
      "Training Batch: 7624 Loss: 3504.103516\n",
      "Training Batch: 7625 Loss: 3200.691895\n",
      "Training Batch: 7626 Loss: 3289.191406\n",
      "Training Batch: 7627 Loss: 3290.604004\n",
      "Training Batch: 7628 Loss: 3171.051270\n",
      "Training Batch: 7629 Loss: 3109.958984\n",
      "Training Batch: 7630 Loss: 3212.838867\n",
      "Training Batch: 7631 Loss: 3187.313232\n",
      "Training Batch: 7632 Loss: 3351.193359\n",
      "Training Batch: 7633 Loss: 3242.762207\n",
      "Training Batch: 7634 Loss: 3110.658203\n",
      "Training Batch: 7635 Loss: 3191.489990\n",
      "Training Batch: 7636 Loss: 3151.049316\n",
      "Training Batch: 7637 Loss: 3227.526123\n",
      "Training Batch: 7638 Loss: 3191.797607\n",
      "Training Batch: 7639 Loss: 3214.946777\n",
      "Training Batch: 7640 Loss: 3106.065674\n",
      "Training Batch: 7641 Loss: 3156.623047\n",
      "Training Batch: 7642 Loss: 3151.664062\n",
      "Training Batch: 7643 Loss: 3240.732422\n",
      "Training Batch: 7644 Loss: 3164.231934\n",
      "Training Batch: 7645 Loss: 3196.046387\n",
      "Training Batch: 7646 Loss: 3093.944824\n",
      "Training Batch: 7647 Loss: 3348.014648\n",
      "Training Batch: 7648 Loss: 3441.173828\n",
      "Training Batch: 7649 Loss: 3285.910645\n",
      "Training Batch: 7650 Loss: 3344.857910\n",
      "Training Batch: 7651 Loss: 3274.387207\n",
      "Training Batch: 7652 Loss: 3231.377930\n",
      "Training Batch: 7653 Loss: 3329.111328\n",
      "Training Batch: 7654 Loss: 3426.458984\n",
      "Training Batch: 7655 Loss: 3274.057617\n",
      "Training Batch: 7656 Loss: 3134.438965\n",
      "Training Batch: 7657 Loss: 3203.898926\n",
      "Training Batch: 7658 Loss: 3399.708984\n",
      "Training Batch: 7659 Loss: 3524.271729\n",
      "Training Batch: 7660 Loss: 3219.632568\n",
      "Training Batch: 7661 Loss: 3507.427246\n",
      "Training Batch: 7662 Loss: 3207.718750\n",
      "Training Batch: 7663 Loss: 3255.471191\n",
      "Training Batch: 7664 Loss: 3154.907471\n",
      "Training Batch: 7665 Loss: 3166.943359\n",
      "Training Batch: 7666 Loss: 3271.316895\n",
      "Training Batch: 7667 Loss: 3316.191895\n",
      "Training Batch: 7668 Loss: 3387.571777\n",
      "Training Batch: 7669 Loss: 3201.048828\n",
      "Training Batch: 7670 Loss: 3301.120117\n",
      "Training Batch: 7671 Loss: 3200.492188\n",
      "Training Batch: 7672 Loss: 3253.992676\n",
      "Training Batch: 7673 Loss: 3260.533691\n",
      "Training Batch: 7674 Loss: 3257.099609\n",
      "Training Batch: 7675 Loss: 3264.083008\n",
      "Training Batch: 7676 Loss: 3101.984863\n",
      "Training Batch: 7677 Loss: 3198.789551\n",
      "Training Batch: 7678 Loss: 3300.582520\n",
      "Training Batch: 7679 Loss: 3106.079102\n",
      "Training Batch: 7680 Loss: 3146.239014\n",
      "Training Batch: 7681 Loss: 3176.819336\n",
      "Training Batch: 7682 Loss: 3249.033203\n",
      "Training Batch: 7683 Loss: 3099.225586\n",
      "Training Batch: 7684 Loss: 3040.773926\n",
      "Training Batch: 7685 Loss: 3353.110840\n",
      "Training Batch: 7686 Loss: 3173.808105\n",
      "Training Batch: 7687 Loss: 3180.512695\n",
      "Training Batch: 7688 Loss: 3141.961426\n",
      "Training Batch: 7689 Loss: 3117.647461\n",
      "Training Batch: 7690 Loss: 3377.549316\n",
      "Training Batch: 7691 Loss: 3096.636230\n",
      "Training Batch: 7692 Loss: 3154.917236\n",
      "Training Batch: 7693 Loss: 3163.151367\n",
      "Training Batch: 7694 Loss: 3148.713379\n",
      "Training Batch: 7695 Loss: 3150.376953\n",
      "Training Batch: 7696 Loss: 3145.584473\n",
      "Training Batch: 7697 Loss: 3213.385254\n",
      "Training Batch: 7698 Loss: 3190.590576\n",
      "Training Batch: 7699 Loss: 3234.040039\n",
      "Training Batch: 7700 Loss: 3146.259766\n",
      "Training Batch: 7701 Loss: 3430.914551\n",
      "Training Batch: 7702 Loss: 3142.053711\n",
      "Training Batch: 7703 Loss: 3434.610840\n",
      "Training Batch: 7704 Loss: 3463.306641\n",
      "Training Batch: 7705 Loss: 3218.367676\n",
      "Training Batch: 7706 Loss: 3176.563721\n",
      "Training Batch: 7707 Loss: 3196.975098\n",
      "Training Batch: 7708 Loss: 3275.991455\n",
      "Training Batch: 7709 Loss: 3291.437256\n",
      "Training Batch: 7710 Loss: 3374.080566\n",
      "Training Batch: 7711 Loss: 3187.637695\n",
      "Training Batch: 7712 Loss: 3280.181152\n",
      "Training Batch: 7713 Loss: 3173.802734\n",
      "Training Batch: 7714 Loss: 3101.146973\n",
      "Training Batch: 7715 Loss: 3190.123535\n",
      "Training Batch: 7716 Loss: 3211.900635\n",
      "Training Batch: 7717 Loss: 3131.201660\n",
      "Training Batch: 7718 Loss: 3159.157471\n",
      "Training Batch: 7719 Loss: 3176.881836\n",
      "Training Batch: 7720 Loss: 3051.566406\n",
      "Training Batch: 7721 Loss: 3098.112305\n",
      "Training Batch: 7722 Loss: 3111.677246\n",
      "Training Batch: 7723 Loss: 3255.874268\n",
      "Training Batch: 7724 Loss: 3211.718750\n",
      "Training Batch: 7725 Loss: 3219.671631\n",
      "Training Batch: 7726 Loss: 3461.377930\n",
      "Training Batch: 7727 Loss: 3232.296875\n",
      "Training Batch: 7728 Loss: 3089.768066\n",
      "Training Batch: 7729 Loss: 3086.027588\n",
      "Training Batch: 7730 Loss: 3133.001465\n",
      "Training Batch: 7731 Loss: 3095.117188\n",
      "Training Batch: 7732 Loss: 3363.008789\n",
      "Training Batch: 7733 Loss: 3152.799316\n",
      "Training Batch: 7734 Loss: 3187.055908\n",
      "Training Batch: 7735 Loss: 3088.247070\n",
      "Training Batch: 7736 Loss: 3217.203125\n",
      "Training Batch: 7737 Loss: 3135.119629\n",
      "Training Batch: 7738 Loss: 3166.380859\n",
      "Training Batch: 7739 Loss: 3220.281494\n",
      "Training Batch: 7740 Loss: 3146.385498\n",
      "Training Batch: 7741 Loss: 3190.342285\n",
      "Training Batch: 7742 Loss: 3182.863770\n",
      "Training Batch: 7743 Loss: 3117.353027\n",
      "Training Batch: 7744 Loss: 3109.571045\n",
      "Training Batch: 7745 Loss: 3180.080078\n",
      "Training Batch: 7746 Loss: 3152.213867\n",
      "Training Batch: 7747 Loss: 3142.344238\n",
      "Training Batch: 7748 Loss: 3057.002197\n",
      "Training Batch: 7749 Loss: 3163.083984\n",
      "Training Batch: 7750 Loss: 3221.350098\n",
      "Training Batch: 7751 Loss: 3119.950195\n",
      "Training Batch: 7752 Loss: 3116.902100\n",
      "Training Batch: 7753 Loss: 3299.974609\n",
      "Training Batch: 7754 Loss: 3169.276611\n",
      "Training Batch: 7755 Loss: 3192.567383\n",
      "Training Batch: 7756 Loss: 3091.831055\n",
      "Training Batch: 7757 Loss: 3161.191895\n",
      "Training Batch: 7758 Loss: 3225.628174\n",
      "Training Batch: 7759 Loss: 3109.687744\n",
      "Training Batch: 7760 Loss: 3248.687012\n",
      "Training Batch: 7761 Loss: 3246.740723\n",
      "Training Batch: 7762 Loss: 3129.683838\n",
      "Training Batch: 7763 Loss: 3218.956543\n",
      "Training Batch: 7764 Loss: 3262.375000\n",
      "Training Batch: 7765 Loss: 3113.662598\n",
      "Training Batch: 7766 Loss: 3416.544922\n",
      "Training Batch: 7767 Loss: 3228.110840\n",
      "Training Batch: 7768 Loss: 3272.484863\n",
      "Training Batch: 7769 Loss: 3173.166016\n",
      "Training Batch: 7770 Loss: 3323.964355\n",
      "Training Batch: 7771 Loss: 3698.975098\n",
      "Training Batch: 7772 Loss: 3332.048828\n",
      "Training Batch: 7773 Loss: 3371.776855\n",
      "Training Batch: 7774 Loss: 3436.184082\n",
      "Training Batch: 7775 Loss: 3190.224121\n",
      "Training Batch: 7776 Loss: 3221.613770\n",
      "Training Batch: 7777 Loss: 3088.946533\n",
      "Training Batch: 7778 Loss: 3123.816895\n",
      "Training Batch: 7779 Loss: 3140.989258\n",
      "Training Batch: 7780 Loss: 3114.613770\n",
      "Training Batch: 7781 Loss: 3307.456055\n",
      "Training Batch: 7782 Loss: 3110.665039\n",
      "Training Batch: 7783 Loss: 3124.217773\n",
      "Training Batch: 7784 Loss: 3227.787598\n",
      "Training Batch: 7785 Loss: 3232.302734\n",
      "Training Batch: 7786 Loss: 3205.592529\n",
      "Training Batch: 7787 Loss: 3340.033691\n",
      "Training Batch: 7788 Loss: 3398.748535\n",
      "Training Batch: 7789 Loss: 3222.165039\n",
      "Training Batch: 7790 Loss: 3207.306396\n",
      "Training Batch: 7791 Loss: 3389.604004\n",
      "Training Batch: 7792 Loss: 3237.480469\n",
      "Training Batch: 7793 Loss: 3347.918457\n",
      "Training Batch: 7794 Loss: 3159.183350\n",
      "Training Batch: 7795 Loss: 3256.537598\n",
      "Training Batch: 7796 Loss: 3243.575684\n",
      "Training Batch: 7797 Loss: 3194.831787\n",
      "Training Batch: 7798 Loss: 3304.722900\n",
      "Training Batch: 7799 Loss: 3197.313721\n",
      "Training Batch: 7800 Loss: 3217.080811\n",
      "Training Batch: 7801 Loss: 3154.562744\n",
      "Training Batch: 7802 Loss: 3260.671875\n",
      "Training Batch: 7803 Loss: 3292.449219\n",
      "Training Batch: 7804 Loss: 3222.671143\n",
      "Training Batch: 7805 Loss: 3349.555908\n",
      "Training Batch: 7806 Loss: 3199.860352\n",
      "Training Batch: 7807 Loss: 3130.289795\n",
      "Training Batch: 7808 Loss: 3176.824219\n",
      "Training Batch: 7809 Loss: 3258.736084\n",
      "Training Batch: 7810 Loss: 3192.785889\n",
      "Training Batch: 7811 Loss: 3219.199707\n",
      "Training Batch: 7812 Loss: 3307.037598\n",
      "Training Batch: 7813 Loss: 3136.210449\n",
      "Training Batch: 7814 Loss: 3185.964355\n",
      "Training Batch: 7815 Loss: 3180.247559\n",
      "Training Batch: 7816 Loss: 3325.544922\n",
      "Training Batch: 7817 Loss: 3180.016602\n",
      "Training Batch: 7818 Loss: 3060.331543\n",
      "Training Batch: 7819 Loss: 3202.441406\n",
      "Training Batch: 7820 Loss: 3138.123535\n",
      "Training Batch: 7821 Loss: 3242.467285\n",
      "Training Batch: 7822 Loss: 3177.252441\n",
      "Training Batch: 7823 Loss: 3306.141846\n",
      "Training Batch: 7824 Loss: 3649.666992\n",
      "Training Batch: 7825 Loss: 3331.792236\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Batch: 7826 Loss: 3135.957520\n",
      "Training Batch: 7827 Loss: 3334.909668\n",
      "Training Batch: 7828 Loss: 3272.776855\n",
      "Training Batch: 7829 Loss: 3129.585938\n",
      "Training Batch: 7830 Loss: 3167.808594\n",
      "Training Batch: 7831 Loss: 4188.759277\n",
      "Training Batch: 7832 Loss: 3176.736816\n",
      "Training Batch: 7833 Loss: 3125.235352\n",
      "Training Batch: 7834 Loss: 3182.082764\n",
      "Training Batch: 7835 Loss: 3523.634277\n",
      "Training Batch: 7836 Loss: 3107.111572\n",
      "Training Batch: 7837 Loss: 3101.996338\n",
      "Training Batch: 7838 Loss: 3156.117676\n",
      "Training Batch: 7839 Loss: 3097.874023\n",
      "Training Batch: 7840 Loss: 3229.058105\n",
      "Training Batch: 7841 Loss: 3256.875000\n",
      "Training Batch: 7842 Loss: 3374.166016\n",
      "Training Batch: 7843 Loss: 3373.933105\n",
      "Training Batch: 7844 Loss: 3197.945312\n",
      "Training Batch: 7845 Loss: 3177.839355\n",
      "Training Batch: 7846 Loss: 3077.248535\n",
      "Training Batch: 7847 Loss: 3160.610352\n",
      "Training Batch: 7848 Loss: 3194.747559\n",
      "Training Batch: 7849 Loss: 3174.954102\n",
      "Training Batch: 7850 Loss: 3196.545654\n",
      "Training Batch: 7851 Loss: 3163.237549\n",
      "Training Batch: 7852 Loss: 3238.113770\n",
      "Training Batch: 7853 Loss: 3117.543213\n",
      "Training Batch: 7854 Loss: 3206.857422\n",
      "Training Batch: 7855 Loss: 3194.401855\n",
      "Training Batch: 7856 Loss: 3156.581055\n",
      "Training Batch: 7857 Loss: 3167.757080\n",
      "Training Batch: 7858 Loss: 3150.160156\n",
      "Training Batch: 7859 Loss: 3195.939941\n",
      "Training Batch: 7860 Loss: 3224.936035\n",
      "Training Batch: 7861 Loss: 3503.987305\n",
      "Training Batch: 7862 Loss: 3110.066895\n",
      "Training Batch: 7863 Loss: 3226.769043\n",
      "Training Batch: 7864 Loss: 3148.595215\n",
      "Training Batch: 7865 Loss: 3194.161865\n",
      "Training Batch: 7866 Loss: 3208.579346\n",
      "Training Batch: 7867 Loss: 3142.688965\n",
      "Training Batch: 7868 Loss: 3345.910645\n",
      "Training Batch: 7869 Loss: 3132.581055\n",
      "Training Batch: 7870 Loss: 3254.749512\n",
      "Training Batch: 7871 Loss: 3160.866699\n",
      "Training Batch: 7872 Loss: 3173.327881\n",
      "Training Batch: 7873 Loss: 3421.439453\n",
      "Training Batch: 7874 Loss: 3228.942383\n",
      "Training Batch: 7875 Loss: 3201.953857\n",
      "Training Batch: 7876 Loss: 3234.757324\n",
      "Training Batch: 7877 Loss: 3124.137939\n",
      "Training Batch: 7878 Loss: 3202.027100\n",
      "Training Batch: 7879 Loss: 3334.993408\n",
      "Training Batch: 7880 Loss: 3215.926270\n",
      "Training Batch: 7881 Loss: 3179.500977\n",
      "Training Batch: 7882 Loss: 3176.822266\n",
      "Training Batch: 7883 Loss: 3202.101074\n",
      "Training Batch: 7884 Loss: 3414.918701\n",
      "Training Batch: 7885 Loss: 3191.271973\n",
      "Training Batch: 7886 Loss: 3189.740723\n",
      "Training Batch: 7887 Loss: 3366.711670\n",
      "Training Batch: 7888 Loss: 3179.052490\n",
      "Training Batch: 7889 Loss: 3293.905762\n",
      "Training Batch: 7890 Loss: 3318.828857\n",
      "Training Batch: 7891 Loss: 3166.378174\n",
      "Training Batch: 7892 Loss: 3269.388672\n",
      "Training Batch: 7893 Loss: 3257.468018\n",
      "Training Batch: 7894 Loss: 3200.153320\n",
      "Training Batch: 7895 Loss: 3256.358154\n",
      "Training Batch: 7896 Loss: 3267.972168\n",
      "Training Batch: 7897 Loss: 3182.008789\n",
      "Training Batch: 7898 Loss: 3133.403320\n",
      "Training Batch: 7899 Loss: 3288.329590\n",
      "Training Batch: 7900 Loss: 3346.198730\n",
      "Training Batch: 7901 Loss: 3156.465820\n",
      "Training Batch: 7902 Loss: 3240.266113\n",
      "Training Batch: 7903 Loss: 3257.969238\n",
      "Training Batch: 7904 Loss: 3270.672852\n",
      "Training Batch: 7905 Loss: 3232.590332\n",
      "Training Batch: 7906 Loss: 3228.994629\n",
      "Training Batch: 7907 Loss: 3160.149414\n",
      "Training Batch: 7908 Loss: 3131.033203\n",
      "Training Batch: 7909 Loss: 3133.173828\n",
      "Training Batch: 7910 Loss: 3257.452637\n",
      "Training Batch: 7911 Loss: 3161.335938\n",
      "Training Batch: 7912 Loss: 3226.013916\n",
      "Training Batch: 7913 Loss: 3055.253906\n",
      "Training Batch: 7914 Loss: 3153.381104\n",
      "Training Batch: 7915 Loss: 3095.476807\n",
      "Training Batch: 7916 Loss: 3235.335205\n",
      "Training Batch: 7917 Loss: 3080.428711\n",
      "Training Batch: 7918 Loss: 3145.210693\n",
      "Training Batch: 7919 Loss: 3066.815186\n",
      "Training Batch: 7920 Loss: 3117.285645\n",
      "Training Batch: 7921 Loss: 3090.099609\n",
      "Training Batch: 7922 Loss: 3256.179443\n",
      "Training Batch: 7923 Loss: 3219.715332\n",
      "Training Batch: 7924 Loss: 3162.974121\n",
      "Training Batch: 7925 Loss: 3287.142578\n",
      "Training Batch: 7926 Loss: 3293.984375\n",
      "Training Batch: 7927 Loss: 3307.773926\n",
      "Training Batch: 7928 Loss: 3286.571045\n",
      "Training Batch: 7929 Loss: 3114.031738\n",
      "Training Batch: 7930 Loss: 3365.044678\n",
      "Training Batch: 7931 Loss: 3356.808838\n",
      "Training Batch: 7932 Loss: 3345.636719\n",
      "Training Batch: 7933 Loss: 3201.810303\n",
      "Training Batch: 7934 Loss: 3163.123047\n",
      "Training Batch: 7935 Loss: 3144.698242\n",
      "Training Batch: 7936 Loss: 3199.061279\n",
      "Training Batch: 7937 Loss: 3160.285645\n",
      "Training Batch: 7938 Loss: 3469.773438\n",
      "Training Batch: 7939 Loss: 3190.682861\n",
      "Training Batch: 7940 Loss: 3192.551270\n",
      "Training Batch: 7941 Loss: 3279.857910\n",
      "Training Batch: 7942 Loss: 3223.486084\n",
      "Training Batch: 7943 Loss: 3320.096680\n",
      "Training Batch: 7944 Loss: 3275.664062\n",
      "Training Batch: 7945 Loss: 3312.657227\n",
      "Training Batch: 7946 Loss: 3225.520020\n",
      "Training Batch: 7947 Loss: 3282.956055\n",
      "Training Batch: 7948 Loss: 3235.962402\n",
      "Training Batch: 7949 Loss: 3357.622559\n",
      "Training Batch: 7950 Loss: 3199.217773\n",
      "Training Batch: 7951 Loss: 3205.089355\n",
      "Training Batch: 7952 Loss: 3369.299316\n",
      "Training Batch: 7953 Loss: 3224.556152\n",
      "Training Batch: 7954 Loss: 3260.209961\n",
      "Training Batch: 7955 Loss: 3245.763672\n",
      "Training Batch: 7956 Loss: 3207.780029\n",
      "Training Batch: 7957 Loss: 3211.511475\n",
      "Training Batch: 7958 Loss: 3052.246338\n",
      "Training Batch: 7959 Loss: 3129.068848\n",
      "Training Batch: 7960 Loss: 3174.575684\n",
      "Training Batch: 7961 Loss: 3368.618164\n",
      "Training Batch: 7962 Loss: 3518.764893\n",
      "Training Batch: 7963 Loss: 3286.076660\n",
      "Training Batch: 7964 Loss: 3345.605957\n",
      "Training Batch: 7965 Loss: 3282.409668\n",
      "Training Batch: 7966 Loss: 3267.705322\n",
      "Training Batch: 7967 Loss: 3347.331787\n",
      "Training Batch: 7968 Loss: 3099.701416\n",
      "Training Batch: 7969 Loss: 3135.770752\n",
      "Training Batch: 7970 Loss: 3357.474854\n",
      "Training Batch: 7971 Loss: 3262.364258\n",
      "Training Batch: 7972 Loss: 3256.824219\n",
      "Training Batch: 7973 Loss: 3433.919434\n",
      "Training Batch: 7974 Loss: 3191.082764\n",
      "Training Batch: 7975 Loss: 3218.496582\n",
      "Training Batch: 7976 Loss: 3180.236572\n",
      "Training Batch: 7977 Loss: 3139.662842\n",
      "Training Batch: 7978 Loss: 3248.421387\n",
      "Training Batch: 7979 Loss: 3154.017578\n",
      "Training Batch: 7980 Loss: 3149.397949\n",
      "Training Batch: 7981 Loss: 3207.316895\n",
      "Training Batch: 7982 Loss: 3234.307617\n",
      "Training Batch: 7983 Loss: 3287.437988\n",
      "Training Batch: 7984 Loss: 3166.762695\n",
      "Training Batch: 7985 Loss: 3429.980957\n",
      "Training Batch: 7986 Loss: 3115.110352\n",
      "Training Batch: 7987 Loss: 3153.663818\n",
      "Training Batch: 7988 Loss: 3274.416016\n",
      "Training Batch: 7989 Loss: 3202.846191\n",
      "Training Batch: 7990 Loss: 3203.344727\n",
      "Training Batch: 7991 Loss: 3464.605957\n",
      "Training Batch: 7992 Loss: 3478.782715\n",
      "Training Batch: 7993 Loss: 3339.704590\n",
      "Training Batch: 7994 Loss: 3679.809570\n",
      "Training Batch: 7995 Loss: 3312.363770\n",
      "Training Batch: 7996 Loss: 3159.835449\n",
      "Training Batch: 7997 Loss: 3183.617920\n",
      "Training Batch: 7998 Loss: 3233.230957\n",
      "Training Batch: 7999 Loss: 3252.919434\n",
      "Training Batch: 8000 Loss: 3309.356445\n",
      "Training Batch: 8001 Loss: 3381.822754\n",
      "Training Batch: 8002 Loss: 3234.402344\n",
      "Training Batch: 8003 Loss: 3129.324707\n",
      "Training Batch: 8004 Loss: 3208.980957\n",
      "Training Batch: 8005 Loss: 3172.342041\n",
      "Training Batch: 8006 Loss: 3432.809326\n",
      "Training Batch: 8007 Loss: 3462.015137\n",
      "Training Batch: 8008 Loss: 3306.541016\n",
      "Training Batch: 8009 Loss: 3224.784668\n",
      "Training Batch: 8010 Loss: 3251.665527\n",
      "Training Batch: 8011 Loss: 3152.604492\n",
      "Training Batch: 8012 Loss: 3176.990723\n",
      "Training Batch: 8013 Loss: 3142.085449\n",
      "Training Batch: 8014 Loss: 3260.188477\n",
      "Training Batch: 8015 Loss: 3145.189941\n",
      "Training Batch: 8016 Loss: 3263.610840\n",
      "Training Batch: 8017 Loss: 3212.571777\n",
      "Training Batch: 8018 Loss: 3494.058105\n",
      "Training Batch: 8019 Loss: 3458.494141\n",
      "Training Batch: 8020 Loss: 3259.341797\n",
      "Training Batch: 8021 Loss: 3217.207520\n",
      "Training Batch: 8022 Loss: 3329.169922\n",
      "Training Batch: 8023 Loss: 3149.397461\n",
      "Training Batch: 8024 Loss: 3243.178711\n",
      "Training Batch: 8025 Loss: 3184.260010\n",
      "Training Batch: 8026 Loss: 3223.281250\n",
      "Training Batch: 8027 Loss: 3213.220215\n",
      "Training Batch: 8028 Loss: 3307.373047\n",
      "Training Batch: 8029 Loss: 3137.447021\n",
      "Training Batch: 8030 Loss: 3219.890137\n",
      "Training Batch: 8031 Loss: 3312.644287\n",
      "Training Batch: 8032 Loss: 3209.121094\n",
      "Training Batch: 8033 Loss: 3177.093262\n",
      "Training Batch: 8034 Loss: 3199.613281\n",
      "Training Batch: 8035 Loss: 3202.233887\n",
      "Training Batch: 8036 Loss: 3176.416016\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Batch: 8037 Loss: 3350.597656\n",
      "Training Batch: 8038 Loss: 3172.148926\n",
      "Training Batch: 8039 Loss: 3617.703613\n",
      "Training Batch: 8040 Loss: 3222.630371\n",
      "Training Batch: 8041 Loss: 3173.152832\n",
      "Training Batch: 8042 Loss: 3189.376465\n",
      "Training Batch: 8043 Loss: 3224.395508\n",
      "Training Batch: 8044 Loss: 3153.021973\n",
      "Training Batch: 8045 Loss: 3301.963379\n",
      "Training Batch: 8046 Loss: 3075.171387\n",
      "Training Batch: 8047 Loss: 3121.752930\n",
      "Training Batch: 8048 Loss: 3168.029053\n",
      "Training Batch: 8049 Loss: 3113.609375\n",
      "Training Batch: 8050 Loss: 3051.522461\n",
      "Training Batch: 8051 Loss: 3297.340332\n",
      "Training Batch: 8052 Loss: 3090.680664\n",
      "Training Batch: 8053 Loss: 3306.041992\n",
      "Training Batch: 8054 Loss: 3352.420166\n",
      "Training Batch: 8055 Loss: 3194.725586\n",
      "Training Batch: 8056 Loss: 3114.403809\n",
      "Training Batch: 8057 Loss: 3178.169434\n",
      "Training Batch: 8058 Loss: 3099.529297\n",
      "Training Batch: 8059 Loss: 3174.529297\n",
      "Training Batch: 8060 Loss: 3247.722168\n",
      "Training Batch: 8061 Loss: 3285.607422\n",
      "Training Batch: 8062 Loss: 3312.218018\n",
      "Training Batch: 8063 Loss: 3305.139893\n",
      "Training Batch: 8064 Loss: 3309.679688\n",
      "Training Batch: 8065 Loss: 3152.708008\n",
      "Training Batch: 8066 Loss: 3219.662598\n",
      "Training Batch: 8067 Loss: 3108.087402\n",
      "Training Batch: 8068 Loss: 3092.594727\n",
      "Training Batch: 8069 Loss: 3330.987549\n",
      "Training Batch: 8070 Loss: 3149.709473\n",
      "Training Batch: 8071 Loss: 3270.162109\n",
      "Training Batch: 8072 Loss: 3235.602539\n",
      "Training Batch: 8073 Loss: 3216.124023\n",
      "Training Batch: 8074 Loss: 3338.531006\n",
      "Training Batch: 8075 Loss: 3235.381348\n",
      "Training Batch: 8076 Loss: 3227.279297\n",
      "Training Batch: 8077 Loss: 3453.611816\n",
      "Training Batch: 8078 Loss: 3109.329102\n",
      "Training Batch: 8079 Loss: 3544.357422\n",
      "Training Batch: 8080 Loss: 3194.176758\n",
      "Training Batch: 8081 Loss: 3146.240967\n",
      "Training Batch: 8082 Loss: 3230.658691\n",
      "Training Batch: 8083 Loss: 3138.806885\n",
      "Training Batch: 8084 Loss: 3182.513184\n",
      "Training Batch: 8085 Loss: 3236.066895\n",
      "Training Batch: 8086 Loss: 3317.570801\n",
      "Training Batch: 8087 Loss: 3042.329590\n",
      "Training Batch: 8088 Loss: 3091.174805\n",
      "Training Batch: 8089 Loss: 3117.959961\n",
      "Training Batch: 8090 Loss: 3060.474365\n",
      "Training Batch: 8091 Loss: 3119.862061\n",
      "Training Batch: 8092 Loss: 3089.136230\n",
      "Training Batch: 8093 Loss: 3269.582031\n",
      "Training Batch: 8094 Loss: 3158.030029\n",
      "Training Batch: 8095 Loss: 3341.046387\n",
      "Training Batch: 8096 Loss: 3382.524902\n",
      "Training Batch: 8097 Loss: 3133.245605\n",
      "Training Batch: 8098 Loss: 3341.196777\n",
      "Training Batch: 8099 Loss: 3144.611328\n",
      "Training Batch: 8100 Loss: 3151.552246\n",
      "Training Batch: 8101 Loss: 3016.415527\n",
      "Training Batch: 8102 Loss: 3111.486572\n",
      "Training Batch: 8103 Loss: 3140.227539\n",
      "Training Batch: 8104 Loss: 3181.603760\n",
      "Training Batch: 8105 Loss: 3173.940918\n",
      "Training Batch: 8106 Loss: 3098.313477\n",
      "Training Batch: 8107 Loss: 3184.590088\n",
      "Training Batch: 8108 Loss: 3330.525879\n",
      "Training Batch: 8109 Loss: 3177.578613\n",
      "Training Batch: 8110 Loss: 3121.932129\n",
      "Training Batch: 8111 Loss: 3213.541748\n",
      "Training Batch: 8112 Loss: 3258.934570\n",
      "Training Batch: 8113 Loss: 3115.874023\n",
      "Training Batch: 8114 Loss: 3127.238281\n",
      "Training Batch: 8115 Loss: 3166.274658\n",
      "Training Batch: 8116 Loss: 3644.508545\n",
      "Training Batch: 8117 Loss: 3167.041992\n",
      "Training Batch: 8118 Loss: 3112.394287\n",
      "Training Batch: 8119 Loss: 3127.977783\n",
      "Training Batch: 8120 Loss: 3215.496582\n",
      "Training Batch: 8121 Loss: 3095.628906\n",
      "Training Batch: 8122 Loss: 3134.040527\n",
      "Training Batch: 8123 Loss: 3175.707520\n",
      "Training Batch: 8124 Loss: 3099.197510\n",
      "Training Batch: 8125 Loss: 3206.072754\n",
      "Training Batch: 8126 Loss: 3362.100098\n",
      "Training Batch: 8127 Loss: 3392.470459\n",
      "Training Batch: 8128 Loss: 3288.110352\n",
      "Training Batch: 8129 Loss: 3211.646973\n",
      "Training Batch: 8130 Loss: 3245.666016\n",
      "Training Batch: 8131 Loss: 3347.318359\n",
      "Training Batch: 8132 Loss: 3222.825684\n",
      "Training Batch: 8133 Loss: 3219.078125\n",
      "Training Batch: 8134 Loss: 3196.746582\n",
      "Training Batch: 8135 Loss: 3101.508545\n",
      "Training Batch: 8136 Loss: 3260.132812\n",
      "Training Batch: 8137 Loss: 3197.714844\n",
      "Training Batch: 8138 Loss: 3597.006836\n",
      "Training Batch: 8139 Loss: 3184.975586\n",
      "Training Batch: 8140 Loss: 3217.955322\n",
      "Training Batch: 8141 Loss: 3223.469727\n",
      "Training Batch: 8142 Loss: 3110.037109\n",
      "Training Batch: 8143 Loss: 3209.992920\n",
      "Training Batch: 8144 Loss: 3341.248535\n",
      "Training Batch: 8145 Loss: 3135.826172\n",
      "Training Batch: 8146 Loss: 3285.340820\n",
      "Training Batch: 8147 Loss: 3113.882812\n",
      "Training Batch: 8148 Loss: 3046.087402\n",
      "Training Batch: 8149 Loss: 3189.650879\n",
      "Training Batch: 8150 Loss: 3252.805664\n",
      "Training Batch: 8151 Loss: 3135.842285\n",
      "Training Batch: 8152 Loss: 3250.974365\n",
      "Training Batch: 8153 Loss: 3063.805664\n",
      "Training Batch: 8154 Loss: 3099.960938\n",
      "Training Batch: 8155 Loss: 3296.501953\n",
      "Training Batch: 8156 Loss: 3296.498291\n",
      "Training Batch: 8157 Loss: 3096.949219\n",
      "Training Batch: 8158 Loss: 3165.073242\n",
      "Training Batch: 8159 Loss: 3109.138428\n",
      "Training Batch: 8160 Loss: 3059.334473\n",
      "Training Batch: 8161 Loss: 3262.748535\n",
      "Training Batch: 8162 Loss: 3168.204590\n",
      "Training Batch: 8163 Loss: 3176.407227\n",
      "Training Batch: 8164 Loss: 3149.312500\n",
      "Training Batch: 8165 Loss: 3204.482666\n",
      "Training Batch: 8166 Loss: 3174.493652\n",
      "Training Batch: 8167 Loss: 3166.925293\n",
      "Training Batch: 8168 Loss: 3159.181152\n",
      "Training Batch: 8169 Loss: 3217.559082\n",
      "Training Batch: 8170 Loss: 3232.481934\n",
      "Training Batch: 8171 Loss: 3243.660889\n",
      "Training Batch: 8172 Loss: 3285.267334\n",
      "Training Batch: 8173 Loss: 3305.644043\n",
      "Training Batch: 8174 Loss: 3237.438965\n",
      "Training Batch: 8175 Loss: 3088.486816\n",
      "Training Batch: 8176 Loss: 3175.958008\n",
      "Training Batch: 8177 Loss: 3186.877441\n",
      "Training Batch: 8178 Loss: 3294.184326\n",
      "Training Batch: 8179 Loss: 3231.595459\n",
      "Training Batch: 8180 Loss: 3365.481934\n",
      "Training Batch: 8181 Loss: 3252.263672\n",
      "Training Batch: 8182 Loss: 3313.211182\n",
      "Training Batch: 8183 Loss: 3216.043457\n",
      "Training Batch: 8184 Loss: 3038.465088\n",
      "Training Batch: 8185 Loss: 3162.495117\n",
      "Training Batch: 8186 Loss: 3259.505859\n",
      "Training Batch: 8187 Loss: 3181.102783\n",
      "Training Batch: 8188 Loss: 3136.691895\n",
      "Training Batch: 8189 Loss: 3096.773438\n",
      "Training Batch: 8190 Loss: 3129.894043\n",
      "Training Batch: 8191 Loss: 3032.106201\n",
      "Training Batch: 8192 Loss: 3156.697266\n",
      "Training Batch: 8193 Loss: 3122.466797\n",
      "Training Batch: 8194 Loss: 3142.476318\n",
      "Training Batch: 8195 Loss: 3257.635010\n",
      "Training Batch: 8196 Loss: 3147.267578\n",
      "Training Batch: 8197 Loss: 3441.537109\n",
      "Training Batch: 8198 Loss: 3244.293457\n",
      "Training Batch: 8199 Loss: 3155.513672\n",
      "Training Batch: 8200 Loss: 3285.017822\n",
      "Training Batch: 8201 Loss: 3166.121582\n",
      "Training Batch: 8202 Loss: 3294.690430\n",
      "Training Batch: 8203 Loss: 3104.351807\n",
      "Training Batch: 8204 Loss: 3142.907715\n",
      "Training Batch: 8205 Loss: 3175.597168\n",
      "Training Batch: 8206 Loss: 3189.215332\n",
      "Training Batch: 8207 Loss: 3116.205322\n",
      "Training Batch: 8208 Loss: 3917.458008\n",
      "Training Batch: 8209 Loss: 3432.489502\n",
      "Training Batch: 8210 Loss: 3438.783936\n",
      "Training Batch: 8211 Loss: 3310.817383\n",
      "Training Batch: 8212 Loss: 3034.964844\n",
      "Training Batch: 8213 Loss: 3133.308105\n",
      "Training Batch: 8214 Loss: 3448.115479\n",
      "Training Batch: 8215 Loss: 3119.958496\n",
      "Training Batch: 8216 Loss: 3123.441406\n",
      "Training Batch: 8217 Loss: 3128.409668\n",
      "Training Batch: 8218 Loss: 3213.557617\n",
      "Training Batch: 8219 Loss: 3223.638184\n",
      "Training Batch: 8220 Loss: 3355.063965\n",
      "Training Batch: 8221 Loss: 3082.330078\n",
      "Training Batch: 8222 Loss: 3388.384521\n",
      "Training Batch: 8223 Loss: 3569.508301\n",
      "Training Batch: 8224 Loss: 3231.360840\n",
      "Training Batch: 8225 Loss: 3051.322266\n",
      "Training Batch: 8226 Loss: 3213.859863\n",
      "Training Batch: 8227 Loss: 3234.325684\n",
      "Training Batch: 8228 Loss: 3164.989502\n",
      "Training Batch: 8229 Loss: 3262.017578\n",
      "Training Batch: 8230 Loss: 3150.609619\n",
      "Training Batch: 8231 Loss: 3525.988770\n",
      "Training Batch: 8232 Loss: 3259.811768\n",
      "Training Batch: 8233 Loss: 3153.017578\n",
      "Training Batch: 8234 Loss: 3185.607910\n",
      "Training Batch: 8235 Loss: 3335.585205\n",
      "Training Batch: 8236 Loss: 3158.099609\n",
      "Training Batch: 8237 Loss: 3127.276855\n",
      "Training Batch: 8238 Loss: 3215.430176\n",
      "Training Batch: 8239 Loss: 3423.858398\n",
      "Training Batch: 8240 Loss: 3217.507324\n",
      "Training Batch: 8241 Loss: 3117.916016\n",
      "Training Batch: 8242 Loss: 3346.114258\n",
      "Training Batch: 8243 Loss: 3409.868652\n",
      "Training Batch: 8244 Loss: 3099.093506\n",
      "Training Batch: 8245 Loss: 3109.690430\n",
      "Training Batch: 8246 Loss: 3115.547852\n",
      "Training Batch: 8247 Loss: 3323.761963\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Batch: 8248 Loss: 3117.188232\n",
      "Training Batch: 8249 Loss: 3232.453857\n",
      "Training Batch: 8250 Loss: 3109.054199\n",
      "Training Batch: 8251 Loss: 3375.554688\n",
      "Training Batch: 8252 Loss: 3213.234375\n",
      "Training Batch: 8253 Loss: 3103.544678\n",
      "Training Batch: 8254 Loss: 3163.706299\n",
      "Training Batch: 8255 Loss: 3722.759277\n",
      "Training Batch: 8256 Loss: 3306.787354\n",
      "Training Batch: 8257 Loss: 3181.569824\n",
      "Training Batch: 8258 Loss: 3326.435791\n",
      "Training Batch: 8259 Loss: 3321.499268\n",
      "Training Batch: 8260 Loss: 3291.837891\n",
      "Training Batch: 8261 Loss: 3247.280029\n",
      "Training Batch: 8262 Loss: 3275.132568\n",
      "Training Batch: 8263 Loss: 3262.408203\n",
      "Training Batch: 8264 Loss: 3139.341797\n",
      "Training Batch: 8265 Loss: 3225.576172\n",
      "Training Batch: 8266 Loss: 3210.196533\n",
      "Training Batch: 8267 Loss: 3296.934814\n",
      "Training Batch: 8268 Loss: 3143.563965\n",
      "Training Batch: 8269 Loss: 3213.720215\n",
      "Training Batch: 8270 Loss: 3189.878906\n",
      "Training Batch: 8271 Loss: 3166.799316\n",
      "Training Batch: 8272 Loss: 3139.946777\n",
      "Training Batch: 8273 Loss: 3182.677246\n",
      "Training Batch: 8274 Loss: 3202.721191\n",
      "Training Batch: 8275 Loss: 3129.631836\n",
      "Training Batch: 8276 Loss: 3206.855225\n",
      "Training Batch: 8277 Loss: 3175.017090\n",
      "Training Batch: 8278 Loss: 3100.733398\n",
      "Training Batch: 8279 Loss: 3298.562256\n",
      "Training Batch: 8280 Loss: 3114.956543\n",
      "Training Batch: 8281 Loss: 2993.865723\n",
      "Training Batch: 8282 Loss: 3215.834717\n",
      "Training Batch: 8283 Loss: 3117.017578\n",
      "Training Batch: 8284 Loss: 3092.825439\n",
      "Training Batch: 8285 Loss: 3200.350342\n",
      "Training Batch: 8286 Loss: 3165.448486\n",
      "Training Batch: 8287 Loss: 3034.568848\n",
      "Training Batch: 8288 Loss: 3037.054688\n",
      "Training Batch: 8289 Loss: 3077.775391\n",
      "Training Batch: 8290 Loss: 3332.983887\n",
      "Training Batch: 8291 Loss: 3449.657227\n",
      "Training Batch: 8292 Loss: 3220.472656\n",
      "Training Batch: 8293 Loss: 3146.412598\n",
      "Training Batch: 8294 Loss: 3109.246094\n",
      "Training Batch: 8295 Loss: 3248.906738\n",
      "Training Batch: 8296 Loss: 3092.751465\n",
      "Training Batch: 8297 Loss: 3529.574219\n",
      "Training Batch: 8298 Loss: 3091.361328\n",
      "Training Batch: 8299 Loss: 3321.376465\n",
      "Training Batch: 8300 Loss: 3219.397217\n",
      "Training Batch: 8301 Loss: 3555.693848\n",
      "Training Batch: 8302 Loss: 3377.307373\n",
      "Training Batch: 8303 Loss: 3232.009033\n",
      "Training Batch: 8304 Loss: 3103.506836\n",
      "Training Batch: 8305 Loss: 3214.702148\n",
      "Training Batch: 8306 Loss: 3165.316406\n",
      "Training Batch: 8307 Loss: 3108.610596\n",
      "Training Batch: 8308 Loss: 3138.996094\n",
      "Training Batch: 8309 Loss: 3264.344727\n",
      "Training Batch: 8310 Loss: 3181.302246\n",
      "Training Batch: 8311 Loss: 3272.418945\n",
      "Training Batch: 8312 Loss: 3232.564941\n",
      "Training Batch: 8313 Loss: 3199.255615\n",
      "Training Batch: 8314 Loss: 3115.341064\n",
      "Training Batch: 8315 Loss: 3125.319092\n",
      "Training Batch: 8316 Loss: 3155.400879\n",
      "Training Batch: 8317 Loss: 3196.638184\n",
      "Training Batch: 8318 Loss: 3199.474365\n",
      "Training Batch: 8319 Loss: 3136.190674\n",
      "Training Batch: 8320 Loss: 3240.678711\n",
      "Training Batch: 8321 Loss: 3115.579102\n",
      "Training Batch: 8322 Loss: 3084.431641\n",
      "Training Batch: 8323 Loss: 3231.499512\n",
      "Training Batch: 8324 Loss: 3238.228516\n",
      "Training Batch: 8325 Loss: 3104.308105\n",
      "Training Batch: 8326 Loss: 3105.423340\n",
      "Training Batch: 8327 Loss: 3130.350342\n",
      "Training Batch: 8328 Loss: 3077.488770\n",
      "Training Batch: 8329 Loss: 3090.051514\n",
      "Training Batch: 8330 Loss: 3051.882080\n",
      "Training Batch: 8331 Loss: 3219.591797\n",
      "Training Batch: 8332 Loss: 3277.172363\n",
      "Training Batch: 8333 Loss: 3272.223389\n",
      "Training Batch: 8334 Loss: 3142.880371\n",
      "Training Batch: 8335 Loss: 3472.731445\n",
      "Training Batch: 8336 Loss: 3218.439453\n",
      "Training Batch: 8337 Loss: 3317.428223\n",
      "Training Batch: 8338 Loss: 3157.244629\n",
      "Training Batch: 8339 Loss: 3167.134277\n",
      "Training Batch: 8340 Loss: 3100.416016\n",
      "Training Batch: 8341 Loss: 3162.541504\n",
      "Training Batch: 8342 Loss: 3299.463867\n",
      "Training Batch: 8343 Loss: 3175.767822\n",
      "Training Batch: 8344 Loss: 3201.956055\n",
      "Training Batch: 8345 Loss: 3092.860352\n",
      "Training Batch: 8346 Loss: 3308.780273\n",
      "Training Batch: 8347 Loss: 3172.321289\n",
      "Training Batch: 8348 Loss: 3206.717773\n",
      "Training Batch: 8349 Loss: 3239.430420\n",
      "Training Batch: 8350 Loss: 3077.242188\n",
      "Training Batch: 8351 Loss: 3186.122314\n",
      "Training Batch: 8352 Loss: 3166.142822\n",
      "Training Batch: 8353 Loss: 3191.901367\n",
      "Training Batch: 8354 Loss: 3199.223633\n",
      "Training Batch: 8355 Loss: 3242.455811\n",
      "Training Batch: 8356 Loss: 3035.678711\n",
      "Training Batch: 8357 Loss: 2998.434570\n",
      "Training Batch: 8358 Loss: 3165.605713\n",
      "Training Batch: 8359 Loss: 3227.831543\n",
      "Training Batch: 8360 Loss: 3342.375000\n",
      "Training Batch: 8361 Loss: 3226.563721\n",
      "Training Batch: 8362 Loss: 3210.333008\n",
      "Training Batch: 8363 Loss: 3132.442383\n",
      "Training Batch: 8364 Loss: 3468.303223\n",
      "Training Batch: 8365 Loss: 3308.778320\n",
      "Training Batch: 8366 Loss: 3257.099121\n",
      "Training Batch: 8367 Loss: 3121.199951\n",
      "Training Batch: 8368 Loss: 3198.116699\n",
      "Training Batch: 8369 Loss: 3153.533691\n",
      "Training Batch: 8370 Loss: 3075.446289\n",
      "Training Batch: 8371 Loss: 3279.899414\n",
      "Training Batch: 8372 Loss: 3158.463379\n",
      "Training Batch: 8373 Loss: 3193.782227\n",
      "Training Batch: 8374 Loss: 3112.073242\n",
      "Training Batch: 8375 Loss: 3225.464600\n",
      "Training Batch: 8376 Loss: 3551.787842\n",
      "Training Batch: 8377 Loss: 3194.772949\n",
      "Training Batch: 8378 Loss: 3157.088379\n",
      "Training Batch: 8379 Loss: 3046.218018\n",
      "Training Batch: 8380 Loss: 3098.242188\n",
      "Training Batch: 8381 Loss: 3261.812500\n",
      "Training Batch: 8382 Loss: 3252.255371\n",
      "Training Batch: 8383 Loss: 3308.270752\n",
      "Training Batch: 8384 Loss: 3158.127441\n",
      "Training Batch: 8385 Loss: 3110.334961\n",
      "Training Batch: 8386 Loss: 3163.845215\n",
      "Training Batch: 8387 Loss: 3441.611084\n",
      "Training Batch: 8388 Loss: 3431.238770\n",
      "Training Batch: 8389 Loss: 3179.499023\n",
      "Training Batch: 8390 Loss: 3413.337891\n",
      "Training Batch: 8391 Loss: 3266.846191\n",
      "Training Batch: 8392 Loss: 3118.198730\n",
      "Training Batch: 8393 Loss: 3298.238281\n",
      "Training Batch: 8394 Loss: 3260.707031\n",
      "Training Batch: 8395 Loss: 3232.330078\n",
      "Training Batch: 8396 Loss: 3235.057129\n",
      "Training Batch: 8397 Loss: 3294.146973\n",
      "Training Batch: 8398 Loss: 3196.670898\n",
      "Training Batch: 8399 Loss: 3276.901855\n",
      "Training Batch: 8400 Loss: 3232.893311\n",
      "Training Batch: 8401 Loss: 3705.006348\n",
      "Training Batch: 8402 Loss: 3402.894043\n",
      "Training Batch: 8403 Loss: 3220.204590\n",
      "Training Batch: 8404 Loss: 3147.500000\n",
      "Training Batch: 8405 Loss: 3429.082031\n",
      "Training Batch: 8406 Loss: 3450.000488\n",
      "Training Batch: 8407 Loss: 3101.529297\n",
      "Training Batch: 8408 Loss: 3393.439209\n",
      "Training Batch: 8409 Loss: 3116.372559\n",
      "Training Batch: 8410 Loss: 3146.511963\n",
      "Training Batch: 8411 Loss: 3155.551270\n",
      "Training Batch: 8412 Loss: 3432.625000\n",
      "Training Batch: 8413 Loss: 3213.461914\n",
      "Training Batch: 8414 Loss: 3256.445312\n",
      "Training Batch: 8415 Loss: 3175.714355\n",
      "Training Batch: 8416 Loss: 3278.510254\n",
      "Training Batch: 8417 Loss: 3260.684814\n",
      "Training Batch: 8418 Loss: 3099.299561\n",
      "Training Batch: 8419 Loss: 3122.096924\n",
      "Training Batch: 8420 Loss: 3165.849121\n",
      "Training Batch: 8421 Loss: 3371.937012\n",
      "Training Batch: 8422 Loss: 3289.173340\n",
      "Training Batch: 8423 Loss: 3241.740234\n",
      "Training Batch: 8424 Loss: 3234.793457\n",
      "Training Batch: 8425 Loss: 3175.170410\n",
      "Training Batch: 8426 Loss: 3086.965820\n",
      "Training Batch: 8427 Loss: 3190.624023\n",
      "Training Batch: 8428 Loss: 3121.032471\n",
      "Training Batch: 8429 Loss: 3176.967285\n",
      "Training Batch: 8430 Loss: 3172.157715\n",
      "Training Batch: 8431 Loss: 3250.189941\n",
      "Training Batch: 8432 Loss: 3180.518311\n",
      "Training Batch: 8433 Loss: 3275.804199\n",
      "Training Batch: 8434 Loss: 3267.087402\n",
      "Training Batch: 8435 Loss: 3135.727051\n",
      "Training Batch: 8436 Loss: 3129.265137\n",
      "Training Batch: 8437 Loss: 3096.128418\n",
      "Training Batch: 8438 Loss: 3196.521240\n",
      "Training Batch: 8439 Loss: 3115.504883\n",
      "Training Batch: 8440 Loss: 3170.267090\n",
      "Training Batch: 8441 Loss: 3183.025879\n",
      "Training Batch: 8442 Loss: 3068.104492\n",
      "Training Batch: 8443 Loss: 3137.018066\n",
      "Training Batch: 8444 Loss: 3200.935791\n",
      "Training Batch: 8445 Loss: 3131.178467\n",
      "Training Batch: 8446 Loss: 3125.988281\n",
      "Training Batch: 8447 Loss: 3127.421387\n",
      "Training Batch: 8448 Loss: 3149.730469\n",
      "Training Batch: 8449 Loss: 3129.202148\n",
      "Training Batch: 8450 Loss: 3014.890137\n",
      "Training Batch: 8451 Loss: 3280.476562\n",
      "Training Batch: 8452 Loss: 3138.351074\n",
      "Training Batch: 8453 Loss: 3319.293457\n",
      "Training Batch: 8454 Loss: 3245.490723\n",
      "Training Batch: 8455 Loss: 3120.900391\n",
      "Training Batch: 8456 Loss: 3080.628906\n",
      "Training Batch: 8457 Loss: 3213.059082\n",
      "Training Batch: 8458 Loss: 3240.875977\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Batch: 8459 Loss: 3176.183105\n",
      "Training Batch: 8460 Loss: 3095.694824\n",
      "Training Batch: 8461 Loss: 3132.653076\n",
      "Training Batch: 8462 Loss: 3115.331543\n",
      "Training Batch: 8463 Loss: 3064.518066\n",
      "Training Batch: 8464 Loss: 3226.585938\n",
      "Training Batch: 8465 Loss: 3180.300293\n",
      "Training Batch: 8466 Loss: 3253.263672\n",
      "Training Batch: 8467 Loss: 3267.342285\n",
      "Training Batch: 8468 Loss: 3144.242920\n",
      "Training Batch: 8469 Loss: 3195.288330\n",
      "Training Batch: 8470 Loss: 3155.384277\n",
      "Training Batch: 8471 Loss: 3161.613770\n",
      "Training Batch: 8472 Loss: 3265.176758\n",
      "Training Batch: 8473 Loss: 3072.001953\n",
      "Training Batch: 8474 Loss: 3156.906738\n",
      "Training Batch: 8475 Loss: 3073.523926\n",
      "Training Batch: 8476 Loss: 3296.042480\n",
      "Training Batch: 8477 Loss: 3252.504395\n",
      "Training Batch: 8478 Loss: 3101.221680\n",
      "Training Batch: 8479 Loss: 3185.452148\n",
      "Training Batch: 8480 Loss: 3248.070312\n",
      "Training Batch: 8481 Loss: 3275.686523\n",
      "Training Batch: 8482 Loss: 3179.404785\n",
      "Training Batch: 8483 Loss: 3164.511230\n",
      "Training Batch: 8484 Loss: 3165.820557\n",
      "Training Batch: 8485 Loss: 3253.611328\n",
      "Training Batch: 8486 Loss: 3188.518066\n",
      "Training Batch: 8487 Loss: 3500.821533\n",
      "Training Batch: 8488 Loss: 3245.138672\n",
      "Training Batch: 8489 Loss: 3124.413086\n",
      "Training Batch: 8490 Loss: 3588.237793\n",
      "Training Batch: 8491 Loss: 3601.188965\n",
      "Training Batch: 8492 Loss: 3190.955811\n",
      "Training Batch: 8493 Loss: 3188.088867\n",
      "Training Batch: 8494 Loss: 3236.978516\n",
      "Training Batch: 8495 Loss: 3266.093262\n",
      "Training Batch: 8496 Loss: 3251.017334\n",
      "Training Batch: 8497 Loss: 3143.829590\n",
      "Training Batch: 8498 Loss: 3140.557129\n",
      "Training Batch: 8499 Loss: 3158.924316\n",
      "Training Batch: 8500 Loss: 3310.467285\n",
      "Training Batch: 8501 Loss: 3114.733398\n",
      "Training Batch: 8502 Loss: 3157.832031\n",
      "Training Batch: 8503 Loss: 3219.402832\n",
      "Training Batch: 8504 Loss: 3230.587402\n",
      "Training Batch: 8505 Loss: 3085.015137\n",
      "Training Batch: 8506 Loss: 3108.619629\n",
      "Training Batch: 8507 Loss: 3149.557617\n",
      "Training Batch: 8508 Loss: 3263.474121\n",
      "Training Batch: 8509 Loss: 3167.919189\n",
      "Training Batch: 8510 Loss: 3317.166016\n",
      "Training Batch: 8511 Loss: 3292.205811\n",
      "Training Batch: 8512 Loss: 3423.289062\n",
      "Training Batch: 8513 Loss: 3297.275879\n",
      "Training Batch: 8514 Loss: 3203.114746\n",
      "Training Batch: 8515 Loss: 3182.914795\n",
      "Training Batch: 8516 Loss: 3240.722412\n",
      "Training Batch: 8517 Loss: 3189.806641\n",
      "Training Batch: 8518 Loss: 3161.767334\n",
      "Training Batch: 8519 Loss: 3056.061035\n",
      "Training Batch: 8520 Loss: 3290.831055\n",
      "Training Batch: 8521 Loss: 3230.057617\n",
      "Training Batch: 8522 Loss: 3109.372559\n",
      "Training Batch: 8523 Loss: 3216.117920\n",
      "Training Batch: 8524 Loss: 3204.542480\n",
      "Training Batch: 8525 Loss: 3197.260498\n",
      "Training Batch: 8526 Loss: 3216.866699\n",
      "Training Batch: 8527 Loss: 3138.890625\n",
      "Training Batch: 8528 Loss: 3427.885254\n",
      "Training Batch: 8529 Loss: 3289.084961\n",
      "Training Batch: 8530 Loss: 3242.196289\n",
      "Training Batch: 8531 Loss: 3320.884277\n",
      "Training Batch: 8532 Loss: 3397.687012\n",
      "Training Batch: 8533 Loss: 3190.207275\n",
      "Training Batch: 8534 Loss: 3151.155273\n",
      "Training Batch: 8535 Loss: 3107.384033\n",
      "Training Batch: 8536 Loss: 3392.924805\n",
      "Training Batch: 8537 Loss: 3211.409180\n",
      "Training Batch: 8538 Loss: 3417.317383\n",
      "Training Batch: 8539 Loss: 3295.550537\n",
      "Training Batch: 8540 Loss: 3219.220703\n",
      "Training Batch: 8541 Loss: 3297.206543\n",
      "Training Batch: 8542 Loss: 3418.128418\n",
      "Training Batch: 8543 Loss: 3400.246582\n",
      "Training Batch: 8544 Loss: 3196.553223\n",
      "Training Batch: 8545 Loss: 3100.819336\n",
      "Training Batch: 8546 Loss: 3283.348145\n",
      "Training Batch: 8547 Loss: 3478.320312\n",
      "Training Batch: 8548 Loss: 3191.973633\n",
      "Training Batch: 8549 Loss: 3226.135742\n",
      "Training Batch: 8550 Loss: 3197.221680\n",
      "Training Batch: 8551 Loss: 3290.068848\n",
      "Training Batch: 8552 Loss: 3074.365723\n",
      "Training Batch: 8553 Loss: 3094.127441\n",
      "Training Batch: 8554 Loss: 3191.274170\n",
      "Training Batch: 8555 Loss: 3108.186035\n",
      "Training Batch: 8556 Loss: 3329.704346\n",
      "Training Batch: 8557 Loss: 3161.908691\n",
      "Training Batch: 8558 Loss: 3295.483398\n",
      "Training Batch: 8559 Loss: 3079.302246\n",
      "Training Batch: 8560 Loss: 3155.007812\n",
      "Training Batch: 8561 Loss: 3104.467773\n",
      "Training Batch: 8562 Loss: 3095.361816\n",
      "Training Batch: 8563 Loss: 3345.024658\n",
      "Training Batch: 8564 Loss: 3152.469482\n",
      "Training Batch: 8565 Loss: 3277.402588\n",
      "Training Batch: 8566 Loss: 3102.213867\n",
      "Training Batch: 8567 Loss: 3143.290527\n",
      "Training Batch: 8568 Loss: 3194.003418\n",
      "Training Batch: 8569 Loss: 3031.930908\n",
      "Training Batch: 8570 Loss: 3148.479980\n",
      "Training Batch: 8571 Loss: 3176.484863\n",
      "Training Batch: 8572 Loss: 3245.239258\n",
      "Training Batch: 8573 Loss: 3338.149902\n",
      "Training Batch: 8574 Loss: 3164.970703\n",
      "Training Batch: 8575 Loss: 3323.043457\n",
      "Training Batch: 8576 Loss: 3378.916992\n",
      "Training Batch: 8577 Loss: 3258.007812\n",
      "Training Batch: 8578 Loss: 3251.209961\n",
      "Training Batch: 8579 Loss: 3074.765625\n",
      "Training Batch: 8580 Loss: 3245.848633\n",
      "Training Batch: 8581 Loss: 3319.122070\n",
      "Training Batch: 8582 Loss: 3294.578857\n",
      "Training Batch: 8583 Loss: 3196.906494\n",
      "Training Batch: 8584 Loss: 3314.662598\n",
      "Training Batch: 8585 Loss: 3256.421387\n",
      "Training Batch: 8586 Loss: 3267.010742\n",
      "Training Batch: 8587 Loss: 3178.755859\n",
      "Training Batch: 8588 Loss: 3221.949707\n",
      "Training Batch: 8589 Loss: 3082.100098\n",
      "Training Batch: 8590 Loss: 3081.614746\n",
      "Training Batch: 8591 Loss: 3172.826904\n",
      "Training Batch: 8592 Loss: 3310.086426\n",
      "Training Batch: 8593 Loss: 3629.258301\n",
      "Training Batch: 8594 Loss: 3159.609863\n",
      "Training Batch: 8595 Loss: 3101.044434\n",
      "Training Batch: 8596 Loss: 3171.832031\n",
      "Training Batch: 8597 Loss: 3167.110840\n",
      "Training Batch: 8598 Loss: 3176.557617\n",
      "Training Batch: 8599 Loss: 3199.059814\n",
      "Training Batch: 8600 Loss: 3235.262939\n",
      "Training Batch: 8601 Loss: 3161.788086\n",
      "Training Batch: 8602 Loss: 3149.664062\n",
      "Training Batch: 8603 Loss: 3205.648438\n",
      "Training Batch: 8604 Loss: 3196.866699\n",
      "Training Batch: 8605 Loss: 3096.633789\n",
      "Training Batch: 8606 Loss: 3064.109375\n",
      "Training Batch: 8607 Loss: 3235.885742\n",
      "Training Batch: 8608 Loss: 3157.977051\n",
      "Training Batch: 8609 Loss: 3122.007812\n",
      "Training Batch: 8610 Loss: 3194.475830\n",
      "Training Batch: 8611 Loss: 3202.266113\n",
      "Training Batch: 8612 Loss: 3201.160645\n",
      "Training Batch: 8613 Loss: 3101.490234\n",
      "Training Batch: 8614 Loss: 3294.710938\n",
      "Training Batch: 8615 Loss: 3343.276123\n",
      "Training Batch: 8616 Loss: 3306.620117\n",
      "Training Batch: 8617 Loss: 3176.536133\n",
      "Training Batch: 8618 Loss: 3359.563477\n",
      "Training Batch: 8619 Loss: 3292.904053\n",
      "Training Batch: 8620 Loss: 3261.146973\n",
      "Training Batch: 8621 Loss: 3094.096191\n",
      "Training Batch: 8622 Loss: 3150.062012\n",
      "Training Batch: 8623 Loss: 3268.978760\n",
      "Training Batch: 8624 Loss: 3170.470703\n",
      "Training Batch: 8625 Loss: 3120.275879\n",
      "Training Batch: 8626 Loss: 3137.300293\n",
      "Training Batch: 8627 Loss: 3147.138184\n",
      "Training Batch: 8628 Loss: 3249.934082\n",
      "Training Batch: 8629 Loss: 3278.162598\n",
      "Training Batch: 8630 Loss: 3085.043701\n",
      "Training Batch: 8631 Loss: 3132.097168\n",
      "Training Batch: 8632 Loss: 3124.151367\n",
      "Training Batch: 8633 Loss: 3175.389160\n",
      "Training Batch: 8634 Loss: 3143.949219\n",
      "Training Batch: 8635 Loss: 3206.190918\n",
      "Training Batch: 8636 Loss: 3255.014404\n",
      "Training Batch: 8637 Loss: 3290.228027\n",
      "Training Batch: 8638 Loss: 3243.956055\n",
      "Training Batch: 8639 Loss: 3118.130371\n",
      "Training Batch: 8640 Loss: 3243.713379\n",
      "Training Batch: 8641 Loss: 3184.410889\n",
      "Training Batch: 8642 Loss: 3068.346191\n",
      "Training Batch: 8643 Loss: 3245.649902\n",
      "Training Batch: 8644 Loss: 3186.078125\n",
      "Training Batch: 8645 Loss: 3229.151367\n",
      "Training Batch: 8646 Loss: 3117.461914\n",
      "Training Batch: 8647 Loss: 3225.949707\n",
      "Training Batch: 8648 Loss: 3076.597168\n",
      "Training Batch: 8649 Loss: 3165.126465\n",
      "Training Batch: 8650 Loss: 3108.183105\n",
      "Training Batch: 8651 Loss: 3153.859375\n",
      "Training Batch: 8652 Loss: 3081.889648\n",
      "Training Batch: 8653 Loss: 3318.713867\n",
      "Training Batch: 8654 Loss: 3071.368652\n",
      "Training Batch: 8655 Loss: 3144.011719\n",
      "Training Batch: 8656 Loss: 3179.743408\n",
      "Training Batch: 8657 Loss: 3086.377441\n",
      "Training Batch: 8658 Loss: 3204.940918\n",
      "Training Batch: 8659 Loss: 3128.776367\n",
      "Training Batch: 8660 Loss: 3147.572998\n",
      "Training Batch: 8661 Loss: 3160.891113\n",
      "Training Batch: 8662 Loss: 3093.744385\n",
      "Training Batch: 8663 Loss: 3150.996582\n",
      "Training Batch: 8664 Loss: 3383.225098\n",
      "Training Batch: 8665 Loss: 3115.042969\n",
      "Training Batch: 8666 Loss: 3710.695312\n",
      "Training Batch: 8667 Loss: 3156.357422\n",
      "Training Batch: 8668 Loss: 3185.637939\n",
      "Training Batch: 8669 Loss: 3107.537109\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Batch: 8670 Loss: 3272.996582\n",
      "Training Batch: 8671 Loss: 3171.872070\n",
      "Training Batch: 8672 Loss: 3214.759277\n",
      "Training Batch: 8673 Loss: 3302.900879\n",
      "Training Batch: 8674 Loss: 3367.682373\n",
      "Training Batch: 8675 Loss: 3189.701172\n",
      "Training Batch: 8676 Loss: 3233.123291\n",
      "Training Batch: 8677 Loss: 3266.369141\n",
      "Training Batch: 8678 Loss: 3233.215820\n",
      "Training Batch: 8679 Loss: 3350.088379\n",
      "Training Batch: 8680 Loss: 3142.210938\n",
      "Training Batch: 8681 Loss: 3171.336670\n",
      "Training Batch: 8682 Loss: 3503.897949\n",
      "Training Batch: 8683 Loss: 3158.906494\n",
      "Training Batch: 8684 Loss: 3346.329102\n",
      "Training Batch: 8685 Loss: 3203.222412\n",
      "Training Batch: 8686 Loss: 3273.889648\n",
      "Training Batch: 8687 Loss: 3100.121826\n",
      "Training Batch: 8688 Loss: 3331.981445\n",
      "Training Batch: 8689 Loss: 3290.288330\n",
      "Training Batch: 8690 Loss: 3556.087891\n",
      "Training Batch: 8691 Loss: 3388.292725\n",
      "Training Batch: 8692 Loss: 3414.386719\n",
      "Training Batch: 8693 Loss: 3195.460205\n",
      "Training Batch: 8694 Loss: 3426.283203\n",
      "Training Batch: 8695 Loss: 3259.760986\n",
      "Training Batch: 8696 Loss: 3171.590332\n",
      "Training Batch: 8697 Loss: 3087.069824\n",
      "Training Batch: 8698 Loss: 3170.076660\n",
      "Training Batch: 8699 Loss: 3382.085938\n",
      "Training Batch: 8700 Loss: 3585.708496\n",
      "Training Batch: 8701 Loss: 3169.299805\n",
      "Training Batch: 8702 Loss: 3121.346680\n",
      "Training Batch: 8703 Loss: 3166.402344\n",
      "Training Batch: 8704 Loss: 3162.595947\n",
      "Training Batch: 8705 Loss: 3132.894531\n",
      "Training Batch: 8706 Loss: 3289.486816\n",
      "Training Batch: 8707 Loss: 3155.663574\n",
      "Training Batch: 8708 Loss: 3144.635254\n",
      "Training Batch: 8709 Loss: 3375.849609\n",
      "Training Batch: 8710 Loss: 3150.231934\n",
      "Training Batch: 8711 Loss: 3161.177734\n",
      "Training Batch: 8712 Loss: 3307.565918\n",
      "Training Batch: 8713 Loss: 3177.838135\n",
      "Training Batch: 8714 Loss: 3144.815186\n",
      "Training Batch: 8715 Loss: 3110.007324\n",
      "Training Batch: 8716 Loss: 3191.945068\n",
      "Training Batch: 8717 Loss: 3109.652832\n",
      "Training Batch: 8718 Loss: 3108.408203\n",
      "Training Batch: 8719 Loss: 3128.385498\n",
      "Training Batch: 8720 Loss: 3105.409424\n",
      "Training Batch: 8721 Loss: 3260.150879\n",
      "Training Batch: 8722 Loss: 3135.118408\n",
      "Training Batch: 8723 Loss: 3166.501953\n",
      "Training Batch: 8724 Loss: 3143.939453\n",
      "Training Batch: 8725 Loss: 3297.264160\n",
      "Training Batch: 8726 Loss: 3038.322266\n",
      "Training Batch: 8727 Loss: 3077.512939\n",
      "Training Batch: 8728 Loss: 3191.268066\n",
      "Training Batch: 8729 Loss: 3398.802734\n",
      "Training Batch: 8730 Loss: 3377.913574\n",
      "Training Batch: 8731 Loss: 3221.366455\n",
      "Training Batch: 8732 Loss: 3171.433594\n",
      "Training Batch: 8733 Loss: 3074.188477\n",
      "Training Batch: 8734 Loss: 3173.635498\n",
      "Training Batch: 8735 Loss: 3190.744141\n",
      "Training Batch: 8736 Loss: 3048.680664\n",
      "Training Batch: 8737 Loss: 3152.050049\n",
      "Training Batch: 8738 Loss: 3198.640137\n",
      "Training Batch: 8739 Loss: 3083.132568\n",
      "Training Batch: 8740 Loss: 3282.688965\n",
      "Training Batch: 8741 Loss: 3203.198242\n",
      "Training Batch: 8742 Loss: 3279.464844\n",
      "Training Batch: 8743 Loss: 3051.066650\n",
      "Training Batch: 8744 Loss: 3214.274414\n",
      "Training Batch: 8745 Loss: 3171.164795\n",
      "Training Batch: 8746 Loss: 3072.757324\n",
      "Training Batch: 8747 Loss: 3086.883057\n",
      "Training Batch: 8748 Loss: 3116.822754\n",
      "Training Batch: 8749 Loss: 3221.725098\n",
      "Training Batch: 8750 Loss: 3166.635498\n",
      "Training Batch: 8751 Loss: 3112.735840\n",
      "Training Batch: 8752 Loss: 3150.895508\n",
      "Training Batch: 8753 Loss: 3106.772217\n",
      "Training Batch: 8754 Loss: 3063.983398\n",
      "Training Batch: 8755 Loss: 3210.368652\n",
      "Training Batch: 8756 Loss: 3254.467285\n",
      "Training Batch: 8757 Loss: 3238.534424\n",
      "Training Batch: 8758 Loss: 3103.006348\n",
      "Training Batch: 8759 Loss: 3115.740723\n",
      "Training Batch: 8760 Loss: 3117.917480\n",
      "Training Batch: 8761 Loss: 3331.735596\n",
      "Training Batch: 8762 Loss: 3144.616211\n",
      "Training Batch: 8763 Loss: 3091.106934\n",
      "Training Batch: 8764 Loss: 3100.262695\n",
      "Training Batch: 8765 Loss: 3296.516602\n",
      "Training Batch: 8766 Loss: 3146.198730\n",
      "Training Batch: 8767 Loss: 3262.074707\n",
      "Training Batch: 8768 Loss: 3261.187012\n",
      "Training Batch: 8769 Loss: 3198.211914\n",
      "Training Batch: 8770 Loss: 3073.046387\n",
      "Training Batch: 8771 Loss: 3082.386719\n",
      "Training Batch: 8772 Loss: 3206.786133\n",
      "Training Batch: 8773 Loss: 3203.072510\n",
      "Training Batch: 8774 Loss: 3099.060547\n",
      "Training Batch: 8775 Loss: 3203.782227\n",
      "Training Batch: 8776 Loss: 3313.541504\n",
      "Training Batch: 8777 Loss: 3161.586182\n",
      "Training Batch: 8778 Loss: 3036.517090\n",
      "Training Batch: 8779 Loss: 3086.479980\n",
      "Training Batch: 8780 Loss: 3150.360596\n",
      "Training Batch: 8781 Loss: 3288.501465\n",
      "Training Batch: 8782 Loss: 3236.868896\n",
      "Training Batch: 8783 Loss: 3137.739258\n",
      "Training Batch: 8784 Loss: 3220.823242\n",
      "Training Batch: 8785 Loss: 3242.607910\n",
      "Training Batch: 8786 Loss: 3134.019043\n",
      "Training Batch: 8787 Loss: 3154.314209\n",
      "Training Batch: 8788 Loss: 3249.828857\n",
      "Training Batch: 8789 Loss: 3096.552246\n",
      "Training Batch: 8790 Loss: 3243.927979\n",
      "Training Batch: 8791 Loss: 3222.308594\n",
      "Training Batch: 8792 Loss: 3082.228516\n",
      "Training Batch: 8793 Loss: 3232.691406\n",
      "Training Batch: 8794 Loss: 3186.125000\n",
      "Training Batch: 8795 Loss: 3394.787842\n",
      "Training Batch: 8796 Loss: 3126.184570\n",
      "Training Batch: 8797 Loss: 3251.390381\n",
      "Training Batch: 8798 Loss: 3133.204590\n",
      "Training Batch: 8799 Loss: 3325.463135\n",
      "Training Batch: 8800 Loss: 3180.363770\n",
      "Training Batch: 8801 Loss: 3217.875000\n",
      "Training Batch: 8802 Loss: 3113.831543\n",
      "Training Batch: 8803 Loss: 3054.829834\n",
      "Training Batch: 8804 Loss: 3304.042480\n",
      "Training Batch: 8805 Loss: 3358.343506\n",
      "Training Batch: 8806 Loss: 3189.090088\n",
      "Training Batch: 8807 Loss: 3204.877930\n",
      "Training Batch: 8808 Loss: 3203.094727\n",
      "Training Batch: 8809 Loss: 3263.250977\n",
      "Training Batch: 8810 Loss: 3170.593506\n",
      "Training Batch: 8811 Loss: 3090.492676\n",
      "Training Batch: 8812 Loss: 3211.896973\n",
      "Training Batch: 8813 Loss: 3233.141113\n",
      "Training Batch: 8814 Loss: 3055.505859\n",
      "Training Batch: 8815 Loss: 3155.282227\n",
      "Training Batch: 8816 Loss: 3070.853516\n",
      "Training Batch: 8817 Loss: 3046.185303\n",
      "Training Batch: 8818 Loss: 3118.379639\n",
      "Training Batch: 8819 Loss: 3142.807617\n",
      "Training Batch: 8820 Loss: 3062.889404\n",
      "Training Batch: 8821 Loss: 3007.107666\n",
      "Training Batch: 8822 Loss: 3529.883789\n",
      "Training Batch: 8823 Loss: 3148.289551\n",
      "Training Batch: 8824 Loss: 3133.504150\n",
      "Training Batch: 8825 Loss: 3028.077393\n",
      "Training Batch: 8826 Loss: 3114.806152\n",
      "Training Batch: 8827 Loss: 3088.458496\n",
      "Training Batch: 8828 Loss: 3108.938965\n",
      "Training Batch: 8829 Loss: 3335.091309\n",
      "Training Batch: 8830 Loss: 3154.072754\n",
      "Training Batch: 8831 Loss: 3093.682129\n",
      "Training Batch: 8832 Loss: 3082.151611\n",
      "Training Batch: 8833 Loss: 3198.250000\n",
      "Training Batch: 8834 Loss: 3177.605957\n",
      "Training Batch: 8835 Loss: 3079.530273\n",
      "Training Batch: 8836 Loss: 3190.515381\n",
      "Training Batch: 8837 Loss: 3122.551758\n",
      "Training Batch: 8838 Loss: 3198.958984\n",
      "Training Batch: 8839 Loss: 3104.274902\n",
      "Training Batch: 8840 Loss: 3325.043945\n",
      "Training Batch: 8841 Loss: 3085.892090\n",
      "Training Batch: 8842 Loss: 3188.737793\n",
      "Training Batch: 8843 Loss: 3202.494385\n",
      "Training Batch: 8844 Loss: 3263.338379\n",
      "Training Batch: 8845 Loss: 3136.390625\n",
      "Training Batch: 8846 Loss: 3096.525879\n",
      "Training Batch: 8847 Loss: 3345.915527\n",
      "Training Batch: 8848 Loss: 3166.821777\n",
      "Training Batch: 8849 Loss: 3165.868652\n",
      "Training Batch: 8850 Loss: 3156.136230\n",
      "Training Batch: 8851 Loss: 3099.333984\n",
      "Training Batch: 8852 Loss: 3354.084961\n",
      "Training Batch: 8853 Loss: 3170.915527\n",
      "Training Batch: 8854 Loss: 3153.160156\n",
      "Training Batch: 8855 Loss: 3200.794922\n",
      "Training Batch: 8856 Loss: 3118.020020\n",
      "Training Batch: 8857 Loss: 3179.478027\n",
      "Training Batch: 8858 Loss: 3301.907227\n",
      "Training Batch: 8859 Loss: 3190.854248\n",
      "Training Batch: 8860 Loss: 3193.595215\n",
      "Training Batch: 8861 Loss: 3135.575439\n",
      "Training Batch: 8862 Loss: 3082.408691\n",
      "Training Batch: 8863 Loss: 3184.948242\n",
      "Training Batch: 8864 Loss: 3077.116211\n",
      "Training Batch: 8865 Loss: 3125.891602\n",
      "Training Batch: 8866 Loss: 3221.902100\n",
      "Training Batch: 8867 Loss: 3194.572510\n",
      "Training Batch: 8868 Loss: 3077.235352\n",
      "Training Batch: 8869 Loss: 3206.132812\n",
      "Training Batch: 8870 Loss: 3411.074219\n",
      "Training Batch: 8871 Loss: 3322.392578\n",
      "Training Batch: 8872 Loss: 3294.430176\n",
      "Training Batch: 8873 Loss: 3329.032227\n",
      "Training Batch: 8874 Loss: 3172.005859\n",
      "Training Batch: 8875 Loss: 3160.940674\n",
      "Training Batch: 8876 Loss: 3197.035645\n",
      "Training Batch: 8877 Loss: 3189.060303\n",
      "Training Batch: 8878 Loss: 3138.511230\n",
      "Training Batch: 8879 Loss: 3322.062012\n",
      "Training Batch: 8880 Loss: 3346.229492\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Batch: 8881 Loss: 3484.958252\n",
      "Training Batch: 8882 Loss: 3151.232422\n",
      "Training Batch: 8883 Loss: 3208.951904\n",
      "Training Batch: 8884 Loss: 3303.958008\n",
      "Training Batch: 8885 Loss: 3306.488037\n",
      "Training Batch: 8886 Loss: 3338.686035\n",
      "Training Batch: 8887 Loss: 3267.608398\n",
      "Training Batch: 8888 Loss: 3131.639160\n",
      "Training Batch: 8889 Loss: 3169.689941\n",
      "Training Batch: 8890 Loss: 3145.949707\n",
      "Training Batch: 8891 Loss: 3111.002686\n",
      "Training Batch: 8892 Loss: 3128.106445\n",
      "Training Batch: 8893 Loss: 3101.212402\n",
      "Training Batch: 8894 Loss: 3153.296875\n",
      "Training Batch: 8895 Loss: 3393.887207\n",
      "Training Batch: 8896 Loss: 3115.873535\n",
      "Training Batch: 8897 Loss: 3117.189453\n",
      "Training Batch: 8898 Loss: 3188.865234\n",
      "Training Batch: 8899 Loss: 3239.729980\n",
      "Training Batch: 8900 Loss: 3102.271973\n",
      "Training Batch: 8901 Loss: 3139.412842\n",
      "Training Batch: 8902 Loss: 3176.125732\n",
      "Training Batch: 8903 Loss: 3131.314941\n",
      "Training Batch: 8904 Loss: 3228.979492\n",
      "Training Batch: 8905 Loss: 3228.248047\n",
      "Training Batch: 8906 Loss: 3457.082764\n",
      "Training Batch: 8907 Loss: 3074.742676\n",
      "Training Batch: 8908 Loss: 3136.286865\n",
      "Training Batch: 8909 Loss: 3159.162598\n",
      "Training Batch: 8910 Loss: 3103.654785\n",
      "Training Batch: 8911 Loss: 3160.377930\n",
      "Training Batch: 8912 Loss: 3283.501709\n",
      "Training Batch: 8913 Loss: 3147.139648\n",
      "Training Batch: 8914 Loss: 3195.989990\n",
      "Training Batch: 8915 Loss: 3120.312988\n",
      "Training Batch: 8916 Loss: 3150.332520\n",
      "Training Batch: 8917 Loss: 3194.877197\n",
      "Training Batch: 8918 Loss: 3129.398438\n",
      "Training Batch: 8919 Loss: 3256.628662\n",
      "Training Batch: 8920 Loss: 3016.639160\n",
      "Training Batch: 8921 Loss: 3139.038086\n",
      "Training Batch: 8922 Loss: 3153.038574\n",
      "Training Batch: 8923 Loss: 3148.291992\n",
      "Training Batch: 8924 Loss: 3193.451660\n",
      "Training Batch: 8925 Loss: 3380.460449\n",
      "Training Batch: 8926 Loss: 3498.368164\n",
      "Training Batch: 8927 Loss: 3214.348145\n",
      "Training Batch: 8928 Loss: 3243.958252\n",
      "Training Batch: 8929 Loss: 3130.680664\n",
      "Training Batch: 8930 Loss: 3126.238037\n",
      "Training Batch: 8931 Loss: 3140.047852\n",
      "Training Batch: 8932 Loss: 3301.376953\n",
      "Training Batch: 8933 Loss: 3246.449219\n",
      "Training Batch: 8934 Loss: 3067.150879\n",
      "Training Batch: 8935 Loss: 3104.178223\n",
      "Training Batch: 8936 Loss: 3189.793701\n",
      "Training Batch: 8937 Loss: 3279.176514\n",
      "Training Batch: 8938 Loss: 3304.818604\n",
      "Training Batch: 8939 Loss: 3305.605469\n",
      "Training Batch: 8940 Loss: 3178.091064\n",
      "Training Batch: 8941 Loss: 3202.646729\n",
      "Training Batch: 8942 Loss: 3170.197754\n",
      "Training Batch: 8943 Loss: 3282.993164\n",
      "Training Batch: 8944 Loss: 3184.472168\n",
      "Training Batch: 8945 Loss: 3047.170898\n",
      "Training Batch: 8946 Loss: 3106.091797\n",
      "Training Batch: 8947 Loss: 3117.461426\n",
      "Training Batch: 8948 Loss: 3091.115723\n",
      "Training Batch: 8949 Loss: 3235.399658\n",
      "Training Batch: 8950 Loss: 3043.515869\n",
      "Training Batch: 8951 Loss: 3150.045166\n",
      "Training Batch: 8952 Loss: 3326.103027\n",
      "Training Batch: 8953 Loss: 3141.684326\n",
      "Training Batch: 8954 Loss: 3163.557129\n",
      "Training Batch: 8955 Loss: 3408.794922\n",
      "Training Batch: 8956 Loss: 3155.610840\n",
      "Training Batch: 8957 Loss: 3167.675293\n",
      "Training Batch: 8958 Loss: 3175.857910\n",
      "Training Batch: 8959 Loss: 3126.271973\n",
      "Training Batch: 8960 Loss: 3177.278564\n",
      "Training Batch: 8961 Loss: 3267.133301\n",
      "Training Batch: 8962 Loss: 3150.059570\n",
      "Training Batch: 8963 Loss: 3173.254395\n",
      "Training Batch: 8964 Loss: 3022.057617\n",
      "Training Batch: 8965 Loss: 3091.765381\n",
      "Training Batch: 8966 Loss: 3199.800293\n",
      "Training Batch: 8967 Loss: 3237.664307\n",
      "Training Batch: 8968 Loss: 3186.009766\n",
      "Training Batch: 8969 Loss: 3357.614746\n",
      "Training Batch: 8970 Loss: 3887.438477\n",
      "Training Batch: 8971 Loss: 3799.918701\n",
      "Training Batch: 8972 Loss: 3509.150879\n",
      "Training Batch: 8973 Loss: 3306.808105\n",
      "Training Batch: 8974 Loss: 3263.269043\n",
      "Training Batch: 8975 Loss: 3218.637207\n",
      "Training Batch: 8976 Loss: 3240.774902\n",
      "Training Batch: 8977 Loss: 3369.929443\n",
      "Training Batch: 8978 Loss: 3207.751953\n",
      "Training Batch: 8979 Loss: 3093.301514\n",
      "Training Batch: 8980 Loss: 3080.650391\n",
      "Training Batch: 8981 Loss: 3128.666992\n",
      "Training Batch: 8982 Loss: 3081.804688\n",
      "Training Batch: 8983 Loss: 3086.010010\n",
      "Training Batch: 8984 Loss: 3216.087646\n",
      "Training Batch: 8985 Loss: 3109.202148\n",
      "Training Batch: 8986 Loss: 3032.594482\n",
      "Training Batch: 8987 Loss: 3165.503662\n",
      "Training Batch: 8988 Loss: 3086.106445\n",
      "Training Batch: 8989 Loss: 3174.420898\n",
      "Training Batch: 8990 Loss: 3214.479492\n",
      "Training Batch: 8991 Loss: 3124.972656\n",
      "Training Batch: 8992 Loss: 3194.244629\n",
      "Training Batch: 8993 Loss: 3122.046387\n",
      "Training Batch: 8994 Loss: 3137.740723\n",
      "Training Batch: 8995 Loss: 3309.493164\n",
      "Training Batch: 8996 Loss: 3249.335938\n",
      "Training Batch: 8997 Loss: 3051.638672\n",
      "Training Batch: 8998 Loss: 3091.101318\n",
      "Training Batch: 8999 Loss: 3067.548828\n",
      "Training Batch: 9000 Loss: 3178.515869\n",
      "Training Batch: 9001 Loss: 3261.636230\n",
      "Training Batch: 9002 Loss: 3299.604492\n",
      "Training Batch: 9003 Loss: 3192.540039\n",
      "Training Batch: 9004 Loss: 3236.472168\n",
      "Training Batch: 9005 Loss: 3095.376465\n",
      "Training Batch: 9006 Loss: 3391.783936\n",
      "Training Batch: 9007 Loss: 3263.586914\n",
      "Training Batch: 9008 Loss: 3172.865723\n",
      "Training Batch: 9009 Loss: 3089.740234\n",
      "Training Batch: 9010 Loss: 3159.462402\n",
      "Training Batch: 9011 Loss: 3309.400879\n",
      "Training Batch: 9012 Loss: 3075.971191\n",
      "Training Batch: 9013 Loss: 3166.622070\n",
      "Training Batch: 9014 Loss: 3157.124023\n",
      "Training Batch: 9015 Loss: 3147.446777\n",
      "Training Batch: 9016 Loss: 3318.104980\n",
      "Training Batch: 9017 Loss: 3559.515137\n",
      "Training Batch: 9018 Loss: 3285.208008\n",
      "Training Batch: 9019 Loss: 3237.823975\n",
      "Training Batch: 9020 Loss: 3261.717773\n",
      "Training Batch: 9021 Loss: 3142.931396\n",
      "Training Batch: 9022 Loss: 3253.478027\n",
      "Training Batch: 9023 Loss: 3287.266113\n",
      "Training Batch: 9024 Loss: 3471.284668\n",
      "Training Batch: 9025 Loss: 3154.293457\n",
      "Training Batch: 9026 Loss: 3189.788574\n",
      "Training Batch: 9027 Loss: 3357.512451\n",
      "Training Batch: 9028 Loss: 3348.333008\n",
      "Training Batch: 9029 Loss: 3353.384033\n",
      "Training Batch: 9030 Loss: 3078.075684\n",
      "Training Batch: 9031 Loss: 3116.572510\n",
      "Training Batch: 9032 Loss: 3230.256836\n",
      "Training Batch: 9033 Loss: 3314.718262\n",
      "Training Batch: 9034 Loss: 3290.201660\n",
      "Training Batch: 9035 Loss: 3199.520996\n",
      "Training Batch: 9036 Loss: 3182.652344\n",
      "Training Batch: 9037 Loss: 3144.364990\n",
      "Training Batch: 9038 Loss: 3063.360352\n",
      "Training Batch: 9039 Loss: 3191.462402\n",
      "Training Batch: 9040 Loss: 3110.447510\n",
      "Training Batch: 9041 Loss: 3172.798096\n",
      "Training Batch: 9042 Loss: 3109.679443\n",
      "Training Batch: 9043 Loss: 3128.128418\n",
      "Training Batch: 9044 Loss: 3268.259766\n",
      "Training Batch: 9045 Loss: 3120.408691\n",
      "Training Batch: 9046 Loss: 3117.342773\n",
      "Training Batch: 9047 Loss: 3092.508057\n",
      "Training Batch: 9048 Loss: 3309.372070\n",
      "Training Batch: 9049 Loss: 3047.663086\n",
      "Training Batch: 9050 Loss: 3056.837891\n",
      "Training Batch: 9051 Loss: 3126.685547\n",
      "Training Batch: 9052 Loss: 3195.143066\n",
      "Training Batch: 9053 Loss: 3217.783691\n",
      "Training Batch: 9054 Loss: 3062.677734\n",
      "Training Batch: 9055 Loss: 3186.746582\n",
      "Training Batch: 9056 Loss: 3098.231445\n",
      "Training Batch: 9057 Loss: 3328.980957\n",
      "Training Batch: 9058 Loss: 3519.208740\n",
      "Training Batch: 9059 Loss: 3095.515381\n",
      "Training Batch: 9060 Loss: 3112.707520\n",
      "Training Batch: 9061 Loss: 3172.247559\n",
      "Training Batch: 9062 Loss: 3215.384521\n",
      "Training Batch: 9063 Loss: 3325.119141\n",
      "Training Batch: 9064 Loss: 3129.380859\n",
      "Training Batch: 9065 Loss: 3244.240234\n",
      "Training Batch: 9066 Loss: 3230.562988\n",
      "Training Batch: 9067 Loss: 3350.019287\n",
      "Training Batch: 9068 Loss: 3129.652344\n",
      "Training Batch: 9069 Loss: 3140.093262\n",
      "Training Batch: 9070 Loss: 3172.826172\n",
      "Training Batch: 9071 Loss: 3203.551025\n",
      "Training Batch: 9072 Loss: 3135.882324\n",
      "Training Batch: 9073 Loss: 3192.767090\n",
      "Training Batch: 9074 Loss: 3136.602539\n",
      "Training Batch: 9075 Loss: 3059.015381\n",
      "Training Batch: 9076 Loss: 3507.766602\n",
      "Training Batch: 9077 Loss: 3236.549561\n",
      "Training Batch: 9078 Loss: 3248.155273\n",
      "Training Batch: 9079 Loss: 3181.791992\n",
      "Training Batch: 9080 Loss: 3205.541992\n",
      "Training Batch: 9081 Loss: 3142.583496\n",
      "Training Batch: 9082 Loss: 3218.057129\n",
      "Training Batch: 9083 Loss: 3061.022461\n",
      "Training Batch: 9084 Loss: 3239.153320\n",
      "Training Batch: 9085 Loss: 3090.287109\n",
      "Training Batch: 9086 Loss: 3390.191406\n",
      "Training Batch: 9087 Loss: 3455.964600\n",
      "Training Batch: 9088 Loss: 3301.365723\n",
      "Training Batch: 9089 Loss: 3087.825684\n",
      "Training Batch: 9090 Loss: 3235.477539\n",
      "Training Batch: 9091 Loss: 3127.346680\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Batch: 9092 Loss: 3134.172852\n",
      "Training Batch: 9093 Loss: 3139.763184\n",
      "Training Batch: 9094 Loss: 3272.728516\n",
      "Training Batch: 9095 Loss: 3070.819336\n",
      "Training Batch: 9096 Loss: 3161.162842\n",
      "Training Batch: 9097 Loss: 3228.189453\n",
      "Training Batch: 9098 Loss: 3254.347900\n",
      "Training Batch: 9099 Loss: 3131.507324\n",
      "Training Batch: 9100 Loss: 3302.472412\n",
      "Training Batch: 9101 Loss: 3283.798828\n",
      "Training Batch: 9102 Loss: 3091.024902\n",
      "Training Batch: 9103 Loss: 3376.280273\n",
      "Training Batch: 9104 Loss: 3178.249023\n",
      "Training Batch: 9105 Loss: 3088.028320\n",
      "Training Batch: 9106 Loss: 3171.309326\n",
      "Training Batch: 9107 Loss: 3335.372070\n",
      "Training Batch: 9108 Loss: 3103.659180\n",
      "Training Batch: 9109 Loss: 3108.049072\n",
      "Training Batch: 9110 Loss: 3282.653320\n",
      "Training Batch: 9111 Loss: 3049.388672\n",
      "Training Batch: 9112 Loss: 3121.827148\n",
      "Training Batch: 9113 Loss: 3129.993652\n",
      "Training Batch: 9114 Loss: 3215.696289\n",
      "Training Batch: 9115 Loss: 3175.351074\n",
      "Training Batch: 9116 Loss: 3215.909668\n",
      "Training Batch: 9117 Loss: 3167.622559\n",
      "Training Batch: 9118 Loss: 3070.804199\n",
      "Training Batch: 9119 Loss: 3065.712402\n",
      "Training Batch: 9120 Loss: 3291.406738\n",
      "Training Batch: 9121 Loss: 3181.197754\n",
      "Training Batch: 9122 Loss: 3233.384033\n",
      "Training Batch: 9123 Loss: 3247.885010\n",
      "Training Batch: 9124 Loss: 3164.044922\n",
      "Training Batch: 9125 Loss: 3191.018066\n",
      "Training Batch: 9126 Loss: 3182.766113\n",
      "Training Batch: 9127 Loss: 3116.708984\n",
      "Training Batch: 9128 Loss: 3167.788330\n",
      "Training Batch: 9129 Loss: 3173.561523\n",
      "Training Batch: 9130 Loss: 3290.670898\n",
      "Training Batch: 9131 Loss: 3105.318604\n",
      "Training Batch: 9132 Loss: 3128.769531\n",
      "Training Batch: 9133 Loss: 3154.831055\n",
      "Training Batch: 9134 Loss: 3236.459961\n",
      "Training Batch: 9135 Loss: 3163.328369\n",
      "Training Batch: 9136 Loss: 3299.774902\n",
      "Training Batch: 9137 Loss: 3290.719727\n",
      "Training Batch: 9138 Loss: 3240.415039\n",
      "Training Batch: 9139 Loss: 3419.501953\n",
      "Training Batch: 9140 Loss: 3194.306641\n",
      "Training Batch: 9141 Loss: 3115.235352\n",
      "Training Batch: 9142 Loss: 3196.477295\n",
      "Training Batch: 9143 Loss: 3226.695801\n",
      "Training Batch: 9144 Loss: 3194.588867\n",
      "Training Batch: 9145 Loss: 3163.676270\n",
      "Training Batch: 9146 Loss: 3170.986328\n",
      "Training Batch: 9147 Loss: 3279.667725\n",
      "Training Batch: 9148 Loss: 3192.722656\n",
      "Training Batch: 9149 Loss: 3447.931641\n",
      "Training Batch: 9150 Loss: 3432.597168\n",
      "Training Batch: 9151 Loss: 3206.232422\n",
      "Training Batch: 9152 Loss: 3483.242432\n",
      "Training Batch: 9153 Loss: 3238.946045\n",
      "Training Batch: 9154 Loss: 3118.521484\n",
      "Training Batch: 9155 Loss: 3208.055176\n",
      "Training Batch: 9156 Loss: 3020.954834\n",
      "Training Batch: 9157 Loss: 3176.246338\n",
      "Training Batch: 9158 Loss: 3166.011230\n",
      "Training Batch: 9159 Loss: 3234.408691\n",
      "Training Batch: 9160 Loss: 3281.762695\n",
      "Training Batch: 9161 Loss: 3424.553223\n",
      "Training Batch: 9162 Loss: 3092.321289\n",
      "Training Batch: 9163 Loss: 3239.935059\n",
      "Training Batch: 9164 Loss: 3132.015625\n",
      "Training Batch: 9165 Loss: 3163.274902\n",
      "Training Batch: 9166 Loss: 3170.764160\n",
      "Training Batch: 9167 Loss: 3276.307617\n",
      "Training Batch: 9168 Loss: 3208.466309\n",
      "Training Batch: 9169 Loss: 3200.527344\n",
      "Training Batch: 9170 Loss: 3213.804688\n",
      "Training Batch: 9171 Loss: 3133.066650\n",
      "Training Batch: 9172 Loss: 3050.242188\n",
      "Training Batch: 9173 Loss: 3166.943115\n",
      "Training Batch: 9174 Loss: 3204.990234\n",
      "Training Batch: 9175 Loss: 3449.529297\n",
      "Training Batch: 9176 Loss: 3183.724121\n",
      "Training Batch: 9177 Loss: 3262.479980\n",
      "Training Batch: 9178 Loss: 3185.726074\n",
      "Training Batch: 9179 Loss: 3061.361084\n",
      "Training Batch: 9180 Loss: 3156.309570\n",
      "Training Batch: 9181 Loss: 3136.715088\n",
      "Training Batch: 9182 Loss: 3145.033691\n",
      "Training Batch: 9183 Loss: 3138.225098\n",
      "Training Batch: 9184 Loss: 3106.636230\n",
      "Training Batch: 9185 Loss: 3173.397461\n",
      "Training Batch: 9186 Loss: 3153.856445\n",
      "Training Batch: 9187 Loss: 3032.517578\n",
      "Training Batch: 9188 Loss: 3319.505859\n",
      "Training Batch: 9189 Loss: 3199.858887\n",
      "Training Batch: 9190 Loss: 3213.905029\n",
      "Training Batch: 9191 Loss: 3078.109375\n",
      "Training Batch: 9192 Loss: 3302.481445\n",
      "Training Batch: 9193 Loss: 3134.198242\n",
      "Training Batch: 9194 Loss: 3069.489014\n",
      "Training Batch: 9195 Loss: 3120.385254\n",
      "Training Batch: 9196 Loss: 3179.218262\n",
      "Training Batch: 9197 Loss: 3140.900146\n",
      "Training Batch: 9198 Loss: 3298.344238\n",
      "Training Batch: 9199 Loss: 3114.315918\n",
      "Training Batch: 9200 Loss: 3081.326172\n",
      "Training Batch: 9201 Loss: 3165.651855\n",
      "Training Batch: 9202 Loss: 3097.463379\n",
      "Training Batch: 9203 Loss: 3124.569580\n",
      "Training Batch: 9204 Loss: 3133.673096\n",
      "Training Batch: 9205 Loss: 3166.838379\n",
      "Training Batch: 9206 Loss: 3157.779785\n",
      "Training Batch: 9207 Loss: 3228.941895\n",
      "Training Batch: 9208 Loss: 3268.163574\n",
      "Training Batch: 9209 Loss: 3182.875244\n",
      "Training Batch: 9210 Loss: 3127.400391\n",
      "Training Batch: 9211 Loss: 3089.458008\n",
      "Training Batch: 9212 Loss: 3120.908447\n",
      "Training Batch: 9213 Loss: 3206.718262\n",
      "Training Batch: 9214 Loss: 3152.696045\n",
      "Training Batch: 9215 Loss: 3172.795410\n",
      "Training Batch: 9216 Loss: 3174.208984\n",
      "Training Batch: 9217 Loss: 3206.180176\n",
      "Training Batch: 9218 Loss: 3156.264648\n",
      "Training Batch: 9219 Loss: 3255.248047\n",
      "Training Batch: 9220 Loss: 3306.138672\n",
      "Training Batch: 9221 Loss: 3149.381348\n",
      "Training Batch: 9222 Loss: 3354.795898\n",
      "Training Batch: 9223 Loss: 3174.356934\n",
      "Training Batch: 9224 Loss: 3185.094238\n",
      "Training Batch: 9225 Loss: 3252.602783\n",
      "Training Batch: 9226 Loss: 3122.991211\n",
      "Training Batch: 9227 Loss: 3050.524902\n",
      "Training Batch: 9228 Loss: 3409.207031\n",
      "Training Batch: 9229 Loss: 3212.955078\n",
      "Training Batch: 9230 Loss: 3175.671875\n",
      "Training Batch: 9231 Loss: 3243.964844\n",
      "Training Batch: 9232 Loss: 3131.531250\n",
      "Training Batch: 9233 Loss: 3121.976318\n",
      "Training Batch: 9234 Loss: 3245.162354\n",
      "Training Batch: 9235 Loss: 3287.109863\n",
      "Training Batch: 9236 Loss: 3287.927734\n",
      "Training Batch: 9237 Loss: 3196.443359\n",
      "Training Batch: 9238 Loss: 3162.360840\n",
      "Training Batch: 9239 Loss: 3164.842041\n",
      "Training Batch: 9240 Loss: 3249.476074\n",
      "Training Batch: 9241 Loss: 3482.577148\n",
      "Training Batch: 9242 Loss: 3201.939209\n",
      "Training Batch: 9243 Loss: 3328.470703\n",
      "Training Batch: 9244 Loss: 3183.350586\n",
      "Training Batch: 9245 Loss: 3186.247803\n",
      "Training Batch: 9246 Loss: 3277.645020\n",
      "Training Batch: 9247 Loss: 3218.016602\n",
      "Training Batch: 9248 Loss: 3107.204590\n",
      "Training Batch: 9249 Loss: 3153.374512\n",
      "Training Batch: 9250 Loss: 3040.941895\n",
      "Training Batch: 9251 Loss: 3184.371338\n",
      "Training Batch: 9252 Loss: 3216.394531\n",
      "Training Batch: 9253 Loss: 3290.101318\n",
      "Training Batch: 9254 Loss: 3244.440186\n",
      "Training Batch: 9255 Loss: 3084.446777\n",
      "Training Batch: 9256 Loss: 3336.933105\n",
      "Training Batch: 9257 Loss: 3206.138184\n",
      "Training Batch: 9258 Loss: 3006.773438\n",
      "Training Batch: 9259 Loss: 3166.875000\n",
      "Training Batch: 9260 Loss: 3227.121826\n",
      "Training Batch: 9261 Loss: 3123.463379\n",
      "Training Batch: 9262 Loss: 3171.514648\n",
      "Training Batch: 9263 Loss: 3176.504395\n",
      "Training Batch: 9264 Loss: 3162.890625\n",
      "Training Batch: 9265 Loss: 3062.452148\n",
      "Training Batch: 9266 Loss: 3146.670898\n",
      "Training Batch: 9267 Loss: 3154.331055\n",
      "Training Batch: 9268 Loss: 3179.526367\n",
      "Training Batch: 9269 Loss: 3405.139893\n",
      "Training Batch: 9270 Loss: 3123.858398\n",
      "Training Batch: 9271 Loss: 3103.594727\n",
      "Training Batch: 9272 Loss: 3115.512939\n",
      "Training Batch: 9273 Loss: 3009.998535\n",
      "Training Batch: 9274 Loss: 3216.754395\n",
      "Training Batch: 9275 Loss: 3251.777100\n",
      "Training Batch: 9276 Loss: 3089.920166\n",
      "Training Batch: 9277 Loss: 3110.480469\n",
      "Training Batch: 9278 Loss: 3091.882568\n",
      "Training Batch: 9279 Loss: 3080.295654\n",
      "Training Batch: 9280 Loss: 3110.228760\n",
      "Training Batch: 9281 Loss: 3094.885986\n",
      "Training Batch: 9282 Loss: 3109.026123\n",
      "Training Batch: 9283 Loss: 3201.721191\n",
      "Training Batch: 9284 Loss: 3186.465576\n",
      "Training Batch: 9285 Loss: 3206.250000\n",
      "Training Batch: 9286 Loss: 3106.797363\n",
      "Training Batch: 9287 Loss: 3042.242432\n",
      "Training Batch: 9288 Loss: 3169.293457\n",
      "Training Batch: 9289 Loss: 3018.053711\n",
      "Training Batch: 9290 Loss: 3176.117920\n",
      "Training Batch: 9291 Loss: 3297.529785\n",
      "Training Batch: 9292 Loss: 3383.279785\n",
      "Training Batch: 9293 Loss: 3520.690430\n",
      "Training Batch: 9294 Loss: 3291.668213\n",
      "Training Batch: 9295 Loss: 3354.562256\n",
      "Training Batch: 9296 Loss: 3182.996582\n",
      "Training Batch: 9297 Loss: 3128.674072\n",
      "Training Batch: 9298 Loss: 3087.520264\n",
      "Training Batch: 9299 Loss: 3141.529297\n",
      "Training Batch: 9300 Loss: 3187.295898\n",
      "Training Batch: 9301 Loss: 3066.479492\n",
      "Training Batch: 9302 Loss: 3270.262695\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Batch: 9303 Loss: 3245.845947\n",
      "Training Batch: 9304 Loss: 3106.979980\n",
      "Training Batch: 9305 Loss: 3156.966797\n",
      "Training Batch: 9306 Loss: 3054.630371\n",
      "Training Batch: 9307 Loss: 3165.066162\n",
      "Training Batch: 9308 Loss: 3226.690918\n",
      "Training Batch: 9309 Loss: 3111.144775\n",
      "Training Batch: 9310 Loss: 3346.562500\n",
      "Training Batch: 9311 Loss: 3237.122559\n",
      "Training Batch: 9312 Loss: 3098.339111\n",
      "Training Batch: 9313 Loss: 3182.264404\n",
      "Training Batch: 9314 Loss: 3137.607666\n",
      "Training Batch: 9315 Loss: 3120.354492\n",
      "Training Batch: 9316 Loss: 3063.330078\n",
      "Training Batch: 9317 Loss: 3090.439697\n",
      "Training Batch: 9318 Loss: 3243.275635\n",
      "Training Batch: 9319 Loss: 3107.498047\n",
      "Training Batch: 9320 Loss: 3263.584717\n",
      "Training Batch: 9321 Loss: 3801.155762\n",
      "Training Batch: 9322 Loss: 3756.651367\n",
      "Training Batch: 9323 Loss: 3513.716309\n",
      "Training Batch: 9324 Loss: 3711.850586\n",
      "Training Batch: 9325 Loss: 3421.782715\n",
      "Training Batch: 9326 Loss: 3136.304688\n",
      "Training Batch: 9327 Loss: 3219.518555\n",
      "Training Batch: 9328 Loss: 3156.642822\n",
      "Training Batch: 9329 Loss: 3117.330566\n",
      "Training Batch: 9330 Loss: 3317.930176\n",
      "Training Batch: 9331 Loss: 3158.995117\n",
      "Training Batch: 9332 Loss: 3471.008057\n",
      "Training Batch: 9333 Loss: 3165.768555\n",
      "Training Batch: 9334 Loss: 3775.143066\n",
      "Training Batch: 9335 Loss: 3422.403809\n",
      "Training Batch: 9336 Loss: 3188.860352\n",
      "Training Batch: 9337 Loss: 3155.122070\n",
      "Training Batch: 9338 Loss: 3089.272705\n",
      "Training Batch: 9339 Loss: 3254.748047\n",
      "Training Batch: 9340 Loss: 3520.732910\n",
      "Training Batch: 9341 Loss: 3116.254883\n",
      "Training Batch: 9342 Loss: 3278.632324\n",
      "Training Batch: 9343 Loss: 3047.817383\n",
      "Training Batch: 9344 Loss: 3360.603516\n",
      "Training Batch: 9345 Loss: 3202.833008\n",
      "Training Batch: 9346 Loss: 3239.274902\n",
      "Training Batch: 9347 Loss: 3115.339355\n",
      "Training Batch: 9348 Loss: 3149.902832\n",
      "Training Batch: 9349 Loss: 3171.039551\n",
      "Training Batch: 9350 Loss: 3160.002441\n",
      "Training Batch: 9351 Loss: 3182.686035\n",
      "Training Batch: 9352 Loss: 3123.465332\n",
      "Training Batch: 9353 Loss: 3222.802490\n",
      "Training Batch: 9354 Loss: 3208.967285\n",
      "Training Batch: 9355 Loss: 3201.045898\n",
      "Training Batch: 9356 Loss: 3473.215820\n",
      "Training Batch: 9357 Loss: 3226.824707\n",
      "Training Batch: 9358 Loss: 3271.720703\n",
      "Training Batch: 9359 Loss: 3688.271484\n",
      "Training Batch: 9360 Loss: 3148.718750\n",
      "Training Batch: 9361 Loss: 3081.816162\n",
      "Training Batch: 9362 Loss: 3164.694336\n",
      "Training Batch: 9363 Loss: 3196.936035\n",
      "Training Batch: 9364 Loss: 3323.525879\n",
      "Training Batch: 9365 Loss: 3199.889648\n",
      "Training Batch: 9366 Loss: 3120.580566\n",
      "Training Batch: 9367 Loss: 3154.987305\n",
      "Training Batch: 9368 Loss: 3047.949707\n",
      "Training Batch: 9369 Loss: 3088.783447\n",
      "Training Batch: 9370 Loss: 3350.699707\n",
      "Training Batch: 9371 Loss: 3360.017090\n",
      "Training Batch: 9372 Loss: 3276.841309\n",
      "Training Batch: 9373 Loss: 3282.014160\n",
      "Training Batch: 9374 Loss: 3198.800781\n",
      "Training Batch: 9375 Loss: 3136.466553\n",
      "Training Batch: 9376 Loss: 3027.240234\n",
      "Training Batch: 9377 Loss: 3156.624756\n",
      "Training Batch: 9378 Loss: 3073.706055\n",
      "Training Batch: 9379 Loss: 3371.494629\n",
      "Training Batch: 9380 Loss: 3095.876953\n",
      "Training Batch: 9381 Loss: 3290.763672\n",
      "Training Batch: 9382 Loss: 3156.296387\n",
      "Training Batch: 9383 Loss: 3052.060547\n",
      "Training Batch: 9384 Loss: 3110.411133\n",
      "Training Batch: 9385 Loss: 3325.329590\n",
      "Training Batch: 9386 Loss: 3170.653809\n",
      "Training Batch: 9387 Loss: 3214.993408\n",
      "Training Batch: 9388 Loss: 3384.924072\n",
      "Training Batch: 9389 Loss: 3123.692383\n",
      "Training Batch: 9390 Loss: 3091.314453\n",
      "Training Batch: 9391 Loss: 3283.819824\n",
      "Training Batch: 9392 Loss: 3287.692383\n",
      "Training Batch: 9393 Loss: 3217.674072\n",
      "Training Batch: 9394 Loss: 3167.733154\n",
      "Training Batch: 9395 Loss: 3066.703125\n",
      "Training Batch: 9396 Loss: 3145.051025\n",
      "Training Batch: 9397 Loss: 3172.515137\n",
      "Training Batch: 9398 Loss: 3272.654053\n",
      "Training Batch: 9399 Loss: 3259.963379\n",
      "Training Batch: 9400 Loss: 4409.379883\n",
      "Training Batch: 9401 Loss: 3210.488770\n",
      "Training Batch: 9402 Loss: 3363.578857\n",
      "Training Batch: 9403 Loss: 3272.080322\n",
      "Training Batch: 9404 Loss: 3348.631348\n",
      "Training Batch: 9405 Loss: 3208.973145\n",
      "Training Batch: 9406 Loss: 3129.293945\n",
      "Training Batch: 9407 Loss: 3555.305176\n",
      "Training Batch: 9408 Loss: 3004.759033\n",
      "Training Batch: 9409 Loss: 3303.379395\n",
      "Training Batch: 9410 Loss: 3162.231934\n",
      "Training Batch: 9411 Loss: 3138.429199\n",
      "Training Batch: 9412 Loss: 3095.130127\n",
      "Training Batch: 9413 Loss: 3073.546875\n",
      "Training Batch: 9414 Loss: 3167.465820\n",
      "Training Batch: 9415 Loss: 3042.743652\n",
      "Training Batch: 9416 Loss: 3179.433105\n",
      "Training Batch: 9417 Loss: 3319.511719\n",
      "Training Batch: 9418 Loss: 3261.551758\n",
      "Training Batch: 9419 Loss: 3180.339844\n",
      "Training Batch: 9420 Loss: 3196.796143\n",
      "Training Batch: 9421 Loss: 3206.628906\n",
      "Training Batch: 9422 Loss: 3123.859375\n",
      "Training Batch: 9423 Loss: 3336.857910\n",
      "Training Batch: 9424 Loss: 3191.281250\n",
      "Training Batch: 9425 Loss: 3054.585449\n",
      "Training Batch: 9426 Loss: 3107.783936\n",
      "Training Batch: 9427 Loss: 3158.883301\n",
      "Training Batch: 9428 Loss: 3193.604004\n",
      "Training Batch: 9429 Loss: 3093.620850\n",
      "Training Batch: 9430 Loss: 3282.191895\n",
      "Training Batch: 9431 Loss: 3974.822510\n",
      "Training Batch: 9432 Loss: 3228.159424\n",
      "Training Batch: 9433 Loss: 3130.715332\n",
      "Training Batch: 9434 Loss: 3103.791016\n",
      "Training Batch: 9435 Loss: 3390.690918\n",
      "Training Batch: 9436 Loss: 3106.125488\n",
      "Training Batch: 9437 Loss: 3182.771240\n",
      "Training Batch: 9438 Loss: 3089.297119\n",
      "Training Batch: 9439 Loss: 3151.528564\n",
      "Training Batch: 9440 Loss: 3072.165039\n",
      "Training Batch: 9441 Loss: 3028.702637\n",
      "Training Batch: 9442 Loss: 3087.276367\n",
      "Training Batch: 9443 Loss: 3069.534424\n",
      "Training Batch: 9444 Loss: 3303.124512\n",
      "Training Batch: 9445 Loss: 3387.618652\n",
      "Training Batch: 9446 Loss: 3099.448975\n",
      "Training Batch: 9447 Loss: 3175.508301\n",
      "Training Batch: 9448 Loss: 3055.558105\n",
      "Training Batch: 9449 Loss: 3068.100586\n",
      "Training Batch: 9450 Loss: 3126.712158\n",
      "Training Batch: 9451 Loss: 3000.429199\n",
      "Training Batch: 9452 Loss: 3146.525879\n",
      "Training Batch: 9453 Loss: 3186.743896\n",
      "Training Batch: 9454 Loss: 3164.795654\n",
      "Training Batch: 9455 Loss: 3377.471924\n",
      "Training Batch: 9456 Loss: 3091.149414\n",
      "Training Batch: 9457 Loss: 3259.636719\n",
      "Training Batch: 9458 Loss: 3369.471924\n",
      "Training Batch: 9459 Loss: 3138.944092\n",
      "Training Batch: 9460 Loss: 3167.726562\n",
      "Training Batch: 9461 Loss: 3190.172363\n",
      "Training Batch: 9462 Loss: 3082.440918\n",
      "Training Batch: 9463 Loss: 3194.276367\n",
      "Training Batch: 9464 Loss: 3066.922119\n",
      "Training Batch: 9465 Loss: 3268.169922\n",
      "Training Batch: 9466 Loss: 3144.856201\n",
      "Training Batch: 9467 Loss: 3120.567871\n",
      "Training Batch: 9468 Loss: 3252.805420\n",
      "Training Batch: 9469 Loss: 3350.907715\n",
      "Training Batch: 9470 Loss: 3364.935791\n",
      "Training Batch: 9471 Loss: 3092.146973\n",
      "Training Batch: 9472 Loss: 3204.763184\n",
      "Training Batch: 9473 Loss: 3283.698242\n",
      "Training Batch: 9474 Loss: 3315.277344\n",
      "Training Batch: 9475 Loss: 3199.979004\n",
      "Training Batch: 9476 Loss: 3261.268066\n",
      "Training Batch: 9477 Loss: 3189.724609\n",
      "Training Batch: 9478 Loss: 3147.860840\n",
      "Training Batch: 9479 Loss: 3152.830566\n",
      "Training Batch: 9480 Loss: 3230.754883\n",
      "Training Batch: 9481 Loss: 3197.046631\n",
      "Training Batch: 9482 Loss: 3244.072754\n",
      "Training Batch: 9483 Loss: 3435.010254\n",
      "Training Batch: 9484 Loss: 3146.810059\n",
      "Training Batch: 9485 Loss: 3160.972656\n",
      "Training Batch: 9486 Loss: 3209.666504\n",
      "Training Batch: 9487 Loss: 3243.445801\n",
      "Training Batch: 9488 Loss: 3429.811523\n",
      "Training Batch: 9489 Loss: 3146.873047\n",
      "Training Batch: 9490 Loss: 3263.772949\n",
      "Training Batch: 9491 Loss: 3157.883789\n",
      "Training Batch: 9492 Loss: 3144.643555\n",
      "Training Batch: 9493 Loss: 3175.928711\n",
      "Training Batch: 9494 Loss: 3173.445068\n",
      "Training Batch: 9495 Loss: 3091.125977\n",
      "Training Batch: 9496 Loss: 3160.009766\n",
      "Training Batch: 9497 Loss: 3089.632812\n",
      "Training Batch: 9498 Loss: 3047.104980\n",
      "Training Batch: 9499 Loss: 3261.027100\n",
      "Training Batch: 9500 Loss: 3080.420410\n",
      "Training Batch: 9501 Loss: 3163.899414\n",
      "Training Batch: 9502 Loss: 3223.612793\n",
      "Training Batch: 9503 Loss: 3112.461426\n",
      "Training Batch: 9504 Loss: 3106.658691\n",
      "Training Batch: 9505 Loss: 3138.605469\n",
      "Training Batch: 9506 Loss: 3114.532227\n",
      "Training Batch: 9507 Loss: 3267.071289\n",
      "Training Batch: 9508 Loss: 3307.131836\n",
      "Training Batch: 9509 Loss: 3114.567383\n",
      "Training Batch: 9510 Loss: 3166.550293\n",
      "Training Batch: 9511 Loss: 3127.647949\n",
      "Training Batch: 9512 Loss: 3167.841797\n",
      "Training Batch: 9513 Loss: 3233.957031\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Batch: 9514 Loss: 3700.842529\n",
      "Training Batch: 9515 Loss: 3338.578613\n",
      "Training Batch: 9516 Loss: 3201.930664\n",
      "Training Batch: 9517 Loss: 3245.755859\n",
      "Training Batch: 9518 Loss: 3117.830811\n",
      "Training Batch: 9519 Loss: 3199.786621\n",
      "Training Batch: 9520 Loss: 3100.254395\n",
      "Training Batch: 9521 Loss: 3103.457764\n",
      "Training Batch: 9522 Loss: 3172.727295\n",
      "Training Batch: 9523 Loss: 3283.676270\n",
      "Training Batch: 9524 Loss: 3451.884033\n",
      "Training Batch: 9525 Loss: 3234.715332\n",
      "Training Batch: 9526 Loss: 3122.126709\n",
      "Training Batch: 9527 Loss: 3233.028320\n",
      "Training Batch: 9528 Loss: 3472.235596\n",
      "Training Batch: 9529 Loss: 3260.431152\n",
      "Training Batch: 9530 Loss: 3288.105957\n",
      "Training Batch: 9531 Loss: 3224.099365\n",
      "Training Batch: 9532 Loss: 3091.205078\n",
      "Training Batch: 9533 Loss: 3321.169922\n",
      "Training Batch: 9534 Loss: 3304.458984\n",
      "Training Batch: 9535 Loss: 3188.681885\n",
      "Training Batch: 9536 Loss: 3307.441406\n",
      "Training Batch: 9537 Loss: 3160.778809\n",
      "Training Batch: 9538 Loss: 3312.406738\n",
      "Training Batch: 9539 Loss: 3310.071045\n",
      "Training Batch: 9540 Loss: 3211.541016\n",
      "Training Batch: 9541 Loss: 3354.395508\n",
      "Training Batch: 9542 Loss: 3177.051025\n",
      "Training Batch: 9543 Loss: 3193.599121\n",
      "Training Batch: 9544 Loss: 3179.155518\n",
      "Training Batch: 9545 Loss: 3209.191895\n",
      "Training Batch: 9546 Loss: 3184.877441\n",
      "Training Batch: 9547 Loss: 3136.138672\n",
      "Training Batch: 9548 Loss: 3164.698730\n",
      "Training Batch: 9549 Loss: 3073.579102\n",
      "Training Batch: 9550 Loss: 3108.754883\n",
      "Training Batch: 9551 Loss: 3344.731689\n",
      "Training Batch: 9552 Loss: 3114.813477\n",
      "Training Batch: 9553 Loss: 3182.508789\n",
      "Training Batch: 9554 Loss: 3230.092529\n",
      "Training Batch: 9555 Loss: 3125.798340\n",
      "Training Batch: 9556 Loss: 3072.858887\n",
      "Training Batch: 9557 Loss: 3137.153076\n",
      "Training Batch: 9558 Loss: 3049.122070\n",
      "Training Batch: 9559 Loss: 3162.426758\n",
      "Training Batch: 9560 Loss: 3130.766113\n",
      "Training Batch: 9561 Loss: 3481.244629\n",
      "Training Batch: 9562 Loss: 3482.664307\n",
      "Training Batch: 9563 Loss: 3246.364258\n",
      "Training Batch: 9564 Loss: 3371.652832\n",
      "Training Batch: 9565 Loss: 3186.284180\n",
      "Training Batch: 9566 Loss: 3196.384766\n",
      "Training Batch: 9567 Loss: 3357.525879\n",
      "Training Batch: 9568 Loss: 3333.465820\n",
      "Training Batch: 9569 Loss: 3214.955078\n",
      "Training Batch: 9570 Loss: 3136.833496\n",
      "Training Batch: 9571 Loss: 3445.081055\n",
      "Training Batch: 9572 Loss: 3358.543945\n",
      "Training Batch: 9573 Loss: 3255.597656\n",
      "Training Batch: 9574 Loss: 3326.538086\n",
      "Training Batch: 9575 Loss: 3589.934814\n",
      "Training Batch: 9576 Loss: 3251.323486\n",
      "Training Batch: 9577 Loss: 3261.859863\n",
      "Training Batch: 9578 Loss: 3150.008789\n",
      "Training Batch: 9579 Loss: 3185.706543\n",
      "Training Batch: 9580 Loss: 3392.898438\n",
      "Training Batch: 9581 Loss: 3243.036377\n",
      "Training Batch: 9582 Loss: 3178.916504\n",
      "Training Batch: 9583 Loss: 3066.922119\n",
      "Training Batch: 9584 Loss: 3188.824707\n",
      "Training Batch: 9585 Loss: 3146.965576\n",
      "Training Batch: 9586 Loss: 3241.795410\n",
      "Training Batch: 9587 Loss: 3348.682861\n",
      "Training Batch: 9588 Loss: 3129.678711\n",
      "Training Batch: 9589 Loss: 3146.745605\n",
      "Training Batch: 9590 Loss: 3204.144287\n",
      "Training Batch: 9591 Loss: 3104.993652\n",
      "Training Batch: 9592 Loss: 3440.687500\n",
      "Training Batch: 9593 Loss: 3666.654297\n",
      "Training Batch: 9594 Loss: 3412.703857\n",
      "Training Batch: 9595 Loss: 3318.796387\n",
      "Training Batch: 9596 Loss: 3267.715332\n",
      "Training Batch: 9597 Loss: 3123.851807\n",
      "Training Batch: 9598 Loss: 3311.672363\n",
      "Training Batch: 9599 Loss: 3179.778320\n",
      "Training Batch: 9600 Loss: 3474.573730\n",
      "Training Batch: 9601 Loss: 3119.597656\n",
      "Training Batch: 9602 Loss: 3248.046387\n",
      "Training Batch: 9603 Loss: 3153.569824\n",
      "Training Batch: 9604 Loss: 3283.233398\n",
      "Training Batch: 9605 Loss: 3095.671387\n",
      "Training Batch: 9606 Loss: 3054.376953\n",
      "Training Batch: 9607 Loss: 3089.063965\n",
      "Training Batch: 9608 Loss: 3131.322266\n",
      "Training Batch: 9609 Loss: 3259.814209\n",
      "Training Batch: 9610 Loss: 3335.878906\n",
      "Training Batch: 9611 Loss: 3219.021484\n",
      "Training Batch: 9612 Loss: 3201.240723\n",
      "Training Batch: 9613 Loss: 3094.816895\n",
      "Training Batch: 9614 Loss: 3208.734863\n",
      "Training Batch: 9615 Loss: 3156.595459\n",
      "Training Batch: 9616 Loss: 3086.003662\n",
      "Training Batch: 9617 Loss: 3163.621826\n",
      "Training Batch: 9618 Loss: 3334.510742\n",
      "Training Batch: 9619 Loss: 3176.855957\n",
      "Training Batch: 9620 Loss: 3165.096680\n",
      "Training Batch: 9621 Loss: 3117.012695\n",
      "Training Batch: 9622 Loss: 3071.044922\n",
      "Training Batch: 9623 Loss: 3159.485840\n",
      "Training Batch: 9624 Loss: 3213.942871\n",
      "Training Batch: 9625 Loss: 3235.935059\n",
      "Training Batch: 9626 Loss: 3308.085449\n",
      "Training Batch: 9627 Loss: 3156.050781\n",
      "Training Batch: 9628 Loss: 3366.104492\n",
      "Training Batch: 9629 Loss: 3370.252441\n",
      "Training Batch: 9630 Loss: 3103.653076\n",
      "Training Batch: 9631 Loss: 3145.468994\n",
      "Training Batch: 9632 Loss: 3059.145996\n",
      "Training Batch: 9633 Loss: 3058.793945\n",
      "Training Batch: 9634 Loss: 3322.974121\n",
      "Training Batch: 9635 Loss: 3258.198242\n",
      "Training Batch: 9636 Loss: 3095.765381\n",
      "Training Batch: 9637 Loss: 3268.406738\n",
      "Training Batch: 9638 Loss: 3323.205566\n",
      "Training Batch: 9639 Loss: 3213.333008\n",
      "Training Batch: 9640 Loss: 3367.457520\n",
      "Training Batch: 9641 Loss: 3176.541992\n",
      "Training Batch: 9642 Loss: 3274.959229\n",
      "Training Batch: 9643 Loss: 3253.920898\n",
      "Training Batch: 9644 Loss: 3584.895508\n",
      "Training Batch: 9645 Loss: 3195.212646\n",
      "Training Batch: 9646 Loss: 3220.337402\n",
      "Training Batch: 9647 Loss: 3157.325439\n",
      "Training Batch: 9648 Loss: 3111.467773\n",
      "Training Batch: 9649 Loss: 3148.537109\n",
      "Training Batch: 9650 Loss: 3117.131104\n",
      "Training Batch: 9651 Loss: 3057.132324\n",
      "Training Batch: 9652 Loss: 3159.983887\n",
      "Training Batch: 9653 Loss: 3178.849121\n",
      "Training Batch: 9654 Loss: 3129.141113\n",
      "Training Batch: 9655 Loss: 3253.073242\n",
      "Training Batch: 9656 Loss: 3360.427246\n",
      "Training Batch: 9657 Loss: 3175.024902\n",
      "Training Batch: 9658 Loss: 3046.508057\n",
      "Training Batch: 9659 Loss: 3323.950195\n",
      "Training Batch: 9660 Loss: 3337.670654\n",
      "Training Batch: 9661 Loss: 3090.863281\n",
      "Training Batch: 9662 Loss: 3183.496338\n",
      "Training Batch: 9663 Loss: 3187.047852\n",
      "Training Batch: 9664 Loss: 3295.448242\n",
      "Training Batch: 9665 Loss: 3090.901367\n",
      "Training Batch: 9666 Loss: 3031.896240\n",
      "Training Batch: 9667 Loss: 3096.666260\n",
      "Training Batch: 9668 Loss: 3091.125488\n",
      "Training Batch: 9669 Loss: 3076.662598\n",
      "Training Batch: 9670 Loss: 3050.891602\n",
      "Training Batch: 9671 Loss: 3134.647461\n",
      "Training Batch: 9672 Loss: 3146.492188\n",
      "Training Batch: 9673 Loss: 3127.296875\n",
      "Training Batch: 9674 Loss: 3101.780518\n",
      "Training Batch: 9675 Loss: 3257.488770\n",
      "Training Batch: 9676 Loss: 3170.224854\n",
      "Training Batch: 9677 Loss: 3075.434082\n",
      "Training Batch: 9678 Loss: 3109.770264\n",
      "Training Batch: 9679 Loss: 3051.587646\n",
      "Training Batch: 9680 Loss: 3366.104980\n",
      "Training Batch: 9681 Loss: 3036.342529\n",
      "Training Batch: 9682 Loss: 3266.683105\n",
      "Training Batch: 9683 Loss: 3270.094238\n",
      "Training Batch: 9684 Loss: 3089.379883\n",
      "Training Batch: 9685 Loss: 3352.112549\n",
      "Training Batch: 9686 Loss: 3189.781738\n",
      "Training Batch: 9687 Loss: 3177.720703\n",
      "Training Batch: 9688 Loss: 3117.129883\n",
      "Training Batch: 9689 Loss: 3225.542236\n",
      "Training Batch: 9690 Loss: 3355.911133\n",
      "Training Batch: 9691 Loss: 3146.128906\n",
      "Training Batch: 9692 Loss: 3078.509277\n",
      "Training Batch: 9693 Loss: 3200.863281\n",
      "Training Batch: 9694 Loss: 3287.635742\n",
      "Training Batch: 9695 Loss: 3147.951172\n",
      "Training Batch: 9696 Loss: 3150.531738\n",
      "Training Batch: 9697 Loss: 3112.726318\n",
      "Training Batch: 9698 Loss: 3106.852051\n",
      "Training Batch: 9699 Loss: 3222.847900\n",
      "Training Batch: 9700 Loss: 3335.405029\n",
      "Training Batch: 9701 Loss: 3153.483398\n",
      "Training Batch: 9702 Loss: 3184.710205\n",
      "Training Batch: 9703 Loss: 3129.146973\n",
      "Training Batch: 9704 Loss: 3343.537598\n",
      "Training Batch: 9705 Loss: 3096.178223\n",
      "Training Batch: 9706 Loss: 3090.553467\n",
      "Training Batch: 9707 Loss: 3201.580078\n",
      "Training Batch: 9708 Loss: 3217.948730\n",
      "Training Batch: 9709 Loss: 3245.801758\n",
      "Training Batch: 9710 Loss: 3111.969238\n",
      "Training Batch: 9711 Loss: 3063.875977\n",
      "Training Batch: 9712 Loss: 3209.968994\n",
      "Training Batch: 9713 Loss: 3143.058105\n",
      "Training Batch: 9714 Loss: 3164.080078\n",
      "Training Batch: 9715 Loss: 3052.012695\n",
      "Training Batch: 9716 Loss: 3135.428711\n",
      "Training Batch: 9717 Loss: 3194.763672\n",
      "Training Batch: 9718 Loss: 3324.068115\n",
      "Training Batch: 9719 Loss: 3376.222656\n",
      "Training Batch: 9720 Loss: 3280.439941\n",
      "Training Batch: 9721 Loss: 3169.326904\n",
      "Training Batch: 9722 Loss: 3136.614990\n",
      "Training Batch: 9723 Loss: 3424.675049\n",
      "Training Batch: 9724 Loss: 3241.770996\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Batch: 9725 Loss: 3161.598877\n",
      "Training Batch: 9726 Loss: 3215.601074\n",
      "Training Batch: 9727 Loss: 3252.630859\n",
      "Training Batch: 9728 Loss: 3325.585449\n",
      "Training Batch: 9729 Loss: 3332.773926\n",
      "Training Batch: 9730 Loss: 3250.801025\n",
      "Training Batch: 9731 Loss: 3266.286377\n",
      "Training Batch: 9732 Loss: 3140.177246\n",
      "Training Batch: 9733 Loss: 3274.776123\n",
      "Training Batch: 9734 Loss: 3176.986328\n",
      "Training Batch: 9735 Loss: 3151.122559\n",
      "Training Batch: 9736 Loss: 3113.781982\n",
      "Training Batch: 9737 Loss: 3275.049316\n",
      "Training Batch: 9738 Loss: 3117.728516\n",
      "Training Batch: 9739 Loss: 3137.002930\n",
      "Training Batch: 9740 Loss: 3283.612305\n",
      "Training Batch: 9741 Loss: 3166.341797\n",
      "Training Batch: 9742 Loss: 3287.313721\n",
      "Training Batch: 9743 Loss: 3141.743652\n",
      "Training Batch: 9744 Loss: 3146.225342\n",
      "Training Batch: 9745 Loss: 3108.128662\n",
      "Training Batch: 9746 Loss: 3428.156738\n",
      "Training Batch: 9747 Loss: 3110.730469\n",
      "Training Batch: 9748 Loss: 3124.402100\n",
      "Training Batch: 9749 Loss: 3190.822266\n",
      "Training Batch: 9750 Loss: 3332.617676\n",
      "Training Batch: 9751 Loss: 3288.270752\n",
      "Training Batch: 9752 Loss: 3182.584473\n",
      "Training Batch: 9753 Loss: 3246.430176\n",
      "Training Batch: 9754 Loss: 3191.354004\n",
      "Training Batch: 9755 Loss: 3166.123535\n",
      "Training Batch: 9756 Loss: 3249.372070\n",
      "Training Batch: 9757 Loss: 3209.917969\n",
      "Training Batch: 9758 Loss: 3093.517090\n",
      "Training Batch: 9759 Loss: 3188.446777\n",
      "Training Batch: 9760 Loss: 3435.895020\n",
      "Training Batch: 9761 Loss: 3273.050781\n",
      "Training Batch: 9762 Loss: 3260.313477\n",
      "Training Batch: 9763 Loss: 3259.237549\n",
      "Training Batch: 9764 Loss: 3039.628906\n",
      "Training Batch: 9765 Loss: 3115.244629\n",
      "Training Batch: 9766 Loss: 3350.847656\n",
      "Training Batch: 9767 Loss: 3135.629395\n",
      "Training Batch: 9768 Loss: 3163.314697\n",
      "Training Batch: 9769 Loss: 3086.171143\n",
      "Training Batch: 9770 Loss: 3211.909668\n",
      "Training Batch: 9771 Loss: 3100.500000\n",
      "Training Batch: 9772 Loss: 3150.967285\n",
      "Training Batch: 9773 Loss: 3178.662842\n",
      "Training Batch: 9774 Loss: 3227.355957\n",
      "Training Batch: 9775 Loss: 3123.759033\n",
      "Training Batch: 9776 Loss: 3129.038330\n",
      "Training Batch: 9777 Loss: 3162.291016\n",
      "Training Batch: 9778 Loss: 3096.499268\n",
      "Training Batch: 9779 Loss: 3702.586670\n",
      "Training Batch: 9780 Loss: 3173.317871\n",
      "Training Batch: 9781 Loss: 3300.365234\n",
      "Training Batch: 9782 Loss: 3141.941895\n",
      "Training Batch: 9783 Loss: 3047.736816\n",
      "Training Batch: 9784 Loss: 3384.345215\n",
      "Training Batch: 9785 Loss: 3311.445801\n",
      "Training Batch: 9786 Loss: 3197.843750\n",
      "Training Batch: 9787 Loss: 3378.624756\n",
      "Training Batch: 9788 Loss: 3295.065430\n",
      "Training Batch: 9789 Loss: 3304.877930\n",
      "Training Batch: 9790 Loss: 3415.129150\n",
      "Training Batch: 9791 Loss: 3237.520996\n",
      "Training Batch: 9792 Loss: 3338.650391\n",
      "Training Batch: 9793 Loss: 3297.229492\n",
      "Training Batch: 9794 Loss: 3215.382812\n",
      "Training Batch: 9795 Loss: 3226.512451\n",
      "Training Batch: 9796 Loss: 3349.313232\n",
      "Training Batch: 9797 Loss: 3090.394287\n",
      "Training Batch: 9798 Loss: 3076.455566\n",
      "Training Batch: 9799 Loss: 3198.403320\n",
      "Training Batch: 9800 Loss: 3235.033203\n",
      "Training Batch: 9801 Loss: 3205.197266\n",
      "Training Batch: 9802 Loss: 3126.535400\n",
      "Training Batch: 9803 Loss: 3208.534180\n",
      "Training Batch: 9804 Loss: 3104.548828\n",
      "Training Batch: 9805 Loss: 3116.160156\n",
      "Training Batch: 9806 Loss: 3120.232910\n",
      "Training Batch: 9807 Loss: 3106.198730\n",
      "Training Batch: 9808 Loss: 3104.469727\n",
      "Training Batch: 9809 Loss: 3144.665527\n",
      "Training Batch: 9810 Loss: 3175.377930\n",
      "Training Batch: 9811 Loss: 3016.460938\n",
      "Training Batch: 9812 Loss: 3167.682373\n",
      "Training Batch: 9813 Loss: 3199.363281\n",
      "Training Batch: 9814 Loss: 3415.446777\n",
      "Training Batch: 9815 Loss: 3205.035156\n",
      "Training Batch: 9816 Loss: 3177.669189\n",
      "Training Batch: 9817 Loss: 3239.195312\n",
      "Training Batch: 9818 Loss: 3173.496826\n",
      "Training Batch: 9819 Loss: 3218.791992\n",
      "Training Batch: 9820 Loss: 3237.179932\n",
      "Training Batch: 9821 Loss: 3134.219238\n",
      "Training Batch: 9822 Loss: 3158.704346\n",
      "Training Batch: 9823 Loss: 3297.153076\n",
      "Training Batch: 9824 Loss: 3162.027344\n",
      "Training Batch: 9825 Loss: 3210.266357\n",
      "Training Batch: 9826 Loss: 3268.428711\n",
      "Training Batch: 9827 Loss: 3368.496582\n",
      "Training Batch: 9828 Loss: 3117.816162\n",
      "Training Batch: 9829 Loss: 3347.461426\n",
      "Training Batch: 9830 Loss: 3084.026611\n",
      "Training Batch: 9831 Loss: 3137.775635\n",
      "Training Batch: 9832 Loss: 3156.784668\n",
      "Training Batch: 9833 Loss: 3091.239258\n",
      "Training Batch: 9834 Loss: 3170.053711\n",
      "Training Batch: 9835 Loss: 3133.901367\n",
      "Training Batch: 9836 Loss: 3180.302734\n",
      "Training Batch: 9837 Loss: 3013.112549\n",
      "Training Batch: 9838 Loss: 3172.269531\n",
      "Training Batch: 9839 Loss: 3158.800537\n",
      "Training Batch: 9840 Loss: 3142.907715\n",
      "Training Batch: 9841 Loss: 3280.074707\n",
      "Training Batch: 9842 Loss: 3196.962402\n",
      "Training Batch: 9843 Loss: 3668.611328\n",
      "Training Batch: 9844 Loss: 3255.904297\n",
      "Training Batch: 9845 Loss: 3126.810547\n",
      "Training Batch: 9846 Loss: 3210.211426\n",
      "Training Batch: 9847 Loss: 3125.240967\n",
      "Training Batch: 9848 Loss: 3170.569824\n",
      "Training Batch: 9849 Loss: 3405.245361\n",
      "Training Batch: 9850 Loss: 3295.518555\n",
      "Training Batch: 9851 Loss: 3439.792725\n",
      "Training Batch: 9852 Loss: 3178.157227\n",
      "Training Batch: 9853 Loss: 3364.848633\n",
      "Training Batch: 9854 Loss: 3729.961670\n",
      "Training Batch: 9855 Loss: 3523.586670\n",
      "Training Batch: 9856 Loss: 3237.486084\n",
      "Training Batch: 9857 Loss: 3188.875488\n",
      "Training Batch: 9858 Loss: 3185.381104\n",
      "Training Batch: 9859 Loss: 3251.138672\n",
      "Training Batch: 9860 Loss: 3235.754395\n",
      "Training Batch: 9861 Loss: 3278.887695\n",
      "Training Batch: 9862 Loss: 3251.496338\n",
      "Training Batch: 9863 Loss: 3262.643555\n",
      "Training Batch: 9864 Loss: 3498.724609\n",
      "Training Batch: 9865 Loss: 3369.418457\n",
      "Training Batch: 9866 Loss: 3121.998291\n",
      "Training Batch: 9867 Loss: 3324.798096\n",
      "Training Batch: 9868 Loss: 3379.347168\n",
      "Training Batch: 9869 Loss: 3041.395508\n",
      "Training Batch: 9870 Loss: 3130.615234\n",
      "Training Batch: 9871 Loss: 3218.625000\n",
      "Training Batch: 9872 Loss: 3089.109375\n",
      "Training Batch: 9873 Loss: 3232.743164\n",
      "Training Batch: 9874 Loss: 3087.152832\n",
      "Training Batch: 9875 Loss: 3074.523438\n",
      "Training Batch: 9876 Loss: 3149.797363\n",
      "Training Batch: 9877 Loss: 3257.670410\n",
      "Training Batch: 9878 Loss: 3232.783691\n",
      "Training Batch: 9879 Loss: 3196.125977\n",
      "Training Batch: 9880 Loss: 3263.709229\n",
      "Training Batch: 9881 Loss: 3188.160156\n",
      "Training Batch: 9882 Loss: 3084.876465\n",
      "Training Batch: 9883 Loss: 3134.934082\n",
      "Training Batch: 9884 Loss: 3090.185547\n",
      "Training Batch: 9885 Loss: 3336.291504\n",
      "Training Batch: 9886 Loss: 3080.965332\n",
      "Training Batch: 9887 Loss: 3133.221191\n",
      "Training Batch: 9888 Loss: 3166.168457\n",
      "Training Batch: 9889 Loss: 3295.982422\n",
      "Training Batch: 9890 Loss: 3120.668457\n",
      "Training Batch: 9891 Loss: 3107.483398\n",
      "Training Batch: 9892 Loss: 3064.222412\n",
      "Training Batch: 9893 Loss: 3301.364990\n",
      "Training Batch: 9894 Loss: 3377.981445\n",
      "Training Batch: 9895 Loss: 3248.741699\n",
      "Training Batch: 9896 Loss: 3267.168457\n",
      "Training Batch: 9897 Loss: 3068.984863\n",
      "Training Batch: 9898 Loss: 3403.304199\n",
      "Training Batch: 9899 Loss: 3203.059082\n",
      "Training Batch: 9900 Loss: 3246.792969\n",
      "Training Batch: 9901 Loss: 3138.364014\n",
      "Training Batch: 9902 Loss: 3128.431152\n",
      "Training Batch: 9903 Loss: 3179.614502\n",
      "Training Batch: 9904 Loss: 3176.306152\n",
      "Training Batch: 9905 Loss: 3122.573730\n",
      "Training Batch: 9906 Loss: 3127.031250\n",
      "Training Batch: 9907 Loss: 3229.535645\n",
      "Training Batch: 9908 Loss: 3126.782715\n",
      "Training Batch: 9909 Loss: 3255.254395\n",
      "Training Batch: 9910 Loss: 3256.122070\n",
      "Training Batch: 9911 Loss: 3290.093994\n",
      "Training Batch: 9912 Loss: 3407.106445\n",
      "Training Batch: 9913 Loss: 3201.059570\n",
      "Training Batch: 9914 Loss: 3172.246582\n",
      "Training Batch: 9915 Loss: 3162.265381\n",
      "Training Batch: 9916 Loss: 3087.010742\n",
      "Training Batch: 9917 Loss: 3434.473145\n",
      "Training Batch: 9918 Loss: 3402.955811\n",
      "Training Batch: 9919 Loss: 3379.163574\n",
      "Training Batch: 9920 Loss: 3198.024170\n",
      "Training Batch: 9921 Loss: 3303.925781\n",
      "Training Batch: 9922 Loss: 3156.235840\n",
      "Training Batch: 9923 Loss: 3275.504395\n",
      "Training Batch: 9924 Loss: 3248.205566\n",
      "Training Batch: 9925 Loss: 3296.244141\n",
      "Training Batch: 9926 Loss: 3343.564941\n",
      "Training Batch: 9927 Loss: 3288.399414\n",
      "Training Batch: 9928 Loss: 3258.911133\n",
      "Training Batch: 9929 Loss: 3208.155762\n",
      "Training Batch: 9930 Loss: 3088.592285\n",
      "Training Batch: 9931 Loss: 3370.609375\n",
      "Training Batch: 9932 Loss: 3149.090576\n",
      "Training Batch: 9933 Loss: 3050.042236\n",
      "Training Batch: 9934 Loss: 3138.323975\n",
      "Training Batch: 9935 Loss: 3124.270508\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Batch: 9936 Loss: 3131.737793\n",
      "Training Batch: 9937 Loss: 3204.818359\n",
      "Training Batch: 9938 Loss: 3178.643555\n",
      "Training Batch: 9939 Loss: 3285.476318\n",
      "Training Batch: 9940 Loss: 3112.437500\n",
      "Training Batch: 9941 Loss: 3220.071533\n",
      "Training Batch: 9942 Loss: 3127.202637\n",
      "Training Batch: 9943 Loss: 3095.513184\n",
      "Training Batch: 9944 Loss: 3065.548828\n",
      "Training Batch: 9945 Loss: 3119.857422\n",
      "Training Batch: 9946 Loss: 3072.250244\n",
      "Training Batch: 9947 Loss: 3173.062988\n",
      "Training Batch: 9948 Loss: 3305.812256\n",
      "Training Batch: 9949 Loss: 3216.776123\n",
      "Training Batch: 9950 Loss: 3569.413574\n",
      "Training Batch: 9951 Loss: 3229.060059\n",
      "Training Batch: 9952 Loss: 3088.809082\n",
      "Training Batch: 9953 Loss: 3168.255615\n",
      "Training Batch: 9954 Loss: 3132.896973\n",
      "Training Batch: 9955 Loss: 3126.634766\n",
      "Training Batch: 9956 Loss: 3215.020508\n",
      "Training Batch: 9957 Loss: 3175.290283\n",
      "Training Batch: 9958 Loss: 3224.188477\n",
      "Training Batch: 9959 Loss: 3189.411133\n",
      "Training Batch: 9960 Loss: 3400.514160\n",
      "Training Batch: 9961 Loss: 3286.748779\n",
      "Training Batch: 9962 Loss: 3173.037842\n",
      "Training Batch: 9963 Loss: 3157.096924\n",
      "Training Batch: 9964 Loss: 3241.691895\n",
      "Training Batch: 9965 Loss: 3033.804199\n",
      "Training Batch: 9966 Loss: 3042.341309\n",
      "Training Batch: 9967 Loss: 3172.887207\n",
      "Training Batch: 9968 Loss: 3128.090332\n",
      "Training Batch: 9969 Loss: 3163.596191\n",
      "Training Batch: 9970 Loss: 3248.162109\n",
      "Training Batch: 9971 Loss: 3541.919922\n",
      "Training Batch: 9972 Loss: 3210.322266\n",
      "Training Batch: 9973 Loss: 3029.560547\n",
      "Training Batch: 9974 Loss: 3143.048828\n",
      "Training Batch: 9975 Loss: 3138.916504\n",
      "Training Batch: 9976 Loss: 3445.556152\n",
      "Training Batch: 9977 Loss: 3274.552490\n",
      "Training Batch: 9978 Loss: 3092.518799\n",
      "Training Batch: 9979 Loss: 3154.703125\n",
      "Training Batch: 9980 Loss: 3135.036621\n",
      "Training Batch: 9981 Loss: 3081.178711\n",
      "Training Batch: 9982 Loss: 3146.387451\n",
      "Training Batch: 9983 Loss: 3065.509766\n",
      "Training Batch: 9984 Loss: 3372.364746\n",
      "Training Batch: 9985 Loss: 3299.096191\n",
      "Training Batch: 9986 Loss: 3124.839355\n",
      "Training Batch: 9987 Loss: 3201.008301\n",
      "Training Batch: 9988 Loss: 3143.929688\n",
      "Training Batch: 9989 Loss: 3069.463379\n",
      "Training Batch: 9990 Loss: 3163.436279\n",
      "Training Batch: 9991 Loss: 3163.597168\n",
      "Training Batch: 9992 Loss: 3197.073975\n",
      "Training Batch: 9993 Loss: 3101.771973\n",
      "Training Batch: 9994 Loss: 3038.162109\n",
      "Training Batch: 9995 Loss: 3307.254883\n",
      "Training Batch: 9996 Loss: 3087.007812\n",
      "Training Batch: 9997 Loss: 3348.951172\n",
      "Training Batch: 9998 Loss: 3261.244141\n",
      "Training Batch: 9999 Loss: 3060.206787\n",
      "Training Batch: 10000 Loss: 3251.775879\n",
      "Training Batch: 10001 Loss: 3132.104736\n",
      "Training Batch: 10002 Loss: 3098.732910\n",
      "Training Batch: 10003 Loss: 3132.879395\n",
      "Training Batch: 10004 Loss: 3088.711426\n",
      "Training Batch: 10005 Loss: 3222.555664\n",
      "Training Batch: 10006 Loss: 3223.000732\n",
      "Training Batch: 10007 Loss: 3229.740479\n",
      "Training Batch: 10008 Loss: 3164.340820\n",
      "Training Batch: 10009 Loss: 3087.113770\n",
      "Training Batch: 10010 Loss: 3284.430420\n",
      "Training Batch: 10011 Loss: 3283.753418\n",
      "Training Batch: 10012 Loss: 3088.769043\n",
      "Training Batch: 10013 Loss: 3465.157471\n",
      "Training Batch: 10014 Loss: 3688.725586\n",
      "Training Batch: 10015 Loss: 3666.933105\n",
      "Training Batch: 10016 Loss: 3365.820801\n",
      "Training Batch: 10017 Loss: 3319.453613\n",
      "Training Batch: 10018 Loss: 3178.690430\n",
      "Training Batch: 10019 Loss: 3149.096191\n",
      "Training Batch: 10020 Loss: 3098.542480\n",
      "Training Batch: 10021 Loss: 3333.882324\n",
      "Training Batch: 10022 Loss: 3189.289551\n",
      "Training Batch: 10023 Loss: 3164.681641\n",
      "Training Batch: 10024 Loss: 3280.608887\n",
      "Training Batch: 10025 Loss: 3099.138672\n",
      "Training Batch: 10026 Loss: 3236.712646\n",
      "Training Batch: 10027 Loss: 3169.972900\n",
      "Training Batch: 10028 Loss: 3123.644531\n",
      "Training Batch: 10029 Loss: 3052.697021\n",
      "Training Batch: 10030 Loss: 3242.071289\n",
      "Training Batch: 10031 Loss: 3091.654297\n",
      "Training Batch: 10032 Loss: 3127.666748\n",
      "Training Batch: 10033 Loss: 3266.719238\n",
      "Training Batch: 10034 Loss: 3151.975586\n",
      "Training Batch: 10035 Loss: 3228.420410\n",
      "Training Batch: 10036 Loss: 3101.576172\n",
      "Training Batch: 10037 Loss: 3135.559570\n",
      "Training Batch: 10038 Loss: 3054.090332\n",
      "Training Batch: 10039 Loss: 3211.327148\n",
      "Training Batch: 10040 Loss: 3185.905273\n",
      "Training Batch: 10041 Loss: 3333.553223\n",
      "Training Batch: 10042 Loss: 3301.952881\n",
      "Training Batch: 10043 Loss: 3263.869141\n",
      "Training Batch: 10044 Loss: 3179.538086\n",
      "Training Batch: 10045 Loss: 3142.238281\n",
      "Training Batch: 10046 Loss: 3281.887207\n",
      "Training Batch: 10047 Loss: 3230.576172\n",
      "Training Batch: 10048 Loss: 3131.139160\n",
      "Training Batch: 10049 Loss: 3376.478516\n",
      "Training Batch: 10050 Loss: 3297.720215\n",
      "Training Batch: 10051 Loss: 3078.171143\n",
      "Training Batch: 10052 Loss: 3172.806152\n",
      "Training Batch: 10053 Loss: 3517.919922\n",
      "Training Batch: 10054 Loss: 3079.824951\n",
      "Training Batch: 10055 Loss: 3046.418457\n",
      "Training Batch: 10056 Loss: 3109.725098\n",
      "Training Batch: 10057 Loss: 3209.964355\n",
      "Training Batch: 10058 Loss: 3513.599121\n",
      "Training Batch: 10059 Loss: 3180.359375\n",
      "Training Batch: 10060 Loss: 3295.743164\n",
      "Training Batch: 10061 Loss: 3177.459961\n",
      "Training Batch: 10062 Loss: 3233.365234\n",
      "Training Batch: 10063 Loss: 3320.295898\n",
      "Training Batch: 10064 Loss: 3047.165527\n",
      "Training Batch: 10065 Loss: 3204.374023\n",
      "Training Batch: 10066 Loss: 3451.162109\n",
      "Training Batch: 10067 Loss: 3294.739746\n",
      "Training Batch: 10068 Loss: 3379.409180\n",
      "Training Batch: 10069 Loss: 3808.663330\n",
      "Training Batch: 10070 Loss: 3317.028320\n",
      "Training Batch: 10071 Loss: 3684.443115\n",
      "Training Batch: 10072 Loss: 3159.923828\n",
      "Training Batch: 10073 Loss: 3365.531738\n",
      "Training Batch: 10074 Loss: 3195.822266\n",
      "Training Batch: 10075 Loss: 3151.172119\n",
      "Training Batch: 10076 Loss: 3265.718018\n",
      "Training Batch: 10077 Loss: 3391.020996\n",
      "Training Batch: 10078 Loss: 3365.156738\n",
      "Training Batch: 10079 Loss: 3275.024414\n",
      "Training Batch: 10080 Loss: 3286.808594\n",
      "Training Batch: 10081 Loss: 3119.794434\n",
      "Training Batch: 10082 Loss: 3086.118652\n",
      "Training Batch: 10083 Loss: 3139.991455\n",
      "Training Batch: 10084 Loss: 3213.500488\n",
      "Training Batch: 10085 Loss: 3243.211426\n",
      "Training Batch: 10086 Loss: 3063.157227\n",
      "Training Batch: 10087 Loss: 3362.270508\n",
      "Training Batch: 10088 Loss: 3111.356445\n",
      "Training Batch: 10089 Loss: 3648.054688\n",
      "Training Batch: 10090 Loss: 3306.255859\n",
      "Training Batch: 10091 Loss: 3220.971436\n",
      "Training Batch: 10092 Loss: 3142.036133\n",
      "Training Batch: 10093 Loss: 3093.595703\n",
      "Training Batch: 10094 Loss: 3183.934814\n",
      "Training Batch: 10095 Loss: 3165.753174\n",
      "Training Batch: 10096 Loss: 3233.093018\n",
      "Training Batch: 10097 Loss: 3180.867188\n",
      "Training Batch: 10098 Loss: 3258.124512\n",
      "Training Batch: 10099 Loss: 3142.278320\n",
      "Training Batch: 10100 Loss: 3208.229492\n",
      "Training Batch: 10101 Loss: 3394.744385\n",
      "Training Batch: 10102 Loss: 3229.411865\n",
      "Training Batch: 10103 Loss: 3033.264160\n",
      "Training Batch: 10104 Loss: 3296.554688\n",
      "Training Batch: 10105 Loss: 3129.433105\n",
      "Training Batch: 10106 Loss: 3275.790527\n",
      "Training Batch: 10107 Loss: 3155.468018\n",
      "Training Batch: 10108 Loss: 3127.129395\n",
      "Training Batch: 10109 Loss: 3104.684082\n",
      "Training Batch: 10110 Loss: 3000.625000\n",
      "Training Batch: 10111 Loss: 3066.479492\n",
      "Training Batch: 10112 Loss: 3260.417480\n",
      "Training Batch: 10113 Loss: 3147.127686\n",
      "Training Batch: 10114 Loss: 3094.568359\n",
      "Training Batch: 10115 Loss: 3065.258789\n",
      "Training Batch: 10116 Loss: 3218.076904\n",
      "Training Batch: 10117 Loss: 3180.987793\n",
      "Training Batch: 10118 Loss: 3169.468994\n",
      "Training Batch: 10119 Loss: 3217.042480\n",
      "Training Batch: 10120 Loss: 3212.059082\n",
      "Training Batch: 10121 Loss: 3068.435059\n",
      "Training Batch: 10122 Loss: 3179.131592\n",
      "Training Batch: 10123 Loss: 3220.588379\n",
      "Training Batch: 10124 Loss: 3299.641113\n",
      "Training Batch: 10125 Loss: 3309.927734\n",
      "Training Batch: 10126 Loss: 3117.157227\n",
      "Training Batch: 10127 Loss: 3083.897217\n",
      "Training Batch: 10128 Loss: 3294.526855\n",
      "Training Batch: 10129 Loss: 3374.090576\n",
      "Training Batch: 10130 Loss: 3293.083008\n",
      "Training Batch: 10131 Loss: 3101.995117\n",
      "Training Batch: 10132 Loss: 3172.171631\n",
      "Training Batch: 10133 Loss: 3208.943848\n",
      "Training Batch: 10134 Loss: 3151.549805\n",
      "Training Batch: 10135 Loss: 3317.440918\n",
      "Training Batch: 10136 Loss: 3455.154785\n",
      "Training Batch: 10137 Loss: 3566.634277\n",
      "Training Batch: 10138 Loss: 3283.134277\n",
      "Training Batch: 10139 Loss: 3177.142578\n",
      "Training Batch: 10140 Loss: 3203.785645\n",
      "Training Batch: 10141 Loss: 3316.729492\n",
      "Training Batch: 10142 Loss: 3123.284180\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Batch: 10143 Loss: 3092.254395\n",
      "Training Batch: 10144 Loss: 3115.001465\n",
      "Training Batch: 10145 Loss: 3085.413818\n",
      "Training Batch: 10146 Loss: 3267.976562\n",
      "Training Batch: 10147 Loss: 3233.383301\n",
      "Training Batch: 10148 Loss: 3157.819092\n",
      "Training Batch: 10149 Loss: 3382.076416\n",
      "Training Batch: 10150 Loss: 3316.415039\n",
      "Training Batch: 10151 Loss: 3142.321777\n",
      "Training Batch: 10152 Loss: 3446.949707\n",
      "Training Batch: 10153 Loss: 3269.313477\n",
      "Training Batch: 10154 Loss: 3589.427734\n",
      "Training Batch: 10155 Loss: 3151.261230\n",
      "Training Batch: 10156 Loss: 3293.085205\n",
      "Training Batch: 10157 Loss: 3376.763184\n",
      "Training Batch: 10158 Loss: 3096.238770\n",
      "Training Batch: 10159 Loss: 3028.173340\n",
      "Training Batch: 10160 Loss: 3204.064697\n",
      "Training Batch: 10161 Loss: 3094.720215\n",
      "Training Batch: 10162 Loss: 3248.344238\n",
      "Training Batch: 10163 Loss: 3198.138672\n",
      "Training Batch: 10164 Loss: 3178.874023\n",
      "Training Batch: 10165 Loss: 3202.256836\n",
      "Training Batch: 10166 Loss: 3195.534668\n",
      "Training Batch: 10167 Loss: 3256.551270\n",
      "Training Batch: 10168 Loss: 3090.147461\n",
      "Training Batch: 10169 Loss: 3231.922363\n",
      "Training Batch: 10170 Loss: 3047.343750\n",
      "Training Batch: 10171 Loss: 3067.993164\n",
      "Training Batch: 10172 Loss: 3249.672852\n",
      "Training Batch: 10173 Loss: 3179.001953\n",
      "Training Batch: 10174 Loss: 3246.907227\n",
      "Training Batch: 10175 Loss: 3116.959961\n",
      "Training Batch: 10176 Loss: 3108.400879\n",
      "Training Batch: 10177 Loss: 3109.044922\n",
      "Training Batch: 10178 Loss: 3264.074707\n",
      "Training Batch: 10179 Loss: 3253.903809\n",
      "Training Batch: 10180 Loss: 3331.304688\n",
      "Training Batch: 10181 Loss: 3140.431152\n",
      "Training Batch: 10182 Loss: 3301.660156\n",
      "Training Batch: 10183 Loss: 3238.102539\n",
      "Training Batch: 10184 Loss: 3152.042480\n",
      "Training Batch: 10185 Loss: 3221.438477\n",
      "Training Batch: 10186 Loss: 3211.916748\n",
      "Training Batch: 10187 Loss: 3001.562012\n",
      "Training Batch: 10188 Loss: 3081.964355\n",
      "Training Batch: 10189 Loss: 3071.449219\n",
      "Training Batch: 10190 Loss: 3185.720703\n",
      "Training Batch: 10191 Loss: 3286.596924\n",
      "Training Batch: 10192 Loss: 3273.836182\n",
      "Training Batch: 10193 Loss: 3218.602539\n",
      "Training Batch: 10194 Loss: 3190.167236\n",
      "Training Batch: 10195 Loss: 3143.850586\n",
      "Training Batch: 10196 Loss: 3172.735840\n",
      "Training Batch: 10197 Loss: 3072.650879\n",
      "Training Batch: 10198 Loss: 3351.000732\n",
      "Training Batch: 10199 Loss: 3181.506836\n",
      "Training Batch: 10200 Loss: 3085.849609\n",
      "Training Batch: 10201 Loss: 3095.614990\n",
      "Training Batch: 10202 Loss: 3329.755371\n",
      "Training Batch: 10203 Loss: 3151.166504\n",
      "Training Batch: 10204 Loss: 3313.075684\n",
      "Training Batch: 10205 Loss: 3162.102539\n",
      "Training Batch: 10206 Loss: 3322.707031\n",
      "Training Batch: 10207 Loss: 3465.987793\n",
      "Training Batch: 10208 Loss: 3133.590820\n",
      "Training Batch: 10209 Loss: 3168.729736\n",
      "Training Batch: 10210 Loss: 3546.597168\n",
      "Training Batch: 10211 Loss: 3254.366699\n",
      "Training Batch: 10212 Loss: 3156.651855\n",
      "Training Batch: 10213 Loss: 3315.746338\n",
      "Training Batch: 10214 Loss: 3216.228271\n",
      "Training Batch: 10215 Loss: 3154.813477\n",
      "Training Batch: 10216 Loss: 3241.424072\n",
      "Training Batch: 10217 Loss: 3163.229736\n",
      "Training Batch: 10218 Loss: 3255.520508\n",
      "Training Batch: 10219 Loss: 3163.352783\n",
      "Training Batch: 10220 Loss: 3103.766113\n",
      "Training Batch: 10221 Loss: 3196.345215\n",
      "Training Batch: 10222 Loss: 3170.451172\n",
      "Training Batch: 10223 Loss: 3241.675537\n",
      "Training Batch: 10224 Loss: 3300.346680\n",
      "Training Batch: 10225 Loss: 3283.702637\n",
      "Training Batch: 10226 Loss: 3146.148926\n",
      "Training Batch: 10227 Loss: 3344.304199\n",
      "Training Batch: 10228 Loss: 3270.249512\n",
      "Training Batch: 10229 Loss: 3228.411133\n",
      "Training Batch: 10230 Loss: 3170.419922\n",
      "Training Batch: 10231 Loss: 3369.935547\n",
      "Training Batch: 10232 Loss: 3085.011963\n",
      "Training Batch: 10233 Loss: 3059.701660\n",
      "Training Batch: 10234 Loss: 3204.207031\n",
      "Training Batch: 10235 Loss: 3166.145020\n",
      "Training Batch: 10236 Loss: 3176.502441\n",
      "Training Batch: 10237 Loss: 3379.356445\n",
      "Training Batch: 10238 Loss: 3280.272949\n",
      "Training Batch: 10239 Loss: 3348.333740\n",
      "Training Batch: 10240 Loss: 3409.658203\n",
      "Training Batch: 10241 Loss: 3405.961182\n",
      "Training Batch: 10242 Loss: 3117.834473\n",
      "Training Batch: 10243 Loss: 3099.517090\n",
      "Training Batch: 10244 Loss: 3110.688477\n",
      "Training Batch: 10245 Loss: 3159.221680\n",
      "Training Batch: 10246 Loss: 3236.444824\n",
      "Training Batch: 10247 Loss: 3075.765625\n",
      "Training Batch: 10248 Loss: 3276.274414\n",
      "Training Batch: 10249 Loss: 3290.213867\n",
      "Training Batch: 10250 Loss: 3051.490723\n",
      "Training Batch: 10251 Loss: 3102.174805\n",
      "Training Batch: 10252 Loss: 3273.511475\n",
      "Training Batch: 10253 Loss: 3206.762207\n",
      "Training Batch: 10254 Loss: 3047.847412\n",
      "Training Batch: 10255 Loss: 3124.066406\n",
      "Training Batch: 10256 Loss: 3182.495605\n",
      "Training Batch: 10257 Loss: 3074.253418\n",
      "Training Batch: 10258 Loss: 3090.588867\n",
      "Training Batch: 10259 Loss: 3153.393555\n",
      "Training Batch: 10260 Loss: 3176.176514\n",
      "Training Batch: 10261 Loss: 3115.325195\n",
      "Training Batch: 10262 Loss: 3363.202881\n",
      "Training Batch: 10263 Loss: 3205.242188\n",
      "Training Batch: 10264 Loss: 3144.547852\n",
      "Training Batch: 10265 Loss: 3038.416504\n",
      "Training Batch: 10266 Loss: 3035.469238\n",
      "Training Batch: 10267 Loss: 3197.299561\n",
      "Training Batch: 10268 Loss: 3135.726074\n",
      "Training Batch: 10269 Loss: 3178.308105\n",
      "Training Batch: 10270 Loss: 3347.598633\n",
      "Training Batch: 10271 Loss: 3061.807129\n",
      "Training Batch: 10272 Loss: 3060.890869\n",
      "Training Batch: 10273 Loss: 3127.082520\n",
      "Training Batch: 10274 Loss: 3155.406250\n",
      "Training Batch: 10275 Loss: 3090.775879\n",
      "Training Batch: 10276 Loss: 3162.357422\n",
      "Training Batch: 10277 Loss: 3171.024170\n",
      "Training Batch: 10278 Loss: 3094.897949\n",
      "Training Batch: 10279 Loss: 3069.938965\n",
      "Training Batch: 10280 Loss: 3151.814453\n",
      "Training Batch: 10281 Loss: 3223.204102\n",
      "Training Batch: 10282 Loss: 3079.838379\n",
      "Training Batch: 10283 Loss: 3168.776123\n",
      "Training Batch: 10284 Loss: 3356.530762\n",
      "Training Batch: 10285 Loss: 3164.230469\n",
      "Training Batch: 10286 Loss: 3244.543457\n",
      "Training Batch: 10287 Loss: 3285.104492\n",
      "Training Batch: 10288 Loss: 3167.542236\n",
      "Training Batch: 10289 Loss: 3429.427246\n",
      "Training Batch: 10290 Loss: 3268.634766\n",
      "Training Batch: 10291 Loss: 3230.321045\n",
      "Training Batch: 10292 Loss: 3284.967773\n",
      "Training Batch: 10293 Loss: 3146.952393\n",
      "Training Batch: 10294 Loss: 3150.212891\n",
      "Training Batch: 10295 Loss: 3313.466797\n",
      "Training Batch: 10296 Loss: 3204.953613\n",
      "Training Batch: 10297 Loss: 3172.087402\n",
      "Training Batch: 10298 Loss: 3314.903076\n",
      "Training Batch: 10299 Loss: 3152.977295\n",
      "Training Batch: 10300 Loss: 3048.598633\n",
      "Training Batch: 10301 Loss: 3212.563477\n",
      "Training Batch: 10302 Loss: 3162.897461\n",
      "Training Batch: 10303 Loss: 3317.401367\n",
      "Training Batch: 10304 Loss: 3502.323242\n",
      "Training Batch: 10305 Loss: 3347.313477\n",
      "Training Batch: 10306 Loss: 3120.762695\n",
      "Training Batch: 10307 Loss: 3218.913086\n",
      "Training Batch: 10308 Loss: 3150.409180\n",
      "Training Batch: 10309 Loss: 3142.898438\n",
      "Training Batch: 10310 Loss: 3180.133545\n",
      "Training Batch: 10311 Loss: 3261.026367\n",
      "Training Batch: 10312 Loss: 3378.441162\n",
      "Training Batch: 10313 Loss: 3471.760742\n",
      "Training Batch: 10314 Loss: 3419.801758\n",
      "Training Batch: 10315 Loss: 3101.857422\n",
      "Training Batch: 10316 Loss: 3200.056396\n",
      "Training Batch: 10317 Loss: 3270.575195\n",
      "Training Batch: 10318 Loss: 3191.613770\n",
      "Training Batch: 10319 Loss: 3130.381348\n",
      "Training Batch: 10320 Loss: 3244.760010\n",
      "Training Batch: 10321 Loss: 3068.341797\n",
      "Training Batch: 10322 Loss: 3116.816895\n",
      "Training Batch: 10323 Loss: 3186.847656\n",
      "Training Batch: 10324 Loss: 3163.857910\n",
      "Training Batch: 10325 Loss: 3196.522461\n",
      "Training Batch: 10326 Loss: 3112.782227\n",
      "Training Batch: 10327 Loss: 3215.822754\n",
      "Training Batch: 10328 Loss: 3070.745605\n",
      "Training Batch: 10329 Loss: 3159.446045\n",
      "Training Batch: 10330 Loss: 3161.833496\n",
      "Training Batch: 10331 Loss: 3177.933594\n",
      "Training Batch: 10332 Loss: 3143.616699\n",
      "Training Batch: 10333 Loss: 3285.427979\n",
      "Training Batch: 10334 Loss: 3062.375977\n",
      "Training Batch: 10335 Loss: 3417.695312\n",
      "Training Batch: 10336 Loss: 3090.600586\n",
      "Training Batch: 10337 Loss: 3157.558350\n",
      "Training Batch: 10338 Loss: 3552.326416\n",
      "Training Batch: 10339 Loss: 3426.665527\n",
      "Training Batch: 10340 Loss: 3180.641113\n",
      "Training Batch: 10341 Loss: 3186.641113\n",
      "Training Batch: 10342 Loss: 3139.852539\n",
      "Training Batch: 10343 Loss: 3158.130371\n",
      "Training Batch: 10344 Loss: 3024.276855\n",
      "Training Batch: 10345 Loss: 3225.435059\n",
      "Training Batch: 10346 Loss: 3127.115479\n",
      "Training Batch: 10347 Loss: 3232.585449\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Batch: 10348 Loss: 3229.120361\n",
      "Training Batch: 10349 Loss: 3301.078613\n",
      "Training Batch: 10350 Loss: 3138.754395\n",
      "Training Batch: 10351 Loss: 3270.214844\n",
      "Training Batch: 10352 Loss: 3196.753662\n",
      "Training Batch: 10353 Loss: 3134.379395\n",
      "Training Batch: 10354 Loss: 3220.096191\n",
      "Training Batch: 10355 Loss: 3298.719727\n",
      "Training Batch: 10356 Loss: 3166.668457\n",
      "Training Batch: 10357 Loss: 3271.062500\n",
      "Training Batch: 10358 Loss: 3249.605469\n",
      "Training Batch: 10359 Loss: 3214.912109\n",
      "Training Batch: 10360 Loss: 3282.087891\n",
      "Training Batch: 10361 Loss: 3225.654297\n",
      "Training Batch: 10362 Loss: 3275.730469\n",
      "Training Batch: 10363 Loss: 3209.417969\n",
      "Training Batch: 10364 Loss: 3166.816406\n",
      "Training Batch: 10365 Loss: 3191.057129\n",
      "Training Batch: 10366 Loss: 3104.998535\n",
      "Training Batch: 10367 Loss: 3102.494141\n",
      "Training Batch: 10368 Loss: 3073.097900\n",
      "Training Batch: 10369 Loss: 3097.848633\n",
      "Training Batch: 10370 Loss: 3056.101562\n",
      "Training Batch: 10371 Loss: 3354.249023\n",
      "Training Batch: 10372 Loss: 3320.126465\n",
      "Training Batch: 10373 Loss: 3180.448730\n",
      "Training Batch: 10374 Loss: 3068.148926\n",
      "Training Batch: 10375 Loss: 3153.572266\n",
      "Training Batch: 10376 Loss: 3308.493652\n",
      "Training Batch: 10377 Loss: 3221.736816\n",
      "Training Batch: 10378 Loss: 3212.807617\n",
      "Training Batch: 10379 Loss: 3104.491699\n",
      "Training Batch: 10380 Loss: 3255.309082\n",
      "Training Batch: 10381 Loss: 3137.441162\n",
      "Training Batch: 10382 Loss: 3186.259033\n",
      "Training Batch: 10383 Loss: 3116.109375\n",
      "Training Batch: 10384 Loss: 3155.479004\n",
      "Training Batch: 10385 Loss: 3149.145020\n",
      "Training Batch: 10386 Loss: 3238.924805\n",
      "Training Batch: 10387 Loss: 3132.120605\n",
      "Training Batch: 10388 Loss: 3222.463379\n",
      "Training Batch: 10389 Loss: 3206.265625\n",
      "Training Batch: 10390 Loss: 3810.975098\n",
      "Training Batch: 10391 Loss: 3390.896973\n",
      "Training Batch: 10392 Loss: 3318.725098\n",
      "Training Batch: 10393 Loss: 3093.084961\n",
      "Training Batch: 10394 Loss: 3102.940186\n",
      "Training Batch: 10395 Loss: 3202.741699\n",
      "Training Batch: 10396 Loss: 3116.982910\n",
      "Training Batch: 10397 Loss: 3161.797852\n",
      "Training Batch: 10398 Loss: 3367.060059\n",
      "Training Batch: 10399 Loss: 3342.252930\n",
      "Training Batch: 10400 Loss: 3305.296387\n",
      "Training Batch: 10401 Loss: 3357.824707\n",
      "Training Batch: 10402 Loss: 3258.267578\n",
      "Training Batch: 10403 Loss: 3347.844727\n",
      "Training Batch: 10404 Loss: 3190.651367\n",
      "Training Batch: 10405 Loss: 3097.770752\n",
      "Training Batch: 10406 Loss: 3111.927734\n",
      "Training Batch: 10407 Loss: 3210.067871\n",
      "Training Batch: 10408 Loss: 3183.762451\n",
      "Training Batch: 10409 Loss: 3221.537598\n",
      "Training Batch: 10410 Loss: 3150.652588\n",
      "Training Batch: 10411 Loss: 3222.359863\n",
      "Training Batch: 10412 Loss: 3138.229492\n",
      "Training Batch: 10413 Loss: 3145.406250\n",
      "Training Batch: 10414 Loss: 3113.203125\n",
      "Training Batch: 10415 Loss: 3015.864746\n",
      "Training Batch: 10416 Loss: 3189.472656\n",
      "Training Batch: 10417 Loss: 3139.999756\n",
      "Training Batch: 10418 Loss: 3075.968750\n",
      "Training Batch: 10419 Loss: 3054.396973\n",
      "Training Batch: 10420 Loss: 3162.256836\n",
      "Training Batch: 10421 Loss: 3155.537109\n",
      "Training Batch: 10422 Loss: 3160.998047\n",
      "Training Batch: 10423 Loss: 3240.338867\n",
      "Training Batch: 10424 Loss: 3120.400146\n",
      "Training Batch: 10425 Loss: 3204.849854\n",
      "Training Batch: 10426 Loss: 3273.099609\n",
      "Training Batch: 10427 Loss: 3262.467041\n",
      "Training Batch: 10428 Loss: 3167.880371\n",
      "Training Batch: 10429 Loss: 3715.500244\n",
      "Training Batch: 10430 Loss: 3207.788086\n",
      "Training Batch: 10431 Loss: 3115.491943\n",
      "Training Batch: 10432 Loss: 3143.575195\n",
      "Training Batch: 10433 Loss: 3118.009521\n",
      "Training Batch: 10434 Loss: 3176.735107\n",
      "Training Batch: 10435 Loss: 3125.049316\n",
      "Training Batch: 10436 Loss: 3244.034668\n",
      "Training Batch: 10437 Loss: 3161.479492\n",
      "Training Batch: 10438 Loss: 3291.332031\n",
      "Training Batch: 10439 Loss: 3140.830078\n",
      "Training Batch: 10440 Loss: 3175.716309\n",
      "Training Batch: 10441 Loss: 3247.050293\n",
      "Training Batch: 10442 Loss: 3139.009277\n",
      "Training Batch: 10443 Loss: 3163.134277\n",
      "Training Batch: 10444 Loss: 3105.208496\n",
      "Training Batch: 10445 Loss: 3046.073486\n",
      "Training Batch: 10446 Loss: 3063.769531\n",
      "Training Batch: 10447 Loss: 3210.360596\n",
      "Training Batch: 10448 Loss: 3026.912109\n",
      "Training Batch: 10449 Loss: 3211.783691\n",
      "Training Batch: 10450 Loss: 3082.940918\n",
      "Training Batch: 10451 Loss: 3167.560059\n",
      "Training Batch: 10452 Loss: 3138.286377\n",
      "Training Batch: 10453 Loss: 3010.616211\n",
      "Training Batch: 10454 Loss: 3053.529785\n",
      "Training Batch: 10455 Loss: 3034.146729\n",
      "Training Batch: 10456 Loss: 3122.936768\n",
      "Training Batch: 10457 Loss: 3075.676758\n",
      "Training Batch: 10458 Loss: 3012.740723\n",
      "Training Batch: 10459 Loss: 3183.113770\n",
      "Training Batch: 10460 Loss: 3105.582520\n",
      "Training Batch: 10461 Loss: 3137.864258\n",
      "Training Batch: 10462 Loss: 3161.247070\n",
      "Training Batch: 10463 Loss: 3108.306152\n",
      "Training Batch: 10464 Loss: 3086.222656\n",
      "Training Batch: 10465 Loss: 3125.642334\n",
      "Training Batch: 10466 Loss: 3161.860352\n",
      "Training Batch: 10467 Loss: 3081.507324\n",
      "Training Batch: 10468 Loss: 3307.786133\n",
      "Training Batch: 10469 Loss: 3370.401855\n",
      "Training Batch: 10470 Loss: 3101.899902\n",
      "Training Batch: 10471 Loss: 3100.119141\n",
      "Training Batch: 10472 Loss: 3103.843750\n",
      "Training Batch: 10473 Loss: 3333.844971\n",
      "Training Batch: 10474 Loss: 3193.381592\n",
      "Training Batch: 10475 Loss: 3224.291992\n",
      "Training Batch: 10476 Loss: 3182.620117\n",
      "Training Batch: 10477 Loss: 3117.556152\n",
      "Training Batch: 10478 Loss: 3337.153809\n",
      "Training Batch: 10479 Loss: 3316.422852\n",
      "Training Batch: 10480 Loss: 3246.401855\n",
      "Training Batch: 10481 Loss: 3105.366699\n",
      "Training Batch: 10482 Loss: 3395.442383\n",
      "Training Batch: 10483 Loss: 3147.967773\n",
      "Training Batch: 10484 Loss: 3259.074219\n",
      "Training Batch: 10485 Loss: 3195.147217\n",
      "Training Batch: 10486 Loss: 3096.604980\n",
      "Training Batch: 10487 Loss: 3170.734375\n",
      "Training Batch: 10488 Loss: 3326.045898\n",
      "Training Batch: 10489 Loss: 3190.256836\n",
      "Training Batch: 10490 Loss: 3369.188477\n",
      "Training Batch: 10491 Loss: 3255.772705\n",
      "Training Batch: 10492 Loss: 3069.187988\n",
      "Training Batch: 10493 Loss: 3153.437012\n",
      "Training Batch: 10494 Loss: 3227.186279\n",
      "Training Batch: 10495 Loss: 3219.794922\n",
      "Training Batch: 10496 Loss: 3149.714844\n",
      "Training Batch: 10497 Loss: 3231.708008\n",
      "Training Batch: 10498 Loss: 3186.821777\n",
      "Training Batch: 10499 Loss: 3242.302246\n",
      "Training Batch: 10500 Loss: 3283.211426\n",
      "Training Batch: 10501 Loss: 3222.221924\n",
      "Training Batch: 10502 Loss: 3205.219727\n",
      "Training Batch: 10503 Loss: 3537.603516\n",
      "Training Batch: 10504 Loss: 3202.311035\n",
      "Training Batch: 10505 Loss: 3150.851318\n",
      "Training Batch: 10506 Loss: 3055.674805\n",
      "Training Batch: 10507 Loss: 3195.066406\n",
      "Training Batch: 10508 Loss: 3202.194580\n",
      "Training Batch: 10509 Loss: 3144.898438\n",
      "Training Batch: 10510 Loss: 3134.616699\n",
      "Training Batch: 10511 Loss: 3063.273926\n",
      "Training Batch: 10512 Loss: 3414.031250\n",
      "Training Batch: 10513 Loss: 3245.888916\n",
      "Training Batch: 10514 Loss: 3226.873047\n",
      "Training Batch: 10515 Loss: 3201.298828\n",
      "Training Batch: 10516 Loss: 3212.227539\n",
      "Training Batch: 10517 Loss: 3224.699707\n",
      "Training Batch: 10518 Loss: 3060.842773\n",
      "Training Batch: 10519 Loss: 3337.903320\n",
      "Training Batch: 10520 Loss: 3244.953125\n",
      "Training Batch: 10521 Loss: 3265.657715\n",
      "Training Batch: 10522 Loss: 3315.544434\n",
      "Training Batch: 10523 Loss: 3164.702148\n",
      "Training Batch: 10524 Loss: 3071.005859\n",
      "Training Batch: 10525 Loss: 3133.947754\n",
      "Training Batch: 10526 Loss: 3543.243164\n",
      "Training Batch: 10527 Loss: 3152.278564\n",
      "Training Batch: 10528 Loss: 3283.276611\n",
      "Training Batch: 10529 Loss: 3314.157959\n",
      "Training Batch: 10530 Loss: 3103.039551\n",
      "Training Batch: 10531 Loss: 3082.159180\n",
      "Training Batch: 10532 Loss: 3363.965332\n",
      "Training Batch: 10533 Loss: 3323.742188\n",
      "Training Batch: 10534 Loss: 3106.923584\n",
      "Training Batch: 10535 Loss: 3092.641113\n",
      "Training Batch: 10536 Loss: 3069.109863\n",
      "Training Batch: 10537 Loss: 3059.333252\n",
      "Training Batch: 10538 Loss: 3155.123047\n",
      "Training Batch: 10539 Loss: 3187.485840\n",
      "Training Batch: 10540 Loss: 3098.893066\n",
      "Training Batch: 10541 Loss: 3030.676270\n",
      "Training Batch: 10542 Loss: 2977.073730\n",
      "Training Batch: 10543 Loss: 3035.436523\n",
      "Training Batch: 10544 Loss: 3138.630371\n",
      "Training Batch: 10545 Loss: 3069.332520\n",
      "Training Batch: 10546 Loss: 3200.651367\n",
      "Training Batch: 10547 Loss: 3148.158936\n",
      "Training Batch: 10548 Loss: 3132.964844\n",
      "Training Batch: 10549 Loss: 3048.252930\n",
      "Training Batch: 10550 Loss: 3255.697021\n",
      "Training Batch: 10551 Loss: 3201.630371\n",
      "Training Batch: 10552 Loss: 3190.467041\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Batch: 10553 Loss: 3163.625244\n",
      "Training Batch: 10554 Loss: 3194.330566\n",
      "Training Batch: 10555 Loss: 3301.992676\n",
      "Training Batch: 10556 Loss: 3362.541504\n",
      "Training Batch: 10557 Loss: 3109.097656\n",
      "Training Batch: 10558 Loss: 3015.954834\n",
      "Training Batch: 10559 Loss: 3345.359863\n",
      "Training Batch: 10560 Loss: 3060.363770\n",
      "Training Batch: 10561 Loss: 3084.192383\n",
      "Training Batch: 10562 Loss: 3180.910645\n",
      "Training Batch: 10563 Loss: 3131.697754\n",
      "Training Batch: 10564 Loss: 3181.718018\n",
      "Training Batch: 10565 Loss: 3063.867920\n",
      "Training Batch: 10566 Loss: 3372.710938\n",
      "Training Batch: 10567 Loss: 3220.535889\n",
      "Training Batch: 10568 Loss: 3116.237305\n",
      "Training Batch: 10569 Loss: 3164.768555\n",
      "Training Batch: 10570 Loss: 3161.759277\n",
      "Training Batch: 10571 Loss: 3056.441895\n",
      "Training Batch: 10572 Loss: 3164.140137\n",
      "Training Batch: 10573 Loss: 3219.873047\n",
      "Training Batch: 10574 Loss: 3199.165527\n",
      "Training Batch: 10575 Loss: 3177.753418\n",
      "Training Batch: 10576 Loss: 3157.394531\n",
      "Training Batch: 10577 Loss: 3253.921875\n",
      "Training Batch: 10578 Loss: 3122.314453\n",
      "Training Batch: 10579 Loss: 3157.673584\n",
      "Training Batch: 10580 Loss: 3282.423340\n",
      "Training Batch: 10581 Loss: 3448.910156\n",
      "Training Batch: 10582 Loss: 3206.547363\n",
      "Training Batch: 10583 Loss: 3395.901855\n",
      "Training Batch: 10584 Loss: 3287.230469\n",
      "Training Batch: 10585 Loss: 3203.814941\n",
      "Training Batch: 10586 Loss: 3074.080078\n",
      "Training Batch: 10587 Loss: 3127.347412\n",
      "Training Batch: 10588 Loss: 3251.669189\n",
      "Training Batch: 10589 Loss: 3139.280762\n",
      "Training Batch: 10590 Loss: 3210.310303\n",
      "Training Batch: 10591 Loss: 3161.908203\n",
      "Training Batch: 10592 Loss: 3085.050781\n",
      "Training Batch: 10593 Loss: 3043.131836\n",
      "Training Batch: 10594 Loss: 3029.889893\n",
      "Training Batch: 10595 Loss: 3012.011230\n",
      "Training Batch: 10596 Loss: 3073.974609\n",
      "Training Batch: 10597 Loss: 3142.371582\n",
      "Training Batch: 10598 Loss: 3113.992188\n",
      "Training Batch: 10599 Loss: 3015.078857\n",
      "Training Batch: 10600 Loss: 3302.142578\n",
      "Training Batch: 10601 Loss: 3116.606445\n",
      "Training Batch: 10602 Loss: 3057.884033\n",
      "Training Batch: 10603 Loss: 3155.149414\n",
      "Training Batch: 10604 Loss: 3452.270996\n",
      "Training Batch: 10605 Loss: 3203.333984\n",
      "Training Batch: 10606 Loss: 3319.553223\n",
      "Training Batch: 10607 Loss: 3083.395996\n",
      "Training Batch: 10608 Loss: 3033.140137\n",
      "Training Batch: 10609 Loss: 3155.018066\n",
      "Training Batch: 10610 Loss: 3099.155762\n",
      "Training Batch: 10611 Loss: 3111.596680\n",
      "Training Batch: 10612 Loss: 3244.786133\n",
      "Training Batch: 10613 Loss: 3173.656738\n",
      "Training Batch: 10614 Loss: 3285.158203\n",
      "Training Batch: 10615 Loss: 3222.985352\n",
      "Training Batch: 10616 Loss: 3032.740723\n",
      "Training Batch: 10617 Loss: 3061.467773\n",
      "Training Batch: 10618 Loss: 3048.341309\n",
      "Training Batch: 10619 Loss: 3086.421875\n",
      "Training Batch: 10620 Loss: 3459.765625\n",
      "Training Batch: 10621 Loss: 3332.454590\n",
      "Training Batch: 10622 Loss: 3315.449707\n",
      "Training Batch: 10623 Loss: 3165.982910\n",
      "Training Batch: 10624 Loss: 3270.801514\n",
      "Training Batch: 10625 Loss: 3240.403564\n",
      "Training Batch: 10626 Loss: 3153.622070\n",
      "Training Batch: 10627 Loss: 3087.866211\n",
      "Training Batch: 10628 Loss: 3165.016602\n",
      "Training Batch: 10629 Loss: 3098.314453\n",
      "Training Batch: 10630 Loss: 3105.593506\n",
      "Training Batch: 10631 Loss: 3080.502197\n",
      "Training Batch: 10632 Loss: 3092.272949\n",
      "Training Batch: 10633 Loss: 3245.623779\n",
      "Training Batch: 10634 Loss: 3051.299316\n",
      "Training Batch: 10635 Loss: 3194.066406\n",
      "Training Batch: 10636 Loss: 3049.837891\n",
      "Training Batch: 10637 Loss: 3217.065186\n",
      "Training Batch: 10638 Loss: 3755.098877\n",
      "Training Batch: 10639 Loss: 3157.406738\n",
      "Training Batch: 10640 Loss: 3118.448242\n",
      "Training Batch: 10641 Loss: 3257.810059\n",
      "Training Batch: 10642 Loss: 3053.060059\n",
      "Training Batch: 10643 Loss: 3190.136719\n",
      "Training Batch: 10644 Loss: 3133.462158\n",
      "Training Batch: 10645 Loss: 3032.415039\n",
      "Training Batch: 10646 Loss: 3251.669434\n",
      "Training Batch: 10647 Loss: 3189.937256\n",
      "Training Batch: 10648 Loss: 3147.234375\n",
      "Training Batch: 10649 Loss: 3250.886230\n",
      "Training Batch: 10650 Loss: 3222.662598\n",
      "Training Batch: 10651 Loss: 3200.825684\n",
      "Training Batch: 10652 Loss: 3262.903809\n",
      "Training Batch: 10653 Loss: 3221.173828\n",
      "Training Batch: 10654 Loss: 3112.355957\n",
      "Training Batch: 10655 Loss: 3280.989014\n",
      "Training Batch: 10656 Loss: 3299.657227\n",
      "Training Batch: 10657 Loss: 3103.669434\n",
      "Training Batch: 10658 Loss: 3046.878906\n",
      "Training Batch: 10659 Loss: 3135.364746\n",
      "Training Batch: 10660 Loss: 3146.009521\n",
      "Training Batch: 10661 Loss: 3201.400391\n",
      "Training Batch: 10662 Loss: 3109.213867\n",
      "Training Batch: 10663 Loss: 3275.305420\n",
      "Training Batch: 10664 Loss: 3058.415039\n",
      "Training Batch: 10665 Loss: 3158.395264\n",
      "Training Batch: 10666 Loss: 2999.987793\n",
      "Training Batch: 10667 Loss: 3251.106689\n",
      "Training Batch: 10668 Loss: 3109.659180\n",
      "Training Batch: 10669 Loss: 3363.886963\n",
      "Training Batch: 10670 Loss: 3293.897461\n",
      "Training Batch: 10671 Loss: 3224.972656\n",
      "Training Batch: 10672 Loss: 3084.906738\n",
      "Training Batch: 10673 Loss: 3222.377930\n",
      "Training Batch: 10674 Loss: 3275.763916\n",
      "Training Batch: 10675 Loss: 3225.594238\n",
      "Training Batch: 10676 Loss: 3321.166992\n",
      "Training Batch: 10677 Loss: 3149.520020\n",
      "Training Batch: 10678 Loss: 3092.778320\n",
      "Training Batch: 10679 Loss: 3350.341309\n",
      "Training Batch: 10680 Loss: 3200.769043\n",
      "Training Batch: 10681 Loss: 3157.112305\n",
      "Training Batch: 10682 Loss: 3829.473877\n",
      "Training Batch: 10683 Loss: 3285.262939\n",
      "Training Batch: 10684 Loss: 3275.582275\n",
      "Training Batch: 10685 Loss: 3261.457031\n",
      "Training Batch: 10686 Loss: 3182.237305\n",
      "Training Batch: 10687 Loss: 3200.357910\n",
      "Training Batch: 10688 Loss: 3261.172363\n",
      "Training Batch: 10689 Loss: 3199.571777\n",
      "Training Batch: 10690 Loss: 3097.181641\n",
      "Training Batch: 10691 Loss: 3251.291016\n",
      "Training Batch: 10692 Loss: 3155.679199\n",
      "Training Batch: 10693 Loss: 3292.296143\n",
      "Training Batch: 10694 Loss: 3081.653809\n",
      "Training Batch: 10695 Loss: 3111.894043\n",
      "Training Batch: 10696 Loss: 3214.750244\n",
      "Training Batch: 10697 Loss: 3118.193848\n",
      "Training Batch: 10698 Loss: 3132.312500\n",
      "Training Batch: 10699 Loss: 3143.142090\n",
      "Training Batch: 10700 Loss: 3092.748047\n",
      "Training Batch: 10701 Loss: 3198.741211\n",
      "Training Batch: 10702 Loss: 3374.457031\n",
      "Training Batch: 10703 Loss: 3313.609863\n",
      "Training Batch: 10704 Loss: 3161.725098\n",
      "Training Batch: 10705 Loss: 3112.821777\n",
      "Training Batch: 10706 Loss: 3201.096436\n",
      "Training Batch: 10707 Loss: 3235.672852\n",
      "Training Batch: 10708 Loss: 3166.822266\n",
      "Training Batch: 10709 Loss: 3171.573242\n",
      "Training Batch: 10710 Loss: 3321.904297\n",
      "Training Batch: 10711 Loss: 3160.290039\n",
      "Training Batch: 10712 Loss: 3254.362061\n",
      "Training Batch: 10713 Loss: 3254.585205\n",
      "Training Batch: 10714 Loss: 3609.934082\n",
      "Training Batch: 10715 Loss: 3443.286133\n",
      "Training Batch: 10716 Loss: 3170.799316\n",
      "Training Batch: 10717 Loss: 3274.512451\n",
      "Training Batch: 10718 Loss: 3385.536377\n",
      "Training Batch: 10719 Loss: 3169.297852\n",
      "Training Batch: 10720 Loss: 3121.565430\n",
      "Training Batch: 10721 Loss: 3257.270020\n",
      "Training Batch: 10722 Loss: 3372.825195\n",
      "Training Batch: 10723 Loss: 3083.577148\n",
      "Training Batch: 10724 Loss: 3014.933105\n",
      "Training Batch: 10725 Loss: 3275.794922\n",
      "Training Batch: 10726 Loss: 3087.133545\n",
      "Training Batch: 10727 Loss: 3141.769531\n",
      "Training Batch: 10728 Loss: 3100.866699\n",
      "Training Batch: 10729 Loss: 3310.426514\n",
      "Training Batch: 10730 Loss: 3138.828857\n",
      "Training Batch: 10731 Loss: 3302.247803\n",
      "Training Batch: 10732 Loss: 3297.426270\n",
      "Training Batch: 10733 Loss: 3188.746094\n",
      "Training Batch: 10734 Loss: 3167.668457\n",
      "Training Batch: 10735 Loss: 3175.502930\n",
      "Training Batch: 10736 Loss: 3155.729980\n",
      "Training Batch: 10737 Loss: 3095.591309\n",
      "Training Batch: 10738 Loss: 3065.811768\n",
      "Training Batch: 10739 Loss: 3143.849365\n",
      "Training Batch: 10740 Loss: 3275.241699\n",
      "Training Batch: 10741 Loss: 3509.913086\n",
      "Training Batch: 10742 Loss: 3383.873779\n",
      "Training Batch: 10743 Loss: 3165.854980\n",
      "Training Batch: 10744 Loss: 3186.501465\n",
      "Training Batch: 10745 Loss: 3184.995605\n",
      "Training Batch: 10746 Loss: 3024.771973\n",
      "Training Batch: 10747 Loss: 3152.801270\n",
      "Training Batch: 10748 Loss: 3217.698242\n",
      "Training Batch: 10749 Loss: 3460.156250\n",
      "Training Batch: 10750 Loss: 3271.078125\n",
      "Training Batch: 10751 Loss: 3182.179443\n",
      "Training Batch: 10752 Loss: 3180.167969\n",
      "Training Batch: 10753 Loss: 3154.252197\n",
      "Training Batch: 10754 Loss: 3034.982910\n",
      "Training Batch: 10755 Loss: 3131.503906\n",
      "Training Batch: 10756 Loss: 3236.340820\n",
      "Training Batch: 10757 Loss: 3150.726074\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Batch: 10758 Loss: 3179.547119\n",
      "Training Batch: 10759 Loss: 3132.718750\n",
      "Training Batch: 10760 Loss: 3173.364502\n",
      "Training Batch: 10761 Loss: 3171.068848\n",
      "Training Batch: 10762 Loss: 3189.022949\n",
      "Training Batch: 10763 Loss: 3054.504883\n",
      "Training Batch: 10764 Loss: 3016.864258\n",
      "Training Batch: 10765 Loss: 2985.395508\n",
      "Training Batch: 10766 Loss: 3154.202148\n",
      "Training Batch: 10767 Loss: 3157.255615\n",
      "Training Batch: 10768 Loss: 3087.978271\n",
      "Training Batch: 10769 Loss: 3027.016113\n",
      "Training Batch: 10770 Loss: 3153.923096\n",
      "Training Batch: 10771 Loss: 3301.366699\n",
      "Training Batch: 10772 Loss: 3117.501221\n",
      "Training Batch: 10773 Loss: 3087.942383\n",
      "Training Batch: 10774 Loss: 3133.352539\n",
      "Training Batch: 10775 Loss: 3096.772949\n",
      "Training Batch: 10776 Loss: 3003.308105\n",
      "Training Batch: 10777 Loss: 3154.201660\n",
      "Training Batch: 10778 Loss: 3160.520752\n",
      "Training Batch: 10779 Loss: 3100.832520\n",
      "Training Batch: 10780 Loss: 3156.090820\n",
      "Training Batch: 10781 Loss: 3117.009033\n",
      "Training Batch: 10782 Loss: 3190.365234\n",
      "Training Batch: 10783 Loss: 3157.651855\n",
      "Training Batch: 10784 Loss: 3105.569824\n",
      "Training Batch: 10785 Loss: 3279.188965\n",
      "Training Batch: 10786 Loss: 3462.525391\n",
      "Training Batch: 10787 Loss: 3128.988770\n",
      "Training Batch: 10788 Loss: 3151.825195\n",
      "Training Batch: 10789 Loss: 3224.552002\n",
      "Training Batch: 10790 Loss: 3244.603271\n",
      "Training Batch: 10791 Loss: 3200.824951\n",
      "Training Batch: 10792 Loss: 3320.326660\n",
      "Training Batch: 10793 Loss: 3232.112061\n",
      "Training Batch: 10794 Loss: 3182.754883\n",
      "Training Batch: 10795 Loss: 3285.628906\n",
      "Training Batch: 10796 Loss: 3138.600586\n",
      "Training Batch: 10797 Loss: 3203.982910\n",
      "Training Batch: 10798 Loss: 3122.802246\n",
      "Training Batch: 10799 Loss: 3483.500244\n",
      "Training Batch: 10800 Loss: 3080.456543\n",
      "Training Batch: 10801 Loss: 3155.629395\n",
      "Training Batch: 10802 Loss: 3398.664062\n",
      "Training Batch: 10803 Loss: 3258.854980\n",
      "Training Batch: 10804 Loss: 3152.097412\n",
      "Training Batch: 10805 Loss: 3188.258301\n",
      "Training Batch: 10806 Loss: 3081.925293\n",
      "Training Batch: 10807 Loss: 3220.632568\n",
      "Training Batch: 10808 Loss: 3108.780762\n",
      "Training Batch: 10809 Loss: 3118.650391\n",
      "Training Batch: 10810 Loss: 3296.993164\n",
      "Training Batch: 10811 Loss: 3089.098633\n",
      "Training Batch: 10812 Loss: 3036.617676\n",
      "Training Batch: 10813 Loss: 3188.254639\n",
      "Training Batch: 10814 Loss: 3180.939697\n",
      "Training Batch: 10815 Loss: 3477.271729\n",
      "Training Batch: 10816 Loss: 3170.867676\n",
      "Training Batch: 10817 Loss: 3057.114014\n",
      "Training Batch: 10818 Loss: 3330.949951\n",
      "Training Batch: 10819 Loss: 3162.444336\n",
      "Training Batch: 10820 Loss: 3090.640869\n",
      "Training Batch: 10821 Loss: 3470.808838\n",
      "Training Batch: 10822 Loss: 3097.966309\n",
      "Training Batch: 10823 Loss: 3219.070312\n",
      "Training Batch: 10824 Loss: 3072.941895\n",
      "Training Batch: 10825 Loss: 3136.051025\n",
      "Training Batch: 10826 Loss: 3092.652344\n",
      "Training Batch: 10827 Loss: 3082.339355\n",
      "Training Batch: 10828 Loss: 3131.559570\n",
      "Training Batch: 10829 Loss: 3081.509277\n",
      "Training Batch: 10830 Loss: 3153.886230\n",
      "Training Batch: 10831 Loss: 3193.069092\n",
      "Training Batch: 10832 Loss: 3181.615967\n",
      "Training Batch: 10833 Loss: 3209.416748\n",
      "Training Batch: 10834 Loss: 3177.636230\n",
      "Training Batch: 10835 Loss: 3256.408203\n",
      "Training Batch: 10836 Loss: 3089.774414\n",
      "Training Batch: 10837 Loss: 3117.303467\n",
      "Training Batch: 10838 Loss: 3089.154297\n",
      "Training Batch: 10839 Loss: 3084.136963\n",
      "Training Batch: 10840 Loss: 3124.898438\n",
      "Training Batch: 10841 Loss: 3148.955078\n",
      "Training Batch: 10842 Loss: 3127.867676\n",
      "Training Batch: 10843 Loss: 3101.420410\n",
      "Training Batch: 10844 Loss: 3132.584961\n",
      "Training Batch: 10845 Loss: 3161.605469\n",
      "Training Batch: 10846 Loss: 3187.404785\n",
      "Training Batch: 10847 Loss: 3186.784668\n",
      "Training Batch: 10848 Loss: 3194.286377\n",
      "Training Batch: 10849 Loss: 3100.930176\n",
      "Training Batch: 10850 Loss: 3266.438721\n",
      "Training Batch: 10851 Loss: 3078.593750\n",
      "Training Batch: 10852 Loss: 3066.496094\n",
      "Training Batch: 10853 Loss: 3069.881348\n",
      "Training Batch: 10854 Loss: 3247.477539\n",
      "Training Batch: 10855 Loss: 3149.446777\n",
      "Training Batch: 10856 Loss: 3193.536133\n",
      "Training Batch: 10857 Loss: 3072.207031\n",
      "Training Batch: 10858 Loss: 3052.880127\n",
      "Training Batch: 10859 Loss: 3110.808105\n",
      "Training Batch: 10860 Loss: 3189.513428\n",
      "Training Batch: 10861 Loss: 3107.596191\n",
      "Training Batch: 10862 Loss: 3120.774414\n",
      "Training Batch: 10863 Loss: 3279.830078\n",
      "Training Batch: 10864 Loss: 3196.659180\n",
      "Training Batch: 10865 Loss: 3189.716309\n",
      "Training Batch: 10866 Loss: 3086.651367\n",
      "Training Batch: 10867 Loss: 3178.823242\n",
      "Training Batch: 10868 Loss: 3141.170654\n",
      "Training Batch: 10869 Loss: 3089.373535\n",
      "Training Batch: 10870 Loss: 3153.913818\n",
      "Training Batch: 10871 Loss: 3176.607422\n",
      "Training Batch: 10872 Loss: 3040.216309\n",
      "Training Batch: 10873 Loss: 3274.440918\n",
      "Training Batch: 10874 Loss: 3173.770508\n",
      "Training Batch: 10875 Loss: 3046.364746\n",
      "Training Batch: 10876 Loss: 3044.625488\n",
      "Training Batch: 10877 Loss: 3181.447998\n",
      "Training Batch: 10878 Loss: 3172.368652\n",
      "Training Batch: 10879 Loss: 3112.060547\n",
      "Training Batch: 10880 Loss: 3165.275635\n",
      "Training Batch: 10881 Loss: 3130.473145\n",
      "Training Batch: 10882 Loss: 3209.580811\n",
      "Training Batch: 10883 Loss: 3150.543945\n",
      "Training Batch: 10884 Loss: 3111.656250\n",
      "Training Batch: 10885 Loss: 3140.921387\n",
      "Training Batch: 10886 Loss: 3115.400391\n",
      "Training Batch: 10887 Loss: 3131.169678\n",
      "Training Batch: 10888 Loss: 3102.081055\n",
      "Training Batch: 10889 Loss: 3163.998535\n",
      "Training Batch: 10890 Loss: 3096.619385\n",
      "Training Batch: 10891 Loss: 3094.591309\n",
      "Training Batch: 10892 Loss: 3013.131104\n",
      "Training Batch: 10893 Loss: 3050.598145\n",
      "Training Batch: 10894 Loss: 3095.743896\n",
      "Training Batch: 10895 Loss: 3084.422363\n",
      "Training Batch: 10896 Loss: 3078.623779\n",
      "Training Batch: 10897 Loss: 3218.481445\n",
      "Training Batch: 10898 Loss: 3060.991211\n",
      "Training Batch: 10899 Loss: 3081.577637\n",
      "Training Batch: 10900 Loss: 3268.883789\n",
      "Training Batch: 10901 Loss: 3222.056152\n",
      "Training Batch: 10902 Loss: 3037.790527\n",
      "Training Batch: 10903 Loss: 3219.929688\n",
      "Training Batch: 10904 Loss: 3152.361816\n",
      "Training Batch: 10905 Loss: 3149.995605\n",
      "Training Batch: 10906 Loss: 3199.399902\n",
      "Training Batch: 10907 Loss: 3005.240723\n",
      "Training Batch: 10908 Loss: 3154.048828\n",
      "Training Batch: 10909 Loss: 3104.242676\n",
      "Training Batch: 10910 Loss: 3191.324707\n",
      "Training Batch: 10911 Loss: 3129.711914\n",
      "Training Batch: 10912 Loss: 3159.844727\n",
      "Training Batch: 10913 Loss: 3043.937012\n",
      "Training Batch: 10914 Loss: 3108.351562\n",
      "Training Batch: 10915 Loss: 3116.907471\n",
      "Training Batch: 10916 Loss: 3125.719727\n",
      "Training Batch: 10917 Loss: 3135.999023\n",
      "Training Batch: 10918 Loss: 3271.156250\n",
      "Training Batch: 10919 Loss: 3371.393555\n",
      "Training Batch: 10920 Loss: 3342.371094\n",
      "Training Batch: 10921 Loss: 3149.489258\n",
      "Training Batch: 10922 Loss: 3432.778320\n",
      "Training Batch: 10923 Loss: 3204.619141\n",
      "Training Batch: 10924 Loss: 3258.425293\n",
      "Training Batch: 10925 Loss: 3213.914062\n",
      "Training Batch: 10926 Loss: 3096.924072\n",
      "Training Batch: 10927 Loss: 3181.053711\n",
      "Training Batch: 10928 Loss: 3219.243408\n",
      "Training Batch: 10929 Loss: 3438.187500\n",
      "Training Batch: 10930 Loss: 3165.139648\n",
      "Training Batch: 10931 Loss: 3217.333984\n",
      "Training Batch: 10932 Loss: 3284.378906\n",
      "Training Batch: 10933 Loss: 3170.442383\n",
      "Training Batch: 10934 Loss: 3058.645508\n",
      "Training Batch: 10935 Loss: 3179.157227\n",
      "Training Batch: 10936 Loss: 3280.341309\n",
      "Training Batch: 10937 Loss: 3054.408691\n",
      "Training Batch: 10938 Loss: 3070.376465\n",
      "Training Batch: 10939 Loss: 3094.039551\n",
      "Training Batch: 10940 Loss: 2993.416992\n",
      "Training Batch: 10941 Loss: 3048.509766\n",
      "Training Batch: 10942 Loss: 3333.843750\n",
      "Training Batch: 10943 Loss: 3058.638184\n",
      "Training Batch: 10944 Loss: 3134.383545\n",
      "Training Batch: 10945 Loss: 3036.715820\n",
      "Training Batch: 10946 Loss: 3305.452148\n",
      "Training Batch: 10947 Loss: 3347.308594\n",
      "Training Batch: 10948 Loss: 3319.611572\n",
      "Training Batch: 10949 Loss: 3148.481445\n",
      "Training Batch: 10950 Loss: 3141.142578\n",
      "Training Batch: 10951 Loss: 3122.941406\n",
      "Training Batch: 10952 Loss: 4001.171631\n",
      "Training Batch: 10953 Loss: 3205.140137\n",
      "Training Batch: 10954 Loss: 3925.114746\n",
      "Training Batch: 10955 Loss: 3191.890625\n",
      "Training Batch: 10956 Loss: 3205.245850\n",
      "Training Batch: 10957 Loss: 3074.963867\n",
      "Training Batch: 10958 Loss: 3080.183105\n",
      "Training Batch: 10959 Loss: 3134.440674\n",
      "Training Batch: 10960 Loss: 3172.266113\n",
      "Training Batch: 10961 Loss: 3050.617676\n",
      "Training Batch: 10962 Loss: 3104.326660\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Batch: 10963 Loss: 3303.206055\n",
      "Training Batch: 10964 Loss: 3030.947021\n",
      "Training Batch: 10965 Loss: 3162.669434\n",
      "Training Batch: 10966 Loss: 3081.504639\n",
      "Training Batch: 10967 Loss: 3081.528809\n",
      "Training Batch: 10968 Loss: 3037.971191\n",
      "Training Batch: 10969 Loss: 3128.118408\n",
      "Training Batch: 10970 Loss: 3130.690430\n",
      "Training Batch: 10971 Loss: 3118.771484\n",
      "Training Batch: 10972 Loss: 3052.166504\n",
      "Training Batch: 10973 Loss: 3189.597656\n",
      "Training Batch: 10974 Loss: 3095.125000\n",
      "Training Batch: 10975 Loss: 3130.823486\n",
      "Training Batch: 10976 Loss: 3159.113525\n",
      "Training Batch: 10977 Loss: 3248.941406\n",
      "Training Batch: 10978 Loss: 3302.150879\n",
      "Training Batch: 10979 Loss: 3300.866943\n",
      "Training Batch: 10980 Loss: 3062.802246\n",
      "Training Batch: 10981 Loss: 3499.964355\n",
      "Training Batch: 10982 Loss: 3179.731934\n",
      "Training Batch: 10983 Loss: 3052.244629\n",
      "Training Batch: 10984 Loss: 3163.302979\n",
      "Training Batch: 10985 Loss: 3152.663086\n",
      "Training Batch: 10986 Loss: 3107.913086\n",
      "Training Batch: 10987 Loss: 3175.922363\n",
      "Training Batch: 10988 Loss: 3453.750488\n",
      "Training Batch: 10989 Loss: 3317.228760\n",
      "Training Batch: 10990 Loss: 3264.492920\n",
      "Training Batch: 10991 Loss: 3070.539551\n",
      "Training Batch: 10992 Loss: 3087.449707\n",
      "Training Batch: 10993 Loss: 3092.784668\n",
      "Training Batch: 10994 Loss: 3316.076416\n",
      "Training Batch: 10995 Loss: 3283.902832\n",
      "Training Batch: 10996 Loss: 3214.078369\n",
      "Training Batch: 10997 Loss: 3193.047119\n",
      "Training Batch: 10998 Loss: 3234.316895\n",
      "Training Batch: 10999 Loss: 3427.598633\n",
      "Training Batch: 11000 Loss: 3475.375488\n",
      "Training Batch: 11001 Loss: 3257.481934\n",
      "Training Batch: 11002 Loss: 3387.817871\n",
      "Training Batch: 11003 Loss: 3116.722168\n",
      "Training Batch: 11004 Loss: 3198.945312\n",
      "Training Batch: 11005 Loss: 3378.750977\n",
      "Training Batch: 11006 Loss: 3233.201660\n",
      "Training Batch: 11007 Loss: 3294.044922\n",
      "Training Batch: 11008 Loss: 3214.896484\n",
      "Training Batch: 11009 Loss: 3233.238281\n",
      "Training Batch: 11010 Loss: 3136.956055\n",
      "Training Batch: 11011 Loss: 3212.030762\n",
      "Training Batch: 11012 Loss: 3221.926758\n",
      "Training Batch: 11013 Loss: 3238.675781\n",
      "Training Batch: 11014 Loss: 3167.681152\n",
      "Training Batch: 11015 Loss: 3294.440430\n",
      "Training Batch: 11016 Loss: 3496.899902\n",
      "Training Batch: 11017 Loss: 3170.296387\n",
      "Training Batch: 11018 Loss: 3213.916504\n",
      "Training Batch: 11019 Loss: 3430.808594\n",
      "Training Batch: 11020 Loss: 3179.418945\n",
      "Training Batch: 11021 Loss: 3235.752441\n",
      "Training Batch: 11022 Loss: 3220.092773\n",
      "Training Batch: 11023 Loss: 3188.284180\n",
      "Training Batch: 11024 Loss: 3156.733398\n",
      "Training Batch: 11025 Loss: 3182.066895\n",
      "Training Batch: 11026 Loss: 3180.341797\n",
      "Training Batch: 11027 Loss: 3149.749512\n",
      "Training Batch: 11028 Loss: 3111.204102\n",
      "Training Batch: 11029 Loss: 3379.705566\n",
      "Training Batch: 11030 Loss: 3110.540771\n",
      "Training Batch: 11031 Loss: 3273.587891\n",
      "Training Batch: 11032 Loss: 3164.662109\n",
      "Training Batch: 11033 Loss: 3075.441650\n",
      "Training Batch: 11034 Loss: 3105.646484\n",
      "Training Batch: 11035 Loss: 3052.470459\n",
      "Training Batch: 11036 Loss: 3129.520508\n",
      "Training Batch: 11037 Loss: 3177.034668\n",
      "Training Batch: 11038 Loss: 3198.728027\n",
      "Training Batch: 11039 Loss: 3178.137207\n",
      "Training Batch: 11040 Loss: 3104.264648\n",
      "Training Batch: 11041 Loss: 3056.424561\n",
      "Training Batch: 11042 Loss: 3313.893555\n",
      "Training Batch: 11043 Loss: 3185.358154\n",
      "Training Batch: 11044 Loss: 3545.314453\n",
      "Training Batch: 11045 Loss: 3337.588623\n",
      "Training Batch: 11046 Loss: 3296.560059\n",
      "Training Batch: 11047 Loss: 3094.440430\n",
      "Training Batch: 11048 Loss: 3233.995117\n",
      "Training Batch: 11049 Loss: 3171.512207\n",
      "Training Batch: 11050 Loss: 3196.765137\n",
      "Training Batch: 11051 Loss: 3195.765137\n",
      "Training Batch: 11052 Loss: 3180.787109\n",
      "Training Batch: 11053 Loss: 3245.789062\n",
      "Training Batch: 11054 Loss: 3140.287109\n",
      "Training Batch: 11055 Loss: 3165.073975\n",
      "Training Batch: 11056 Loss: 3099.750000\n",
      "Training Batch: 11057 Loss: 3848.067627\n",
      "Training Batch: 11058 Loss: 3224.413086\n",
      "Training Batch: 11059 Loss: 3074.780029\n",
      "Training Batch: 11060 Loss: 3227.262695\n",
      "Training Batch: 11061 Loss: 3287.598877\n",
      "Training Batch: 11062 Loss: 3176.816406\n",
      "Training Batch: 11063 Loss: 3211.894775\n",
      "Training Batch: 11064 Loss: 3140.442383\n",
      "Training Batch: 11065 Loss: 3091.664795\n",
      "Training Batch: 11066 Loss: 3103.440674\n",
      "Training Batch: 11067 Loss: 3122.620605\n",
      "Training Batch: 11068 Loss: 3185.506348\n",
      "Training Batch: 11069 Loss: 3327.378906\n",
      "Training Batch: 11070 Loss: 3125.428223\n",
      "Training Batch: 11071 Loss: 3156.833496\n",
      "Training Batch: 11072 Loss: 3069.576172\n",
      "Training Batch: 11073 Loss: 3244.703613\n",
      "Training Batch: 11074 Loss: 3200.111328\n",
      "Training Batch: 11075 Loss: 3021.265625\n",
      "Training Batch: 11076 Loss: 3110.727539\n",
      "Training Batch: 11077 Loss: 3296.395508\n",
      "Training Batch: 11078 Loss: 3109.094482\n",
      "Training Batch: 11079 Loss: 3136.993408\n",
      "Training Batch: 11080 Loss: 3139.320801\n",
      "Training Batch: 11081 Loss: 3123.569336\n",
      "Training Batch: 11082 Loss: 3184.010986\n",
      "Training Batch: 11083 Loss: 3088.751953\n",
      "Training Batch: 11084 Loss: 3049.530518\n",
      "Training Batch: 11085 Loss: 3269.161377\n",
      "Training Batch: 11086 Loss: 3094.760254\n",
      "Training Batch: 11087 Loss: 3115.540771\n",
      "Training Batch: 11088 Loss: 3074.518066\n",
      "Training Batch: 11089 Loss: 3171.183594\n",
      "Training Batch: 11090 Loss: 3306.794678\n",
      "Training Batch: 11091 Loss: 3071.436279\n",
      "Training Batch: 11092 Loss: 3213.162598\n",
      "Training Batch: 11093 Loss: 3163.383789\n",
      "Training Batch: 11094 Loss: 3135.897949\n",
      "Training Batch: 11095 Loss: 3104.799561\n",
      "Training Batch: 11096 Loss: 3155.085449\n",
      "Training Batch: 11097 Loss: 3127.132812\n",
      "Training Batch: 11098 Loss: 3209.567871\n",
      "Training Batch: 11099 Loss: 3291.990967\n",
      "Training Batch: 11100 Loss: 3133.733398\n",
      "Training Batch: 11101 Loss: 3150.218994\n",
      "Training Batch: 11102 Loss: 3132.139648\n",
      "Training Batch: 11103 Loss: 3152.585449\n",
      "Training Batch: 11104 Loss: 3105.762207\n",
      "Training Batch: 11105 Loss: 3306.867920\n",
      "Training Batch: 11106 Loss: 3055.762207\n",
      "Training Batch: 11107 Loss: 3348.353516\n",
      "Training Batch: 11108 Loss: 3071.936768\n",
      "Training Batch: 11109 Loss: 3105.937012\n",
      "Training Batch: 11110 Loss: 3210.491699\n",
      "Training Batch: 11111 Loss: 3205.648438\n",
      "Training Batch: 11112 Loss: 3418.345947\n",
      "Training Batch: 11113 Loss: 3331.120850\n",
      "Training Batch: 11114 Loss: 3106.754150\n",
      "Training Batch: 11115 Loss: 3013.820068\n",
      "Training Batch: 11116 Loss: 3151.478027\n",
      "Training Batch: 11117 Loss: 3161.445312\n",
      "Training Batch: 11118 Loss: 3202.596436\n",
      "Training Batch: 11119 Loss: 3206.484375\n",
      "Training Batch: 11120 Loss: 3111.423828\n",
      "Training Batch: 11121 Loss: 3078.965820\n",
      "Training Batch: 11122 Loss: 3143.909180\n",
      "Training Batch: 11123 Loss: 3088.032715\n",
      "Training Batch: 11124 Loss: 3028.056152\n",
      "Training Batch: 11125 Loss: 3223.968750\n",
      "Training Batch: 11126 Loss: 3187.227539\n",
      "Training Batch: 11127 Loss: 3302.613037\n",
      "Training Batch: 11128 Loss: 3381.968750\n",
      "Training Batch: 11129 Loss: 3121.817383\n",
      "Training Batch: 11130 Loss: 3178.394531\n",
      "Training Batch: 11131 Loss: 3099.927734\n",
      "Training Batch: 11132 Loss: 3233.382324\n",
      "Training Batch: 11133 Loss: 3246.373535\n",
      "Training Batch: 11134 Loss: 3211.095215\n",
      "Training Batch: 11135 Loss: 3170.352051\n",
      "Training Batch: 11136 Loss: 3181.946045\n",
      "Training Batch: 11137 Loss: 3197.792480\n",
      "Training Batch: 11138 Loss: 3340.399902\n",
      "Training Batch: 11139 Loss: 3245.395996\n",
      "Training Batch: 11140 Loss: 3635.839844\n",
      "Training Batch: 11141 Loss: 3178.435547\n",
      "Training Batch: 11142 Loss: 3132.895020\n",
      "Training Batch: 11143 Loss: 3294.424072\n",
      "Training Batch: 11144 Loss: 3067.206055\n",
      "Training Batch: 11145 Loss: 3165.099609\n",
      "Training Batch: 11146 Loss: 3112.863770\n",
      "Training Batch: 11147 Loss: 3032.629883\n",
      "Training Batch: 11148 Loss: 3116.872070\n",
      "Training Batch: 11149 Loss: 3306.303711\n",
      "Training Batch: 11150 Loss: 3120.416504\n",
      "Training Batch: 11151 Loss: 3145.747070\n",
      "Training Batch: 11152 Loss: 3023.611328\n",
      "Training Batch: 11153 Loss: 3301.590332\n",
      "Training Batch: 11154 Loss: 3157.596436\n",
      "Training Batch: 11155 Loss: 3274.915039\n",
      "Training Batch: 11156 Loss: 3313.564697\n",
      "Training Batch: 11157 Loss: 3063.892090\n",
      "Training Batch: 11158 Loss: 3043.386719\n",
      "Training Batch: 11159 Loss: 3170.678955\n",
      "Training Batch: 11160 Loss: 3029.571045\n",
      "Training Batch: 11161 Loss: 3202.564453\n",
      "Training Batch: 11162 Loss: 3056.521973\n",
      "Training Batch: 11163 Loss: 3083.157715\n",
      "Training Batch: 11164 Loss: 3097.740723\n",
      "Training Batch: 11165 Loss: 3066.927734\n",
      "Training Batch: 11166 Loss: 3135.021973\n",
      "Training Batch: 11167 Loss: 3290.331543\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Batch: 11168 Loss: 3198.341797\n",
      "Training Batch: 11169 Loss: 3198.706299\n",
      "Training Batch: 11170 Loss: 3110.670410\n",
      "Training Batch: 11171 Loss: 3069.043701\n",
      "Training Batch: 11172 Loss: 3337.346924\n",
      "Training Batch: 11173 Loss: 3052.259277\n",
      "Training Batch: 11174 Loss: 3009.692383\n",
      "Training Batch: 11175 Loss: 3394.653809\n",
      "Training Batch: 11176 Loss: 3390.995850\n",
      "Training Batch: 11177 Loss: 3105.423340\n",
      "Training Batch: 11178 Loss: 3145.891357\n",
      "Training Batch: 11179 Loss: 3136.713867\n",
      "Training Batch: 11180 Loss: 3079.866699\n",
      "Training Batch: 11181 Loss: 3078.037109\n",
      "Training Batch: 11182 Loss: 3160.614746\n",
      "Training Batch: 11183 Loss: 3119.807617\n",
      "Training Batch: 11184 Loss: 3103.616211\n",
      "Training Batch: 11185 Loss: 3121.634521\n",
      "Training Batch: 11186 Loss: 2942.862793\n",
      "Training Batch: 11187 Loss: 2997.697998\n",
      "Training Batch: 11188 Loss: 3056.470215\n",
      "Training Batch: 11189 Loss: 3313.745117\n",
      "Training Batch: 11190 Loss: 3027.693359\n",
      "Training Batch: 11191 Loss: 3213.896484\n",
      "Training Batch: 11192 Loss: 3144.783691\n",
      "Training Batch: 11193 Loss: 3207.447266\n",
      "Training Batch: 11194 Loss: 3293.836182\n",
      "Training Batch: 11195 Loss: 3151.644531\n",
      "Training Batch: 11196 Loss: 3034.379883\n",
      "Training Batch: 11197 Loss: 3180.009766\n",
      "Training Batch: 11198 Loss: 3076.219971\n",
      "Training Batch: 11199 Loss: 3082.045410\n",
      "Training Batch: 11200 Loss: 3077.885254\n",
      "Training Batch: 11201 Loss: 3032.727539\n",
      "Training Batch: 11202 Loss: 3213.834717\n",
      "Training Batch: 11203 Loss: 3066.782715\n",
      "Training Batch: 11204 Loss: 3179.719238\n",
      "Training Batch: 11205 Loss: 3068.188477\n",
      "Training Batch: 11206 Loss: 3056.966553\n",
      "Training Batch: 11207 Loss: 3110.622070\n",
      "Training Batch: 11208 Loss: 3128.095215\n",
      "Training Batch: 11209 Loss: 3244.868164\n",
      "Training Batch: 11210 Loss: 3129.927734\n",
      "Training Batch: 11211 Loss: 3103.051270\n",
      "Training Batch: 11212 Loss: 3107.429443\n",
      "Training Batch: 11213 Loss: 2963.489746\n",
      "Training Batch: 11214 Loss: 3095.641602\n",
      "Training Batch: 11215 Loss: 3087.651855\n",
      "Training Batch: 11216 Loss: 3172.792969\n",
      "Training Batch: 11217 Loss: 3152.050293\n",
      "Training Batch: 11218 Loss: 3218.569824\n",
      "Training Batch: 11219 Loss: 3163.207275\n",
      "Training Batch: 11220 Loss: 3181.069092\n",
      "Training Batch: 11221 Loss: 3082.538330\n",
      "Training Batch: 11222 Loss: 3120.752441\n",
      "Training Batch: 11223 Loss: 3256.530273\n",
      "Training Batch: 11224 Loss: 3163.024414\n",
      "Training Batch: 11225 Loss: 3103.124512\n",
      "Training Batch: 11226 Loss: 3062.901855\n",
      "Training Batch: 11227 Loss: 3173.786621\n",
      "Training Batch: 11228 Loss: 3094.805176\n",
      "Training Batch: 11229 Loss: 3072.659912\n",
      "Training Batch: 11230 Loss: 3069.826660\n",
      "Training Batch: 11231 Loss: 3183.959473\n",
      "Training Batch: 11232 Loss: 3020.303467\n",
      "Training Batch: 11233 Loss: 3368.361328\n",
      "Training Batch: 11234 Loss: 3335.333008\n",
      "Training Batch: 11235 Loss: 3276.719238\n",
      "Training Batch: 11236 Loss: 3205.599854\n",
      "Training Batch: 11237 Loss: 3132.824951\n",
      "Training Batch: 11238 Loss: 3333.104980\n",
      "Training Batch: 11239 Loss: 3246.776855\n",
      "Training Batch: 11240 Loss: 3438.201172\n",
      "Training Batch: 11241 Loss: 3240.103027\n",
      "Training Batch: 11242 Loss: 3195.751465\n",
      "Training Batch: 11243 Loss: 3375.849121\n",
      "Training Batch: 11244 Loss: 3309.034180\n",
      "Training Batch: 11245 Loss: 3142.060547\n",
      "Training Batch: 11246 Loss: 3294.395020\n",
      "Training Batch: 11247 Loss: 3062.749512\n",
      "Training Batch: 11248 Loss: 3086.562988\n",
      "Training Batch: 11249 Loss: 3237.054688\n",
      "Training Batch: 11250 Loss: 3215.470215\n",
      "Training Batch: 11251 Loss: 3238.056885\n",
      "Training Batch: 11252 Loss: 3056.478760\n",
      "Training Batch: 11253 Loss: 3100.741455\n",
      "Training Batch: 11254 Loss: 3081.400879\n",
      "Training Batch: 11255 Loss: 3036.233887\n",
      "Training Batch: 11256 Loss: 3165.265625\n",
      "Training Batch: 11257 Loss: 3545.289307\n",
      "Training Batch: 11258 Loss: 3187.716553\n",
      "Training Batch: 11259 Loss: 3462.909180\n",
      "Training Batch: 11260 Loss: 3098.486572\n",
      "Training Batch: 11261 Loss: 3137.084473\n",
      "Training Batch: 11262 Loss: 3400.533203\n",
      "Training Batch: 11263 Loss: 3225.705566\n",
      "Training Batch: 11264 Loss: 3046.144775\n",
      "Training Batch: 11265 Loss: 3093.236816\n",
      "Training Batch: 11266 Loss: 3107.062500\n",
      "Training Batch: 11267 Loss: 3063.135742\n",
      "Training Batch: 11268 Loss: 3043.162109\n",
      "Training Batch: 11269 Loss: 3069.562744\n",
      "Training Batch: 11270 Loss: 3080.822754\n",
      "Training Batch: 11271 Loss: 3109.913818\n",
      "Training Batch: 11272 Loss: 3111.680908\n",
      "Training Batch: 11273 Loss: 3156.816895\n",
      "Training Batch: 11274 Loss: 3039.724121\n",
      "Training Batch: 11275 Loss: 3270.184570\n",
      "Training Batch: 11276 Loss: 3130.817139\n",
      "Training Batch: 11277 Loss: 3076.887207\n",
      "Training Batch: 11278 Loss: 3201.698486\n",
      "Training Batch: 11279 Loss: 3159.353516\n",
      "Training Batch: 11280 Loss: 3125.464844\n",
      "Training Batch: 11281 Loss: 3033.055176\n",
      "Training Batch: 11282 Loss: 3103.329590\n",
      "Training Batch: 11283 Loss: 3329.167725\n",
      "Training Batch: 11284 Loss: 3599.231689\n",
      "Training Batch: 11285 Loss: 3225.139404\n",
      "Training Batch: 11286 Loss: 3108.522949\n",
      "Training Batch: 11287 Loss: 3152.324219\n",
      "Training Batch: 11288 Loss: 3120.755859\n",
      "Training Batch: 11289 Loss: 3063.861328\n",
      "Training Batch: 11290 Loss: 3162.225586\n",
      "Training Batch: 11291 Loss: 3238.535645\n",
      "Training Batch: 11292 Loss: 3043.724609\n",
      "Training Batch: 11293 Loss: 3139.367676\n",
      "Training Batch: 11294 Loss: 3164.249512\n",
      "Training Batch: 11295 Loss: 3158.412109\n",
      "Training Batch: 11296 Loss: 3095.892090\n",
      "Training Batch: 11297 Loss: 3157.687256\n",
      "Training Batch: 11298 Loss: 3365.277344\n",
      "Training Batch: 11299 Loss: 3442.943848\n",
      "Training Batch: 11300 Loss: 3223.714111\n",
      "Training Batch: 11301 Loss: 3379.673828\n",
      "Training Batch: 11302 Loss: 3085.202148\n",
      "Training Batch: 11303 Loss: 3095.651855\n",
      "Training Batch: 11304 Loss: 3196.022949\n",
      "Training Batch: 11305 Loss: 3141.873047\n",
      "Training Batch: 11306 Loss: 3103.069092\n",
      "Training Batch: 11307 Loss: 3097.478271\n",
      "Training Batch: 11308 Loss: 3321.105469\n",
      "Training Batch: 11309 Loss: 3073.083252\n",
      "Training Batch: 11310 Loss: 3178.310059\n",
      "Training Batch: 11311 Loss: 3040.674316\n",
      "Training Batch: 11312 Loss: 3107.211426\n",
      "Training Batch: 11313 Loss: 3153.774902\n",
      "Training Batch: 11314 Loss: 3123.833984\n",
      "Training Batch: 11315 Loss: 3156.678955\n",
      "Training Batch: 11316 Loss: 3122.681152\n",
      "Training Batch: 11317 Loss: 3051.466309\n",
      "Training Batch: 11318 Loss: 3158.645996\n",
      "Training Batch: 11319 Loss: 3186.492676\n",
      "Training Batch: 11320 Loss: 3371.681641\n",
      "Training Batch: 11321 Loss: 3252.159668\n",
      "Training Batch: 11322 Loss: 3103.550781\n",
      "Training Batch: 11323 Loss: 3067.060059\n",
      "Training Batch: 11324 Loss: 3218.842285\n",
      "Training Batch: 11325 Loss: 3195.313477\n",
      "Training Batch: 11326 Loss: 3385.514160\n",
      "Training Batch: 11327 Loss: 3021.528320\n",
      "Training Batch: 11328 Loss: 3086.400391\n",
      "Training Batch: 11329 Loss: 3158.385254\n",
      "Training Batch: 11330 Loss: 3071.176270\n",
      "Training Batch: 11331 Loss: 3102.763184\n",
      "Training Batch: 11332 Loss: 3032.593262\n",
      "Training Batch: 11333 Loss: 3224.688965\n",
      "Training Batch: 11334 Loss: 3234.526367\n",
      "Training Batch: 11335 Loss: 3208.870117\n",
      "Training Batch: 11336 Loss: 3220.298340\n",
      "Training Batch: 11337 Loss: 3202.463867\n",
      "Training Batch: 11338 Loss: 3231.882324\n",
      "Training Batch: 11339 Loss: 3114.860840\n",
      "Training Batch: 11340 Loss: 2989.562988\n",
      "Training Batch: 11341 Loss: 3106.516113\n",
      "Training Batch: 11342 Loss: 2975.413086\n",
      "Training Batch: 11343 Loss: 3015.751709\n",
      "Training Batch: 11344 Loss: 3091.199219\n",
      "Training Batch: 11345 Loss: 3113.542480\n",
      "Training Batch: 11346 Loss: 3200.624023\n",
      "Training Batch: 11347 Loss: 3205.411621\n",
      "Training Batch: 11348 Loss: 3391.646973\n",
      "Training Batch: 11349 Loss: 3266.739258\n",
      "Training Batch: 11350 Loss: 3194.892578\n",
      "Training Batch: 11351 Loss: 3086.565674\n",
      "Training Batch: 11352 Loss: 3367.190918\n",
      "Training Batch: 11353 Loss: 3481.288818\n",
      "Training Batch: 11354 Loss: 3261.779297\n",
      "Training Batch: 11355 Loss: 3335.143555\n",
      "Training Batch: 11356 Loss: 3134.314941\n",
      "Training Batch: 11357 Loss: 3057.188965\n",
      "Training Batch: 11358 Loss: 3034.885254\n",
      "Training Batch: 11359 Loss: 3050.729004\n",
      "Training Batch: 11360 Loss: 3092.465088\n",
      "Training Batch: 11361 Loss: 3197.060547\n",
      "Training Batch: 11362 Loss: 3039.094727\n",
      "Training Batch: 11363 Loss: 3151.375488\n",
      "Training Batch: 11364 Loss: 3270.229492\n",
      "Training Batch: 11365 Loss: 3107.973145\n",
      "Training Batch: 11366 Loss: 3296.292236\n",
      "Training Batch: 11367 Loss: 3060.687012\n",
      "Training Batch: 11368 Loss: 3108.009521\n",
      "Training Batch: 11369 Loss: 3067.533691\n",
      "Training Batch: 11370 Loss: 3141.562012\n",
      "Training Batch: 11371 Loss: 3183.550293\n",
      "Training Batch: 11372 Loss: 2978.513184\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Batch: 11373 Loss: 3145.726562\n",
      "Training Batch: 11374 Loss: 3029.762695\n",
      "Training Batch: 11375 Loss: 3567.225098\n",
      "Training Batch: 11376 Loss: 3169.143066\n",
      "Training Batch: 11377 Loss: 3386.088867\n",
      "Training Batch: 11378 Loss: 3262.749512\n",
      "Training Batch: 11379 Loss: 3137.233643\n",
      "Training Batch: 11380 Loss: 3110.708740\n",
      "Training Batch: 11381 Loss: 3274.028320\n",
      "Training Batch: 11382 Loss: 3053.864258\n",
      "Training Batch: 11383 Loss: 3137.182373\n",
      "Training Batch: 11384 Loss: 3157.354980\n",
      "Training Batch: 11385 Loss: 3630.072998\n",
      "Training Batch: 11386 Loss: 3049.263672\n",
      "Training Batch: 11387 Loss: 3129.086182\n",
      "Training Batch: 11388 Loss: 3212.085938\n",
      "Training Batch: 11389 Loss: 3119.811523\n",
      "Training Batch: 11390 Loss: 3311.889648\n",
      "Training Batch: 11391 Loss: 3183.562500\n",
      "Training Batch: 11392 Loss: 3113.787598\n",
      "Training Batch: 11393 Loss: 3118.143311\n",
      "Training Batch: 11394 Loss: 3096.209229\n",
      "Training Batch: 11395 Loss: 3012.071533\n",
      "Training Batch: 11396 Loss: 3155.905762\n",
      "Training Batch: 11397 Loss: 3058.131104\n",
      "Training Batch: 11398 Loss: 3068.490723\n",
      "Training Batch: 11399 Loss: 3095.716064\n",
      "Training Batch: 11400 Loss: 3126.644775\n",
      "Training Batch: 11401 Loss: 3072.668213\n",
      "Training Batch: 11402 Loss: 3050.648193\n",
      "Training Batch: 11403 Loss: 3057.082031\n",
      "Training Batch: 11404 Loss: 3133.926758\n",
      "Training Batch: 11405 Loss: 3107.834961\n",
      "Training Batch: 11406 Loss: 3050.796875\n",
      "Training Batch: 11407 Loss: 3191.343262\n",
      "Training Batch: 11408 Loss: 3139.241211\n",
      "Training Batch: 11409 Loss: 3071.007812\n",
      "Training Batch: 11410 Loss: 3099.203125\n",
      "Training Batch: 11411 Loss: 3121.700439\n",
      "Training Batch: 11412 Loss: 3241.342529\n",
      "Training Batch: 11413 Loss: 3288.562988\n",
      "Training Batch: 11414 Loss: 3093.859863\n",
      "Training Batch: 11415 Loss: 3155.810303\n",
      "Training Batch: 11416 Loss: 3155.489746\n",
      "Training Batch: 11417 Loss: 3296.344727\n",
      "Training Batch: 11418 Loss: 3115.397217\n",
      "Training Batch: 11419 Loss: 3053.263672\n",
      "Training Batch: 11420 Loss: 3143.019043\n",
      "Training Batch: 11421 Loss: 2985.258789\n",
      "Training Batch: 11422 Loss: 3073.087402\n",
      "Training Batch: 11423 Loss: 3098.573242\n",
      "Training Batch: 11424 Loss: 3292.732422\n",
      "Training Batch: 11425 Loss: 3225.599609\n",
      "Training Batch: 11426 Loss: 3064.620117\n",
      "Training Batch: 11427 Loss: 3290.638916\n",
      "Training Batch: 11428 Loss: 3046.530273\n",
      "Training Batch: 11429 Loss: 3410.244141\n",
      "Training Batch: 11430 Loss: 3166.190918\n",
      "Training Batch: 11431 Loss: 3175.143555\n",
      "Training Batch: 11432 Loss: 3108.410889\n",
      "Training Batch: 11433 Loss: 3192.907227\n",
      "Training Batch: 11434 Loss: 3188.877930\n",
      "Training Batch: 11435 Loss: 3105.773926\n",
      "Training Batch: 11436 Loss: 3106.378662\n",
      "Training Batch: 11437 Loss: 3104.520508\n",
      "Training Batch: 11438 Loss: 3103.133789\n",
      "Training Batch: 11439 Loss: 3156.089355\n",
      "Training Batch: 11440 Loss: 3355.983887\n",
      "Training Batch: 11441 Loss: 3148.704102\n",
      "Training Batch: 11442 Loss: 3264.249268\n",
      "Training Batch: 11443 Loss: 3370.860352\n",
      "Training Batch: 11444 Loss: 3308.174316\n",
      "Training Batch: 11445 Loss: 4725.205566\n",
      "Training Batch: 11446 Loss: 4879.528320\n",
      "Training Batch: 11447 Loss: 3494.983154\n",
      "Training Batch: 11448 Loss: 3204.656738\n",
      "Training Batch: 11449 Loss: 3205.242432\n",
      "Training Batch: 11450 Loss: 3114.019043\n",
      "Training Batch: 11451 Loss: 3232.112305\n",
      "Training Batch: 11452 Loss: 3205.051270\n",
      "Training Batch: 11453 Loss: 3197.667969\n",
      "Training Batch: 11454 Loss: 3228.119629\n",
      "Training Batch: 11455 Loss: 3133.379395\n",
      "Training Batch: 11456 Loss: 3163.019531\n",
      "Training Batch: 11457 Loss: 3159.393066\n",
      "Training Batch: 11458 Loss: 3101.078369\n",
      "Training Batch: 11459 Loss: 3134.020508\n",
      "Training Batch: 11460 Loss: 3564.025879\n",
      "Training Batch: 11461 Loss: 3277.627930\n",
      "Training Batch: 11462 Loss: 3133.828613\n",
      "Training Batch: 11463 Loss: 3246.817383\n",
      "Training Batch: 11464 Loss: 3255.108643\n",
      "Training Batch: 11465 Loss: 3072.512207\n",
      "Training Batch: 11466 Loss: 3089.109131\n",
      "Training Batch: 11467 Loss: 3048.982910\n",
      "Training Batch: 11468 Loss: 3125.847168\n",
      "Training Batch: 11469 Loss: 3134.231445\n",
      "Training Batch: 11470 Loss: 3088.581543\n",
      "Training Batch: 11471 Loss: 3125.095459\n",
      "Training Batch: 11472 Loss: 3106.051270\n",
      "Training Batch: 11473 Loss: 3110.728516\n",
      "Training Batch: 11474 Loss: 3199.067383\n",
      "Training Batch: 11475 Loss: 3071.339355\n",
      "Training Batch: 11476 Loss: 3139.604004\n",
      "Training Batch: 11477 Loss: 3064.693359\n",
      "Training Batch: 11478 Loss: 3254.739502\n",
      "Training Batch: 11479 Loss: 3115.272949\n",
      "Training Batch: 11480 Loss: 3128.789551\n",
      "Training Batch: 11481 Loss: 3314.296387\n",
      "Training Batch: 11482 Loss: 3283.553223\n",
      "Training Batch: 11483 Loss: 3203.814697\n",
      "Training Batch: 11484 Loss: 3299.440430\n",
      "Training Batch: 11485 Loss: 3316.616211\n",
      "Training Batch: 11486 Loss: 3215.815918\n",
      "Training Batch: 11487 Loss: 3075.340332\n",
      "Training Batch: 11488 Loss: 3199.412842\n",
      "Training Batch: 11489 Loss: 3309.389648\n",
      "Training Batch: 11490 Loss: 3383.471924\n",
      "Training Batch: 11491 Loss: 3155.589844\n",
      "Training Batch: 11492 Loss: 3425.213867\n",
      "Training Batch: 11493 Loss: 3225.924316\n",
      "Training Batch: 11494 Loss: 3377.318848\n",
      "Training Batch: 11495 Loss: 3250.179443\n",
      "Training Batch: 11496 Loss: 3153.150879\n",
      "Training Batch: 11497 Loss: 3117.978760\n",
      "Training Batch: 11498 Loss: 3217.045166\n",
      "Training Batch: 11499 Loss: 3149.666016\n",
      "Training Batch: 11500 Loss: 3159.498779\n",
      "Training Batch: 11501 Loss: 3275.969971\n",
      "Training Batch: 11502 Loss: 3174.570312\n",
      "Training Batch: 11503 Loss: 3231.087402\n",
      "Training Batch: 11504 Loss: 3269.172852\n",
      "Training Batch: 11505 Loss: 3220.666016\n",
      "Training Batch: 11506 Loss: 3251.278076\n",
      "Training Batch: 11507 Loss: 3088.595459\n",
      "Training Batch: 11508 Loss: 3157.265625\n",
      "Training Batch: 11509 Loss: 3050.583008\n",
      "Training Batch: 11510 Loss: 3183.870117\n",
      "Training Batch: 11511 Loss: 3111.842773\n",
      "Training Batch: 11512 Loss: 3162.523193\n",
      "Training Batch: 11513 Loss: 3090.584961\n",
      "Training Batch: 11514 Loss: 3032.797363\n",
      "Training Batch: 11515 Loss: 3327.370117\n",
      "Training Batch: 11516 Loss: 3149.856934\n",
      "Training Batch: 11517 Loss: 3108.454102\n",
      "Training Batch: 11518 Loss: 3268.002686\n",
      "Training Batch: 11519 Loss: 3054.438477\n",
      "Training Batch: 11520 Loss: 3867.505859\n",
      "Training Batch: 11521 Loss: 3224.894531\n",
      "Training Batch: 11522 Loss: 3228.802246\n",
      "Training Batch: 11523 Loss: 3187.987793\n",
      "Training Batch: 11524 Loss: 3002.995605\n",
      "Training Batch: 11525 Loss: 3157.219727\n",
      "Training Batch: 11526 Loss: 3107.634277\n",
      "Training Batch: 11527 Loss: 3151.556152\n",
      "Training Batch: 11528 Loss: 3087.111328\n",
      "Training Batch: 11529 Loss: 3236.585449\n",
      "Training Batch: 11530 Loss: 3226.406738\n",
      "Training Batch: 11531 Loss: 3055.088135\n",
      "Training Batch: 11532 Loss: 3018.145020\n",
      "Training Batch: 11533 Loss: 3144.934570\n",
      "Training Batch: 11534 Loss: 3114.344727\n",
      "Training Batch: 11535 Loss: 3152.330078\n",
      "Training Batch: 11536 Loss: 2999.762939\n",
      "Training Batch: 11537 Loss: 3174.914062\n",
      "Training Batch: 11538 Loss: 3020.377197\n",
      "Training Batch: 11539 Loss: 3231.765137\n",
      "Training Batch: 11540 Loss: 3115.698242\n",
      "Training Batch: 11541 Loss: 3251.979004\n",
      "Training Batch: 11542 Loss: 3124.449707\n",
      "Training Batch: 11543 Loss: 3065.376953\n",
      "Training Batch: 11544 Loss: 3126.368652\n",
      "Training Batch: 11545 Loss: 3176.253906\n",
      "Training Batch: 11546 Loss: 3107.312988\n",
      "Training Batch: 11547 Loss: 3184.525879\n",
      "Training Batch: 11548 Loss: 3324.333008\n",
      "Training Batch: 11549 Loss: 3286.191406\n",
      "Training Batch: 11550 Loss: 3140.105469\n",
      "Training Batch: 11551 Loss: 3232.849121\n",
      "Training Batch: 11552 Loss: 3123.983887\n",
      "Training Batch: 11553 Loss: 3131.973633\n",
      "Training Batch: 11554 Loss: 3145.819336\n",
      "Training Batch: 11555 Loss: 3046.856445\n",
      "Training Batch: 11556 Loss: 3081.875000\n",
      "Training Batch: 11557 Loss: 3178.165527\n",
      "Training Batch: 11558 Loss: 3114.954834\n",
      "Training Batch: 11559 Loss: 3174.314697\n",
      "Training Batch: 11560 Loss: 3155.715820\n",
      "Training Batch: 11561 Loss: 3063.338867\n",
      "Training Batch: 11562 Loss: 3196.398926\n",
      "Training Batch: 11563 Loss: 3077.043945\n",
      "Training Batch: 11564 Loss: 3165.361328\n",
      "Training Batch: 11565 Loss: 3071.431641\n",
      "Training Batch: 11566 Loss: 3090.345703\n",
      "Training Batch: 11567 Loss: 3152.927246\n",
      "Training Batch: 11568 Loss: 3237.041504\n",
      "Training Batch: 11569 Loss: 3046.238770\n",
      "Training Batch: 11570 Loss: 3037.621582\n",
      "Training Batch: 11571 Loss: 3097.421631\n",
      "Training Batch: 11572 Loss: 3150.066406\n",
      "Training Batch: 11573 Loss: 3204.215332\n",
      "Training Batch: 11574 Loss: 3017.640625\n",
      "Training Batch: 11575 Loss: 3356.714600\n",
      "Training Batch: 11576 Loss: 3220.522461\n",
      "Training Batch: 11577 Loss: 3311.144043\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Batch: 11578 Loss: 3192.320557\n",
      "Training Batch: 11579 Loss: 3064.009277\n",
      "Training Batch: 11580 Loss: 3101.221680\n",
      "Training Batch: 11581 Loss: 3076.252441\n",
      "Training Batch: 11582 Loss: 3245.099121\n",
      "Training Batch: 11583 Loss: 3388.321289\n",
      "Training Batch: 11584 Loss: 3243.696289\n",
      "Training Batch: 11585 Loss: 3199.747314\n",
      "Training Batch: 11586 Loss: 3153.327148\n",
      "Training Batch: 11587 Loss: 3363.352051\n",
      "Training Batch: 11588 Loss: 3244.780762\n",
      "Training Batch: 11589 Loss: 3239.717773\n",
      "Training Batch: 11590 Loss: 3072.685791\n",
      "Training Batch: 11591 Loss: 3116.470703\n",
      "Training Batch: 11592 Loss: 3125.692383\n",
      "Training Batch: 11593 Loss: 3134.131836\n",
      "Training Batch: 11594 Loss: 3317.017578\n",
      "Training Batch: 11595 Loss: 3176.062988\n",
      "Training Batch: 11596 Loss: 3277.435059\n",
      "Training Batch: 11597 Loss: 3111.181152\n",
      "Training Batch: 11598 Loss: 3118.730469\n",
      "Training Batch: 11599 Loss: 3090.540039\n",
      "Training Batch: 11600 Loss: 3126.393555\n",
      "Training Batch: 11601 Loss: 3041.113770\n",
      "Training Batch: 11602 Loss: 3036.756836\n",
      "Training Batch: 11603 Loss: 3121.812988\n",
      "Training Batch: 11604 Loss: 3091.702148\n",
      "Training Batch: 11605 Loss: 3289.990723\n",
      "Training Batch: 11606 Loss: 3068.867188\n",
      "Training Batch: 11607 Loss: 3428.439941\n",
      "Training Batch: 11608 Loss: 3088.273926\n",
      "Training Batch: 11609 Loss: 3213.834473\n",
      "Training Batch: 11610 Loss: 3209.719238\n",
      "Training Batch: 11611 Loss: 3130.898926\n",
      "Training Batch: 11612 Loss: 3139.604980\n",
      "Training Batch: 11613 Loss: 3216.489746\n",
      "Training Batch: 11614 Loss: 3111.788574\n",
      "Training Batch: 11615 Loss: 3151.352539\n",
      "Training Batch: 11616 Loss: 3063.405762\n",
      "Training Batch: 11617 Loss: 3400.052734\n",
      "Training Batch: 11618 Loss: 3331.097900\n",
      "Training Batch: 11619 Loss: 3237.671631\n",
      "Training Batch: 11620 Loss: 3131.039307\n",
      "Training Batch: 11621 Loss: 3154.679688\n",
      "Training Batch: 11622 Loss: 3096.863525\n",
      "Training Batch: 11623 Loss: 3059.460449\n",
      "Training Batch: 11624 Loss: 3008.064941\n",
      "Training Batch: 11625 Loss: 3091.148926\n",
      "Training Batch: 11626 Loss: 3246.346680\n",
      "Training Batch: 11627 Loss: 3226.547607\n",
      "Training Batch: 11628 Loss: 3263.704346\n",
      "Training Batch: 11629 Loss: 3213.767334\n",
      "Training Batch: 11630 Loss: 3122.631836\n",
      "Training Batch: 11631 Loss: 3140.432129\n",
      "Training Batch: 11632 Loss: 3035.470459\n",
      "Training Batch: 11633 Loss: 3041.998535\n",
      "Training Batch: 11634 Loss: 3001.182129\n",
      "Training Batch: 11635 Loss: 3157.500488\n",
      "Training Batch: 11636 Loss: 3055.578369\n",
      "Training Batch: 11637 Loss: 3131.804688\n",
      "Training Batch: 11638 Loss: 3546.279053\n",
      "Training Batch: 11639 Loss: 3299.255127\n",
      "Training Batch: 11640 Loss: 3164.914551\n",
      "Training Batch: 11641 Loss: 3201.352051\n",
      "Training Batch: 11642 Loss: 3189.476562\n",
      "Training Batch: 11643 Loss: 3142.167480\n",
      "Training Batch: 11644 Loss: 3082.564697\n",
      "Training Batch: 11645 Loss: 3073.784668\n",
      "Training Batch: 11646 Loss: 3132.256836\n",
      "Training Batch: 11647 Loss: 3125.714844\n",
      "Training Batch: 11648 Loss: 3525.212891\n",
      "Training Batch: 11649 Loss: 3178.661133\n",
      "Training Batch: 11650 Loss: 3089.083984\n",
      "Training Batch: 11651 Loss: 3097.033447\n",
      "Training Batch: 11652 Loss: 3477.167969\n",
      "Training Batch: 11653 Loss: 3137.236816\n",
      "Training Batch: 11654 Loss: 3158.488037\n",
      "Training Batch: 11655 Loss: 3055.436768\n",
      "Training Batch: 11656 Loss: 3070.090088\n",
      "Training Batch: 11657 Loss: 3045.407227\n",
      "Training Batch: 11658 Loss: 3072.632812\n",
      "Training Batch: 11659 Loss: 3050.284180\n",
      "Training Batch: 11660 Loss: 3133.986328\n",
      "Training Batch: 11661 Loss: 3137.741699\n",
      "Training Batch: 11662 Loss: 3202.134766\n",
      "Training Batch: 11663 Loss: 3200.666504\n",
      "Training Batch: 11664 Loss: 3170.419678\n",
      "Training Batch: 11665 Loss: 3201.000977\n",
      "Training Batch: 11666 Loss: 3190.673584\n",
      "Training Batch: 11667 Loss: 4096.836426\n",
      "Training Batch: 11668 Loss: 3892.182617\n",
      "Training Batch: 11669 Loss: 3423.839844\n",
      "Training Batch: 11670 Loss: 3121.282715\n",
      "Training Batch: 11671 Loss: 3143.463379\n",
      "Training Batch: 11672 Loss: 3237.603760\n",
      "Training Batch: 11673 Loss: 3152.003418\n",
      "Training Batch: 11674 Loss: 3106.022949\n",
      "Training Batch: 11675 Loss: 3010.484375\n",
      "Training Batch: 11676 Loss: 3377.494141\n",
      "Training Batch: 11677 Loss: 3074.582031\n",
      "Training Batch: 11678 Loss: 3061.731689\n",
      "Training Batch: 11679 Loss: 3079.301270\n",
      "Training Batch: 11680 Loss: 3143.975098\n",
      "Training Batch: 11681 Loss: 3202.036621\n",
      "Training Batch: 11682 Loss: 3134.534180\n",
      "Training Batch: 11683 Loss: 3187.871582\n",
      "Training Batch: 11684 Loss: 3456.423340\n",
      "Training Batch: 11685 Loss: 3224.312500\n",
      "Training Batch: 11686 Loss: 3229.522461\n",
      "Training Batch: 11687 Loss: 3171.560059\n",
      "Training Batch: 11688 Loss: 3157.766113\n",
      "Training Batch: 11689 Loss: 3128.983887\n",
      "Training Batch: 11690 Loss: 3199.062500\n",
      "Training Batch: 11691 Loss: 3271.547852\n",
      "Training Batch: 11692 Loss: 3073.985352\n",
      "Training Batch: 11693 Loss: 3449.441162\n",
      "Training Batch: 11694 Loss: 3222.980469\n",
      "Training Batch: 11695 Loss: 3072.782715\n",
      "Training Batch: 11696 Loss: 3253.702393\n",
      "Training Batch: 11697 Loss: 3278.785156\n",
      "Training Batch: 11698 Loss: 3126.840332\n",
      "Training Batch: 11699 Loss: 3401.330078\n",
      "Training Batch: 11700 Loss: 3130.314453\n",
      "Training Batch: 11701 Loss: 3053.737793\n",
      "Training Batch: 11702 Loss: 3110.956055\n",
      "Training Batch: 11703 Loss: 3148.628906\n",
      "Training Batch: 11704 Loss: 3267.916992\n",
      "Training Batch: 11705 Loss: 3344.665527\n",
      "Training Batch: 11706 Loss: 3193.545898\n",
      "Training Batch: 11707 Loss: 3203.855957\n",
      "Training Batch: 11708 Loss: 3342.350830\n",
      "Training Batch: 11709 Loss: 3155.380371\n",
      "Training Batch: 11710 Loss: 3286.703125\n",
      "Training Batch: 11711 Loss: 3172.632324\n",
      "Training Batch: 11712 Loss: 3223.274902\n",
      "Training Batch: 11713 Loss: 3193.669434\n",
      "Training Batch: 11714 Loss: 3513.680664\n",
      "Training Batch: 11715 Loss: 3061.661377\n",
      "Training Batch: 11716 Loss: 3090.836914\n",
      "Training Batch: 11717 Loss: 3219.885986\n",
      "Training Batch: 11718 Loss: 3058.566895\n",
      "Training Batch: 11719 Loss: 3254.401855\n",
      "Training Batch: 11720 Loss: 3048.489258\n",
      "Training Batch: 11721 Loss: 3437.489990\n",
      "Training Batch: 11722 Loss: 3057.209473\n",
      "Training Batch: 11723 Loss: 3157.722168\n",
      "Training Batch: 11724 Loss: 3072.773438\n",
      "Training Batch: 11725 Loss: 2986.149414\n",
      "Training Batch: 11726 Loss: 3124.821777\n",
      "Training Batch: 11727 Loss: 3124.439453\n",
      "Training Batch: 11728 Loss: 3149.924805\n",
      "Training Batch: 11729 Loss: 3266.715820\n",
      "Training Batch: 11730 Loss: 3158.391357\n",
      "Training Batch: 11731 Loss: 3429.932373\n",
      "Training Batch: 11732 Loss: 3351.986572\n",
      "Training Batch: 11733 Loss: 3146.029785\n",
      "Training Batch: 11734 Loss: 3070.528076\n",
      "Training Batch: 11735 Loss: 3122.014648\n",
      "Training Batch: 11736 Loss: 3332.095947\n",
      "Training Batch: 11737 Loss: 3345.592529\n",
      "Training Batch: 11738 Loss: 3252.506592\n",
      "Training Batch: 11739 Loss: 3231.324951\n",
      "Training Batch: 11740 Loss: 3190.167969\n",
      "Training Batch: 11741 Loss: 3293.416504\n",
      "Training Batch: 11742 Loss: 3187.632080\n",
      "Training Batch: 11743 Loss: 3289.287598\n",
      "Training Batch: 11744 Loss: 3366.894775\n",
      "Training Batch: 11745 Loss: 3229.249023\n",
      "Training Batch: 11746 Loss: 3272.073730\n",
      "Training Batch: 11747 Loss: 3194.018066\n",
      "Training Batch: 11748 Loss: 3296.749512\n",
      "Training Batch: 11749 Loss: 3176.439209\n",
      "Training Batch: 11750 Loss: 3266.527344\n",
      "Training Batch: 11751 Loss: 3077.395996\n",
      "Training Batch: 11752 Loss: 3441.153564\n",
      "Training Batch: 11753 Loss: 3206.193848\n",
      "Training Batch: 11754 Loss: 3191.337891\n",
      "Training Batch: 11755 Loss: 3078.164795\n",
      "Training Batch: 11756 Loss: 3189.556152\n",
      "Training Batch: 11757 Loss: 3116.970703\n",
      "Training Batch: 11758 Loss: 3178.635742\n",
      "Training Batch: 11759 Loss: 3129.622559\n",
      "Training Batch: 11760 Loss: 3144.134766\n",
      "Training Batch: 11761 Loss: 3128.800537\n",
      "Training Batch: 11762 Loss: 3149.038574\n",
      "Training Batch: 11763 Loss: 3179.243164\n",
      "Training Batch: 11764 Loss: 3140.629883\n",
      "Training Batch: 11765 Loss: 3043.203613\n",
      "Training Batch: 11766 Loss: 3095.991943\n",
      "Training Batch: 11767 Loss: 3104.538086\n",
      "Training Batch: 11768 Loss: 3033.593018\n",
      "Training Batch: 11769 Loss: 3071.058838\n",
      "Training Batch: 11770 Loss: 3061.336182\n",
      "Training Batch: 11771 Loss: 3122.867432\n",
      "Training Batch: 11772 Loss: 3142.692139\n",
      "Training Batch: 11773 Loss: 3115.038330\n",
      "Training Batch: 11774 Loss: 3000.339844\n",
      "Training Batch: 11775 Loss: 3107.245117\n",
      "Training Batch: 11776 Loss: 3012.509277\n",
      "Training Batch: 11777 Loss: 3005.788574\n",
      "Training Batch: 11778 Loss: 3059.746094\n",
      "Training Batch: 11779 Loss: 3249.439209\n",
      "Training Batch: 11780 Loss: 3299.861328\n",
      "Training Batch: 11781 Loss: 3047.608398\n",
      "Training Batch: 11782 Loss: 3122.574707\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Batch: 11783 Loss: 3239.836914\n",
      "Training Batch: 11784 Loss: 3180.510254\n",
      "Training Batch: 11785 Loss: 3098.859375\n",
      "Training Batch: 11786 Loss: 3076.986084\n",
      "Training Batch: 11787 Loss: 3183.542480\n",
      "Training Batch: 11788 Loss: 3108.248779\n",
      "Training Batch: 11789 Loss: 3294.755859\n",
      "Training Batch: 11790 Loss: 3085.313477\n",
      "Training Batch: 11791 Loss: 3062.958496\n",
      "Training Batch: 11792 Loss: 2997.842773\n",
      "Training Batch: 11793 Loss: 3119.551270\n",
      "Training Batch: 11794 Loss: 3280.881348\n",
      "Training Batch: 11795 Loss: 3083.944336\n",
      "Training Batch: 11796 Loss: 3066.020508\n",
      "Training Batch: 11797 Loss: 3060.260254\n",
      "Training Batch: 11798 Loss: 3049.077148\n",
      "Training Batch: 11799 Loss: 3188.855957\n",
      "Training Batch: 11800 Loss: 3126.370117\n",
      "Training Batch: 11801 Loss: 3197.792480\n",
      "Training Batch: 11802 Loss: 3051.261963\n",
      "Training Batch: 11803 Loss: 3089.253906\n",
      "Training Batch: 11804 Loss: 3116.302246\n",
      "Training Batch: 11805 Loss: 3066.028809\n",
      "Training Batch: 11806 Loss: 3076.375977\n",
      "Training Batch: 11807 Loss: 3104.252441\n",
      "Training Batch: 11808 Loss: 3154.634277\n",
      "Training Batch: 11809 Loss: 3398.100098\n",
      "Training Batch: 11810 Loss: 3483.629883\n",
      "Training Batch: 11811 Loss: 3215.497559\n",
      "Training Batch: 11812 Loss: 3078.050781\n",
      "Training Batch: 11813 Loss: 3040.293457\n",
      "Training Batch: 11814 Loss: 3109.294434\n",
      "Training Batch: 11815 Loss: 3123.627441\n",
      "Training Batch: 11816 Loss: 3129.880371\n",
      "Training Batch: 11817 Loss: 3191.625488\n",
      "Training Batch: 11818 Loss: 3309.721191\n",
      "Training Batch: 11819 Loss: 3136.398438\n",
      "Training Batch: 11820 Loss: 3225.337402\n",
      "Training Batch: 11821 Loss: 3268.553223\n",
      "Training Batch: 11822 Loss: 3127.756592\n",
      "Training Batch: 11823 Loss: 3063.999756\n",
      "Training Batch: 11824 Loss: 3156.273926\n",
      "Training Batch: 11825 Loss: 3066.222656\n",
      "Training Batch: 11826 Loss: 3296.323730\n",
      "Training Batch: 11827 Loss: 3206.484863\n",
      "Training Batch: 11828 Loss: 3115.351562\n",
      "Training Batch: 11829 Loss: 3136.803223\n",
      "Training Batch: 11830 Loss: 3235.546875\n",
      "Training Batch: 11831 Loss: 3187.750977\n",
      "Training Batch: 11832 Loss: 3087.865723\n",
      "Training Batch: 11833 Loss: 3090.124268\n",
      "Training Batch: 11834 Loss: 3008.260010\n",
      "Training Batch: 11835 Loss: 3070.754883\n",
      "Training Batch: 11836 Loss: 3133.737061\n",
      "Training Batch: 11837 Loss: 3224.966309\n",
      "Training Batch: 11838 Loss: 3156.812988\n",
      "Training Batch: 11839 Loss: 3049.027344\n",
      "Training Batch: 11840 Loss: 3051.913330\n",
      "Training Batch: 11841 Loss: 3063.361328\n",
      "Training Batch: 11842 Loss: 3157.833984\n",
      "Training Batch: 11843 Loss: 3074.890137\n",
      "Training Batch: 11844 Loss: 3244.334717\n",
      "Training Batch: 11845 Loss: 3086.707520\n",
      "Training Batch: 11846 Loss: 3014.374268\n",
      "Training Batch: 11847 Loss: 3136.042480\n",
      "Training Batch: 11848 Loss: 3301.073242\n",
      "Training Batch: 11849 Loss: 3248.582031\n",
      "Training Batch: 11850 Loss: 3111.660645\n",
      "Training Batch: 11851 Loss: 3229.393066\n",
      "Training Batch: 11852 Loss: 3119.113525\n",
      "Training Batch: 11853 Loss: 3062.200684\n",
      "Training Batch: 11854 Loss: 3146.676025\n",
      "Training Batch: 11855 Loss: 3025.677979\n",
      "Training Batch: 11856 Loss: 3063.109863\n",
      "Training Batch: 11857 Loss: 3102.096680\n",
      "Training Batch: 11858 Loss: 3161.905762\n",
      "Training Batch: 11859 Loss: 3071.730957\n",
      "Training Batch: 11860 Loss: 3061.910400\n",
      "Training Batch: 11861 Loss: 3048.740234\n",
      "Training Batch: 11862 Loss: 3293.603516\n",
      "Training Batch: 11863 Loss: 3200.464844\n",
      "Training Batch: 11864 Loss: 3212.955322\n",
      "Training Batch: 11865 Loss: 3060.170898\n",
      "Training Batch: 11866 Loss: 3114.979980\n",
      "Training Batch: 11867 Loss: 3183.423096\n",
      "Training Batch: 11868 Loss: 3126.589111\n",
      "Training Batch: 11869 Loss: 2985.586670\n",
      "Training Batch: 11870 Loss: 3127.170898\n",
      "Training Batch: 11871 Loss: 3085.208008\n",
      "Training Batch: 11872 Loss: 3141.291016\n",
      "Training Batch: 11873 Loss: 3025.783203\n",
      "Training Batch: 11874 Loss: 3101.325684\n",
      "Training Batch: 11875 Loss: 3278.146973\n",
      "Training Batch: 11876 Loss: 3172.914551\n",
      "Training Batch: 11877 Loss: 3020.068115\n",
      "Training Batch: 11878 Loss: 3063.965576\n",
      "Training Batch: 11879 Loss: 2984.583008\n",
      "Training Batch: 11880 Loss: 3023.231445\n",
      "Training Batch: 11881 Loss: 3251.272705\n",
      "Training Batch: 11882 Loss: 3031.602051\n",
      "Training Batch: 11883 Loss: 3196.379639\n",
      "Training Batch: 11884 Loss: 3159.354492\n",
      "Training Batch: 11885 Loss: 3076.446777\n",
      "Training Batch: 11886 Loss: 3157.633301\n",
      "Training Batch: 11887 Loss: 3149.250488\n",
      "Training Batch: 11888 Loss: 3042.074707\n",
      "Training Batch: 11889 Loss: 3215.505127\n",
      "Training Batch: 11890 Loss: 3028.117432\n",
      "Training Batch: 11891 Loss: 3119.217773\n",
      "Training Batch: 11892 Loss: 3170.993652\n",
      "Training Batch: 11893 Loss: 3039.974609\n",
      "Training Batch: 11894 Loss: 3113.226074\n",
      "Training Batch: 11895 Loss: 3160.814209\n",
      "Training Batch: 11896 Loss: 3129.821289\n",
      "Training Batch: 11897 Loss: 3098.083008\n",
      "Training Batch: 11898 Loss: 3306.934570\n",
      "Training Batch: 11899 Loss: 3331.852539\n",
      "Training Batch: 11900 Loss: 3102.709473\n",
      "Training Batch: 11901 Loss: 3231.555908\n",
      "Training Batch: 11902 Loss: 3119.516357\n",
      "Training Batch: 11903 Loss: 3222.671875\n",
      "Training Batch: 11904 Loss: 3327.701660\n",
      "Training Batch: 11905 Loss: 3148.444824\n",
      "Training Batch: 11906 Loss: 3288.234619\n",
      "Training Batch: 11907 Loss: 3203.742188\n",
      "Training Batch: 11908 Loss: 3265.944824\n",
      "Training Batch: 11909 Loss: 3145.485352\n",
      "Training Batch: 11910 Loss: 3139.722656\n",
      "Training Batch: 11911 Loss: 3179.554199\n",
      "Training Batch: 11912 Loss: 3155.869873\n",
      "Training Batch: 11913 Loss: 3291.591309\n",
      "Training Batch: 11914 Loss: 3203.280273\n",
      "Training Batch: 11915 Loss: 3050.172852\n",
      "Training Batch: 11916 Loss: 3244.532227\n",
      "Training Batch: 11917 Loss: 3221.668701\n",
      "Training Batch: 11918 Loss: 3056.891602\n",
      "Training Batch: 11919 Loss: 3133.452148\n",
      "Training Batch: 11920 Loss: 3131.271973\n",
      "Training Batch: 11921 Loss: 3264.032959\n",
      "Training Batch: 11922 Loss: 3106.944824\n",
      "Training Batch: 11923 Loss: 3181.213623\n",
      "Training Batch: 11924 Loss: 3081.839355\n",
      "Training Batch: 11925 Loss: 3261.464600\n",
      "Training Batch: 11926 Loss: 3062.209229\n",
      "Training Batch: 11927 Loss: 3230.799805\n",
      "Training Batch: 11928 Loss: 3136.055420\n",
      "Training Batch: 11929 Loss: 3086.515625\n",
      "Training Batch: 11930 Loss: 3051.404297\n",
      "Training Batch: 11931 Loss: 3078.695801\n",
      "Training Batch: 11932 Loss: 3249.383301\n",
      "Training Batch: 11933 Loss: 3111.234619\n",
      "Training Batch: 11934 Loss: 3151.003662\n",
      "Training Batch: 11935 Loss: 3151.174561\n",
      "Training Batch: 11936 Loss: 3233.344482\n",
      "Training Batch: 11937 Loss: 3049.437012\n",
      "Training Batch: 11938 Loss: 3210.986816\n",
      "Training Batch: 11939 Loss: 3164.538086\n",
      "Training Batch: 11940 Loss: 2968.573730\n",
      "Training Batch: 11941 Loss: 3211.272949\n",
      "Training Batch: 11942 Loss: 3175.911133\n",
      "Training Batch: 11943 Loss: 3223.909180\n",
      "Training Batch: 11944 Loss: 3128.909180\n",
      "Training Batch: 11945 Loss: 3020.153320\n",
      "Training Batch: 11946 Loss: 3321.745850\n",
      "Training Batch: 11947 Loss: 3092.674561\n",
      "Training Batch: 11948 Loss: 3195.848389\n",
      "Training Batch: 11949 Loss: 3079.346680\n",
      "Training Batch: 11950 Loss: 3112.760254\n",
      "Training Batch: 11951 Loss: 3215.854736\n",
      "Training Batch: 11952 Loss: 3195.835938\n",
      "Training Batch: 11953 Loss: 3144.958252\n",
      "Training Batch: 11954 Loss: 3185.822266\n",
      "Training Batch: 11955 Loss: 3150.538574\n",
      "Training Batch: 11956 Loss: 3278.302734\n",
      "Training Batch: 11957 Loss: 3054.189941\n",
      "Training Batch: 11958 Loss: 3044.044434\n",
      "Training Batch: 11959 Loss: 3156.005615\n",
      "Training Batch: 11960 Loss: 3039.354980\n",
      "Training Batch: 11961 Loss: 3093.397949\n",
      "Training Batch: 11962 Loss: 3120.942383\n",
      "Training Batch: 11963 Loss: 3472.255371\n",
      "Training Batch: 11964 Loss: 3068.896973\n",
      "Training Batch: 11965 Loss: 3254.767578\n",
      "Training Batch: 11966 Loss: 3089.422119\n",
      "Training Batch: 11967 Loss: 3251.310547\n",
      "Training Batch: 11968 Loss: 3049.998047\n",
      "Training Batch: 11969 Loss: 3098.498047\n",
      "Training Batch: 11970 Loss: 3193.521240\n",
      "Training Batch: 11971 Loss: 3110.730225\n",
      "Training Batch: 11972 Loss: 3107.644287\n",
      "Training Batch: 11973 Loss: 3153.881348\n",
      "Training Batch: 11974 Loss: 3113.357666\n",
      "Training Batch: 11975 Loss: 3268.474121\n",
      "Training Batch: 11976 Loss: 3150.989746\n",
      "Training Batch: 11977 Loss: 3106.703613\n",
      "Training Batch: 11978 Loss: 3159.576416\n",
      "Training Batch: 11979 Loss: 2992.310059\n",
      "Training Batch: 11980 Loss: 2968.536865\n",
      "Training Batch: 11981 Loss: 3143.004883\n",
      "Training Batch: 11982 Loss: 3137.071777\n",
      "Training Batch: 11983 Loss: 3097.154297\n",
      "Training Batch: 11984 Loss: 3103.439697\n",
      "Training Batch: 11985 Loss: 3150.570312\n",
      "Training Batch: 11986 Loss: 3099.100586\n",
      "Training Batch: 11987 Loss: 3121.752930\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Batch: 11988 Loss: 3228.741699\n",
      "Training Batch: 11989 Loss: 3225.046387\n",
      "Training Batch: 11990 Loss: 3092.753418\n",
      "Training Batch: 11991 Loss: 3117.860840\n",
      "Training Batch: 11992 Loss: 3066.291016\n",
      "Training Batch: 11993 Loss: 3056.868652\n",
      "Training Batch: 11994 Loss: 3123.543457\n",
      "Training Batch: 11995 Loss: 3076.148438\n",
      "Training Batch: 11996 Loss: 3362.860596\n",
      "Training Batch: 11997 Loss: 3122.755859\n",
      "Training Batch: 11998 Loss: 3095.108887\n",
      "Training Batch: 11999 Loss: 3094.028809\n",
      "Training Batch: 12000 Loss: 3276.659180\n",
      "Training Batch: 12001 Loss: 3010.392578\n",
      "Training Batch: 12002 Loss: 3203.185059\n",
      "Training Batch: 12003 Loss: 3086.632324\n",
      "Training Batch: 12004 Loss: 3046.663330\n",
      "Training Batch: 12005 Loss: 3062.450195\n",
      "Training Batch: 12006 Loss: 3051.260986\n",
      "Training Batch: 12007 Loss: 3357.662354\n",
      "Training Batch: 12008 Loss: 3173.757812\n",
      "Training Batch: 12009 Loss: 3022.245117\n",
      "Training Batch: 12010 Loss: 3072.523926\n",
      "Training Batch: 12011 Loss: 3051.298828\n",
      "Training Batch: 12012 Loss: 3268.014648\n",
      "Training Batch: 12013 Loss: 3127.047852\n",
      "Training Batch: 12014 Loss: 3039.131348\n",
      "Training Batch: 12015 Loss: 2986.441162\n",
      "Training Batch: 12016 Loss: 3050.934082\n",
      "Training Batch: 12017 Loss: 3715.407959\n",
      "Training Batch: 12018 Loss: 3123.814941\n",
      "Training Batch: 12019 Loss: 3062.385254\n",
      "Training Batch: 12020 Loss: 3104.749512\n",
      "Training Batch: 12021 Loss: 3068.538330\n",
      "Training Batch: 12022 Loss: 3223.193604\n",
      "Training Batch: 12023 Loss: 3132.617920\n",
      "Training Batch: 12024 Loss: 3149.876953\n",
      "Training Batch: 12025 Loss: 3317.096924\n",
      "Training Batch: 12026 Loss: 3119.580078\n",
      "Training Batch: 12027 Loss: 3165.395020\n",
      "Training Batch: 12028 Loss: 3056.103516\n",
      "Training Batch: 12029 Loss: 3034.258301\n",
      "Training Batch: 12030 Loss: 3366.691406\n",
      "Training Batch: 12031 Loss: 3013.422119\n",
      "Training Batch: 12032 Loss: 3104.278809\n",
      "Training Batch: 12033 Loss: 3145.461914\n",
      "Training Batch: 12034 Loss: 3096.263672\n",
      "Training Batch: 12035 Loss: 3116.022949\n",
      "Training Batch: 12036 Loss: 3168.868652\n",
      "Training Batch: 12037 Loss: 3119.173828\n",
      "Training Batch: 12038 Loss: 3117.685303\n",
      "Training Batch: 12039 Loss: 3124.593750\n",
      "Training Batch: 12040 Loss: 3089.771240\n",
      "Training Batch: 12041 Loss: 3097.173828\n",
      "Training Batch: 12042 Loss: 3214.996826\n",
      "Training Batch: 12043 Loss: 3095.655029\n",
      "Training Batch: 12044 Loss: 3171.437500\n",
      "Training Batch: 12045 Loss: 3077.811035\n",
      "Training Batch: 12046 Loss: 3174.004883\n",
      "Training Batch: 12047 Loss: 3057.366211\n",
      "Training Batch: 12048 Loss: 3156.025391\n",
      "Training Batch: 12049 Loss: 3206.111084\n",
      "Training Batch: 12050 Loss: 3116.707031\n",
      "Training Batch: 12051 Loss: 3062.378418\n",
      "Training Batch: 12052 Loss: 3353.428711\n",
      "Training Batch: 12053 Loss: 3110.117188\n",
      "Training Batch: 12054 Loss: 3092.066406\n",
      "Training Batch: 12055 Loss: 3447.644043\n",
      "Training Batch: 12056 Loss: 3249.511963\n",
      "Training Batch: 12057 Loss: 3121.671875\n",
      "Training Batch: 12058 Loss: 3125.289551\n",
      "Training Batch: 12059 Loss: 3051.252441\n",
      "Training Batch: 12060 Loss: 3064.874512\n",
      "Training Batch: 12061 Loss: 2986.184082\n",
      "Training Batch: 12062 Loss: 3256.659180\n",
      "Training Batch: 12063 Loss: 3083.911133\n",
      "Training Batch: 12064 Loss: 3001.154297\n",
      "Training Batch: 12065 Loss: 3023.829834\n",
      "Training Batch: 12066 Loss: 3112.000000\n",
      "Training Batch: 12067 Loss: 3243.829590\n",
      "Training Batch: 12068 Loss: 3059.420410\n",
      "Training Batch: 12069 Loss: 3217.430176\n",
      "Training Batch: 12070 Loss: 3183.454834\n",
      "Training Batch: 12071 Loss: 2992.181152\n",
      "Training Batch: 12072 Loss: 3043.329590\n",
      "Training Batch: 12073 Loss: 3096.707520\n",
      "Training Batch: 12074 Loss: 3292.244141\n",
      "Training Batch: 12075 Loss: 3236.169434\n",
      "Training Batch: 12076 Loss: 3178.416016\n",
      "Training Batch: 12077 Loss: 3197.037109\n",
      "Training Batch: 12078 Loss: 2970.187744\n",
      "Training Batch: 12079 Loss: 3167.904297\n",
      "Training Batch: 12080 Loss: 3066.346680\n",
      "Training Batch: 12081 Loss: 3321.657959\n",
      "Training Batch: 12082 Loss: 3159.549805\n",
      "Training Batch: 12083 Loss: 3250.464355\n",
      "Training Batch: 12084 Loss: 3129.049561\n",
      "Training Batch: 12085 Loss: 3111.087646\n",
      "Training Batch: 12086 Loss: 3136.755859\n",
      "Training Batch: 12087 Loss: 3104.656738\n",
      "Training Batch: 12088 Loss: 3170.239746\n",
      "Training Batch: 12089 Loss: 3211.965332\n",
      "Training Batch: 12090 Loss: 3189.787842\n",
      "Training Batch: 12091 Loss: 3059.460449\n",
      "Training Batch: 12092 Loss: 3178.949219\n",
      "Training Batch: 12093 Loss: 3215.343262\n",
      "Training Batch: 12094 Loss: 3127.683594\n",
      "Training Batch: 12095 Loss: 3304.671631\n",
      "Training Batch: 12096 Loss: 3031.403320\n",
      "Training Batch: 12097 Loss: 3007.151367\n",
      "Training Batch: 12098 Loss: 3088.778809\n",
      "Training Batch: 12099 Loss: 3047.546143\n",
      "Training Batch: 12100 Loss: 3302.197266\n",
      "Training Batch: 12101 Loss: 3442.760010\n",
      "Training Batch: 12102 Loss: 3193.291992\n",
      "Training Batch: 12103 Loss: 3221.739746\n",
      "Training Batch: 12104 Loss: 3334.778809\n",
      "Training Batch: 12105 Loss: 3070.579102\n",
      "Training Batch: 12106 Loss: 3006.354492\n",
      "Training Batch: 12107 Loss: 3111.619629\n",
      "Training Batch: 12108 Loss: 3098.478516\n",
      "Training Batch: 12109 Loss: 3078.669678\n",
      "Training Batch: 12110 Loss: 3163.465332\n",
      "Training Batch: 12111 Loss: 3206.866943\n",
      "Training Batch: 12112 Loss: 3195.345947\n",
      "Training Batch: 12113 Loss: 3275.472656\n",
      "Training Batch: 12114 Loss: 3168.857910\n",
      "Training Batch: 12115 Loss: 3144.003418\n",
      "Training Batch: 12116 Loss: 3402.412598\n",
      "Training Batch: 12117 Loss: 3379.209229\n",
      "Training Batch: 12118 Loss: 3101.427246\n",
      "Training Batch: 12119 Loss: 3068.886475\n",
      "Training Batch: 12120 Loss: 3149.503418\n",
      "Training Batch: 12121 Loss: 3234.246582\n",
      "Training Batch: 12122 Loss: 3076.617676\n",
      "Training Batch: 12123 Loss: 3149.699219\n",
      "Training Batch: 12124 Loss: 2980.824219\n",
      "Training Batch: 12125 Loss: 3091.450439\n",
      "Training Batch: 12126 Loss: 3115.773682\n",
      "Training Batch: 12127 Loss: 3118.548584\n",
      "Training Batch: 12128 Loss: 3018.522949\n",
      "Training Batch: 12129 Loss: 3036.461914\n",
      "Training Batch: 12130 Loss: 3195.165039\n",
      "Training Batch: 12131 Loss: 3038.689453\n",
      "Training Batch: 12132 Loss: 3167.560547\n",
      "Training Batch: 12133 Loss: 3159.235352\n",
      "Training Batch: 12134 Loss: 3040.202637\n",
      "Training Batch: 12135 Loss: 3076.915771\n",
      "Training Batch: 12136 Loss: 3058.174805\n",
      "Training Batch: 12137 Loss: 3002.902588\n",
      "Training Batch: 12138 Loss: 3077.038574\n",
      "Training Batch: 12139 Loss: 3139.659180\n",
      "Training Batch: 12140 Loss: 3050.435059\n",
      "Training Batch: 12141 Loss: 3132.916016\n",
      "Training Batch: 12142 Loss: 3046.259033\n",
      "Training Batch: 12143 Loss: 3358.377441\n",
      "Training Batch: 12144 Loss: 3097.576172\n",
      "Training Batch: 12145 Loss: 3285.916016\n",
      "Training Batch: 12146 Loss: 3154.975586\n",
      "Training Batch: 12147 Loss: 3049.333008\n",
      "Training Batch: 12148 Loss: 3158.821289\n",
      "Training Batch: 12149 Loss: 2974.775391\n",
      "Training Batch: 12150 Loss: 3195.873779\n",
      "Training Batch: 12151 Loss: 3101.228516\n",
      "Training Batch: 12152 Loss: 3057.180176\n",
      "Training Batch: 12153 Loss: 3147.697266\n",
      "Training Batch: 12154 Loss: 3230.608887\n",
      "Training Batch: 12155 Loss: 3037.431152\n",
      "Training Batch: 12156 Loss: 3181.698242\n",
      "Training Batch: 12157 Loss: 3231.403320\n",
      "Training Batch: 12158 Loss: 2994.805664\n",
      "Training Batch: 12159 Loss: 3153.144043\n",
      "Training Batch: 12160 Loss: 3329.856445\n",
      "Training Batch: 12161 Loss: 3184.372070\n",
      "Training Batch: 12162 Loss: 3291.707520\n",
      "Training Batch: 12163 Loss: 3028.288086\n",
      "Training Batch: 12164 Loss: 3234.074707\n",
      "Training Batch: 12165 Loss: 3173.533203\n",
      "Training Batch: 12166 Loss: 3253.467773\n",
      "Training Batch: 12167 Loss: 3139.656250\n",
      "Training Batch: 12168 Loss: 3089.811523\n",
      "Training Batch: 12169 Loss: 3166.599121\n",
      "Training Batch: 12170 Loss: 3053.891846\n",
      "Training Batch: 12171 Loss: 3105.925293\n",
      "Training Batch: 12172 Loss: 3098.720947\n",
      "Training Batch: 12173 Loss: 3179.374268\n",
      "Training Batch: 12174 Loss: 3425.041504\n",
      "Training Batch: 12175 Loss: 3140.784668\n",
      "Training Batch: 12176 Loss: 3057.440430\n",
      "Training Batch: 12177 Loss: 2982.060303\n",
      "Training Batch: 12178 Loss: 3015.714844\n",
      "Training Batch: 12179 Loss: 3041.319824\n",
      "Training Batch: 12180 Loss: 3095.951172\n",
      "Training Batch: 12181 Loss: 3196.546875\n",
      "Training Batch: 12182 Loss: 3173.634766\n",
      "Training Batch: 12183 Loss: 3206.046387\n",
      "Training Batch: 12184 Loss: 3053.164551\n",
      "Training Batch: 12185 Loss: 3080.163574\n",
      "Training Batch: 12186 Loss: 3217.443848\n",
      "Training Batch: 12187 Loss: 3058.021484\n",
      "Training Batch: 12188 Loss: 3052.354980\n",
      "Training Batch: 12189 Loss: 3121.313232\n",
      "Training Batch: 12190 Loss: 3229.058105\n",
      "Training Batch: 12191 Loss: 3207.085449\n",
      "Training Batch: 12192 Loss: 3093.774658\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Batch: 12193 Loss: 3224.118164\n",
      "Training Batch: 12194 Loss: 3110.610596\n",
      "Training Batch: 12195 Loss: 3200.943115\n",
      "Training Batch: 12196 Loss: 3253.355957\n",
      "Training Batch: 12197 Loss: 3174.576172\n",
      "Training Batch: 12198 Loss: 3117.364258\n",
      "Training Batch: 12199 Loss: 3253.798828\n",
      "Training Batch: 12200 Loss: 3155.547363\n",
      "Training Batch: 12201 Loss: 3208.637695\n",
      "Training Batch: 12202 Loss: 3340.555664\n",
      "Training Batch: 12203 Loss: 3114.776855\n",
      "Training Batch: 12204 Loss: 3397.820312\n",
      "Training Batch: 12205 Loss: 3190.937012\n",
      "Training Batch: 12206 Loss: 3142.536133\n",
      "Training Batch: 12207 Loss: 3612.746826\n",
      "Training Batch: 12208 Loss: 3210.543457\n",
      "Training Batch: 12209 Loss: 3271.517090\n",
      "Training Batch: 12210 Loss: 3208.182129\n",
      "Training Batch: 12211 Loss: 3238.910889\n",
      "Training Batch: 12212 Loss: 3173.492188\n",
      "Training Batch: 12213 Loss: 3257.723389\n",
      "Training Batch: 12214 Loss: 3166.665283\n",
      "Training Batch: 12215 Loss: 3144.979492\n",
      "Training Batch: 12216 Loss: 3418.864746\n",
      "Training Batch: 12217 Loss: 3367.634277\n",
      "Training Batch: 12218 Loss: 3168.837402\n",
      "Training Batch: 12219 Loss: 3120.521729\n",
      "Training Batch: 12220 Loss: 3413.035645\n",
      "Training Batch: 12221 Loss: 3260.654541\n",
      "Training Batch: 12222 Loss: 3213.484131\n",
      "Training Batch: 12223 Loss: 3079.570801\n",
      "Training Batch: 12224 Loss: 3158.541504\n",
      "Training Batch: 12225 Loss: 3105.732666\n",
      "Training Batch: 12226 Loss: 3075.029053\n",
      "Training Batch: 12227 Loss: 3214.830566\n",
      "Training Batch: 12228 Loss: 3057.793945\n",
      "Training Batch: 12229 Loss: 3196.496094\n",
      "Training Batch: 12230 Loss: 3545.868164\n",
      "Training Batch: 12231 Loss: 3154.370117\n",
      "Training Batch: 12232 Loss: 3227.779785\n",
      "Training Batch: 12233 Loss: 3344.629883\n",
      "Training Batch: 12234 Loss: 3104.812256\n",
      "Training Batch: 12235 Loss: 3163.354736\n",
      "Training Batch: 12236 Loss: 3361.480225\n",
      "Training Batch: 12237 Loss: 3281.146484\n",
      "Training Batch: 12238 Loss: 3133.244873\n",
      "Training Batch: 12239 Loss: 3271.759277\n",
      "Training Batch: 12240 Loss: 3336.534668\n",
      "Training Batch: 12241 Loss: 3237.513916\n",
      "Training Batch: 12242 Loss: 3297.152832\n",
      "Training Batch: 12243 Loss: 3163.104980\n",
      "Training Batch: 12244 Loss: 3164.163818\n",
      "Training Batch: 12245 Loss: 3263.666016\n",
      "Training Batch: 12246 Loss: 3056.304688\n",
      "Training Batch: 12247 Loss: 3131.019531\n",
      "Training Batch: 12248 Loss: 3301.535156\n",
      "Training Batch: 12249 Loss: 3152.880859\n",
      "Training Batch: 12250 Loss: 3115.342773\n",
      "Training Batch: 12251 Loss: 3088.578613\n",
      "Training Batch: 12252 Loss: 3134.961426\n",
      "Training Batch: 12253 Loss: 3169.395020\n",
      "Training Batch: 12254 Loss: 3153.238281\n",
      "Training Batch: 12255 Loss: 3065.868164\n",
      "Training Batch: 12256 Loss: 3329.644531\n",
      "Training Batch: 12257 Loss: 3282.204346\n",
      "Training Batch: 12258 Loss: 3042.294922\n",
      "Training Batch: 12259 Loss: 3125.627441\n",
      "Training Batch: 12260 Loss: 3194.394287\n",
      "Training Batch: 12261 Loss: 3095.861328\n",
      "Training Batch: 12262 Loss: 3099.635742\n",
      "Training Batch: 12263 Loss: 3114.690430\n",
      "Training Batch: 12264 Loss: 3088.994873\n",
      "Training Batch: 12265 Loss: 3225.084473\n",
      "Training Batch: 12266 Loss: 3036.178711\n",
      "Training Batch: 12267 Loss: 3116.879395\n",
      "Training Batch: 12268 Loss: 3135.049561\n",
      "Training Batch: 12269 Loss: 3104.051514\n",
      "Training Batch: 12270 Loss: 3091.486328\n",
      "Training Batch: 12271 Loss: 3036.845703\n",
      "Training Batch: 12272 Loss: 3122.140625\n",
      "Training Batch: 12273 Loss: 3257.243164\n",
      "Training Batch: 12274 Loss: 3227.329590\n",
      "Training Batch: 12275 Loss: 3109.784668\n",
      "Training Batch: 12276 Loss: 3045.502441\n",
      "Training Batch: 12277 Loss: 3136.882812\n",
      "Training Batch: 12278 Loss: 3174.802490\n",
      "Training Batch: 12279 Loss: 3132.982422\n",
      "Training Batch: 12280 Loss: 3140.471191\n",
      "Training Batch: 12281 Loss: 3030.391602\n",
      "Training Batch: 12282 Loss: 3049.460938\n",
      "Training Batch: 12283 Loss: 3082.486084\n",
      "Training Batch: 12284 Loss: 3178.909180\n",
      "Training Batch: 12285 Loss: 3120.413086\n",
      "Training Batch: 12286 Loss: 3142.350098\n",
      "Training Batch: 12287 Loss: 3060.690674\n",
      "Training Batch: 12288 Loss: 3083.329102\n",
      "Training Batch: 12289 Loss: 3231.105957\n",
      "Training Batch: 12290 Loss: 3032.004395\n",
      "Training Batch: 12291 Loss: 2950.208984\n",
      "Training Batch: 12292 Loss: 3105.777832\n",
      "Training Batch: 12293 Loss: 3116.781738\n",
      "Training Batch: 12294 Loss: 3130.465332\n",
      "Training Batch: 12295 Loss: 3165.119629\n",
      "Training Batch: 12296 Loss: 3105.230713\n",
      "Training Batch: 12297 Loss: 3104.020020\n",
      "Training Batch: 12298 Loss: 3110.715820\n",
      "Training Batch: 12299 Loss: 3115.415527\n",
      "Training Batch: 12300 Loss: 3137.769531\n",
      "Training Batch: 12301 Loss: 3126.619141\n",
      "Training Batch: 12302 Loss: 3077.285645\n",
      "Training Batch: 12303 Loss: 3134.388672\n",
      "Training Batch: 12304 Loss: 3130.282227\n",
      "Training Batch: 12305 Loss: 3136.709473\n",
      "Training Batch: 12306 Loss: 3191.847900\n",
      "Training Batch: 12307 Loss: 3180.139160\n",
      "Training Batch: 12308 Loss: 3197.706543\n",
      "Training Batch: 12309 Loss: 3334.324219\n",
      "Training Batch: 12310 Loss: 3038.732422\n",
      "Training Batch: 12311 Loss: 3440.793457\n",
      "Training Batch: 12312 Loss: 3024.906250\n",
      "Training Batch: 12313 Loss: 3191.679199\n",
      "Training Batch: 12314 Loss: 3129.048340\n",
      "Training Batch: 12315 Loss: 3194.056152\n",
      "Training Batch: 12316 Loss: 3176.635254\n",
      "Training Batch: 12317 Loss: 3049.446777\n",
      "Training Batch: 12318 Loss: 3078.792969\n",
      "Training Batch: 12319 Loss: 3283.099121\n",
      "Training Batch: 12320 Loss: 3049.790039\n",
      "Training Batch: 12321 Loss: 3170.790771\n",
      "Training Batch: 12322 Loss: 3233.853271\n",
      "Training Batch: 12323 Loss: 3170.851807\n",
      "Training Batch: 12324 Loss: 3019.397217\n",
      "Training Batch: 12325 Loss: 3305.702881\n",
      "Training Batch: 12326 Loss: 3118.067383\n",
      "Training Batch: 12327 Loss: 3147.728027\n",
      "Training Batch: 12328 Loss: 3098.802246\n",
      "Training Batch: 12329 Loss: 3051.719238\n",
      "Training Batch: 12330 Loss: 3218.098633\n",
      "Training Batch: 12331 Loss: 3254.987305\n",
      "Training Batch: 12332 Loss: 3158.942871\n",
      "Training Batch: 12333 Loss: 3132.429688\n",
      "Training Batch: 12334 Loss: 3081.299072\n",
      "Training Batch: 12335 Loss: 3050.836426\n",
      "Training Batch: 12336 Loss: 3309.987793\n",
      "Training Batch: 12337 Loss: 3220.113525\n",
      "Training Batch: 12338 Loss: 3129.561035\n",
      "Training Batch: 12339 Loss: 3264.771973\n",
      "Training Batch: 12340 Loss: 3083.659180\n",
      "Training Batch: 12341 Loss: 3196.924561\n",
      "Training Batch: 12342 Loss: 3175.907715\n",
      "Training Batch: 12343 Loss: 3346.399170\n",
      "Training Batch: 12344 Loss: 3185.306396\n",
      "Training Batch: 12345 Loss: 3045.761475\n",
      "Training Batch: 12346 Loss: 3046.443848\n",
      "Training Batch: 12347 Loss: 3097.848389\n",
      "Training Batch: 12348 Loss: 3197.448730\n",
      "Training Batch: 12349 Loss: 3187.811523\n",
      "Training Batch: 12350 Loss: 3163.649414\n",
      "Training Batch: 12351 Loss: 3165.218750\n",
      "Training Batch: 12352 Loss: 3279.512207\n",
      "Training Batch: 12353 Loss: 3036.566650\n",
      "Training Batch: 12354 Loss: 3101.065918\n",
      "Training Batch: 12355 Loss: 3150.017334\n",
      "Training Batch: 12356 Loss: 3208.571777\n",
      "Training Batch: 12357 Loss: 3226.677734\n",
      "Training Batch: 12358 Loss: 3300.800293\n",
      "Training Batch: 12359 Loss: 3231.507568\n",
      "Training Batch: 12360 Loss: 3209.208740\n",
      "Training Batch: 12361 Loss: 3171.499023\n",
      "Training Batch: 12362 Loss: 3090.991455\n",
      "Training Batch: 12363 Loss: 3287.992920\n",
      "Training Batch: 12364 Loss: 3265.924072\n",
      "Training Batch: 12365 Loss: 3294.652100\n",
      "Training Batch: 12366 Loss: 3077.916748\n",
      "Training Batch: 12367 Loss: 3121.359375\n",
      "Training Batch: 12368 Loss: 3039.398682\n",
      "Training Batch: 12369 Loss: 3076.434570\n",
      "Training Batch: 12370 Loss: 3167.895752\n",
      "Training Batch: 12371 Loss: 3108.018066\n",
      "Training Batch: 12372 Loss: 3318.924805\n",
      "Training Batch: 12373 Loss: 3338.829346\n",
      "Training Batch: 12374 Loss: 3081.351562\n",
      "Training Batch: 12375 Loss: 3154.800293\n",
      "Training Batch: 12376 Loss: 3093.892090\n",
      "Training Batch: 12377 Loss: 3096.681885\n",
      "Training Batch: 12378 Loss: 3118.096191\n",
      "Training Batch: 12379 Loss: 3030.089844\n",
      "Training Batch: 12380 Loss: 3060.277832\n",
      "Training Batch: 12381 Loss: 3017.300781\n",
      "Training Batch: 12382 Loss: 3195.333008\n",
      "Training Batch: 12383 Loss: 3497.513672\n",
      "Training Batch: 12384 Loss: 3150.659180\n",
      "Training Batch: 12385 Loss: 3299.058105\n",
      "Training Batch: 12386 Loss: 3026.450195\n",
      "Training Batch: 12387 Loss: 3044.471191\n",
      "Training Batch: 12388 Loss: 3023.108398\n",
      "Training Batch: 12389 Loss: 3060.983398\n",
      "Training Batch: 12390 Loss: 3083.169434\n",
      "Training Batch: 12391 Loss: 3202.755371\n",
      "Training Batch: 12392 Loss: 3156.310059\n",
      "Training Batch: 12393 Loss: 3345.789062\n",
      "Training Batch: 12394 Loss: 3167.881836\n",
      "Training Batch: 12395 Loss: 3108.646729\n",
      "Training Batch: 12396 Loss: 3222.966309\n",
      "Training Batch: 12397 Loss: 3162.994629\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Batch: 12398 Loss: 3179.333496\n",
      "Training Batch: 12399 Loss: 3125.239502\n",
      "Training Batch: 12400 Loss: 3241.945312\n",
      "Training Batch: 12401 Loss: 3051.912109\n",
      "Training Batch: 12402 Loss: 3136.252441\n",
      "Training Batch: 12403 Loss: 3100.094238\n",
      "Training Batch: 12404 Loss: 3119.612061\n",
      "Training Batch: 12405 Loss: 3226.982178\n",
      "Training Batch: 12406 Loss: 3008.733887\n",
      "Training Batch: 12407 Loss: 3319.500977\n",
      "Training Batch: 12408 Loss: 3230.175293\n",
      "Training Batch: 12409 Loss: 3268.803711\n",
      "Training Batch: 12410 Loss: 3133.733398\n",
      "Training Batch: 12411 Loss: 3358.383057\n",
      "Training Batch: 12412 Loss: 3077.141846\n",
      "Training Batch: 12413 Loss: 3120.785645\n",
      "Training Batch: 12414 Loss: 3212.397217\n",
      "Training Batch: 12415 Loss: 3143.145020\n",
      "Training Batch: 12416 Loss: 3228.859131\n",
      "Training Batch: 12417 Loss: 3246.081055\n",
      "Training Batch: 12418 Loss: 3046.167480\n",
      "Training Batch: 12419 Loss: 3067.076660\n",
      "Training Batch: 12420 Loss: 3200.398682\n",
      "Training Batch: 12421 Loss: 3181.949219\n",
      "Training Batch: 12422 Loss: 3128.416992\n",
      "Training Batch: 12423 Loss: 3048.986816\n",
      "Training Batch: 12424 Loss: 3169.853271\n",
      "Training Batch: 12425 Loss: 3077.582520\n",
      "Training Batch: 12426 Loss: 3067.780762\n",
      "Training Batch: 12427 Loss: 3111.648438\n",
      "Training Batch: 12428 Loss: 3151.716797\n",
      "Training Batch: 12429 Loss: 3062.161133\n",
      "Training Batch: 12430 Loss: 3090.197266\n",
      "Training Batch: 12431 Loss: 3098.591309\n",
      "Training Batch: 12432 Loss: 2998.385254\n",
      "Training Batch: 12433 Loss: 3218.908691\n",
      "Training Batch: 12434 Loss: 3500.421631\n",
      "Training Batch: 12435 Loss: 3257.927002\n",
      "Training Batch: 12436 Loss: 3146.239014\n",
      "Training Batch: 12437 Loss: 3031.921631\n",
      "Training Batch: 12438 Loss: 3223.923340\n",
      "Training Batch: 12439 Loss: 2996.715576\n",
      "Training Batch: 12440 Loss: 3076.991699\n",
      "Training Batch: 12441 Loss: 3086.099609\n",
      "Training Batch: 12442 Loss: 3157.901855\n",
      "Training Batch: 12443 Loss: 3128.997559\n",
      "Training Batch: 12444 Loss: 2972.749512\n",
      "Training Batch: 12445 Loss: 3090.480957\n",
      "Training Batch: 12446 Loss: 3049.316406\n",
      "Training Batch: 12447 Loss: 3139.183105\n",
      "Training Batch: 12448 Loss: 3058.665039\n",
      "Training Batch: 12449 Loss: 3122.359375\n",
      "Training Batch: 12450 Loss: 3043.104004\n",
      "Training Batch: 12451 Loss: 3136.261475\n",
      "Training Batch: 12452 Loss: 3114.212402\n",
      "Training Batch: 12453 Loss: 3051.363281\n",
      "Training Batch: 12454 Loss: 3225.020020\n",
      "Training Batch: 12455 Loss: 3090.977539\n",
      "Training Batch: 12456 Loss: 3260.376221\n",
      "Training Batch: 12457 Loss: 3269.138672\n",
      "Training Batch: 12458 Loss: 3184.360352\n",
      "Training Batch: 12459 Loss: 3095.384766\n",
      "Training Batch: 12460 Loss: 3025.147461\n",
      "Training Batch: 12461 Loss: 3051.754395\n",
      "Training Batch: 12462 Loss: 3046.567383\n",
      "Training Batch: 12463 Loss: 3259.207031\n",
      "Training Batch: 12464 Loss: 3139.790039\n",
      "Training Batch: 12465 Loss: 3255.551270\n",
      "Training Batch: 12466 Loss: 3343.215332\n",
      "Training Batch: 12467 Loss: 3219.927490\n",
      "Training Batch: 12468 Loss: 3231.108887\n",
      "Training Batch: 12469 Loss: 3132.695801\n",
      "Training Batch: 12470 Loss: 3083.614746\n",
      "Training Batch: 12471 Loss: 3068.165527\n",
      "Training Batch: 12472 Loss: 3148.479492\n",
      "Training Batch: 12473 Loss: 3113.494141\n",
      "Training Batch: 12474 Loss: 3120.603760\n",
      "Training Batch: 12475 Loss: 3247.487793\n",
      "Training Batch: 12476 Loss: 3201.298828\n",
      "Training Batch: 12477 Loss: 3160.325195\n",
      "Training Batch: 12478 Loss: 3084.290527\n",
      "Training Batch: 12479 Loss: 3127.067383\n",
      "Training Batch: 12480 Loss: 3267.234863\n",
      "Training Batch: 12481 Loss: 3089.064941\n",
      "Training Batch: 12482 Loss: 3168.802979\n",
      "Training Batch: 12483 Loss: 3312.112305\n",
      "Training Batch: 12484 Loss: 3061.081055\n",
      "Training Batch: 12485 Loss: 3219.426514\n",
      "Training Batch: 12486 Loss: 3106.809326\n",
      "Training Batch: 12487 Loss: 3023.711426\n",
      "Training Batch: 12488 Loss: 3075.675781\n",
      "Training Batch: 12489 Loss: 3119.933105\n",
      "Training Batch: 12490 Loss: 3113.488281\n",
      "Training Batch: 12491 Loss: 3070.381836\n",
      "Training Batch: 12492 Loss: 2974.825439\n",
      "Training Batch: 12493 Loss: 3258.315918\n",
      "Training Batch: 12494 Loss: 3151.048340\n",
      "Training Batch: 12495 Loss: 3104.558105\n",
      "Training Batch: 12496 Loss: 3328.547852\n",
      "Training Batch: 12497 Loss: 3266.285645\n",
      "Training Batch: 12498 Loss: 3156.600586\n",
      "Training Batch: 12499 Loss: 3101.677734\n",
      "Training Batch: 12500 Loss: 3294.116943\n",
      "Training Batch: 12501 Loss: 3266.122070\n",
      "Training Batch: 12502 Loss: 3258.648682\n",
      "Training Batch: 12503 Loss: 3060.310059\n",
      "Training Batch: 12504 Loss: 3225.334961\n",
      "Training Batch: 12505 Loss: 3086.770508\n",
      "Training Batch: 12506 Loss: 3095.503418\n",
      "Training Batch: 12507 Loss: 3162.870605\n",
      "Training Batch: 12508 Loss: 3194.379395\n",
      "Training Batch: 12509 Loss: 3133.653809\n",
      "Training Batch: 12510 Loss: 3083.505371\n",
      "Training Batch: 12511 Loss: 3075.390137\n",
      "Training Batch: 12512 Loss: 3228.794434\n",
      "Training Batch: 12513 Loss: 3166.963867\n",
      "Training Batch: 12514 Loss: 3191.701416\n",
      "Training Batch: 12515 Loss: 3094.188477\n",
      "Training Batch: 12516 Loss: 3143.848389\n",
      "Training Batch: 12517 Loss: 3179.501465\n",
      "Training Batch: 12518 Loss: 3081.590088\n",
      "Training Batch: 12519 Loss: 3085.167969\n",
      "Training Batch: 12520 Loss: 3150.689453\n",
      "Training Batch: 12521 Loss: 3197.157715\n",
      "Training Batch: 12522 Loss: 3086.255859\n",
      "Training Batch: 12523 Loss: 3085.419922\n",
      "Training Batch: 12524 Loss: 3176.933105\n",
      "Training Batch: 12525 Loss: 3214.395020\n",
      "Training Batch: 12526 Loss: 3367.929688\n",
      "Training Batch: 12527 Loss: 3035.712891\n",
      "Training Batch: 12528 Loss: 3109.997559\n",
      "Training Batch: 12529 Loss: 3053.570801\n",
      "Training Batch: 12530 Loss: 3153.277344\n",
      "Training Batch: 12531 Loss: 3292.718262\n",
      "Training Batch: 12532 Loss: 3045.734863\n",
      "Training Batch: 12533 Loss: 3424.580566\n",
      "Training Batch: 12534 Loss: 3079.963623\n",
      "Training Batch: 12535 Loss: 3132.643066\n",
      "Training Batch: 12536 Loss: 3183.481934\n",
      "Training Batch: 12537 Loss: 3062.345703\n",
      "Training Batch: 12538 Loss: 3312.707764\n",
      "Training Batch: 12539 Loss: 3015.908203\n",
      "Training Batch: 12540 Loss: 3274.515869\n",
      "Training Batch: 12541 Loss: 3060.700928\n",
      "Training Batch: 12542 Loss: 3097.332031\n",
      "Training Batch: 12543 Loss: 3075.562988\n",
      "Training Batch: 12544 Loss: 3302.847168\n",
      "Training Batch: 12545 Loss: 3135.820068\n",
      "Training Batch: 12546 Loss: 3302.927246\n",
      "Training Batch: 12547 Loss: 3047.130371\n",
      "Training Batch: 12548 Loss: 3250.307129\n",
      "Training Batch: 12549 Loss: 3132.195557\n",
      "Training Batch: 12550 Loss: 3289.847168\n",
      "Training Batch: 12551 Loss: 3230.729004\n",
      "Training Batch: 12552 Loss: 3217.417480\n",
      "Training Batch: 12553 Loss: 3069.580566\n",
      "Training Batch: 12554 Loss: 3161.199219\n",
      "Training Batch: 12555 Loss: 3308.908203\n",
      "Training Batch: 12556 Loss: 3272.233398\n",
      "Training Batch: 12557 Loss: 3108.341797\n",
      "Training Batch: 12558 Loss: 3046.051758\n",
      "Training Batch: 12559 Loss: 3105.518311\n",
      "Training Batch: 12560 Loss: 3085.458984\n",
      "Training Batch: 12561 Loss: 3180.345703\n",
      "Training Batch: 12562 Loss: 3046.490967\n",
      "Training Batch: 12563 Loss: 3152.034180\n",
      "Training Batch: 12564 Loss: 3088.040039\n",
      "Training Batch: 12565 Loss: 3170.384766\n",
      "Training Batch: 12566 Loss: 3445.200684\n",
      "Training Batch: 12567 Loss: 3108.708008\n",
      "Training Batch: 12568 Loss: 3318.581543\n",
      "Training Batch: 12569 Loss: 3189.780273\n",
      "Training Batch: 12570 Loss: 3266.359863\n",
      "Training Batch: 12571 Loss: 3419.382324\n",
      "Training Batch: 12572 Loss: 3172.860107\n",
      "Training Batch: 12573 Loss: 3108.221924\n",
      "Training Batch: 12574 Loss: 3132.603516\n",
      "Training Batch: 12575 Loss: 3144.201416\n",
      "Training Batch: 12576 Loss: 3422.685547\n",
      "Training Batch: 12577 Loss: 3409.501465\n",
      "Training Batch: 12578 Loss: 3276.779785\n",
      "Training Batch: 12579 Loss: 3223.687744\n",
      "Training Batch: 12580 Loss: 3130.003174\n",
      "Training Batch: 12581 Loss: 3135.754883\n",
      "Training Batch: 12582 Loss: 3115.941650\n",
      "Training Batch: 12583 Loss: 3352.310547\n",
      "Training Batch: 12584 Loss: 3125.041992\n",
      "Training Batch: 12585 Loss: 3120.505127\n",
      "Training Batch: 12586 Loss: 3214.444092\n",
      "Training Batch: 12587 Loss: 3130.960938\n",
      "Training Batch: 12588 Loss: 3190.015869\n",
      "Training Batch: 12589 Loss: 3266.283203\n",
      "Training Batch: 12590 Loss: 3086.973145\n",
      "Training Batch: 12591 Loss: 3063.141602\n",
      "Training Batch: 12592 Loss: 3491.731689\n",
      "Training Batch: 12593 Loss: 3147.486084\n",
      "Training Batch: 12594 Loss: 3278.747559\n",
      "Training Batch: 12595 Loss: 3177.712891\n",
      "Training Batch: 12596 Loss: 3087.189453\n",
      "Training Batch: 12597 Loss: 3210.564453\n",
      "Training Batch: 12598 Loss: 3132.562012\n",
      "Training Batch: 12599 Loss: 3028.206543\n",
      "Training Batch: 12600 Loss: 3421.291504\n",
      "Training Batch: 12601 Loss: 3763.905029\n",
      "Training Batch: 12602 Loss: 3154.853516\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Batch: 12603 Loss: 3119.033203\n",
      "Training Batch: 12604 Loss: 3003.681641\n",
      "Training Batch: 12605 Loss: 3070.108887\n",
      "Training Batch: 12606 Loss: 3116.330078\n",
      "Training Batch: 12607 Loss: 3117.354980\n",
      "Training Batch: 12608 Loss: 3194.410645\n",
      "Training Batch: 12609 Loss: 3092.535156\n",
      "Training Batch: 12610 Loss: 3124.459473\n",
      "Training Batch: 12611 Loss: 3237.769043\n",
      "Training Batch: 12612 Loss: 2999.353516\n",
      "Training Batch: 12613 Loss: 2990.823730\n",
      "Training Batch: 12614 Loss: 3058.901611\n",
      "Training Batch: 12615 Loss: 3010.492188\n",
      "Training Batch: 12616 Loss: 3270.374268\n",
      "Training Batch: 12617 Loss: 3029.917725\n",
      "Training Batch: 12618 Loss: 3169.593750\n",
      "Training Batch: 12619 Loss: 3178.823975\n",
      "Training Batch: 12620 Loss: 3127.023193\n",
      "Training Batch: 12621 Loss: 3013.576172\n",
      "Training Batch: 12622 Loss: 3328.650879\n",
      "Training Batch: 12623 Loss: 3070.149414\n",
      "Training Batch: 12624 Loss: 3273.895020\n",
      "Training Batch: 12625 Loss: 3077.189453\n",
      "Training Batch: 12626 Loss: 3053.064941\n",
      "Training Batch: 12627 Loss: 3031.671875\n",
      "Training Batch: 12628 Loss: 3172.382080\n",
      "Training Batch: 12629 Loss: 3115.669922\n",
      "Training Batch: 12630 Loss: 3169.200439\n",
      "Training Batch: 12631 Loss: 3114.915527\n",
      "Training Batch: 12632 Loss: 3083.772461\n",
      "Training Batch: 12633 Loss: 3500.037842\n",
      "Training Batch: 12634 Loss: 3226.117920\n",
      "Training Batch: 12635 Loss: 3331.022217\n",
      "Training Batch: 12636 Loss: 3303.118164\n",
      "Training Batch: 12637 Loss: 3095.964355\n",
      "Training Batch: 12638 Loss: 3225.742188\n",
      "Training Batch: 12639 Loss: 3193.571289\n",
      "Training Batch: 12640 Loss: 3203.710205\n",
      "Training Batch: 12641 Loss: 3151.248779\n",
      "Training Batch: 12642 Loss: 3138.588867\n",
      "Training Batch: 12643 Loss: 3084.811523\n",
      "Training Batch: 12644 Loss: 3158.861328\n",
      "Training Batch: 12645 Loss: 3159.187988\n",
      "Training Batch: 12646 Loss: 3271.292725\n",
      "Training Batch: 12647 Loss: 3102.766846\n",
      "Training Batch: 12648 Loss: 3163.541016\n",
      "Training Batch: 12649 Loss: 3237.103027\n",
      "Training Batch: 12650 Loss: 3156.366211\n",
      "Training Batch: 12651 Loss: 3026.182129\n",
      "Training Batch: 12652 Loss: 3203.851562\n",
      "Training Batch: 12653 Loss: 3142.941162\n",
      "Training Batch: 12654 Loss: 3090.411621\n",
      "Training Batch: 12655 Loss: 3122.208008\n",
      "Training Batch: 12656 Loss: 3060.081055\n",
      "Training Batch: 12657 Loss: 3243.504883\n",
      "Training Batch: 12658 Loss: 3155.887207\n",
      "Training Batch: 12659 Loss: 3207.583984\n",
      "Training Batch: 12660 Loss: 3115.779541\n",
      "Training Batch: 12661 Loss: 3092.195801\n",
      "Training Batch: 12662 Loss: 3049.088379\n",
      "Training Batch: 12663 Loss: 3124.418945\n",
      "Training Batch: 12664 Loss: 3200.169434\n",
      "Training Batch: 12665 Loss: 3075.951172\n",
      "Training Batch: 12666 Loss: 3681.782471\n",
      "Training Batch: 12667 Loss: 3117.635742\n",
      "Training Batch: 12668 Loss: 3081.417236\n",
      "Training Batch: 12669 Loss: 3254.099609\n",
      "Training Batch: 12670 Loss: 3178.902832\n",
      "Training Batch: 12671 Loss: 3148.908203\n",
      "Training Batch: 12672 Loss: 3130.134766\n",
      "Training Batch: 12673 Loss: 3063.625244\n",
      "Training Batch: 12674 Loss: 3104.129883\n",
      "Training Batch: 12675 Loss: 3112.917236\n",
      "Training Batch: 12676 Loss: 3173.562012\n",
      "Training Batch: 12677 Loss: 3089.951172\n",
      "Training Batch: 12678 Loss: 3082.387207\n",
      "Training Batch: 12679 Loss: 3099.207031\n",
      "Training Batch: 12680 Loss: 3497.739746\n",
      "Training Batch: 12681 Loss: 3284.250977\n",
      "Training Batch: 12682 Loss: 3123.137207\n",
      "Training Batch: 12683 Loss: 3156.630859\n",
      "Training Batch: 12684 Loss: 3253.870117\n",
      "Training Batch: 12685 Loss: 3187.969238\n",
      "Training Batch: 12686 Loss: 3137.523438\n",
      "Training Batch: 12687 Loss: 3180.011230\n",
      "Training Batch: 12688 Loss: 3104.766602\n",
      "Training Batch: 12689 Loss: 3061.855957\n",
      "Training Batch: 12690 Loss: 3282.515137\n",
      "Training Batch: 12691 Loss: 3184.626465\n",
      "Training Batch: 12692 Loss: 3077.320801\n",
      "Training Batch: 12693 Loss: 3126.408447\n",
      "Training Batch: 12694 Loss: 3176.735840\n",
      "Training Batch: 12695 Loss: 3115.032715\n",
      "Training Batch: 12696 Loss: 3147.944824\n",
      "Training Batch: 12697 Loss: 3056.419434\n",
      "Training Batch: 12698 Loss: 3219.030273\n",
      "Training Batch: 12699 Loss: 3269.967773\n",
      "Training Batch: 12700 Loss: 3276.112793\n",
      "Training Batch: 12701 Loss: 3146.590820\n",
      "Training Batch: 12702 Loss: 3079.724121\n",
      "Training Batch: 12703 Loss: 3184.203125\n",
      "Training Batch: 12704 Loss: 3131.029785\n",
      "Training Batch: 12705 Loss: 3137.939941\n",
      "Training Batch: 12706 Loss: 3060.443115\n",
      "Training Batch: 12707 Loss: 3163.250977\n",
      "Training Batch: 12708 Loss: 3115.360352\n",
      "Training Batch: 12709 Loss: 3244.827637\n",
      "Training Batch: 12710 Loss: 3077.295898\n",
      "Training Batch: 12711 Loss: 3125.784668\n",
      "Training Batch: 12712 Loss: 3155.259277\n",
      "Training Batch: 12713 Loss: 2991.043457\n",
      "Training Batch: 12714 Loss: 3264.447998\n",
      "Training Batch: 12715 Loss: 3010.747559\n",
      "Training Batch: 12716 Loss: 3038.016602\n",
      "Training Batch: 12717 Loss: 4037.915283\n",
      "Training Batch: 12718 Loss: 3321.651367\n",
      "Training Batch: 12719 Loss: 3167.664062\n",
      "Training Batch: 12720 Loss: 3369.063477\n",
      "Training Batch: 12721 Loss: 3675.640137\n",
      "Training Batch: 12722 Loss: 3294.694336\n",
      "Training Batch: 12723 Loss: 3310.437988\n",
      "Training Batch: 12724 Loss: 3371.166992\n",
      "Training Batch: 12725 Loss: 3191.283447\n",
      "Training Batch: 12726 Loss: 3341.505859\n",
      "Training Batch: 12727 Loss: 3206.800293\n",
      "Training Batch: 12728 Loss: 3097.595459\n",
      "Training Batch: 12729 Loss: 3338.800293\n",
      "Training Batch: 12730 Loss: 3149.850098\n",
      "Training Batch: 12731 Loss: 3107.105469\n",
      "Training Batch: 12732 Loss: 3156.517090\n",
      "Training Batch: 12733 Loss: 3092.962402\n",
      "Training Batch: 12734 Loss: 3198.008545\n",
      "Training Batch: 12735 Loss: 3054.745605\n",
      "Training Batch: 12736 Loss: 3262.663574\n",
      "Training Batch: 12737 Loss: 3210.880859\n",
      "Training Batch: 12738 Loss: 3348.616211\n",
      "Training Batch: 12739 Loss: 3322.864990\n",
      "Training Batch: 12740 Loss: 3418.293457\n",
      "Training Batch: 12741 Loss: 3168.074707\n",
      "Training Batch: 12742 Loss: 3319.377197\n",
      "Training Batch: 12743 Loss: 3232.680420\n",
      "Training Batch: 12744 Loss: 3107.574707\n",
      "Training Batch: 12745 Loss: 3130.844727\n",
      "Training Batch: 12746 Loss: 3096.052246\n",
      "Training Batch: 12747 Loss: 3076.738037\n",
      "Training Batch: 12748 Loss: 3163.900391\n",
      "Training Batch: 12749 Loss: 3303.791992\n",
      "Training Batch: 12750 Loss: 3148.832275\n",
      "Training Batch: 12751 Loss: 3131.671875\n",
      "Training Batch: 12752 Loss: 3192.707520\n",
      "Training Batch: 12753 Loss: 3116.742188\n",
      "Training Batch: 12754 Loss: 3299.963867\n",
      "Training Batch: 12755 Loss: 3198.136230\n",
      "Training Batch: 12756 Loss: 3252.499023\n",
      "Training Batch: 12757 Loss: 3504.531738\n",
      "Training Batch: 12758 Loss: 3222.437256\n",
      "Training Batch: 12759 Loss: 3334.612061\n",
      "Training Batch: 12760 Loss: 3470.514648\n",
      "Training Batch: 12761 Loss: 3240.980957\n",
      "Training Batch: 12762 Loss: 3173.483154\n",
      "Training Batch: 12763 Loss: 3208.126953\n",
      "Training Batch: 12764 Loss: 3192.407227\n",
      "Training Batch: 12765 Loss: 3114.939697\n",
      "Training Batch: 12766 Loss: 3419.526367\n",
      "Training Batch: 12767 Loss: 3375.793457\n",
      "Training Batch: 12768 Loss: 3242.168213\n",
      "Training Batch: 12769 Loss: 3144.721436\n",
      "Training Batch: 12770 Loss: 3032.759033\n",
      "Training Batch: 12771 Loss: 3155.039551\n",
      "Training Batch: 12772 Loss: 3071.625977\n",
      "Training Batch: 12773 Loss: 3110.057129\n",
      "Training Batch: 12774 Loss: 3146.777832\n",
      "Training Batch: 12775 Loss: 3141.112061\n",
      "Training Batch: 12776 Loss: 3318.140381\n",
      "Training Batch: 12777 Loss: 3131.466797\n",
      "Training Batch: 12778 Loss: 3167.513672\n",
      "Training Batch: 12779 Loss: 3080.740967\n",
      "Training Batch: 12780 Loss: 3195.827637\n",
      "Training Batch: 12781 Loss: 3225.696777\n",
      "Training Batch: 12782 Loss: 3270.964844\n",
      "Training Batch: 12783 Loss: 3364.930176\n",
      "Training Batch: 12784 Loss: 3313.435547\n",
      "Training Batch: 12785 Loss: 3296.571533\n",
      "Training Batch: 12786 Loss: 3181.083008\n",
      "Training Batch: 12787 Loss: 3263.753418\n",
      "Training Batch: 12788 Loss: 3069.000488\n",
      "Training Batch: 12789 Loss: 3005.685791\n",
      "Training Batch: 12790 Loss: 3375.976318\n",
      "Training Batch: 12791 Loss: 3080.092773\n",
      "Training Batch: 12792 Loss: 3068.467529\n",
      "Training Batch: 12793 Loss: 3078.245117\n",
      "Training Batch: 12794 Loss: 3090.651855\n",
      "Training Batch: 12795 Loss: 3036.165283\n",
      "Training Batch: 12796 Loss: 2992.833740\n",
      "Training Batch: 12797 Loss: 3266.826172\n",
      "Training Batch: 12798 Loss: 3162.845215\n",
      "Training Batch: 12799 Loss: 3146.151367\n",
      "Training Batch: 12800 Loss: 3078.109863\n",
      "Training Batch: 12801 Loss: 3325.818604\n",
      "Training Batch: 12802 Loss: 3186.761719\n",
      "Training Batch: 12803 Loss: 3104.481934\n",
      "Training Batch: 12804 Loss: 3022.883545\n",
      "Training Batch: 12805 Loss: 3081.590332\n",
      "Training Batch: 12806 Loss: 3102.729980\n",
      "Training Batch: 12807 Loss: 3093.272949\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Batch: 12808 Loss: 3074.779785\n",
      "Training Batch: 12809 Loss: 3131.055420\n",
      "Training Batch: 12810 Loss: 3126.997070\n",
      "Training Batch: 12811 Loss: 3119.311035\n",
      "Training Batch: 12812 Loss: 3200.094971\n",
      "Training Batch: 12813 Loss: 3018.942627\n",
      "Training Batch: 12814 Loss: 3162.067871\n",
      "Training Batch: 12815 Loss: 3092.762207\n",
      "Training Batch: 12816 Loss: 3226.746094\n",
      "Training Batch: 12817 Loss: 3218.288086\n",
      "Training Batch: 12818 Loss: 3046.266357\n",
      "Training Batch: 12819 Loss: 2994.303467\n",
      "Training Batch: 12820 Loss: 3078.628418\n",
      "Training Batch: 12821 Loss: 3126.468506\n",
      "Training Batch: 12822 Loss: 3309.592773\n",
      "Training Batch: 12823 Loss: 3236.753174\n",
      "Training Batch: 12824 Loss: 3159.205078\n",
      "Training Batch: 12825 Loss: 2992.389160\n",
      "Training Batch: 12826 Loss: 3247.247314\n",
      "Training Batch: 12827 Loss: 3260.579346\n",
      "Training Batch: 12828 Loss: 3065.187988\n",
      "Training Batch: 12829 Loss: 3171.990723\n",
      "Training Batch: 12830 Loss: 3031.159180\n",
      "Training Batch: 12831 Loss: 3093.981689\n",
      "Training Batch: 12832 Loss: 3166.915527\n",
      "Training Batch: 12833 Loss: 3332.128662\n",
      "Training Batch: 12834 Loss: 3315.293457\n",
      "Training Batch: 12835 Loss: 3060.871094\n",
      "Training Batch: 12836 Loss: 3275.091309\n",
      "Training Batch: 12837 Loss: 3207.170654\n",
      "Training Batch: 12838 Loss: 3055.810059\n",
      "Training Batch: 12839 Loss: 3182.309082\n",
      "Training Batch: 12840 Loss: 2961.156006\n",
      "Training Batch: 12841 Loss: 3146.537598\n",
      "Training Batch: 12842 Loss: 3081.333984\n",
      "Training Batch: 12843 Loss: 3131.441406\n",
      "Training Batch: 12844 Loss: 2994.381836\n",
      "Training Batch: 12845 Loss: 3137.763672\n",
      "Training Batch: 12846 Loss: 3356.909668\n",
      "Training Batch: 12847 Loss: 3221.135742\n",
      "Training Batch: 12848 Loss: 3184.060547\n",
      "Training Batch: 12849 Loss: 3050.887207\n",
      "Training Batch: 12850 Loss: 3015.957031\n",
      "Training Batch: 12851 Loss: 3194.433594\n",
      "Training Batch: 12852 Loss: 3013.586914\n",
      "Training Batch: 12853 Loss: 3093.520996\n",
      "Training Batch: 12854 Loss: 3077.886719\n",
      "Training Batch: 12855 Loss: 3109.787109\n",
      "Training Batch: 12856 Loss: 3085.693359\n",
      "Training Batch: 12857 Loss: 3489.281982\n",
      "Training Batch: 12858 Loss: 3111.461914\n",
      "Training Batch: 12859 Loss: 3150.219727\n",
      "Training Batch: 12860 Loss: 3163.641113\n",
      "Training Batch: 12861 Loss: 3156.300537\n",
      "Training Batch: 12862 Loss: 3165.523438\n",
      "Training Batch: 12863 Loss: 3087.078125\n",
      "Training Batch: 12864 Loss: 3149.053467\n",
      "Training Batch: 12865 Loss: 2955.075928\n",
      "Training Batch: 12866 Loss: 3201.824951\n",
      "Training Batch: 12867 Loss: 3086.247803\n",
      "Training Batch: 12868 Loss: 3309.921875\n",
      "Training Batch: 12869 Loss: 3153.916016\n",
      "Training Batch: 12870 Loss: 3205.823730\n",
      "Training Batch: 12871 Loss: 3101.218018\n",
      "Training Batch: 12872 Loss: 3134.165527\n",
      "Training Batch: 12873 Loss: 3028.635254\n",
      "Training Batch: 12874 Loss: 3011.421143\n",
      "Training Batch: 12875 Loss: 3012.424072\n",
      "Training Batch: 12876 Loss: 3323.435547\n",
      "Training Batch: 12877 Loss: 3206.185547\n",
      "Training Batch: 12878 Loss: 3083.792480\n",
      "Training Batch: 12879 Loss: 3050.150391\n",
      "Training Batch: 12880 Loss: 3093.529541\n",
      "Training Batch: 12881 Loss: 3097.167969\n",
      "Training Batch: 12882 Loss: 3091.582031\n",
      "Training Batch: 12883 Loss: 3068.036133\n",
      "Training Batch: 12884 Loss: 3302.751953\n",
      "Training Batch: 12885 Loss: 3071.657959\n",
      "Training Batch: 12886 Loss: 3178.438721\n",
      "Training Batch: 12887 Loss: 3132.723389\n",
      "Training Batch: 12888 Loss: 3287.789062\n",
      "Training Batch: 12889 Loss: 3128.934570\n",
      "Training Batch: 12890 Loss: 3045.826660\n",
      "Training Batch: 12891 Loss: 3060.507812\n",
      "Training Batch: 12892 Loss: 3283.557129\n",
      "Training Batch: 12893 Loss: 3518.615723\n",
      "Training Batch: 12894 Loss: 3072.517822\n",
      "Training Batch: 12895 Loss: 3197.003418\n",
      "Training Batch: 12896 Loss: 3038.298584\n",
      "Training Batch: 12897 Loss: 3135.722656\n",
      "Training Batch: 12898 Loss: 3154.034668\n",
      "Training Batch: 12899 Loss: 3099.124512\n",
      "Training Batch: 12900 Loss: 3130.568848\n",
      "Training Batch: 12901 Loss: 3057.629395\n",
      "Training Batch: 12902 Loss: 2963.089355\n",
      "Training Batch: 12903 Loss: 3016.545410\n",
      "Training Batch: 12904 Loss: 3327.040039\n",
      "Training Batch: 12905 Loss: 3078.883789\n",
      "Training Batch: 12906 Loss: 3057.134766\n",
      "Training Batch: 12907 Loss: 3087.351074\n",
      "Training Batch: 12908 Loss: 3143.821289\n",
      "Training Batch: 12909 Loss: 2991.816406\n",
      "Training Batch: 12910 Loss: 3227.467285\n",
      "Training Batch: 12911 Loss: 3255.853516\n",
      "Training Batch: 12912 Loss: 3100.529297\n",
      "Training Batch: 12913 Loss: 3278.515137\n",
      "Training Batch: 12914 Loss: 3160.317383\n",
      "Training Batch: 12915 Loss: 3062.438965\n",
      "Training Batch: 12916 Loss: 3183.012695\n",
      "Training Batch: 12917 Loss: 3081.094238\n",
      "Training Batch: 12918 Loss: 3224.989746\n",
      "Training Batch: 12919 Loss: 3118.163574\n",
      "Training Batch: 12920 Loss: 3065.112305\n",
      "Training Batch: 12921 Loss: 3080.616699\n",
      "Training Batch: 12922 Loss: 3499.189941\n",
      "Training Batch: 12923 Loss: 3277.603027\n",
      "Training Batch: 12924 Loss: 3100.866699\n",
      "Training Batch: 12925 Loss: 3136.148926\n",
      "Training Batch: 12926 Loss: 3037.066895\n",
      "Training Batch: 12927 Loss: 3123.372070\n",
      "Training Batch: 12928 Loss: 3152.236328\n",
      "Training Batch: 12929 Loss: 3045.609375\n",
      "Training Batch: 12930 Loss: 2985.081787\n",
      "Training Batch: 12931 Loss: 3070.836182\n",
      "Training Batch: 12932 Loss: 3048.309082\n",
      "Training Batch: 12933 Loss: 3023.153320\n",
      "Training Batch: 12934 Loss: 3137.251465\n",
      "Training Batch: 12935 Loss: 3208.339844\n",
      "Training Batch: 12936 Loss: 3650.038086\n",
      "Training Batch: 12937 Loss: 3102.604004\n",
      "Training Batch: 12938 Loss: 3047.351562\n",
      "Training Batch: 12939 Loss: 3099.242920\n",
      "Training Batch: 12940 Loss: 3271.958008\n",
      "Training Batch: 12941 Loss: 3120.597656\n",
      "Training Batch: 12942 Loss: 3154.936035\n",
      "Training Batch: 12943 Loss: 3010.779053\n",
      "Training Batch: 12944 Loss: 3173.520020\n",
      "Training Batch: 12945 Loss: 3061.155762\n",
      "Training Batch: 12946 Loss: 3298.824707\n",
      "Training Batch: 12947 Loss: 3262.227051\n",
      "Training Batch: 12948 Loss: 4459.080078\n",
      "Training Batch: 12949 Loss: 3085.996094\n",
      "Training Batch: 12950 Loss: 3102.445312\n",
      "Training Batch: 12951 Loss: 3152.710938\n",
      "Training Batch: 12952 Loss: 3223.143555\n",
      "Training Batch: 12953 Loss: 3335.521484\n",
      "Training Batch: 12954 Loss: 3286.011963\n",
      "Training Batch: 12955 Loss: 3086.544678\n",
      "Training Batch: 12956 Loss: 3103.494629\n",
      "Training Batch: 12957 Loss: 3180.154785\n",
      "Training Batch: 12958 Loss: 2988.461670\n",
      "Training Batch: 12959 Loss: 3273.359375\n",
      "Training Batch: 12960 Loss: 3141.078857\n",
      "Training Batch: 12961 Loss: 3258.281250\n",
      "Training Batch: 12962 Loss: 3050.502441\n",
      "Training Batch: 12963 Loss: 3091.090820\n",
      "Training Batch: 12964 Loss: 3067.221191\n",
      "Training Batch: 12965 Loss: 3271.197754\n",
      "Training Batch: 12966 Loss: 3060.344482\n",
      "Training Batch: 12967 Loss: 3165.472656\n",
      "Training Batch: 12968 Loss: 3111.937012\n",
      "Training Batch: 12969 Loss: 3042.031250\n",
      "Training Batch: 12970 Loss: 3123.508789\n",
      "Training Batch: 12971 Loss: 3216.612305\n",
      "Training Batch: 12972 Loss: 3153.809814\n",
      "Training Batch: 12973 Loss: 3063.206543\n",
      "Training Batch: 12974 Loss: 3115.636719\n",
      "Training Batch: 12975 Loss: 3104.671387\n",
      "Training Batch: 12976 Loss: 3279.519043\n",
      "Training Batch: 12977 Loss: 3327.721191\n",
      "Training Batch: 12978 Loss: 3230.925293\n",
      "Training Batch: 12979 Loss: 3169.531982\n",
      "Training Batch: 12980 Loss: 2991.101562\n",
      "Training Batch: 12981 Loss: 3233.474609\n",
      "Training Batch: 12982 Loss: 3179.024658\n",
      "Training Batch: 12983 Loss: 3612.643311\n",
      "Training Batch: 12984 Loss: 3417.007324\n",
      "Training Batch: 12985 Loss: 3171.211426\n",
      "Training Batch: 12986 Loss: 3128.587646\n",
      "Training Batch: 12987 Loss: 3068.412109\n",
      "Training Batch: 12988 Loss: 3290.062012\n",
      "Training Batch: 12989 Loss: 3220.967773\n",
      "Training Batch: 12990 Loss: 3255.694824\n",
      "Training Batch: 12991 Loss: 3229.277100\n",
      "Training Batch: 12992 Loss: 3328.160645\n",
      "Training Batch: 12993 Loss: 3231.924805\n",
      "Training Batch: 12994 Loss: 3302.677979\n",
      "Training Batch: 12995 Loss: 3234.761719\n",
      "Training Batch: 12996 Loss: 3116.894043\n",
      "Training Batch: 12997 Loss: 3112.934570\n",
      "Training Batch: 12998 Loss: 3089.253662\n",
      "Training Batch: 12999 Loss: 3110.023682\n",
      "Training Batch: 13000 Loss: 3108.114258\n",
      "Training Batch: 13001 Loss: 3153.582275\n",
      "Training Batch: 13002 Loss: 3123.343506\n",
      "Training Batch: 13003 Loss: 3131.111328\n",
      "Training Batch: 13004 Loss: 3149.767090\n",
      "Training Batch: 13005 Loss: 3261.939453\n",
      "Training Batch: 13006 Loss: 3106.703369\n",
      "Training Batch: 13007 Loss: 3266.330811\n",
      "Training Batch: 13008 Loss: 3177.312988\n",
      "Training Batch: 13009 Loss: 3072.430908\n",
      "Training Batch: 13010 Loss: 3316.704102\n",
      "Training Batch: 13011 Loss: 3053.046875\n",
      "Training Batch: 13012 Loss: 3361.567383\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Batch: 13013 Loss: 3121.672363\n",
      "Training Batch: 13014 Loss: 3065.734863\n",
      "Training Batch: 13015 Loss: 3087.917969\n",
      "Training Batch: 13016 Loss: 3075.313965\n",
      "Training Batch: 13017 Loss: 3107.344727\n",
      "Training Batch: 13018 Loss: 3221.497070\n",
      "Training Batch: 13019 Loss: 3303.752441\n",
      "Training Batch: 13020 Loss: 3090.208008\n",
      "Training Batch: 13021 Loss: 3072.538086\n",
      "Training Batch: 13022 Loss: 3215.164795\n",
      "Training Batch: 13023 Loss: 3088.891602\n",
      "Training Batch: 13024 Loss: 3138.543945\n",
      "Training Batch: 13025 Loss: 3081.789551\n",
      "Training Batch: 13026 Loss: 3178.273438\n",
      "Training Batch: 13027 Loss: 2999.926758\n",
      "Training Batch: 13028 Loss: 3084.959473\n",
      "Training Batch: 13029 Loss: 3108.285645\n",
      "Training Batch: 13030 Loss: 3130.738770\n",
      "Training Batch: 13031 Loss: 3139.024414\n",
      "Training Batch: 13032 Loss: 3130.234131\n",
      "Training Batch: 13033 Loss: 3207.515137\n",
      "Training Batch: 13034 Loss: 3154.425293\n",
      "Training Batch: 13035 Loss: 3037.907715\n",
      "Training Batch: 13036 Loss: 3077.385986\n",
      "Training Batch: 13037 Loss: 3003.907227\n",
      "Training Batch: 13038 Loss: 3164.599121\n",
      "Training Batch: 13039 Loss: 3052.539795\n",
      "Training Batch: 13040 Loss: 3094.683594\n",
      "Training Batch: 13041 Loss: 3145.890625\n",
      "Training Batch: 13042 Loss: 3064.529297\n",
      "Training Batch: 13043 Loss: 3280.128418\n",
      "Training Batch: 13044 Loss: 3241.179688\n",
      "Training Batch: 13045 Loss: 3331.025635\n",
      "Training Batch: 13046 Loss: 3332.931885\n",
      "Training Batch: 13047 Loss: 3052.467529\n",
      "Training Batch: 13048 Loss: 3095.716309\n",
      "Training Batch: 13049 Loss: 3162.708008\n",
      "Training Batch: 13050 Loss: 3147.851562\n",
      "Training Batch: 13051 Loss: 3011.585205\n",
      "Training Batch: 13052 Loss: 3198.575684\n",
      "Training Batch: 13053 Loss: 3101.399902\n",
      "Training Batch: 13054 Loss: 3234.170166\n",
      "Training Batch: 13055 Loss: 3149.491455\n",
      "Training Batch: 13056 Loss: 3032.285645\n",
      "Training Batch: 13057 Loss: 3493.451660\n",
      "Training Batch: 13058 Loss: 4232.954590\n",
      "Training Batch: 13059 Loss: 4014.371094\n",
      "Training Batch: 13060 Loss: 3413.699707\n",
      "Training Batch: 13061 Loss: 3262.413086\n",
      "Training Batch: 13062 Loss: 3536.840820\n",
      "Training Batch: 13063 Loss: 3318.905762\n",
      "Training Batch: 13064 Loss: 3291.699219\n",
      "Training Batch: 13065 Loss: 3248.441406\n",
      "Training Batch: 13066 Loss: 3224.841064\n",
      "Training Batch: 13067 Loss: 3091.574219\n",
      "Training Batch: 13068 Loss: 3405.049561\n",
      "Training Batch: 13069 Loss: 3390.430176\n",
      "Training Batch: 13070 Loss: 3186.181641\n",
      "Training Batch: 13071 Loss: 3254.214355\n",
      "Training Batch: 13072 Loss: 3069.156738\n",
      "Training Batch: 13073 Loss: 3096.011719\n",
      "Training Batch: 13074 Loss: 3201.463867\n",
      "Training Batch: 13075 Loss: 3281.787598\n",
      "Training Batch: 13076 Loss: 3104.780029\n",
      "Training Batch: 13077 Loss: 3098.514160\n",
      "Training Batch: 13078 Loss: 3206.249512\n",
      "Training Batch: 13079 Loss: 3286.740967\n",
      "Training Batch: 13080 Loss: 3256.033691\n",
      "Training Batch: 13081 Loss: 3062.262695\n",
      "Training Batch: 13082 Loss: 3068.318359\n",
      "Training Batch: 13083 Loss: 3186.904297\n",
      "Training Batch: 13084 Loss: 3040.281738\n",
      "Training Batch: 13085 Loss: 3039.571533\n",
      "Training Batch: 13086 Loss: 3162.861084\n",
      "Training Batch: 13087 Loss: 3159.624023\n",
      "Training Batch: 13088 Loss: 3105.287354\n",
      "Training Batch: 13089 Loss: 3134.111816\n",
      "Training Batch: 13090 Loss: 3027.881348\n",
      "Training Batch: 13091 Loss: 3270.225342\n",
      "Training Batch: 13092 Loss: 3094.484375\n",
      "Training Batch: 13093 Loss: 3022.836670\n",
      "Training Batch: 13094 Loss: 3138.518066\n",
      "Training Batch: 13095 Loss: 3211.364990\n",
      "Training Batch: 13096 Loss: 3144.527832\n",
      "Training Batch: 13097 Loss: 3308.136475\n",
      "Training Batch: 13098 Loss: 3099.900635\n",
      "Training Batch: 13099 Loss: 3101.387695\n",
      "Training Batch: 13100 Loss: 3192.551758\n",
      "Training Batch: 13101 Loss: 3166.421875\n",
      "Training Batch: 13102 Loss: 3102.369385\n",
      "Training Batch: 13103 Loss: 3402.306885\n",
      "Training Batch: 13104 Loss: 3277.612549\n",
      "Training Batch: 13105 Loss: 3215.766602\n",
      "Training Batch: 13106 Loss: 3189.467041\n",
      "Training Batch: 13107 Loss: 3155.635742\n",
      "Training Batch: 13108 Loss: 3265.003662\n",
      "Training Batch: 13109 Loss: 3187.145508\n",
      "Training Batch: 13110 Loss: 3066.930908\n",
      "Training Batch: 13111 Loss: 3164.607422\n",
      "Training Batch: 13112 Loss: 3379.208740\n",
      "Training Batch: 13113 Loss: 3186.119141\n",
      "Training Batch: 13114 Loss: 3176.897461\n",
      "Training Batch: 13115 Loss: 2994.265137\n",
      "Training Batch: 13116 Loss: 3213.641846\n",
      "Training Batch: 13117 Loss: 3065.635986\n",
      "Training Batch: 13118 Loss: 3116.766602\n",
      "Training Batch: 13119 Loss: 3125.284668\n",
      "Training Batch: 13120 Loss: 3067.723389\n",
      "Training Batch: 13121 Loss: 3088.753906\n",
      "Training Batch: 13122 Loss: 3210.005127\n",
      "Training Batch: 13123 Loss: 3080.813477\n",
      "Training Batch: 13124 Loss: 3071.053711\n",
      "Training Batch: 13125 Loss: 3129.892090\n",
      "Training Batch: 13126 Loss: 3276.204834\n",
      "Training Batch: 13127 Loss: 3248.008789\n",
      "Training Batch: 13128 Loss: 3184.951172\n",
      "Training Batch: 13129 Loss: 3106.040039\n",
      "Training Batch: 13130 Loss: 3202.514648\n",
      "Training Batch: 13131 Loss: 3165.780273\n",
      "Training Batch: 13132 Loss: 3022.227295\n",
      "Training Batch: 13133 Loss: 3099.037109\n",
      "Training Batch: 13134 Loss: 2996.255371\n",
      "Training Batch: 13135 Loss: 3052.146484\n",
      "Training Batch: 13136 Loss: 3046.956299\n",
      "Training Batch: 13137 Loss: 3291.882080\n",
      "Training Batch: 13138 Loss: 3011.102783\n",
      "Training Batch: 13139 Loss: 3011.575928\n",
      "Training Batch: 13140 Loss: 3000.864258\n",
      "Training Batch: 13141 Loss: 3042.353516\n",
      "Training Batch: 13142 Loss: 3147.488770\n",
      "Training Batch: 13143 Loss: 3289.936035\n",
      "Training Batch: 13144 Loss: 3221.086914\n",
      "Training Batch: 13145 Loss: 3196.349121\n",
      "Training Batch: 13146 Loss: 3180.011719\n",
      "Training Batch: 13147 Loss: 3046.730957\n",
      "Training Batch: 13148 Loss: 3288.696289\n",
      "Training Batch: 13149 Loss: 3093.592041\n",
      "Training Batch: 13150 Loss: 3387.932617\n",
      "Training Batch: 13151 Loss: 3197.816895\n",
      "Training Batch: 13152 Loss: 3112.248047\n",
      "Training Batch: 13153 Loss: 3149.276611\n",
      "Training Batch: 13154 Loss: 3241.241943\n",
      "Training Batch: 13155 Loss: 3067.789551\n",
      "Training Batch: 13156 Loss: 3150.800537\n",
      "Training Batch: 13157 Loss: 3458.345215\n",
      "Training Batch: 13158 Loss: 3167.485840\n",
      "Training Batch: 13159 Loss: 3099.230469\n",
      "Training Batch: 13160 Loss: 3186.529297\n",
      "Training Batch: 13161 Loss: 3225.862305\n",
      "Training Batch: 13162 Loss: 3214.601562\n",
      "Training Batch: 13163 Loss: 3279.326660\n",
      "Training Batch: 13164 Loss: 3128.494629\n",
      "Training Batch: 13165 Loss: 3224.641602\n",
      "Training Batch: 13166 Loss: 3151.830078\n",
      "Training Batch: 13167 Loss: 3144.725098\n",
      "Training Batch: 13168 Loss: 3188.422852\n",
      "Training Batch: 13169 Loss: 3509.719238\n",
      "Training Batch: 13170 Loss: 3568.389893\n",
      "Training Batch: 13171 Loss: 3363.938232\n",
      "Training Batch: 13172 Loss: 3419.375000\n",
      "Training Batch: 13173 Loss: 3216.192871\n",
      "Training Batch: 13174 Loss: 3176.514648\n",
      "Training Batch: 13175 Loss: 3175.056152\n",
      "Training Batch: 13176 Loss: 3137.582031\n",
      "Training Batch: 13177 Loss: 3049.177490\n",
      "Training Batch: 13178 Loss: 3124.498779\n",
      "Training Batch: 13179 Loss: 3308.840332\n",
      "Training Batch: 13180 Loss: 3253.189453\n",
      "Training Batch: 13181 Loss: 3162.950195\n",
      "Training Batch: 13182 Loss: 3352.631592\n",
      "Training Batch: 13183 Loss: 3131.782471\n",
      "Training Batch: 13184 Loss: 3099.564453\n",
      "Training Batch: 13185 Loss: 3199.225098\n",
      "Training Batch: 13186 Loss: 3028.107422\n",
      "Training Batch: 13187 Loss: 3086.078613\n",
      "Training Batch: 13188 Loss: 3022.586426\n",
      "Training Batch: 13189 Loss: 3192.854004\n",
      "Training Batch: 13190 Loss: 3156.026855\n",
      "Training Batch: 13191 Loss: 3135.075684\n",
      "Training Batch: 13192 Loss: 3184.019043\n",
      "Training Batch: 13193 Loss: 3135.250977\n",
      "Training Batch: 13194 Loss: 3038.564941\n",
      "Training Batch: 13195 Loss: 3043.706055\n",
      "Training Batch: 13196 Loss: 3050.665283\n",
      "Training Batch: 13197 Loss: 3091.547363\n",
      "Training Batch: 13198 Loss: 3089.533691\n",
      "Training Batch: 13199 Loss: 3161.710938\n",
      "Training Batch: 13200 Loss: 3116.190186\n",
      "Training Batch: 13201 Loss: 3207.737061\n",
      "Training Batch: 13202 Loss: 3082.499756\n",
      "Training Batch: 13203 Loss: 3281.604492\n",
      "Training Batch: 13204 Loss: 3092.413086\n",
      "Training Batch: 13205 Loss: 3082.067383\n",
      "Training Batch: 13206 Loss: 3109.768311\n",
      "Training Batch: 13207 Loss: 3058.655762\n",
      "Training Batch: 13208 Loss: 2982.562012\n",
      "Training Batch: 13209 Loss: 3095.561035\n",
      "Training Batch: 13210 Loss: 3172.918945\n",
      "Training Batch: 13211 Loss: 2983.131592\n",
      "Training Batch: 13212 Loss: 2982.615234\n",
      "Training Batch: 13213 Loss: 3081.339355\n",
      "Training Batch: 13214 Loss: 3118.154053\n",
      "Training Batch: 13215 Loss: 3052.901367\n",
      "Training Batch: 13216 Loss: 3152.563232\n",
      "Training Batch: 13217 Loss: 3076.683105\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Batch: 13218 Loss: 3239.306641\n",
      "Training Batch: 13219 Loss: 3124.515869\n",
      "Training Batch: 13220 Loss: 3265.114258\n",
      "Training Batch: 13221 Loss: 3064.755371\n",
      "Training Batch: 13222 Loss: 3632.370850\n",
      "Training Batch: 13223 Loss: 3032.315674\n",
      "Training Batch: 13224 Loss: 3263.734375\n",
      "Training Batch: 13225 Loss: 3087.740723\n",
      "Training Batch: 13226 Loss: 3148.331543\n",
      "Training Batch: 13227 Loss: 3150.576904\n",
      "Training Batch: 13228 Loss: 3262.337158\n",
      "Training Batch: 13229 Loss: 3144.995117\n",
      "Training Batch: 13230 Loss: 3109.823242\n",
      "Training Batch: 13231 Loss: 3167.757080\n",
      "Training Batch: 13232 Loss: 3042.425781\n",
      "Training Batch: 13233 Loss: 3152.757324\n",
      "Training Batch: 13234 Loss: 3055.778809\n",
      "Training Batch: 13235 Loss: 2996.266846\n",
      "Training Batch: 13236 Loss: 3249.743652\n",
      "Training Batch: 13237 Loss: 3222.826660\n",
      "Training Batch: 13238 Loss: 3048.453125\n",
      "Training Batch: 13239 Loss: 3026.547363\n",
      "Training Batch: 13240 Loss: 3107.130859\n",
      "Training Batch: 13241 Loss: 3092.566406\n",
      "Training Batch: 13242 Loss: 3368.382568\n",
      "Training Batch: 13243 Loss: 3267.031738\n",
      "Training Batch: 13244 Loss: 3201.724121\n",
      "Training Batch: 13245 Loss: 3108.021484\n",
      "Training Batch: 13246 Loss: 3125.635986\n",
      "Training Batch: 13247 Loss: 3141.922119\n",
      "Training Batch: 13248 Loss: 3188.063965\n",
      "Training Batch: 13249 Loss: 3286.026855\n",
      "Training Batch: 13250 Loss: 3057.295410\n",
      "Training Batch: 13251 Loss: 3101.465332\n",
      "Training Batch: 13252 Loss: 3226.942627\n",
      "Training Batch: 13253 Loss: 3080.659180\n",
      "Training Batch: 13254 Loss: 3243.874512\n",
      "Training Batch: 13255 Loss: 3122.213379\n",
      "Training Batch: 13256 Loss: 3025.496582\n",
      "Training Batch: 13257 Loss: 3136.441895\n",
      "Training Batch: 13258 Loss: 3008.104492\n",
      "Training Batch: 13259 Loss: 3288.087402\n",
      "Training Batch: 13260 Loss: 3242.502441\n",
      "Training Batch: 13261 Loss: 3235.616699\n",
      "Training Batch: 13262 Loss: 3012.604980\n",
      "Training Batch: 13263 Loss: 2996.600098\n",
      "Training Batch: 13264 Loss: 3047.517578\n",
      "Training Batch: 13265 Loss: 3214.885254\n",
      "Training Batch: 13266 Loss: 3063.618652\n",
      "Training Batch: 13267 Loss: 3190.846680\n",
      "Training Batch: 13268 Loss: 3082.840820\n",
      "Training Batch: 13269 Loss: 3201.428711\n",
      "Training Batch: 13270 Loss: 3085.552979\n",
      "Training Batch: 13271 Loss: 3165.681641\n",
      "Training Batch: 13272 Loss: 3045.462891\n",
      "Training Batch: 13273 Loss: 2979.764160\n",
      "Training Batch: 13274 Loss: 3048.875977\n",
      "Training Batch: 13275 Loss: 3046.385742\n",
      "Training Batch: 13276 Loss: 3138.841309\n",
      "Training Batch: 13277 Loss: 3244.785156\n",
      "Training Batch: 13278 Loss: 3149.601562\n",
      "Training Batch: 13279 Loss: 3035.736328\n",
      "Training Batch: 13280 Loss: 3010.275635\n",
      "Training Batch: 13281 Loss: 2975.030273\n",
      "Training Batch: 13282 Loss: 3041.458984\n",
      "Training Batch: 13283 Loss: 3051.355469\n",
      "Training Batch: 13284 Loss: 3079.681152\n",
      "Training Batch: 13285 Loss: 3036.912598\n",
      "Training Batch: 13286 Loss: 3128.076660\n",
      "Training Batch: 13287 Loss: 3220.092773\n",
      "Training Batch: 13288 Loss: 3016.123535\n",
      "Training Batch: 13289 Loss: 3112.399414\n",
      "Training Batch: 13290 Loss: 3227.006836\n",
      "Training Batch: 13291 Loss: 3091.361328\n",
      "Training Batch: 13292 Loss: 3381.877441\n",
      "Training Batch: 13293 Loss: 3054.291260\n",
      "Training Batch: 13294 Loss: 3127.979248\n",
      "Training Batch: 13295 Loss: 3172.692383\n",
      "Training Batch: 13296 Loss: 3136.117676\n",
      "Training Batch: 13297 Loss: 3020.703857\n",
      "Training Batch: 13298 Loss: 3119.214111\n",
      "Training Batch: 13299 Loss: 3052.635254\n",
      "Training Batch: 13300 Loss: 3121.653809\n",
      "Training Batch: 13301 Loss: 3279.209961\n",
      "Training Batch: 13302 Loss: 3065.434570\n",
      "Training Batch: 13303 Loss: 3124.968262\n",
      "Training Batch: 13304 Loss: 3053.248047\n",
      "Training Batch: 13305 Loss: 3004.599609\n",
      "Training Batch: 13306 Loss: 3161.265625\n",
      "Training Batch: 13307 Loss: 3286.998047\n",
      "Training Batch: 13308 Loss: 3436.202148\n",
      "Training Batch: 13309 Loss: 3228.065674\n",
      "Training Batch: 13310 Loss: 3069.720215\n",
      "Training Batch: 13311 Loss: 3054.390381\n",
      "Training Batch: 13312 Loss: 3104.363037\n",
      "Training Batch: 13313 Loss: 3110.714844\n",
      "Training Batch: 13314 Loss: 3331.683105\n",
      "Training Batch: 13315 Loss: 3345.368652\n",
      "Training Batch: 13316 Loss: 3445.918457\n",
      "Training Batch: 13317 Loss: 3210.635254\n",
      "Training Batch: 13318 Loss: 3208.293213\n",
      "Training Batch: 13319 Loss: 3359.269043\n",
      "Training Batch: 13320 Loss: 3149.120605\n",
      "Training Batch: 13321 Loss: 3293.778076\n",
      "Training Batch: 13322 Loss: 3229.426758\n",
      "Training Batch: 13323 Loss: 3312.170410\n",
      "Training Batch: 13324 Loss: 3166.474365\n",
      "Training Batch: 13325 Loss: 3142.308350\n",
      "Training Batch: 13326 Loss: 3292.475098\n",
      "Training Batch: 13327 Loss: 3035.572754\n",
      "Training Batch: 13328 Loss: 3238.603516\n",
      "Training Batch: 13329 Loss: 3274.916992\n",
      "Training Batch: 13330 Loss: 3165.832764\n",
      "Training Batch: 13331 Loss: 3233.458984\n",
      "Training Batch: 13332 Loss: 3144.034668\n",
      "Training Batch: 13333 Loss: 3181.639160\n",
      "Training Batch: 13334 Loss: 3068.898438\n",
      "Training Batch: 13335 Loss: 3162.784668\n",
      "Training Batch: 13336 Loss: 3029.671387\n",
      "Training Batch: 13337 Loss: 3251.716064\n",
      "Training Batch: 13338 Loss: 3326.299316\n",
      "Training Batch: 13339 Loss: 3080.068359\n",
      "Training Batch: 13340 Loss: 3180.030273\n",
      "Training Batch: 13341 Loss: 3265.468750\n",
      "Training Batch: 13342 Loss: 3017.622314\n",
      "Training Batch: 13343 Loss: 3121.431152\n",
      "Training Batch: 13344 Loss: 3157.622070\n",
      "Training Batch: 13345 Loss: 3103.407715\n",
      "Training Batch: 13346 Loss: 3020.684570\n",
      "Training Batch: 13347 Loss: 3052.010742\n",
      "Training Batch: 13348 Loss: 3369.501953\n",
      "Training Batch: 13349 Loss: 3203.632812\n",
      "Training Batch: 13350 Loss: 3122.051758\n",
      "Training Batch: 13351 Loss: 3191.144775\n",
      "Training Batch: 13352 Loss: 3126.859131\n",
      "Training Batch: 13353 Loss: 3228.690918\n",
      "Training Batch: 13354 Loss: 3033.284668\n",
      "Training Batch: 13355 Loss: 3034.753662\n",
      "Training Batch: 13356 Loss: 3165.242188\n",
      "Training Batch: 13357 Loss: 3090.480469\n",
      "Training Batch: 13358 Loss: 3144.573242\n",
      "Training Batch: 13359 Loss: 3078.041016\n",
      "Training Batch: 13360 Loss: 3062.590332\n",
      "Training Batch: 13361 Loss: 3120.050293\n",
      "Training Batch: 13362 Loss: 3125.332520\n",
      "Training Batch: 13363 Loss: 3110.767090\n",
      "Training Batch: 13364 Loss: 3073.422119\n",
      "Training Batch: 13365 Loss: 3062.826904\n",
      "Training Batch: 13366 Loss: 2999.849609\n",
      "Training Batch: 13367 Loss: 3064.167236\n",
      "Training Batch: 13368 Loss: 3044.985840\n",
      "Training Batch: 13369 Loss: 3154.350098\n",
      "Training Batch: 13370 Loss: 3222.365234\n",
      "Training Batch: 13371 Loss: 3160.984375\n",
      "Training Batch: 13372 Loss: 3153.977051\n",
      "Training Batch: 13373 Loss: 3062.624023\n",
      "Training Batch: 13374 Loss: 3105.162109\n",
      "Training Batch: 13375 Loss: 3021.810059\n",
      "Training Batch: 13376 Loss: 3175.585205\n",
      "Training Batch: 13377 Loss: 3045.208984\n",
      "Training Batch: 13378 Loss: 3143.807129\n",
      "Training Batch: 13379 Loss: 3123.700195\n",
      "Training Batch: 13380 Loss: 3063.874023\n",
      "Training Batch: 13381 Loss: 3082.867676\n",
      "Training Batch: 13382 Loss: 2982.470459\n",
      "Training Batch: 13383 Loss: 3348.603271\n",
      "Training Batch: 13384 Loss: 3020.493896\n",
      "Training Batch: 13385 Loss: 3195.164062\n",
      "Training Batch: 13386 Loss: 3334.431641\n",
      "Training Batch: 13387 Loss: 3277.993164\n",
      "Training Batch: 13388 Loss: 3251.403320\n",
      "Training Batch: 13389 Loss: 3128.565430\n",
      "Training Batch: 13390 Loss: 3055.866699\n",
      "Training Batch: 13391 Loss: 3045.020264\n",
      "Training Batch: 13392 Loss: 3279.590332\n",
      "Training Batch: 13393 Loss: 3060.109375\n",
      "Training Batch: 13394 Loss: 3386.923340\n",
      "Training Batch: 13395 Loss: 3061.097656\n",
      "Training Batch: 13396 Loss: 3070.625977\n",
      "Training Batch: 13397 Loss: 3459.353516\n",
      "Training Batch: 13398 Loss: 3129.532227\n",
      "Training Batch: 13399 Loss: 3163.537598\n",
      "Training Batch: 13400 Loss: 3162.989746\n",
      "Training Batch: 13401 Loss: 3189.944336\n",
      "Training Batch: 13402 Loss: 3200.560791\n",
      "Training Batch: 13403 Loss: 3153.810547\n",
      "Training Batch: 13404 Loss: 3151.002441\n",
      "Training Batch: 13405 Loss: 3253.105469\n",
      "Training Batch: 13406 Loss: 3246.652100\n",
      "Training Batch: 13407 Loss: 3049.749268\n",
      "Training Batch: 13408 Loss: 3187.060059\n",
      "Training Batch: 13409 Loss: 3212.795898\n",
      "Training Batch: 13410 Loss: 3087.084717\n",
      "Training Batch: 13411 Loss: 3012.494629\n",
      "Training Batch: 13412 Loss: 3067.483398\n",
      "Training Batch: 13413 Loss: 3085.115723\n",
      "Training Batch: 13414 Loss: 3060.902344\n",
      "Training Batch: 13415 Loss: 3094.853516\n",
      "Training Batch: 13416 Loss: 3183.480469\n",
      "Training Batch: 13417 Loss: 3066.788574\n",
      "Training Batch: 13418 Loss: 3318.876953\n",
      "Training Batch: 13419 Loss: 3224.031738\n",
      "Training Batch: 13420 Loss: 3043.297363\n",
      "Training Batch: 13421 Loss: 3095.038574\n",
      "Training Batch: 13422 Loss: 3212.232422\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Batch: 13423 Loss: 3011.046143\n",
      "Training Batch: 13424 Loss: 3284.332520\n",
      "Training Batch: 13425 Loss: 3070.057861\n",
      "Training Batch: 13426 Loss: 3272.054199\n",
      "Training Batch: 13427 Loss: 3475.295898\n",
      "Training Batch: 13428 Loss: 3153.910400\n",
      "Training Batch: 13429 Loss: 3328.287842\n",
      "Training Batch: 13430 Loss: 3095.596191\n",
      "Training Batch: 13431 Loss: 3375.059082\n",
      "Training Batch: 13432 Loss: 3194.773926\n",
      "Training Batch: 13433 Loss: 3325.427246\n",
      "Training Batch: 13434 Loss: 3123.587402\n",
      "Training Batch: 13435 Loss: 3075.652832\n",
      "Training Batch: 13436 Loss: 3033.476074\n",
      "Training Batch: 13437 Loss: 3049.819336\n",
      "Training Batch: 13438 Loss: 3058.762939\n",
      "Training Batch: 13439 Loss: 3060.651611\n",
      "Training Batch: 13440 Loss: 3094.593750\n",
      "Training Batch: 13441 Loss: 3029.392334\n",
      "Training Batch: 13442 Loss: 2991.677246\n",
      "Training Batch: 13443 Loss: 3152.224121\n",
      "Training Batch: 13444 Loss: 3118.511230\n",
      "Training Batch: 13445 Loss: 3095.583008\n",
      "Training Batch: 13446 Loss: 3112.174072\n",
      "Training Batch: 13447 Loss: 3094.726562\n",
      "Training Batch: 13448 Loss: 3161.009277\n",
      "Training Batch: 13449 Loss: 3104.475098\n",
      "Training Batch: 13450 Loss: 3075.392090\n",
      "Training Batch: 13451 Loss: 3068.901367\n",
      "Training Batch: 13452 Loss: 3159.232422\n",
      "Training Batch: 13453 Loss: 3297.821045\n",
      "Training Batch: 13454 Loss: 3053.889160\n",
      "Training Batch: 13455 Loss: 2973.614746\n",
      "Training Batch: 13456 Loss: 3180.527832\n",
      "Training Batch: 13457 Loss: 3004.402832\n",
      "Training Batch: 13458 Loss: 2985.286133\n",
      "Training Batch: 13459 Loss: 3074.838135\n",
      "Training Batch: 13460 Loss: 2991.258545\n",
      "Training Batch: 13461 Loss: 3019.129395\n",
      "Training Batch: 13462 Loss: 3061.181396\n",
      "Training Batch: 13463 Loss: 3163.511719\n",
      "Training Batch: 13464 Loss: 3047.451904\n",
      "Training Batch: 13465 Loss: 2968.883545\n",
      "Training Batch: 13466 Loss: 3237.517090\n",
      "Training Batch: 13467 Loss: 3366.808105\n",
      "Training Batch: 13468 Loss: 3335.644531\n",
      "Training Batch: 13469 Loss: 3090.981445\n",
      "Training Batch: 13470 Loss: 3174.976562\n",
      "Training Batch: 13471 Loss: 3358.853027\n",
      "Training Batch: 13472 Loss: 3426.772461\n",
      "Training Batch: 13473 Loss: 3013.335449\n",
      "Training Batch: 13474 Loss: 3234.314941\n",
      "Training Batch: 13475 Loss: 3559.455078\n",
      "Training Batch: 13476 Loss: 3265.573242\n",
      "Training Batch: 13477 Loss: 3465.583008\n",
      "Training Batch: 13478 Loss: 3103.706055\n",
      "Training Batch: 13479 Loss: 3201.227539\n",
      "Training Batch: 13480 Loss: 3073.927734\n",
      "Training Batch: 13481 Loss: 3111.178223\n",
      "Training Batch: 13482 Loss: 3311.031738\n",
      "Training Batch: 13483 Loss: 3019.303467\n",
      "Training Batch: 13484 Loss: 3154.424316\n",
      "Training Batch: 13485 Loss: 3366.331055\n",
      "Training Batch: 13486 Loss: 3170.565430\n",
      "Training Batch: 13487 Loss: 3203.035645\n",
      "Training Batch: 13488 Loss: 3027.346924\n",
      "Training Batch: 13489 Loss: 3122.968262\n",
      "Training Batch: 13490 Loss: 3430.372803\n",
      "Training Batch: 13491 Loss: 3155.607422\n",
      "Training Batch: 13492 Loss: 3090.837646\n",
      "Training Batch: 13493 Loss: 3152.396973\n",
      "Training Batch: 13494 Loss: 2984.749023\n",
      "Training Batch: 13495 Loss: 3259.012695\n",
      "Training Batch: 13496 Loss: 3141.150635\n",
      "Training Batch: 13497 Loss: 3155.705078\n",
      "Training Batch: 13498 Loss: 3215.685059\n",
      "Training Batch: 13499 Loss: 3167.928223\n",
      "Training Batch: 13500 Loss: 3125.549805\n",
      "Training Batch: 13501 Loss: 3054.939941\n",
      "Training Batch: 13502 Loss: 3122.171875\n",
      "Training Batch: 13503 Loss: 3080.772461\n",
      "Training Batch: 13504 Loss: 3066.354004\n",
      "Training Batch: 13505 Loss: 3141.342285\n",
      "Training Batch: 13506 Loss: 3119.824219\n",
      "Training Batch: 13507 Loss: 3144.052734\n",
      "Training Batch: 13508 Loss: 3077.687256\n",
      "Training Batch: 13509 Loss: 3081.643555\n",
      "Training Batch: 13510 Loss: 3190.252686\n",
      "Training Batch: 13511 Loss: 2974.945801\n",
      "Training Batch: 13512 Loss: 3140.365234\n",
      "Training Batch: 13513 Loss: 3476.771973\n",
      "Training Batch: 13514 Loss: 3108.346680\n",
      "Training Batch: 13515 Loss: 3239.687256\n",
      "Training Batch: 13516 Loss: 3337.286621\n",
      "Training Batch: 13517 Loss: 3111.524170\n",
      "Training Batch: 13518 Loss: 3034.970215\n",
      "Training Batch: 13519 Loss: 3151.354248\n",
      "Training Batch: 13520 Loss: 3010.808350\n",
      "Training Batch: 13521 Loss: 3105.931641\n",
      "Training Batch: 13522 Loss: 3014.855713\n",
      "Training Batch: 13523 Loss: 3144.098633\n",
      "Training Batch: 13524 Loss: 3061.542480\n",
      "Training Batch: 13525 Loss: 3036.623535\n",
      "Training Batch: 13526 Loss: 3097.763184\n",
      "Training Batch: 13527 Loss: 3082.406250\n",
      "Training Batch: 13528 Loss: 3112.486328\n",
      "Training Batch: 13529 Loss: 2990.586914\n",
      "Training Batch: 13530 Loss: 2998.108398\n",
      "Training Batch: 13531 Loss: 3101.129883\n",
      "Training Batch: 13532 Loss: 3062.839844\n",
      "Training Batch: 13533 Loss: 3072.675293\n",
      "Training Batch: 13534 Loss: 3011.562012\n",
      "Training Batch: 13535 Loss: 3204.770996\n",
      "Training Batch: 13536 Loss: 3127.887207\n",
      "Training Batch: 13537 Loss: 3080.224609\n",
      "Training Batch: 13538 Loss: 3075.622559\n",
      "Training Batch: 13539 Loss: 3014.044678\n",
      "Training Batch: 13540 Loss: 3332.493408\n",
      "Training Batch: 13541 Loss: 3030.139648\n",
      "Training Batch: 13542 Loss: 3132.833008\n",
      "Training Batch: 13543 Loss: 3022.779785\n",
      "Training Batch: 13544 Loss: 3008.236328\n",
      "Training Batch: 13545 Loss: 3042.159668\n",
      "Training Batch: 13546 Loss: 3087.608398\n",
      "Training Batch: 13547 Loss: 3026.253906\n",
      "Training Batch: 13548 Loss: 2938.709473\n",
      "Training Batch: 13549 Loss: 3082.485840\n",
      "Training Batch: 13550 Loss: 3184.417480\n",
      "Training Batch: 13551 Loss: 3306.596191\n",
      "Training Batch: 13552 Loss: 3047.269531\n",
      "Training Batch: 13553 Loss: 3045.008301\n",
      "Training Batch: 13554 Loss: 3036.851074\n",
      "Training Batch: 13555 Loss: 3087.311768\n",
      "Training Batch: 13556 Loss: 3433.500732\n",
      "Training Batch: 13557 Loss: 3277.593018\n",
      "Training Batch: 13558 Loss: 3310.082520\n",
      "Training Batch: 13559 Loss: 3181.750488\n",
      "Training Batch: 13560 Loss: 3214.769775\n",
      "Training Batch: 13561 Loss: 3083.978027\n",
      "Training Batch: 13562 Loss: 3132.934082\n",
      "Training Batch: 13563 Loss: 3054.682861\n",
      "Training Batch: 13564 Loss: 3045.320801\n",
      "Training Batch: 13565 Loss: 3091.079834\n",
      "Training Batch: 13566 Loss: 3230.687012\n",
      "Training Batch: 13567 Loss: 3351.167480\n",
      "Training Batch: 13568 Loss: 3169.650635\n",
      "Training Batch: 13569 Loss: 3117.340332\n",
      "Training Batch: 13570 Loss: 3094.132812\n",
      "Training Batch: 13571 Loss: 3233.153320\n",
      "Training Batch: 13572 Loss: 3507.976562\n",
      "Training Batch: 13573 Loss: 3188.991699\n",
      "Training Batch: 13574 Loss: 3094.086670\n",
      "Training Batch: 13575 Loss: 2984.256592\n",
      "Training Batch: 13576 Loss: 3204.495605\n",
      "Training Batch: 13577 Loss: 3303.587158\n",
      "Training Batch: 13578 Loss: 3451.958252\n",
      "Training Batch: 13579 Loss: 3084.670410\n",
      "Training Batch: 13580 Loss: 3062.203125\n",
      "Training Batch: 13581 Loss: 3352.040039\n",
      "Training Batch: 13582 Loss: 3173.529053\n",
      "Training Batch: 13583 Loss: 3167.295410\n",
      "Training Batch: 13584 Loss: 3000.896973\n",
      "Training Batch: 13585 Loss: 3021.423096\n",
      "Training Batch: 13586 Loss: 3079.975342\n",
      "Training Batch: 13587 Loss: 3164.507812\n",
      "Training Batch: 13588 Loss: 3141.040527\n",
      "Training Batch: 13589 Loss: 3109.281250\n",
      "Training Batch: 13590 Loss: 3263.863281\n",
      "Training Batch: 13591 Loss: 3250.480713\n",
      "Training Batch: 13592 Loss: 3087.096191\n",
      "Training Batch: 13593 Loss: 3117.002686\n",
      "Training Batch: 13594 Loss: 3113.814941\n",
      "Training Batch: 13595 Loss: 3191.301025\n",
      "Training Batch: 13596 Loss: 3223.512451\n",
      "Training Batch: 13597 Loss: 3210.724609\n",
      "Training Batch: 13598 Loss: 3132.852539\n",
      "Training Batch: 13599 Loss: 3178.669922\n",
      "Training Batch: 13600 Loss: 3031.563477\n",
      "Training Batch: 13601 Loss: 3000.386230\n",
      "Training Batch: 13602 Loss: 3146.801270\n",
      "Training Batch: 13603 Loss: 3022.071777\n",
      "Training Batch: 13604 Loss: 3104.193848\n",
      "Training Batch: 13605 Loss: 3269.640137\n",
      "Training Batch: 13606 Loss: 3029.079346\n",
      "Training Batch: 13607 Loss: 3410.142090\n",
      "Training Batch: 13608 Loss: 3052.438965\n",
      "Training Batch: 13609 Loss: 3158.702148\n",
      "Training Batch: 13610 Loss: 3143.071289\n",
      "Training Batch: 13611 Loss: 3094.898926\n",
      "Training Batch: 13612 Loss: 3118.156250\n",
      "Training Batch: 13613 Loss: 3219.121582\n",
      "Training Batch: 13614 Loss: 3133.557129\n",
      "Training Batch: 13615 Loss: 3294.960449\n",
      "Training Batch: 13616 Loss: 3059.572266\n",
      "Training Batch: 13617 Loss: 3144.973633\n",
      "Training Batch: 13618 Loss: 3484.747559\n",
      "Training Batch: 13619 Loss: 3186.656006\n",
      "Training Batch: 13620 Loss: 3094.940674\n",
      "Training Batch: 13621 Loss: 3144.683594\n",
      "Training Batch: 13622 Loss: 3139.554199\n",
      "Training Batch: 13623 Loss: 3160.704590\n",
      "Training Batch: 13624 Loss: 3032.382812\n",
      "Training Batch: 13625 Loss: 3071.789062\n",
      "Training Batch: 13626 Loss: 3243.484131\n",
      "Training Batch: 13627 Loss: 3000.920654\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Batch: 13628 Loss: 3029.517334\n",
      "Training Batch: 13629 Loss: 2990.086426\n",
      "Training Batch: 13630 Loss: 3069.150879\n",
      "Training Batch: 13631 Loss: 2994.053711\n",
      "Training Batch: 13632 Loss: 3185.405273\n",
      "Training Batch: 13633 Loss: 3077.436035\n",
      "Training Batch: 13634 Loss: 3033.129150\n",
      "Training Batch: 13635 Loss: 3374.475586\n",
      "Training Batch: 13636 Loss: 3037.106201\n",
      "Training Batch: 13637 Loss: 3145.876953\n",
      "Training Batch: 13638 Loss: 3061.212158\n",
      "Training Batch: 13639 Loss: 2989.120850\n",
      "Training Batch: 13640 Loss: 3159.249023\n",
      "Training Batch: 13641 Loss: 3078.099121\n",
      "Training Batch: 13642 Loss: 3073.377441\n",
      "Training Batch: 13643 Loss: 3217.198486\n",
      "Training Batch: 13644 Loss: 3210.316895\n",
      "Training Batch: 13645 Loss: 3254.041260\n",
      "Training Batch: 13646 Loss: 3072.944824\n",
      "Training Batch: 13647 Loss: 3080.354980\n",
      "Training Batch: 13648 Loss: 3084.399902\n",
      "Training Batch: 13649 Loss: 3087.469238\n",
      "Training Batch: 13650 Loss: 3595.791504\n",
      "Training Batch: 13651 Loss: 3041.191650\n",
      "Training Batch: 13652 Loss: 3237.545166\n",
      "Training Batch: 13653 Loss: 3083.599121\n",
      "Training Batch: 13654 Loss: 2998.501221\n",
      "Training Batch: 13655 Loss: 3143.124023\n",
      "Training Batch: 13656 Loss: 3361.097412\n",
      "Training Batch: 13657 Loss: 3179.904297\n",
      "Training Batch: 13658 Loss: 3386.340332\n",
      "Training Batch: 13659 Loss: 3257.812012\n",
      "Training Batch: 13660 Loss: 3216.697754\n",
      "Training Batch: 13661 Loss: 3786.131348\n",
      "Training Batch: 13662 Loss: 3153.059082\n",
      "Training Batch: 13663 Loss: 3232.395508\n",
      "Training Batch: 13664 Loss: 3047.824219\n",
      "Training Batch: 13665 Loss: 3166.555908\n",
      "Training Batch: 13666 Loss: 3074.374512\n",
      "Training Batch: 13667 Loss: 3105.373291\n",
      "Training Batch: 13668 Loss: 3061.191895\n",
      "Training Batch: 13669 Loss: 3035.654297\n",
      "Training Batch: 13670 Loss: 3093.010498\n",
      "Training Batch: 13671 Loss: 3344.409912\n",
      "Training Batch: 13672 Loss: 3117.317871\n",
      "Training Batch: 13673 Loss: 2993.888428\n",
      "Training Batch: 13674 Loss: 3253.086426\n",
      "Training Batch: 13675 Loss: 3145.592773\n",
      "Training Batch: 13676 Loss: 3049.993896\n",
      "Training Batch: 13677 Loss: 3108.373047\n",
      "Training Batch: 13678 Loss: 3115.295410\n",
      "Training Batch: 13679 Loss: 3111.969238\n",
      "Training Batch: 13680 Loss: 3204.576172\n",
      "Training Batch: 13681 Loss: 3261.883301\n",
      "Training Batch: 13682 Loss: 3232.683105\n",
      "Training Batch: 13683 Loss: 3190.977539\n",
      "Training Batch: 13684 Loss: 3266.602051\n",
      "Training Batch: 13685 Loss: 3069.975098\n",
      "Training Batch: 13686 Loss: 3174.044678\n",
      "Training Batch: 13687 Loss: 3275.932617\n",
      "Training Batch: 13688 Loss: 3335.538574\n",
      "Training Batch: 13689 Loss: 3373.234619\n",
      "Training Batch: 13690 Loss: 3232.550781\n",
      "Training Batch: 13691 Loss: 3319.106445\n",
      "Training Batch: 13692 Loss: 3199.859619\n",
      "Training Batch: 13693 Loss: 3199.919434\n",
      "Training Batch: 13694 Loss: 3144.772949\n",
      "Training Batch: 13695 Loss: 3184.883789\n",
      "Training Batch: 13696 Loss: 3018.041992\n",
      "Training Batch: 13697 Loss: 3026.740967\n",
      "Training Batch: 13698 Loss: 3127.383057\n",
      "Training Batch: 13699 Loss: 3145.174805\n",
      "Training Batch: 13700 Loss: 3019.320801\n",
      "Training Batch: 13701 Loss: 3156.317871\n",
      "Training Batch: 13702 Loss: 3162.630859\n",
      "Training Batch: 13703 Loss: 3145.942627\n",
      "Training Batch: 13704 Loss: 3150.227783\n",
      "Training Batch: 13705 Loss: 3099.935059\n",
      "Training Batch: 13706 Loss: 3134.265137\n",
      "Training Batch: 13707 Loss: 3104.719727\n",
      "Training Batch: 13708 Loss: 3243.682373\n",
      "Training Batch: 13709 Loss: 3084.948730\n",
      "Training Batch: 13710 Loss: 3111.557129\n",
      "Training Batch: 13711 Loss: 3022.503906\n",
      "Training Batch: 13712 Loss: 3025.618652\n",
      "Training Batch: 13713 Loss: 3238.603027\n",
      "Training Batch: 13714 Loss: 3210.378174\n",
      "Training Batch: 13715 Loss: 3118.825439\n",
      "Training Batch: 13716 Loss: 3149.435059\n",
      "Training Batch: 13717 Loss: 3064.752930\n",
      "Training Batch: 13718 Loss: 3147.259766\n",
      "Training Batch: 13719 Loss: 3070.563965\n",
      "Training Batch: 13720 Loss: 3170.638184\n",
      "Training Batch: 13721 Loss: 3382.661621\n",
      "Training Batch: 13722 Loss: 3160.301514\n",
      "Training Batch: 13723 Loss: 3064.521484\n",
      "Training Batch: 13724 Loss: 3059.123779\n",
      "Training Batch: 13725 Loss: 3047.114014\n",
      "Training Batch: 13726 Loss: 3012.121826\n",
      "Training Batch: 13727 Loss: 3207.237793\n",
      "Training Batch: 13728 Loss: 3175.132812\n",
      "Training Batch: 13729 Loss: 3311.664062\n",
      "Training Batch: 13730 Loss: 3184.236328\n",
      "Training Batch: 13731 Loss: 3461.426758\n",
      "Training Batch: 13732 Loss: 3281.395508\n",
      "Training Batch: 13733 Loss: 3306.416016\n",
      "Training Batch: 13734 Loss: 3439.861328\n",
      "Training Batch: 13735 Loss: 3144.035400\n",
      "Training Batch: 13736 Loss: 3138.951172\n",
      "Training Batch: 13737 Loss: 3145.224365\n",
      "Training Batch: 13738 Loss: 3459.208008\n",
      "Training Batch: 13739 Loss: 3254.121094\n",
      "Training Batch: 13740 Loss: 3359.576660\n",
      "Training Batch: 13741 Loss: 3134.273438\n",
      "Training Batch: 13742 Loss: 3362.749512\n",
      "Training Batch: 13743 Loss: 3211.184570\n",
      "Training Batch: 13744 Loss: 3229.866699\n",
      "Training Batch: 13745 Loss: 3194.269531\n",
      "Training Batch: 13746 Loss: 3108.422852\n",
      "Training Batch: 13747 Loss: 3150.583496\n",
      "Training Batch: 13748 Loss: 3305.612305\n",
      "Training Batch: 13749 Loss: 3017.138916\n",
      "Training Batch: 13750 Loss: 3091.494385\n",
      "Training Batch: 13751 Loss: 3324.875732\n",
      "Training Batch: 13752 Loss: 2945.557617\n",
      "Training Batch: 13753 Loss: 3149.197266\n",
      "Training Batch: 13754 Loss: 3042.885254\n",
      "Training Batch: 13755 Loss: 3124.110352\n",
      "Training Batch: 13756 Loss: 3141.887939\n",
      "Training Batch: 13757 Loss: 3082.926514\n",
      "Training Batch: 13758 Loss: 3004.023438\n",
      "Training Batch: 13759 Loss: 3232.892578\n",
      "Training Batch: 13760 Loss: 3125.736084\n",
      "Training Batch: 13761 Loss: 3100.528320\n",
      "Training Batch: 13762 Loss: 3104.980713\n",
      "Training Batch: 13763 Loss: 3343.880127\n",
      "Training Batch: 13764 Loss: 3165.338867\n",
      "Training Batch: 13765 Loss: 3254.752930\n",
      "Training Batch: 13766 Loss: 3150.738525\n",
      "Training Batch: 13767 Loss: 3156.286133\n",
      "Training Batch: 13768 Loss: 3130.127441\n",
      "Training Batch: 13769 Loss: 3247.487549\n",
      "Training Batch: 13770 Loss: 3168.519531\n",
      "Training Batch: 13771 Loss: 3285.925293\n",
      "Training Batch: 13772 Loss: 3167.245850\n",
      "Training Batch: 13773 Loss: 3067.064209\n",
      "Training Batch: 13774 Loss: 3206.195312\n",
      "Training Batch: 13775 Loss: 3026.551270\n",
      "Training Batch: 13776 Loss: 3215.336914\n",
      "Training Batch: 13777 Loss: 3235.991699\n",
      "Training Batch: 13778 Loss: 3175.033203\n",
      "Training Batch: 13779 Loss: 3094.799316\n",
      "Training Batch: 13780 Loss: 3117.602783\n",
      "Training Batch: 13781 Loss: 2996.357910\n",
      "Training Batch: 13782 Loss: 3022.561523\n",
      "Training Batch: 13783 Loss: 3105.793945\n",
      "Training Batch: 13784 Loss: 3090.934570\n",
      "Training Batch: 13785 Loss: 3014.023682\n",
      "Training Batch: 13786 Loss: 3027.776367\n",
      "Training Batch: 13787 Loss: 3136.958008\n",
      "Training Batch: 13788 Loss: 3106.166504\n",
      "Training Batch: 13789 Loss: 3129.051758\n",
      "Training Batch: 13790 Loss: 3171.735352\n",
      "Training Batch: 13791 Loss: 3104.527588\n",
      "Training Batch: 13792 Loss: 3133.451172\n",
      "Training Batch: 13793 Loss: 3036.453613\n",
      "Training Batch: 13794 Loss: 3176.130371\n",
      "Training Batch: 13795 Loss: 3366.607422\n",
      "Training Batch: 13796 Loss: 3617.402100\n",
      "Training Batch: 13797 Loss: 3476.693848\n",
      "Training Batch: 13798 Loss: 3064.170898\n",
      "Training Batch: 13799 Loss: 3112.360107\n",
      "Training Batch: 13800 Loss: 3065.746582\n",
      "Training Batch: 13801 Loss: 3232.979492\n",
      "Training Batch: 13802 Loss: 3276.239746\n",
      "Training Batch: 13803 Loss: 3095.808594\n",
      "Training Batch: 13804 Loss: 3252.481445\n",
      "Training Batch: 13805 Loss: 3060.048096\n",
      "Training Batch: 13806 Loss: 3061.225098\n",
      "Training Batch: 13807 Loss: 3107.768799\n",
      "Training Batch: 13808 Loss: 3226.380127\n",
      "Training Batch: 13809 Loss: 3282.982666\n",
      "Training Batch: 13810 Loss: 3088.613281\n",
      "Training Batch: 13811 Loss: 3034.255127\n",
      "Training Batch: 13812 Loss: 3087.566406\n",
      "Training Batch: 13813 Loss: 3085.072021\n",
      "Training Batch: 13814 Loss: 3042.497070\n",
      "Training Batch: 13815 Loss: 3193.624512\n",
      "Training Batch: 13816 Loss: 3013.427734\n",
      "Training Batch: 13817 Loss: 3064.079102\n",
      "Training Batch: 13818 Loss: 3432.151611\n",
      "Training Batch: 13819 Loss: 3093.416016\n",
      "Training Batch: 13820 Loss: 3068.997803\n",
      "Training Batch: 13821 Loss: 3439.384766\n",
      "Training Batch: 13822 Loss: 3077.905029\n",
      "Training Batch: 13823 Loss: 3044.334473\n",
      "Training Batch: 13824 Loss: 3144.051758\n",
      "Training Batch: 13825 Loss: 3140.563721\n",
      "Training Batch: 13826 Loss: 3239.005127\n",
      "Training Batch: 13827 Loss: 3690.574707\n",
      "Training Batch: 13828 Loss: 3319.511230\n",
      "Training Batch: 13829 Loss: 3039.626465\n",
      "Training Batch: 13830 Loss: 3082.916016\n",
      "Training Batch: 13831 Loss: 3069.544434\n",
      "Training Batch: 13832 Loss: 3188.631592\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Batch: 13833 Loss: 2990.555664\n",
      "Training Batch: 13834 Loss: 3277.618408\n",
      "Training Batch: 13835 Loss: 3272.868164\n",
      "Training Batch: 13836 Loss: 3092.135742\n",
      "Training Batch: 13837 Loss: 3324.486816\n",
      "Training Batch: 13838 Loss: 3129.072266\n",
      "Training Batch: 13839 Loss: 3081.199219\n",
      "Training Batch: 13840 Loss: 3045.995605\n",
      "Training Batch: 13841 Loss: 3084.146484\n",
      "Training Batch: 13842 Loss: 3150.634766\n",
      "Training Batch: 13843 Loss: 3182.798828\n",
      "Training Batch: 13844 Loss: 3122.216797\n",
      "Training Batch: 13845 Loss: 3088.855225\n",
      "Training Batch: 13846 Loss: 3111.449219\n",
      "Training Batch: 13847 Loss: 3154.463379\n",
      "Training Batch: 13848 Loss: 3036.322266\n",
      "Training Batch: 13849 Loss: 3088.693359\n",
      "Training Batch: 13850 Loss: 3112.997070\n",
      "Training Batch: 13851 Loss: 3142.275146\n",
      "Training Batch: 13852 Loss: 3164.586914\n",
      "Training Batch: 13853 Loss: 3051.533691\n",
      "Training Batch: 13854 Loss: 3116.186035\n",
      "Training Batch: 13855 Loss: 3375.836914\n",
      "Training Batch: 13856 Loss: 3017.320312\n",
      "Training Batch: 13857 Loss: 3234.484131\n",
      "Training Batch: 13858 Loss: 3031.827148\n",
      "Training Batch: 13859 Loss: 3366.800293\n",
      "Training Batch: 13860 Loss: 3102.266602\n",
      "Training Batch: 13861 Loss: 3248.155762\n",
      "Training Batch: 13862 Loss: 3050.325439\n",
      "Training Batch: 13863 Loss: 3333.785645\n",
      "Training Batch: 13864 Loss: 3112.745361\n",
      "Training Batch: 13865 Loss: 3070.632568\n",
      "Training Batch: 13866 Loss: 3210.301025\n",
      "Training Batch: 13867 Loss: 3026.255371\n",
      "Training Batch: 13868 Loss: 3201.119873\n",
      "Training Batch: 13869 Loss: 3124.386963\n",
      "Training Batch: 13870 Loss: 3114.516113\n",
      "Training Batch: 13871 Loss: 3371.867676\n",
      "Training Batch: 13872 Loss: 3535.506836\n",
      "Training Batch: 13873 Loss: 3166.628906\n",
      "Training Batch: 13874 Loss: 2990.029785\n",
      "Training Batch: 13875 Loss: 3256.621338\n",
      "Training Batch: 13876 Loss: 4151.348633\n",
      "Training Batch: 13877 Loss: 3252.136230\n",
      "Training Batch: 13878 Loss: 3146.756836\n",
      "Training Batch: 13879 Loss: 3071.476074\n",
      "Training Batch: 13880 Loss: 3170.977295\n",
      "Training Batch: 13881 Loss: 3314.868652\n",
      "Training Batch: 13882 Loss: 3158.195801\n",
      "Training Batch: 13883 Loss: 3186.533691\n",
      "Training Batch: 13884 Loss: 3107.928955\n",
      "Training Batch: 13885 Loss: 3158.072266\n",
      "Training Batch: 13886 Loss: 3092.657715\n",
      "Training Batch: 13887 Loss: 3050.178223\n",
      "Training Batch: 13888 Loss: 3289.572754\n",
      "Training Batch: 13889 Loss: 3201.538086\n",
      "Training Batch: 13890 Loss: 3130.859863\n",
      "Training Batch: 13891 Loss: 3245.579590\n",
      "Training Batch: 13892 Loss: 3207.526367\n",
      "Training Batch: 13893 Loss: 3534.034912\n",
      "Training Batch: 13894 Loss: 3311.788818\n",
      "Training Batch: 13895 Loss: 3087.163086\n",
      "Training Batch: 13896 Loss: 3118.838379\n",
      "Training Batch: 13897 Loss: 3054.405762\n",
      "Training Batch: 13898 Loss: 3128.898926\n",
      "Training Batch: 13899 Loss: 3052.406738\n",
      "Training Batch: 13900 Loss: 3313.296875\n",
      "Training Batch: 13901 Loss: 3346.841064\n",
      "Training Batch: 13902 Loss: 3392.778076\n",
      "Training Batch: 13903 Loss: 3164.712891\n",
      "Training Batch: 13904 Loss: 3127.456787\n",
      "Training Batch: 13905 Loss: 3090.083984\n",
      "Training Batch: 13906 Loss: 3064.710693\n",
      "Training Batch: 13907 Loss: 3098.743164\n",
      "Training Batch: 13908 Loss: 3226.218750\n",
      "Training Batch: 13909 Loss: 3143.291748\n",
      "Training Batch: 13910 Loss: 3118.890137\n",
      "Training Batch: 13911 Loss: 3144.297119\n",
      "Training Batch: 13912 Loss: 3129.174561\n",
      "Training Batch: 13913 Loss: 3109.597900\n",
      "Training Batch: 13914 Loss: 3064.179443\n",
      "Training Batch: 13915 Loss: 3033.324219\n",
      "Training Batch: 13916 Loss: 3153.866699\n",
      "Training Batch: 13917 Loss: 3241.747559\n",
      "Training Batch: 13918 Loss: 3198.499512\n",
      "Training Batch: 13919 Loss: 3105.033203\n",
      "Training Batch: 13920 Loss: 3187.657959\n",
      "Training Batch: 13921 Loss: 3123.118164\n",
      "Training Batch: 13922 Loss: 3056.849609\n",
      "Training Batch: 13923 Loss: 3002.674316\n",
      "Training Batch: 13924 Loss: 3036.491455\n",
      "Training Batch: 13925 Loss: 3126.807129\n",
      "Training Batch: 13926 Loss: 3088.155762\n",
      "Training Batch: 13927 Loss: 3002.130371\n",
      "Training Batch: 13928 Loss: 3115.429199\n",
      "Training Batch: 13929 Loss: 3301.238525\n",
      "Training Batch: 13930 Loss: 3086.492432\n",
      "Training Batch: 13931 Loss: 3148.361816\n",
      "Training Batch: 13932 Loss: 3122.525391\n",
      "Training Batch: 13933 Loss: 3060.834961\n",
      "Training Batch: 13934 Loss: 2991.160400\n",
      "Training Batch: 13935 Loss: 3106.398926\n",
      "Training Batch: 13936 Loss: 3087.649658\n",
      "Training Batch: 13937 Loss: 3022.034668\n",
      "Training Batch: 13938 Loss: 3032.968262\n",
      "Training Batch: 13939 Loss: 3216.349609\n",
      "Training Batch: 13940 Loss: 3215.630859\n",
      "Training Batch: 13941 Loss: 3135.327148\n",
      "Training Batch: 13942 Loss: 3144.280762\n",
      "Training Batch: 13943 Loss: 3170.474609\n",
      "Training Batch: 13944 Loss: 3108.641602\n",
      "Training Batch: 13945 Loss: 3276.670166\n",
      "Training Batch: 13946 Loss: 3065.705078\n",
      "Training Batch: 13947 Loss: 3167.038574\n",
      "Training Batch: 13948 Loss: 3305.371582\n",
      "Training Batch: 13949 Loss: 3131.944336\n",
      "Training Batch: 13950 Loss: 3406.466797\n",
      "Training Batch: 13951 Loss: 3215.848389\n",
      "Training Batch: 13952 Loss: 3135.311523\n",
      "Training Batch: 13953 Loss: 3159.509766\n",
      "Training Batch: 13954 Loss: 3025.693359\n",
      "Training Batch: 13955 Loss: 3262.209961\n",
      "Training Batch: 13956 Loss: 3260.059326\n",
      "Training Batch: 13957 Loss: 2996.185547\n",
      "Training Batch: 13958 Loss: 3023.376221\n",
      "Training Batch: 13959 Loss: 3112.830566\n",
      "Training Batch: 13960 Loss: 3068.338379\n",
      "Training Batch: 13961 Loss: 3084.685059\n",
      "Training Batch: 13962 Loss: 3152.273926\n",
      "Training Batch: 13963 Loss: 3080.733887\n",
      "Training Batch: 13964 Loss: 3051.813232\n",
      "Training Batch: 13965 Loss: 3070.188721\n",
      "Training Batch: 13966 Loss: 3170.764648\n",
      "Training Batch: 13967 Loss: 3108.704102\n",
      "Training Batch: 13968 Loss: 3092.485107\n",
      "Training Batch: 13969 Loss: 3455.482910\n",
      "Training Batch: 13970 Loss: 3464.513916\n",
      "Training Batch: 13971 Loss: 3151.661621\n",
      "Training Batch: 13972 Loss: 3185.050781\n",
      "Training Batch: 13973 Loss: 3099.804688\n",
      "Training Batch: 13974 Loss: 3173.323730\n",
      "Training Batch: 13975 Loss: 3157.746094\n",
      "Training Batch: 13976 Loss: 3109.131348\n",
      "Training Batch: 13977 Loss: 3211.909180\n",
      "Training Batch: 13978 Loss: 3177.266113\n",
      "Training Batch: 13979 Loss: 3311.317871\n",
      "Training Batch: 13980 Loss: 3254.847168\n",
      "Training Batch: 13981 Loss: 3168.512939\n",
      "Training Batch: 13982 Loss: 2934.480957\n",
      "Training Batch: 13983 Loss: 3004.843750\n",
      "Training Batch: 13984 Loss: 3131.750977\n",
      "Training Batch: 13985 Loss: 2973.479004\n",
      "Training Batch: 13986 Loss: 3148.250977\n",
      "Training Batch: 13987 Loss: 2994.986084\n",
      "Training Batch: 13988 Loss: 3144.618652\n",
      "Training Batch: 13989 Loss: 3072.831787\n",
      "Training Batch: 13990 Loss: 2970.546875\n",
      "Training Batch: 13991 Loss: 3239.022461\n",
      "Training Batch: 13992 Loss: 2953.411621\n",
      "Training Batch: 13993 Loss: 3302.586914\n",
      "Training Batch: 13994 Loss: 3009.874023\n",
      "Training Batch: 13995 Loss: 3209.839111\n",
      "Training Batch: 13996 Loss: 3233.485352\n",
      "Training Batch: 13997 Loss: 3227.710449\n",
      "Training Batch: 13998 Loss: 3106.185303\n",
      "Training Batch: 13999 Loss: 3408.787842\n",
      "Training Batch: 14000 Loss: 3093.935547\n",
      "Training Batch: 14001 Loss: 3027.854492\n",
      "Training Batch: 14002 Loss: 3116.085205\n",
      "Training Batch: 14003 Loss: 3179.602539\n",
      "Training Batch: 14004 Loss: 2971.553223\n",
      "Training Batch: 14005 Loss: 3044.284180\n",
      "Training Batch: 14006 Loss: 3040.208008\n",
      "Training Batch: 14007 Loss: 3083.054199\n",
      "Training Batch: 14008 Loss: 3130.187500\n",
      "Training Batch: 14009 Loss: 3264.916016\n",
      "Training Batch: 14010 Loss: 3181.132080\n",
      "Training Batch: 14011 Loss: 3237.370850\n",
      "Training Batch: 14012 Loss: 3134.866699\n",
      "Training Batch: 14013 Loss: 3215.554443\n",
      "Training Batch: 14014 Loss: 3089.586426\n",
      "Training Batch: 14015 Loss: 3051.680176\n",
      "Training Batch: 14016 Loss: 3151.764893\n",
      "Training Batch: 14017 Loss: 2985.767578\n",
      "Training Batch: 14018 Loss: 3191.540527\n",
      "Training Batch: 14019 Loss: 3100.875000\n",
      "Training Batch: 14020 Loss: 3271.958252\n",
      "Training Batch: 14021 Loss: 3142.733887\n",
      "Training Batch: 14022 Loss: 3257.302734\n",
      "Training Batch: 14023 Loss: 3037.552246\n",
      "Training Batch: 14024 Loss: 3109.313232\n",
      "Training Batch: 14025 Loss: 3023.119629\n",
      "Training Batch: 14026 Loss: 3062.830566\n",
      "Training Batch: 14027 Loss: 3076.607422\n",
      "Training Batch: 14028 Loss: 2980.433838\n",
      "Training Batch: 14029 Loss: 3165.458984\n",
      "Training Batch: 14030 Loss: 3101.544678\n",
      "Training Batch: 14031 Loss: 3246.218506\n",
      "Training Batch: 14032 Loss: 3450.283203\n",
      "Training Batch: 14033 Loss: 2951.825684\n",
      "Training Batch: 14034 Loss: 3078.060791\n",
      "Training Batch: 14035 Loss: 3093.271973\n",
      "Training Batch: 14036 Loss: 3135.124512\n",
      "Training Batch: 14037 Loss: 3124.126953\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Batch: 14038 Loss: 3109.735840\n",
      "Training Batch: 14039 Loss: 3073.105225\n",
      "Training Batch: 14040 Loss: 3081.923828\n",
      "Training Batch: 14041 Loss: 3050.623779\n",
      "Training Batch: 14042 Loss: 3171.832764\n",
      "Training Batch: 14043 Loss: 3134.199707\n",
      "Training Batch: 14044 Loss: 3084.498535\n",
      "Training Batch: 14045 Loss: 3033.519043\n",
      "Training Batch: 14046 Loss: 3100.447266\n",
      "Training Batch: 14047 Loss: 3251.256836\n",
      "Training Batch: 14048 Loss: 3017.659912\n",
      "Training Batch: 14049 Loss: 3210.349121\n",
      "Training Batch: 14050 Loss: 3113.036133\n",
      "Training Batch: 14051 Loss: 3107.115723\n",
      "Training Batch: 14052 Loss: 3015.045410\n",
      "Training Batch: 14053 Loss: 3080.050781\n",
      "Training Batch: 14054 Loss: 3105.023926\n",
      "Training Batch: 14055 Loss: 3132.407715\n",
      "Training Batch: 14056 Loss: 3150.292480\n",
      "Training Batch: 14057 Loss: 3268.008301\n",
      "Training Batch: 14058 Loss: 3035.487793\n",
      "Training Batch: 14059 Loss: 3055.921875\n",
      "Training Batch: 14060 Loss: 3073.594238\n",
      "Training Batch: 14061 Loss: 3078.394043\n",
      "Training Batch: 14062 Loss: 3132.858398\n",
      "Training Batch: 14063 Loss: 3123.329834\n",
      "Training Batch: 14064 Loss: 3065.253418\n",
      "Training Batch: 14065 Loss: 3034.894531\n",
      "Training Batch: 14066 Loss: 3159.643799\n",
      "Training Batch: 14067 Loss: 3111.575928\n",
      "Training Batch: 14068 Loss: 3103.041016\n",
      "Training Batch: 14069 Loss: 3117.219238\n",
      "Training Batch: 14070 Loss: 3083.606934\n",
      "Training Batch: 14071 Loss: 3302.283691\n",
      "Training Batch: 14072 Loss: 3161.446045\n",
      "Training Batch: 14073 Loss: 3126.452637\n",
      "Training Batch: 14074 Loss: 3043.763672\n",
      "Training Batch: 14075 Loss: 3122.718018\n",
      "Training Batch: 14076 Loss: 3015.626221\n",
      "Training Batch: 14077 Loss: 3094.656494\n",
      "Training Batch: 14078 Loss: 3279.823975\n",
      "Training Batch: 14079 Loss: 3086.380859\n",
      "Training Batch: 14080 Loss: 3115.801270\n",
      "Training Batch: 14081 Loss: 3243.619141\n",
      "Training Batch: 14082 Loss: 3266.275635\n",
      "Training Batch: 14083 Loss: 3202.033203\n",
      "Training Batch: 14084 Loss: 3029.250244\n",
      "Training Batch: 14085 Loss: 3049.154785\n",
      "Training Batch: 14086 Loss: 3072.133301\n",
      "Training Batch: 14087 Loss: 3075.182129\n",
      "Training Batch: 14088 Loss: 2948.880371\n",
      "Training Batch: 14089 Loss: 3144.288086\n",
      "Training Batch: 14090 Loss: 3100.832764\n",
      "Training Batch: 14091 Loss: 3179.504150\n",
      "Training Batch: 14092 Loss: 3032.566162\n",
      "Training Batch: 14093 Loss: 3250.905273\n",
      "Training Batch: 14094 Loss: 3186.127930\n",
      "Training Batch: 14095 Loss: 3461.256836\n",
      "Training Batch: 14096 Loss: 3093.209961\n",
      "Training Batch: 14097 Loss: 3094.801270\n",
      "Training Batch: 14098 Loss: 3084.570801\n",
      "Training Batch: 14099 Loss: 2976.693115\n",
      "Training Batch: 14100 Loss: 3046.729980\n",
      "Training Batch: 14101 Loss: 3233.318359\n",
      "Training Batch: 14102 Loss: 3081.770508\n",
      "Training Batch: 14103 Loss: 3066.061523\n",
      "Training Batch: 14104 Loss: 3043.187500\n",
      "Training Batch: 14105 Loss: 3174.861572\n",
      "Training Batch: 14106 Loss: 3290.133789\n",
      "Training Batch: 14107 Loss: 3071.261230\n",
      "Training Batch: 14108 Loss: 3151.191895\n",
      "Training Batch: 14109 Loss: 3095.252441\n",
      "Training Batch: 14110 Loss: 3129.719482\n",
      "Training Batch: 14111 Loss: 3189.223145\n",
      "Training Batch: 14112 Loss: 3253.470703\n",
      "Training Batch: 14113 Loss: 3106.225342\n",
      "Training Batch: 14114 Loss: 3129.828857\n",
      "Training Batch: 14115 Loss: 3520.880859\n",
      "Training Batch: 14116 Loss: 3086.850586\n",
      "Training Batch: 14117 Loss: 3137.599609\n",
      "Training Batch: 14118 Loss: 3162.393066\n",
      "Training Batch: 14119 Loss: 3125.031738\n",
      "Training Batch: 14120 Loss: 3080.231201\n",
      "Training Batch: 14121 Loss: 3241.831055\n",
      "Training Batch: 14122 Loss: 3146.255859\n",
      "Training Batch: 14123 Loss: 2946.735352\n",
      "Training Batch: 14124 Loss: 3114.513184\n",
      "Training Batch: 14125 Loss: 3054.901855\n",
      "Training Batch: 14126 Loss: 3313.031250\n",
      "Training Batch: 14127 Loss: 3082.523438\n",
      "Training Batch: 14128 Loss: 3364.672363\n",
      "Training Batch: 14129 Loss: 3070.917725\n",
      "Training Batch: 14130 Loss: 3048.001953\n",
      "Training Batch: 14131 Loss: 3070.589844\n",
      "Training Batch: 14132 Loss: 3164.368164\n",
      "Training Batch: 14133 Loss: 3139.223145\n",
      "Training Batch: 14134 Loss: 3252.421387\n",
      "Training Batch: 14135 Loss: 3131.162109\n",
      "Training Batch: 14136 Loss: 3023.554688\n",
      "Training Batch: 14137 Loss: 3045.491211\n",
      "Training Batch: 14138 Loss: 3049.730469\n",
      "Training Batch: 14139 Loss: 3046.133301\n",
      "Training Batch: 14140 Loss: 3114.871338\n",
      "Training Batch: 14141 Loss: 3038.572754\n",
      "Training Batch: 14142 Loss: 2960.027344\n",
      "Training Batch: 14143 Loss: 3250.478027\n",
      "Training Batch: 14144 Loss: 3029.473633\n",
      "Training Batch: 14145 Loss: 3297.881592\n",
      "Training Batch: 14146 Loss: 3195.126953\n",
      "Training Batch: 14147 Loss: 3015.243652\n",
      "Training Batch: 14148 Loss: 3108.711914\n",
      "Training Batch: 14149 Loss: 3098.338379\n",
      "Training Batch: 14150 Loss: 3044.463623\n",
      "Training Batch: 14151 Loss: 3131.402832\n",
      "Training Batch: 14152 Loss: 3214.187012\n",
      "Training Batch: 14153 Loss: 3068.270508\n",
      "Training Batch: 14154 Loss: 3045.853516\n",
      "Training Batch: 14155 Loss: 3113.835693\n",
      "Training Batch: 14156 Loss: 2987.166992\n",
      "Training Batch: 14157 Loss: 3148.044434\n",
      "Training Batch: 14158 Loss: 3107.584961\n",
      "Training Batch: 14159 Loss: 3279.073242\n",
      "Training Batch: 14160 Loss: 3049.959473\n",
      "Training Batch: 14161 Loss: 2984.805908\n",
      "Training Batch: 14162 Loss: 3181.063721\n",
      "Training Batch: 14163 Loss: 3129.060791\n",
      "Training Batch: 14164 Loss: 3126.009277\n",
      "Training Batch: 14165 Loss: 3288.723389\n",
      "Training Batch: 14166 Loss: 3074.234375\n",
      "Training Batch: 14167 Loss: 3139.259033\n",
      "Training Batch: 14168 Loss: 3068.193848\n",
      "Training Batch: 14169 Loss: 3056.687988\n",
      "Training Batch: 14170 Loss: 3124.208252\n",
      "Training Batch: 14171 Loss: 3202.210449\n",
      "Training Batch: 14172 Loss: 2986.121094\n",
      "Training Batch: 14173 Loss: 3015.450439\n",
      "Training Batch: 14174 Loss: 3004.426514\n",
      "Training Batch: 14175 Loss: 3058.180176\n",
      "Training Batch: 14176 Loss: 2975.273926\n",
      "Training Batch: 14177 Loss: 3016.432617\n",
      "Training Batch: 14178 Loss: 2959.255615\n",
      "Training Batch: 14179 Loss: 3075.051025\n",
      "Training Batch: 14180 Loss: 3064.992188\n",
      "Training Batch: 14181 Loss: 2994.948730\n",
      "Training Batch: 14182 Loss: 3096.931152\n",
      "Training Batch: 14183 Loss: 2954.170166\n",
      "Training Batch: 14184 Loss: 3138.614746\n",
      "Training Batch: 14185 Loss: 3079.581299\n",
      "Training Batch: 14186 Loss: 3035.262695\n",
      "Training Batch: 14187 Loss: 3116.904297\n",
      "Training Batch: 14188 Loss: 3075.246094\n",
      "Training Batch: 14189 Loss: 3104.277832\n",
      "Training Batch: 14190 Loss: 3126.490967\n",
      "Training Batch: 14191 Loss: 3165.291016\n",
      "Training Batch: 14192 Loss: 2987.149414\n",
      "Training Batch: 14193 Loss: 3048.577637\n",
      "Training Batch: 14194 Loss: 3222.040527\n",
      "Training Batch: 14195 Loss: 3182.837891\n",
      "Training Batch: 14196 Loss: 2958.318115\n",
      "Training Batch: 14197 Loss: 3030.176514\n",
      "Training Batch: 14198 Loss: 2982.568359\n",
      "Training Batch: 14199 Loss: 3075.546387\n",
      "Training Batch: 14200 Loss: 3073.049805\n",
      "Training Batch: 14201 Loss: 3050.684570\n",
      "Training Batch: 14202 Loss: 3149.102539\n",
      "Training Batch: 14203 Loss: 3110.190430\n",
      "Training Batch: 14204 Loss: 3141.013184\n",
      "Training Batch: 14205 Loss: 3126.428711\n",
      "Training Batch: 14206 Loss: 2989.783691\n",
      "Training Batch: 14207 Loss: 2993.861816\n",
      "Training Batch: 14208 Loss: 3001.115234\n",
      "Training Batch: 14209 Loss: 3028.160156\n",
      "Training Batch: 14210 Loss: 3084.637695\n",
      "Training Batch: 14211 Loss: 3184.787598\n",
      "Training Batch: 14212 Loss: 3211.688477\n",
      "Training Batch: 14213 Loss: 3039.474609\n",
      "Training Batch: 14214 Loss: 3268.790527\n",
      "Training Batch: 14215 Loss: 3495.632080\n",
      "Training Batch: 14216 Loss: 3277.634277\n",
      "Training Batch: 14217 Loss: 3082.958252\n",
      "Training Batch: 14218 Loss: 3084.231689\n",
      "Training Batch: 14219 Loss: 3182.763672\n",
      "Training Batch: 14220 Loss: 3097.321533\n",
      "Training Batch: 14221 Loss: 3029.189941\n",
      "Training Batch: 14222 Loss: 3101.758301\n",
      "Training Batch: 14223 Loss: 3098.138916\n",
      "Training Batch: 14224 Loss: 3185.024902\n",
      "Training Batch: 14225 Loss: 2953.566895\n",
      "Training Batch: 14226 Loss: 3319.942871\n",
      "Training Batch: 14227 Loss: 3031.366455\n",
      "Training Batch: 14228 Loss: 3174.700684\n",
      "Training Batch: 14229 Loss: 3102.610596\n",
      "Training Batch: 14230 Loss: 3134.431152\n",
      "Training Batch: 14231 Loss: 3183.227295\n",
      "Training Batch: 14232 Loss: 3277.362305\n",
      "Training Batch: 14233 Loss: 3091.712646\n",
      "Training Batch: 14234 Loss: 2980.161621\n",
      "Training Batch: 14235 Loss: 3013.374512\n",
      "Training Batch: 14236 Loss: 2980.660645\n",
      "Training Batch: 14237 Loss: 3032.770752\n",
      "Training Batch: 14238 Loss: 3025.657715\n",
      "Training Batch: 14239 Loss: 3141.043457\n",
      "Training Batch: 14240 Loss: 3037.695312\n",
      "Training Batch: 14241 Loss: 3130.914062\n",
      "Training Batch: 14242 Loss: 3063.167480\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Batch: 14243 Loss: 3107.253906\n",
      "Training Batch: 14244 Loss: 3076.360840\n",
      "Training Batch: 14245 Loss: 3161.836914\n",
      "Training Batch: 14246 Loss: 3064.566895\n",
      "Training Batch: 14247 Loss: 3041.395020\n",
      "Training Batch: 14248 Loss: 3210.677734\n",
      "Training Batch: 14249 Loss: 3060.357910\n",
      "Training Batch: 14250 Loss: 3136.410645\n",
      "Training Batch: 14251 Loss: 3070.611328\n",
      "Training Batch: 14252 Loss: 3080.813965\n",
      "Training Batch: 14253 Loss: 2982.104492\n",
      "Training Batch: 14254 Loss: 3223.533203\n",
      "Training Batch: 14255 Loss: 2953.027832\n",
      "Training Batch: 14256 Loss: 3127.279541\n",
      "Training Batch: 14257 Loss: 3108.898193\n",
      "Training Batch: 14258 Loss: 3132.743896\n",
      "Training Batch: 14259 Loss: 3043.086914\n",
      "Training Batch: 14260 Loss: 3101.367188\n",
      "Training Batch: 14261 Loss: 3072.368652\n",
      "Training Batch: 14262 Loss: 2966.085449\n",
      "Training Batch: 14263 Loss: 3093.958496\n",
      "Training Batch: 14264 Loss: 3105.839355\n",
      "Training Batch: 14265 Loss: 3068.632080\n",
      "Training Batch: 14266 Loss: 3083.974121\n",
      "Training Batch: 14267 Loss: 3157.215820\n",
      "Training Batch: 14268 Loss: 2961.519775\n",
      "Training Batch: 14269 Loss: 3152.145264\n",
      "Training Batch: 14270 Loss: 3040.541992\n",
      "Training Batch: 14271 Loss: 3115.067871\n",
      "Training Batch: 14272 Loss: 3073.394043\n",
      "Training Batch: 14273 Loss: 3087.872314\n",
      "Training Batch: 14274 Loss: 3238.694336\n",
      "Training Batch: 14275 Loss: 3269.367188\n",
      "Training Batch: 14276 Loss: 3024.442383\n",
      "Training Batch: 14277 Loss: 3202.811523\n",
      "Training Batch: 14278 Loss: 2930.262695\n",
      "Training Batch: 14279 Loss: 3133.699707\n",
      "Training Batch: 14280 Loss: 3098.950928\n",
      "Training Batch: 14281 Loss: 3016.107666\n",
      "Training Batch: 14282 Loss: 3073.104492\n",
      "Training Batch: 14283 Loss: 3049.204834\n",
      "Training Batch: 14284 Loss: 3276.811035\n",
      "Training Batch: 14285 Loss: 3336.234619\n",
      "Training Batch: 14286 Loss: 3156.359619\n",
      "Training Batch: 14287 Loss: 3429.176270\n",
      "Training Batch: 14288 Loss: 3040.823242\n",
      "Training Batch: 14289 Loss: 3275.280273\n",
      "Training Batch: 14290 Loss: 3000.422363\n",
      "Training Batch: 14291 Loss: 3059.601562\n",
      "Training Batch: 14292 Loss: 3096.147949\n",
      "Training Batch: 14293 Loss: 3045.924805\n",
      "Training Batch: 14294 Loss: 3187.638672\n",
      "Training Batch: 14295 Loss: 3077.899170\n",
      "Training Batch: 14296 Loss: 3029.420898\n",
      "Training Batch: 14297 Loss: 3008.639893\n",
      "Training Batch: 14298 Loss: 3148.791016\n",
      "Training Batch: 14299 Loss: 3473.360840\n",
      "Training Batch: 14300 Loss: 3039.842041\n",
      "Training Batch: 14301 Loss: 3299.185547\n",
      "Training Batch: 14302 Loss: 3147.032715\n",
      "Training Batch: 14303 Loss: 3279.389648\n",
      "Training Batch: 14304 Loss: 3196.697021\n",
      "Training Batch: 14305 Loss: 3190.276855\n",
      "Training Batch: 14306 Loss: 3136.062988\n",
      "Training Batch: 14307 Loss: 3130.144043\n",
      "Training Batch: 14308 Loss: 3178.237793\n",
      "Training Batch: 14309 Loss: 3075.787598\n",
      "Training Batch: 14310 Loss: 3038.026855\n",
      "Training Batch: 14311 Loss: 3222.555664\n",
      "Training Batch: 14312 Loss: 3184.707031\n",
      "Training Batch: 14313 Loss: 3086.910156\n",
      "Training Batch: 14314 Loss: 3042.824707\n",
      "Training Batch: 14315 Loss: 3017.161865\n",
      "Training Batch: 14316 Loss: 3058.735840\n",
      "Training Batch: 14317 Loss: 3053.626465\n",
      "Training Batch: 14318 Loss: 3159.275391\n",
      "Training Batch: 14319 Loss: 3033.100098\n",
      "Training Batch: 14320 Loss: 3091.475098\n",
      "Training Batch: 14321 Loss: 3030.302734\n",
      "Training Batch: 14322 Loss: 3112.466553\n",
      "Training Batch: 14323 Loss: 3178.303711\n",
      "Training Batch: 14324 Loss: 3092.307129\n",
      "Training Batch: 14325 Loss: 3208.267578\n",
      "Training Batch: 14326 Loss: 3108.630615\n",
      "Training Batch: 14327 Loss: 3063.680664\n",
      "Training Batch: 14328 Loss: 2967.026855\n",
      "Training Batch: 14329 Loss: 3102.427490\n",
      "Training Batch: 14330 Loss: 3130.380371\n",
      "Training Batch: 14331 Loss: 3206.364258\n",
      "Training Batch: 14332 Loss: 3315.884277\n",
      "Training Batch: 14333 Loss: 3071.096680\n",
      "Training Batch: 14334 Loss: 3040.142578\n",
      "Training Batch: 14335 Loss: 3313.442139\n",
      "Training Batch: 14336 Loss: 3122.354980\n",
      "Training Batch: 14337 Loss: 3011.035156\n",
      "Training Batch: 14338 Loss: 3017.733398\n",
      "Training Batch: 14339 Loss: 3323.603516\n",
      "Training Batch: 14340 Loss: 3043.580566\n",
      "Training Batch: 14341 Loss: 3180.641113\n",
      "Training Batch: 14342 Loss: 2951.775391\n",
      "Training Batch: 14343 Loss: 3078.323730\n",
      "Training Batch: 14344 Loss: 3148.976562\n",
      "Training Batch: 14345 Loss: 3247.793213\n",
      "Training Batch: 14346 Loss: 3020.027344\n",
      "Training Batch: 14347 Loss: 3103.252930\n",
      "Training Batch: 14348 Loss: 3071.492188\n",
      "Training Batch: 14349 Loss: 2955.396484\n",
      "Training Batch: 14350 Loss: 3103.261719\n",
      "Training Batch: 14351 Loss: 3050.803711\n",
      "Training Batch: 14352 Loss: 3111.008545\n",
      "Training Batch: 14353 Loss: 3244.092529\n",
      "Training Batch: 14354 Loss: 3052.618652\n",
      "Training Batch: 14355 Loss: 3093.199219\n",
      "Training Batch: 14356 Loss: 3161.062012\n",
      "Training Batch: 14357 Loss: 3292.898438\n",
      "Training Batch: 14358 Loss: 3132.878418\n",
      "Training Batch: 14359 Loss: 3005.255371\n",
      "Training Batch: 14360 Loss: 3012.567871\n",
      "Training Batch: 14361 Loss: 2987.746338\n",
      "Training Batch: 14362 Loss: 3024.456055\n",
      "Training Batch: 14363 Loss: 2988.839111\n",
      "Training Batch: 14364 Loss: 3000.885498\n",
      "Training Batch: 14365 Loss: 3022.993652\n",
      "Training Batch: 14366 Loss: 3089.418945\n",
      "Training Batch: 14367 Loss: 3080.656738\n",
      "Training Batch: 14368 Loss: 3072.865723\n",
      "Training Batch: 14369 Loss: 3216.431152\n",
      "Training Batch: 14370 Loss: 3003.954102\n",
      "Training Batch: 14371 Loss: 3095.951172\n",
      "Training Batch: 14372 Loss: 3003.079102\n",
      "Training Batch: 14373 Loss: 3211.370117\n",
      "Training Batch: 14374 Loss: 3177.097900\n",
      "Training Batch: 14375 Loss: 3088.929199\n",
      "Training Batch: 14376 Loss: 3185.632568\n",
      "Training Batch: 14377 Loss: 3047.303711\n",
      "Training Batch: 14378 Loss: 3185.254395\n",
      "Training Batch: 14379 Loss: 3181.377930\n",
      "Training Batch: 14380 Loss: 3161.891357\n",
      "Training Batch: 14381 Loss: 3043.464844\n",
      "Training Batch: 14382 Loss: 3528.048828\n",
      "Training Batch: 14383 Loss: 3127.251953\n",
      "Training Batch: 14384 Loss: 3123.637939\n",
      "Training Batch: 14385 Loss: 3129.841797\n",
      "Training Batch: 14386 Loss: 3131.936279\n",
      "Training Batch: 14387 Loss: 3088.281250\n",
      "Training Batch: 14388 Loss: 3407.066895\n",
      "Training Batch: 14389 Loss: 3112.593750\n",
      "Training Batch: 14390 Loss: 3237.109375\n",
      "Training Batch: 14391 Loss: 3285.954834\n",
      "Training Batch: 14392 Loss: 3090.572754\n",
      "Training Batch: 14393 Loss: 3089.641113\n",
      "Training Batch: 14394 Loss: 3024.278809\n",
      "Training Batch: 14395 Loss: 3116.840332\n",
      "Training Batch: 14396 Loss: 2989.909668\n",
      "Training Batch: 14397 Loss: 3099.254639\n",
      "Training Batch: 14398 Loss: 3188.267090\n",
      "Training Batch: 14399 Loss: 3042.520020\n",
      "Training Batch: 14400 Loss: 3050.907227\n",
      "Training Batch: 14401 Loss: 3190.456543\n",
      "Training Batch: 14402 Loss: 3159.550781\n",
      "Training Batch: 14403 Loss: 3145.510254\n",
      "Training Batch: 14404 Loss: 2991.020508\n",
      "Training Batch: 14405 Loss: 3005.100586\n",
      "Training Batch: 14406 Loss: 3167.683594\n",
      "Training Batch: 14407 Loss: 3012.716797\n",
      "Training Batch: 14408 Loss: 3308.980957\n",
      "Training Batch: 14409 Loss: 3121.704834\n",
      "Training Batch: 14410 Loss: 3148.607422\n",
      "Training Batch: 14411 Loss: 3228.523438\n",
      "Training Batch: 14412 Loss: 3022.132324\n",
      "Training Batch: 14413 Loss: 3011.945312\n",
      "Training Batch: 14414 Loss: 3086.521973\n",
      "Training Batch: 14415 Loss: 3142.920166\n",
      "Training Batch: 14416 Loss: 2984.470703\n",
      "Training Batch: 14417 Loss: 3050.228027\n",
      "Training Batch: 14418 Loss: 3290.449707\n",
      "Training Batch: 14419 Loss: 2965.179688\n",
      "Training Batch: 14420 Loss: 3158.207520\n",
      "Training Batch: 14421 Loss: 3084.005371\n",
      "Training Batch: 14422 Loss: 3056.950195\n",
      "Training Batch: 14423 Loss: 3087.579102\n",
      "Training Batch: 14424 Loss: 3111.966797\n",
      "Training Batch: 14425 Loss: 3022.505859\n",
      "Training Batch: 14426 Loss: 3089.307861\n",
      "Training Batch: 14427 Loss: 3137.447754\n",
      "Training Batch: 14428 Loss: 3106.325684\n",
      "Training Batch: 14429 Loss: 3095.298096\n",
      "Training Batch: 14430 Loss: 3207.378906\n",
      "Training Batch: 14431 Loss: 3169.852295\n",
      "Training Batch: 14432 Loss: 3001.448242\n",
      "Training Batch: 14433 Loss: 3015.202637\n",
      "Training Batch: 14434 Loss: 3248.620117\n",
      "Training Batch: 14435 Loss: 3381.256348\n",
      "Training Batch: 14436 Loss: 3041.617188\n",
      "Training Batch: 14437 Loss: 2988.910400\n",
      "Training Batch: 14438 Loss: 2930.985352\n",
      "Training Batch: 14439 Loss: 3191.841064\n",
      "Training Batch: 14440 Loss: 3126.802246\n",
      "Training Batch: 14441 Loss: 3303.051270\n",
      "Training Batch: 14442 Loss: 2972.299072\n",
      "Training Batch: 14443 Loss: 3137.804932\n",
      "Training Batch: 14444 Loss: 3232.208740\n",
      "Training Batch: 14445 Loss: 3171.766602\n",
      "Training Batch: 14446 Loss: 3197.978516\n",
      "Training Batch: 14447 Loss: 3017.226074\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Batch: 14448 Loss: 3196.524902\n",
      "Training Batch: 14449 Loss: 3148.622559\n",
      "Training Batch: 14450 Loss: 2984.755615\n",
      "Training Batch: 14451 Loss: 3129.286133\n",
      "Training Batch: 14452 Loss: 3306.231934\n",
      "Training Batch: 14453 Loss: 3054.970215\n",
      "Training Batch: 14454 Loss: 3225.619629\n",
      "Training Batch: 14455 Loss: 3058.751953\n",
      "Training Batch: 14456 Loss: 3074.090820\n",
      "Training Batch: 14457 Loss: 3124.153809\n",
      "Training Batch: 14458 Loss: 3052.125488\n",
      "Training Batch: 14459 Loss: 3078.021484\n",
      "Training Batch: 14460 Loss: 3115.148682\n",
      "Training Batch: 14461 Loss: 3063.098145\n",
      "Training Batch: 14462 Loss: 2959.734863\n",
      "Training Batch: 14463 Loss: 3028.255859\n",
      "Training Batch: 14464 Loss: 3128.274658\n",
      "Training Batch: 14465 Loss: 2995.687012\n",
      "Training Batch: 14466 Loss: 2932.615967\n",
      "Training Batch: 14467 Loss: 3176.556152\n",
      "Training Batch: 14468 Loss: 3234.091797\n",
      "Training Batch: 14469 Loss: 3142.561279\n",
      "Training Batch: 14470 Loss: 3023.488770\n",
      "Training Batch: 14471 Loss: 3108.949219\n",
      "Training Batch: 14472 Loss: 3007.577148\n",
      "Training Batch: 14473 Loss: 3058.206543\n",
      "Training Batch: 14474 Loss: 3054.261719\n",
      "Training Batch: 14475 Loss: 3077.056152\n",
      "Training Batch: 14476 Loss: 3098.295410\n",
      "Training Batch: 14477 Loss: 3388.317871\n",
      "Training Batch: 14478 Loss: 3131.140869\n",
      "Training Batch: 14479 Loss: 3210.578613\n",
      "Training Batch: 14480 Loss: 3209.195801\n",
      "Training Batch: 14481 Loss: 3300.864502\n",
      "Training Batch: 14482 Loss: 3037.007568\n",
      "Training Batch: 14483 Loss: 3074.669434\n",
      "Training Batch: 14484 Loss: 3184.317871\n",
      "Training Batch: 14485 Loss: 3276.855469\n",
      "Training Batch: 14486 Loss: 3164.952637\n",
      "Training Batch: 14487 Loss: 3102.308594\n",
      "Training Batch: 14488 Loss: 3218.845215\n",
      "Training Batch: 14489 Loss: 3126.864746\n",
      "Training Batch: 14490 Loss: 3072.499023\n",
      "Training Batch: 14491 Loss: 3120.448242\n",
      "Training Batch: 14492 Loss: 3261.710449\n",
      "Training Batch: 14493 Loss: 3059.494629\n",
      "Training Batch: 14494 Loss: 3114.666992\n",
      "Training Batch: 14495 Loss: 3244.774902\n",
      "Training Batch: 14496 Loss: 3393.942383\n",
      "Training Batch: 14497 Loss: 3113.060059\n",
      "Training Batch: 14498 Loss: 3281.951172\n",
      "Training Batch: 14499 Loss: 3247.812012\n",
      "Training Batch: 14500 Loss: 3077.227539\n",
      "Training Batch: 14501 Loss: 3134.339355\n",
      "Training Batch: 14502 Loss: 3192.718750\n",
      "Training Batch: 14503 Loss: 3056.556396\n",
      "Training Batch: 14504 Loss: 3012.502686\n",
      "Training Batch: 14505 Loss: 3085.283447\n",
      "Training Batch: 14506 Loss: 3031.438965\n",
      "Training Batch: 14507 Loss: 3078.772461\n",
      "Training Batch: 14508 Loss: 3051.820068\n",
      "Training Batch: 14509 Loss: 3100.295898\n",
      "Training Batch: 14510 Loss: 2966.929199\n",
      "Training Batch: 14511 Loss: 3083.655762\n",
      "Training Batch: 14512 Loss: 2993.035156\n",
      "Training Batch: 14513 Loss: 3056.864258\n",
      "Training Batch: 14514 Loss: 3305.242188\n",
      "Training Batch: 14515 Loss: 3588.199707\n",
      "Training Batch: 14516 Loss: 3060.829346\n",
      "Training Batch: 14517 Loss: 3062.743652\n",
      "Training Batch: 14518 Loss: 3072.279053\n",
      "Training Batch: 14519 Loss: 3136.460938\n",
      "Training Batch: 14520 Loss: 3107.002441\n",
      "Training Batch: 14521 Loss: 3220.103516\n",
      "Training Batch: 14522 Loss: 2996.601562\n",
      "Training Batch: 14523 Loss: 3145.662109\n",
      "Training Batch: 14524 Loss: 3086.524658\n",
      "Training Batch: 14525 Loss: 3046.245605\n",
      "Training Batch: 14526 Loss: 3200.389160\n",
      "Training Batch: 14527 Loss: 3100.169922\n",
      "Training Batch: 14528 Loss: 3153.102539\n",
      "Training Batch: 14529 Loss: 3017.725098\n",
      "Training Batch: 14530 Loss: 3069.936035\n",
      "Training Batch: 14531 Loss: 3238.351074\n",
      "Training Batch: 14532 Loss: 3112.788330\n",
      "Training Batch: 14533 Loss: 3171.249268\n",
      "Training Batch: 14534 Loss: 2987.256348\n",
      "Training Batch: 14535 Loss: 3269.177734\n",
      "Training Batch: 14536 Loss: 3002.639648\n",
      "Training Batch: 14537 Loss: 3044.477295\n",
      "Training Batch: 14538 Loss: 3262.044434\n",
      "Training Batch: 14539 Loss: 3024.743164\n",
      "Training Batch: 14540 Loss: 3106.532715\n",
      "Training Batch: 14541 Loss: 3101.055664\n",
      "Training Batch: 14542 Loss: 3021.652344\n",
      "Training Batch: 14543 Loss: 3016.745361\n",
      "Training Batch: 14544 Loss: 3322.985107\n",
      "Training Batch: 14545 Loss: 3044.338867\n",
      "Training Batch: 14546 Loss: 3444.033203\n",
      "Training Batch: 14547 Loss: 3115.015625\n",
      "Training Batch: 14548 Loss: 3074.412842\n",
      "Training Batch: 14549 Loss: 3311.958984\n",
      "Training Batch: 14550 Loss: 3176.667969\n",
      "Training Batch: 14551 Loss: 2975.812500\n",
      "Training Batch: 14552 Loss: 3142.998535\n",
      "Training Batch: 14553 Loss: 3134.759277\n",
      "Training Batch: 14554 Loss: 3090.501465\n",
      "Training Batch: 14555 Loss: 3145.861328\n",
      "Training Batch: 14556 Loss: 3091.877930\n",
      "Training Batch: 14557 Loss: 3000.297852\n",
      "Training Batch: 14558 Loss: 3945.681641\n",
      "Training Batch: 14559 Loss: 3109.208496\n",
      "Training Batch: 14560 Loss: 3152.567383\n",
      "Training Batch: 14561 Loss: 3063.545898\n",
      "Training Batch: 14562 Loss: 3024.836914\n",
      "Training Batch: 14563 Loss: 3237.980469\n",
      "Training Batch: 14564 Loss: 3116.156006\n",
      "Training Batch: 14565 Loss: 3009.063965\n",
      "Training Batch: 14566 Loss: 3238.808594\n",
      "Training Batch: 14567 Loss: 3456.305176\n",
      "Training Batch: 14568 Loss: 3352.422852\n",
      "Training Batch: 14569 Loss: 3300.819336\n",
      "Training Batch: 14570 Loss: 3392.184082\n",
      "Training Batch: 14571 Loss: 3181.382812\n",
      "Training Batch: 14572 Loss: 3372.035156\n",
      "Training Batch: 14573 Loss: 3198.338623\n",
      "Training Batch: 14574 Loss: 3212.136230\n",
      "Training Batch: 14575 Loss: 3333.556396\n",
      "Training Batch: 14576 Loss: 3074.539062\n",
      "Training Batch: 14577 Loss: 3316.834473\n",
      "Training Batch: 14578 Loss: 3077.302246\n",
      "Training Batch: 14579 Loss: 3014.861816\n",
      "Training Batch: 14580 Loss: 3073.979736\n",
      "Training Batch: 14581 Loss: 3022.827148\n",
      "Training Batch: 14582 Loss: 3128.724121\n",
      "Training Batch: 14583 Loss: 3101.013672\n",
      "Training Batch: 14584 Loss: 3382.883789\n",
      "Training Batch: 14585 Loss: 3033.086670\n",
      "Training Batch: 14586 Loss: 3217.064941\n",
      "Training Batch: 14587 Loss: 3185.420898\n",
      "Training Batch: 14588 Loss: 3257.964355\n",
      "Training Batch: 14589 Loss: 3130.215332\n",
      "Training Batch: 14590 Loss: 3164.100586\n",
      "Training Batch: 14591 Loss: 3114.208496\n",
      "Training Batch: 14592 Loss: 3262.672852\n",
      "Training Batch: 14593 Loss: 3081.623047\n",
      "Training Batch: 14594 Loss: 3103.007080\n",
      "Training Batch: 14595 Loss: 3192.869629\n",
      "Training Batch: 14596 Loss: 3126.447266\n",
      "Training Batch: 14597 Loss: 3090.510254\n",
      "Training Batch: 14598 Loss: 3139.884277\n",
      "Training Batch: 14599 Loss: 3207.318115\n",
      "Training Batch: 14600 Loss: 3099.612305\n",
      "Training Batch: 14601 Loss: 3272.288086\n",
      "Training Batch: 14602 Loss: 2990.427246\n",
      "Training Batch: 14603 Loss: 3138.508789\n",
      "Training Batch: 14604 Loss: 3169.566162\n",
      "Training Batch: 14605 Loss: 3249.928467\n",
      "Training Batch: 14606 Loss: 3024.205811\n",
      "Training Batch: 14607 Loss: 3058.892090\n",
      "Training Batch: 14608 Loss: 3108.117676\n",
      "Training Batch: 14609 Loss: 3087.217529\n",
      "Training Batch: 14610 Loss: 3178.078125\n",
      "Training Batch: 14611 Loss: 3104.194824\n",
      "Training Batch: 14612 Loss: 2971.466064\n",
      "Training Batch: 14613 Loss: 3083.054688\n",
      "Training Batch: 14614 Loss: 3163.647461\n",
      "Training Batch: 14615 Loss: 3080.634033\n",
      "Training Batch: 14616 Loss: 3064.248535\n",
      "Training Batch: 14617 Loss: 3020.148438\n",
      "Training Batch: 14618 Loss: 3027.245117\n",
      "Training Batch: 14619 Loss: 3041.297363\n",
      "Training Batch: 14620 Loss: 3102.653564\n",
      "Training Batch: 14621 Loss: 3099.930664\n",
      "Training Batch: 14622 Loss: 3211.481689\n",
      "Training Batch: 14623 Loss: 3139.486572\n",
      "Training Batch: 14624 Loss: 3277.748047\n",
      "Training Batch: 14625 Loss: 3126.944336\n",
      "Training Batch: 14626 Loss: 3106.361328\n",
      "Training Batch: 14627 Loss: 3102.686035\n",
      "Training Batch: 14628 Loss: 3410.711914\n",
      "Training Batch: 14629 Loss: 3422.752930\n",
      "Training Batch: 14630 Loss: 3238.779541\n",
      "Training Batch: 14631 Loss: 3165.403564\n",
      "Training Batch: 14632 Loss: 3071.769043\n",
      "Training Batch: 14633 Loss: 3196.927490\n",
      "Training Batch: 14634 Loss: 3246.615967\n",
      "Training Batch: 14635 Loss: 3178.500000\n",
      "Training Batch: 14636 Loss: 3202.212891\n",
      "Training Batch: 14637 Loss: 3126.938721\n",
      "Training Batch: 14638 Loss: 3150.678711\n",
      "Training Batch: 14639 Loss: 3189.608398\n",
      "Training Batch: 14640 Loss: 3138.392578\n",
      "Training Batch: 14641 Loss: 3152.210449\n",
      "Training Batch: 14642 Loss: 3105.994141\n",
      "Training Batch: 14643 Loss: 3090.959229\n",
      "Training Batch: 14644 Loss: 3066.371582\n",
      "Training Batch: 14645 Loss: 3169.666260\n",
      "Training Batch: 14646 Loss: 3141.032715\n",
      "Training Batch: 14647 Loss: 3140.191162\n",
      "Training Batch: 14648 Loss: 3069.840088\n",
      "Training Batch: 14649 Loss: 3236.670410\n",
      "Training Batch: 14650 Loss: 3266.191406\n",
      "Training Batch: 14651 Loss: 3122.626465\n",
      "Training Batch: 14652 Loss: 3374.699707\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Batch: 14653 Loss: 3285.700439\n",
      "Training Batch: 14654 Loss: 3421.208252\n",
      "Training Batch: 14655 Loss: 3217.792969\n",
      "Training Batch: 14656 Loss: 3149.547852\n",
      "Training Batch: 14657 Loss: 3108.750488\n",
      "Training Batch: 14658 Loss: 3040.108398\n",
      "Training Batch: 14659 Loss: 3103.418945\n",
      "Training Batch: 14660 Loss: 3081.720459\n",
      "Training Batch: 14661 Loss: 3213.240723\n",
      "Training Batch: 14662 Loss: 2962.814697\n",
      "Training Batch: 14663 Loss: 3114.358398\n",
      "Training Batch: 14664 Loss: 3319.917969\n",
      "Training Batch: 14665 Loss: 2978.599121\n",
      "Training Batch: 14666 Loss: 3137.996338\n",
      "Training Batch: 14667 Loss: 3172.004150\n",
      "Training Batch: 14668 Loss: 3028.561768\n",
      "Training Batch: 14669 Loss: 3113.488770\n",
      "Training Batch: 14670 Loss: 3126.423828\n",
      "Training Batch: 14671 Loss: 3078.385254\n",
      "Training Batch: 14672 Loss: 3115.664062\n",
      "Training Batch: 14673 Loss: 3101.441406\n",
      "Training Batch: 14674 Loss: 3180.414551\n",
      "Training Batch: 14675 Loss: 3168.603760\n",
      "Training Batch: 14676 Loss: 3153.490723\n",
      "Training Batch: 14677 Loss: 3014.192871\n",
      "Training Batch: 14678 Loss: 2990.037354\n",
      "Training Batch: 14679 Loss: 3119.332520\n",
      "Training Batch: 14680 Loss: 2985.123047\n",
      "Training Batch: 14681 Loss: 3123.439453\n",
      "Training Batch: 14682 Loss: 2984.026367\n",
      "Training Batch: 14683 Loss: 3152.565186\n",
      "Training Batch: 14684 Loss: 3069.449707\n",
      "Training Batch: 14685 Loss: 3154.656738\n",
      "Training Batch: 14686 Loss: 3684.736816\n",
      "Training Batch: 14687 Loss: 3231.228516\n",
      "Training Batch: 14688 Loss: 3186.500000\n",
      "Training Batch: 14689 Loss: 3147.819092\n",
      "Training Batch: 14690 Loss: 3045.808350\n",
      "Training Batch: 14691 Loss: 3080.975098\n",
      "Training Batch: 14692 Loss: 3162.060059\n",
      "Training Batch: 14693 Loss: 3024.461914\n",
      "Training Batch: 14694 Loss: 3152.900146\n",
      "Training Batch: 14695 Loss: 3222.947998\n",
      "Training Batch: 14696 Loss: 2966.714111\n",
      "Training Batch: 14697 Loss: 3163.412598\n",
      "Training Batch: 14698 Loss: 3135.054688\n",
      "Training Batch: 14699 Loss: 3129.786133\n",
      "Training Batch: 14700 Loss: 3115.465820\n",
      "Training Batch: 14701 Loss: 3052.197266\n",
      "Training Batch: 14702 Loss: 3061.692383\n",
      "Training Batch: 14703 Loss: 2945.999023\n",
      "Training Batch: 14704 Loss: 3082.163086\n",
      "Training Batch: 14705 Loss: 3073.963623\n",
      "Training Batch: 14706 Loss: 3218.055664\n",
      "Training Batch: 14707 Loss: 3061.488770\n",
      "Training Batch: 14708 Loss: 3142.242676\n",
      "Training Batch: 14709 Loss: 3169.084961\n",
      "Training Batch: 14710 Loss: 3106.167969\n",
      "Training Batch: 14711 Loss: 3019.986816\n",
      "Training Batch: 14712 Loss: 3124.135742\n",
      "Training Batch: 14713 Loss: 3172.041016\n",
      "Training Batch: 14714 Loss: 3174.791992\n",
      "Training Batch: 14715 Loss: 3191.012695\n",
      "Training Batch: 14716 Loss: 3023.151855\n",
      "Training Batch: 14717 Loss: 3076.069336\n",
      "Training Batch: 14718 Loss: 3102.308105\n",
      "Training Batch: 14719 Loss: 3327.047363\n",
      "Training Batch: 14720 Loss: 3530.618164\n",
      "Training Batch: 14721 Loss: 3549.307617\n",
      "Training Batch: 14722 Loss: 3124.240234\n",
      "Training Batch: 14723 Loss: 3045.930176\n",
      "Training Batch: 14724 Loss: 3198.552490\n",
      "Training Batch: 14725 Loss: 3026.604980\n",
      "Training Batch: 14726 Loss: 3057.043457\n",
      "Training Batch: 14727 Loss: 3344.017822\n",
      "Training Batch: 14728 Loss: 3023.679199\n",
      "Training Batch: 14729 Loss: 3062.850098\n",
      "Training Batch: 14730 Loss: 3120.829590\n",
      "Training Batch: 14731 Loss: 3312.337891\n",
      "Training Batch: 14732 Loss: 3188.356934\n",
      "Training Batch: 14733 Loss: 3177.045898\n",
      "Training Batch: 14734 Loss: 3358.860840\n",
      "Training Batch: 14735 Loss: 3328.889160\n",
      "Training Batch: 14736 Loss: 3381.506592\n",
      "Training Batch: 14737 Loss: 3109.584961\n",
      "Training Batch: 14738 Loss: 3101.375244\n",
      "Training Batch: 14739 Loss: 3132.408447\n",
      "Training Batch: 14740 Loss: 3198.642578\n",
      "Training Batch: 14741 Loss: 3017.182861\n",
      "Training Batch: 14742 Loss: 2999.150879\n",
      "Training Batch: 14743 Loss: 3179.522461\n",
      "Training Batch: 14744 Loss: 3262.145020\n",
      "Training Batch: 14745 Loss: 3270.397461\n",
      "Training Batch: 14746 Loss: 3217.562500\n",
      "Training Batch: 14747 Loss: 3013.330811\n",
      "Training Batch: 14748 Loss: 3316.945801\n",
      "Training Batch: 14749 Loss: 3129.728027\n",
      "Training Batch: 14750 Loss: 3317.020508\n",
      "Training Batch: 14751 Loss: 3366.106445\n",
      "Training Batch: 14752 Loss: 3238.996094\n",
      "Training Batch: 14753 Loss: 3288.706543\n",
      "Training Batch: 14754 Loss: 3181.874512\n",
      "Training Batch: 14755 Loss: 3177.583008\n",
      "Training Batch: 14756 Loss: 3113.974121\n",
      "Training Batch: 14757 Loss: 3237.220459\n",
      "Training Batch: 14758 Loss: 3351.394531\n",
      "Training Batch: 14759 Loss: 3029.723145\n",
      "Training Batch: 14760 Loss: 3067.693359\n",
      "Training Batch: 14761 Loss: 3036.468994\n",
      "Training Batch: 14762 Loss: 3035.609863\n",
      "Training Batch: 14763 Loss: 3162.887451\n",
      "Training Batch: 14764 Loss: 3043.119141\n",
      "Training Batch: 14765 Loss: 3034.812500\n",
      "Training Batch: 14766 Loss: 3053.157715\n",
      "Training Batch: 14767 Loss: 3271.986816\n",
      "Training Batch: 14768 Loss: 3317.005371\n",
      "Training Batch: 14769 Loss: 3127.885254\n",
      "Training Batch: 14770 Loss: 3128.199707\n",
      "Training Batch: 14771 Loss: 3138.419922\n",
      "Training Batch: 14772 Loss: 3137.487793\n",
      "Training Batch: 14773 Loss: 3134.586914\n",
      "Training Batch: 14774 Loss: 3054.500488\n",
      "Training Batch: 14775 Loss: 3056.853516\n",
      "Training Batch: 14776 Loss: 3097.691406\n",
      "Training Batch: 14777 Loss: 3246.715820\n",
      "Training Batch: 14778 Loss: 3081.156250\n",
      "Training Batch: 14779 Loss: 3101.322754\n",
      "Training Batch: 14780 Loss: 2996.679688\n",
      "Training Batch: 14781 Loss: 3227.942383\n",
      "Training Batch: 14782 Loss: 2981.032715\n",
      "Training Batch: 14783 Loss: 3143.838867\n",
      "Training Batch: 14784 Loss: 3269.743896\n",
      "Training Batch: 14785 Loss: 3073.368164\n",
      "Training Batch: 14786 Loss: 3198.175537\n",
      "Training Batch: 14787 Loss: 3064.840088\n",
      "Training Batch: 14788 Loss: 3094.716553\n",
      "Training Batch: 14789 Loss: 2987.225098\n",
      "Training Batch: 14790 Loss: 3087.253906\n",
      "Training Batch: 14791 Loss: 3159.515137\n",
      "Training Batch: 14792 Loss: 3226.218262\n",
      "Training Batch: 14793 Loss: 3104.622070\n",
      "Training Batch: 14794 Loss: 3067.357666\n",
      "Training Batch: 14795 Loss: 3030.741211\n",
      "Training Batch: 14796 Loss: 2994.695801\n",
      "Training Batch: 14797 Loss: 3066.195557\n",
      "Training Batch: 14798 Loss: 3153.499023\n",
      "Training Batch: 14799 Loss: 3087.230957\n",
      "Training Batch: 14800 Loss: 3121.075684\n",
      "Training Batch: 14801 Loss: 3208.869629\n",
      "Training Batch: 14802 Loss: 3151.568115\n",
      "Training Batch: 14803 Loss: 3145.299805\n",
      "Training Batch: 14804 Loss: 3141.446777\n",
      "Training Batch: 14805 Loss: 3217.372070\n",
      "Training Batch: 14806 Loss: 2998.272461\n",
      "Training Batch: 14807 Loss: 2968.827148\n",
      "Training Batch: 14808 Loss: 2997.784180\n",
      "Training Batch: 14809 Loss: 3057.491211\n",
      "Training Batch: 14810 Loss: 3119.038818\n",
      "Training Batch: 14811 Loss: 3062.198975\n",
      "Training Batch: 14812 Loss: 3126.379883\n",
      "Training Batch: 14813 Loss: 3163.788330\n",
      "Training Batch: 14814 Loss: 3245.044922\n",
      "Training Batch: 14815 Loss: 2994.796875\n",
      "Training Batch: 14816 Loss: 3039.062012\n",
      "Training Batch: 14817 Loss: 3036.191406\n",
      "Training Batch: 14818 Loss: 3044.052734\n",
      "Training Batch: 14819 Loss: 3059.922852\n",
      "Training Batch: 14820 Loss: 3453.422852\n",
      "Training Batch: 14821 Loss: 3132.455811\n",
      "Training Batch: 14822 Loss: 3072.484131\n",
      "Training Batch: 14823 Loss: 3177.934082\n",
      "Training Batch: 14824 Loss: 3019.702637\n",
      "Training Batch: 14825 Loss: 3159.308105\n",
      "Training Batch: 14826 Loss: 3325.812988\n",
      "Training Batch: 14827 Loss: 3097.203613\n",
      "Training Batch: 14828 Loss: 3001.476074\n",
      "Training Batch: 14829 Loss: 3049.987061\n",
      "Training Batch: 14830 Loss: 3114.415527\n",
      "Training Batch: 14831 Loss: 3075.323730\n",
      "Training Batch: 14832 Loss: 3089.383301\n",
      "Training Batch: 14833 Loss: 3163.620117\n",
      "Training Batch: 14834 Loss: 3047.661621\n",
      "Training Batch: 14835 Loss: 3044.795898\n",
      "Training Batch: 14836 Loss: 3253.178223\n",
      "Training Batch: 14837 Loss: 3081.026123\n",
      "Training Batch: 14838 Loss: 3450.604004\n",
      "Training Batch: 14839 Loss: 3124.873779\n",
      "Training Batch: 14840 Loss: 3237.490479\n",
      "Training Batch: 14841 Loss: 3246.667480\n",
      "Training Batch: 14842 Loss: 3082.000977\n",
      "Training Batch: 14843 Loss: 3493.498047\n",
      "Training Batch: 14844 Loss: 3085.391602\n",
      "Training Batch: 14845 Loss: 3015.281006\n",
      "Training Batch: 14846 Loss: 3067.334229\n",
      "Training Batch: 14847 Loss: 2971.220459\n",
      "Training Batch: 14848 Loss: 3171.784180\n",
      "Training Batch: 14849 Loss: 3057.493164\n",
      "Training Batch: 14850 Loss: 3170.557617\n",
      "Training Batch: 14851 Loss: 3114.342773\n",
      "Training Batch: 14852 Loss: 3057.874023\n",
      "Training Batch: 14853 Loss: 3226.960938\n",
      "Training Batch: 14854 Loss: 3100.041504\n",
      "Training Batch: 14855 Loss: 3135.054199\n",
      "Training Batch: 14856 Loss: 3054.129395\n",
      "Training Batch: 14857 Loss: 3146.719727\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Batch: 14858 Loss: 3128.923584\n",
      "Training Batch: 14859 Loss: 3073.168945\n",
      "Training Batch: 14860 Loss: 3316.876465\n",
      "Training Batch: 14861 Loss: 3049.900391\n",
      "Training Batch: 14862 Loss: 3099.009766\n",
      "Training Batch: 14863 Loss: 3076.625000\n",
      "Training Batch: 14864 Loss: 3096.183838\n",
      "Training Batch: 14865 Loss: 3392.812012\n",
      "Training Batch: 14866 Loss: 3092.371582\n",
      "Training Batch: 14867 Loss: 3145.446533\n",
      "Training Batch: 14868 Loss: 3184.437988\n",
      "Training Batch: 14869 Loss: 3400.014648\n",
      "Training Batch: 14870 Loss: 3250.122314\n",
      "Training Batch: 14871 Loss: 3008.888428\n",
      "Training Batch: 14872 Loss: 3000.007568\n",
      "Training Batch: 14873 Loss: 3033.909668\n",
      "Training Batch: 14874 Loss: 3582.626953\n",
      "Training Batch: 14875 Loss: 3169.524414\n",
      "Training Batch: 14876 Loss: 3107.345703\n",
      "Training Batch: 14877 Loss: 3301.786377\n",
      "Training Batch: 14878 Loss: 3082.898682\n",
      "Training Batch: 14879 Loss: 3058.768555\n",
      "Training Batch: 14880 Loss: 3000.966797\n",
      "Training Batch: 14881 Loss: 3248.549316\n",
      "Training Batch: 14882 Loss: 3028.605469\n",
      "Training Batch: 14883 Loss: 3004.820312\n",
      "Training Batch: 14884 Loss: 3159.960449\n",
      "Training Batch: 14885 Loss: 3056.667480\n",
      "Training Batch: 14886 Loss: 3217.904297\n",
      "Training Batch: 14887 Loss: 3104.537598\n",
      "Training Batch: 14888 Loss: 3231.489502\n",
      "Training Batch: 14889 Loss: 3124.165039\n",
      "Training Batch: 14890 Loss: 3188.656006\n",
      "Training Batch: 14891 Loss: 3123.914551\n",
      "Training Batch: 14892 Loss: 3157.337891\n",
      "Training Batch: 14893 Loss: 2989.584961\n",
      "Training Batch: 14894 Loss: 3313.704590\n",
      "Training Batch: 14895 Loss: 3106.717773\n",
      "Training Batch: 14896 Loss: 3029.122559\n",
      "Training Batch: 14897 Loss: 3069.691406\n",
      "Training Batch: 14898 Loss: 3240.849121\n",
      "Training Batch: 14899 Loss: 3157.657715\n",
      "Training Batch: 14900 Loss: 3185.054199\n",
      "Training Batch: 14901 Loss: 3040.881836\n",
      "Training Batch: 14902 Loss: 3224.600586\n",
      "Training Batch: 14903 Loss: 3126.273926\n",
      "Training Batch: 14904 Loss: 3001.940430\n",
      "Training Batch: 14905 Loss: 3176.303223\n",
      "Training Batch: 14906 Loss: 3141.338379\n",
      "Training Batch: 14907 Loss: 3206.016846\n",
      "Training Batch: 14908 Loss: 3217.444824\n",
      "Training Batch: 14909 Loss: 3077.393066\n",
      "Training Batch: 14910 Loss: 2990.856445\n",
      "Training Batch: 14911 Loss: 3322.225098\n",
      "Training Batch: 14912 Loss: 3122.776855\n",
      "Training Batch: 14913 Loss: 3029.144531\n",
      "Training Batch: 14914 Loss: 3083.544434\n",
      "Training Batch: 14915 Loss: 3051.027832\n",
      "Training Batch: 14916 Loss: 3118.379639\n",
      "Training Batch: 14917 Loss: 3061.225342\n",
      "Training Batch: 14918 Loss: 3232.907471\n",
      "Training Batch: 14919 Loss: 3099.362061\n",
      "Training Batch: 14920 Loss: 3096.356934\n",
      "Training Batch: 14921 Loss: 2968.055908\n",
      "Training Batch: 14922 Loss: 3199.150391\n",
      "Training Batch: 14923 Loss: 3089.111816\n",
      "Training Batch: 14924 Loss: 2961.296387\n",
      "Training Batch: 14925 Loss: 3194.039795\n",
      "Training Batch: 14926 Loss: 3054.232910\n",
      "Training Batch: 14927 Loss: 2958.650391\n",
      "Training Batch: 14928 Loss: 3338.392578\n",
      "Training Batch: 14929 Loss: 3378.560791\n",
      "Training Batch: 14930 Loss: 3092.160645\n",
      "Training Batch: 14931 Loss: 3071.242188\n",
      "Training Batch: 14932 Loss: 3014.970215\n",
      "Training Batch: 14933 Loss: 3121.309082\n",
      "Training Batch: 14934 Loss: 3136.606934\n",
      "Training Batch: 14935 Loss: 3019.103027\n",
      "Training Batch: 14936 Loss: 3043.317871\n",
      "Training Batch: 14937 Loss: 3158.004883\n",
      "Training Batch: 14938 Loss: 3123.438965\n",
      "Training Batch: 14939 Loss: 3065.938965\n",
      "Training Batch: 14940 Loss: 3030.522217\n",
      "Training Batch: 14941 Loss: 3266.892578\n",
      "Training Batch: 14942 Loss: 3089.830566\n",
      "Training Batch: 14943 Loss: 3037.651855\n",
      "Training Batch: 14944 Loss: 3188.033936\n",
      "Training Batch: 14945 Loss: 3000.734863\n",
      "Training Batch: 14946 Loss: 2996.897461\n",
      "Training Batch: 14947 Loss: 3290.609131\n",
      "Training Batch: 14948 Loss: 3069.432129\n",
      "Training Batch: 14949 Loss: 3180.174561\n",
      "Training Batch: 14950 Loss: 3104.878418\n",
      "Training Batch: 14951 Loss: 3085.014893\n",
      "Training Batch: 14952 Loss: 3281.639893\n",
      "Training Batch: 14953 Loss: 3055.979004\n",
      "Training Batch: 14954 Loss: 3088.797852\n",
      "Training Batch: 14955 Loss: 3243.830566\n",
      "Training Batch: 14956 Loss: 3089.686279\n",
      "Training Batch: 14957 Loss: 3206.869385\n",
      "Training Batch: 14958 Loss: 3087.379150\n",
      "Training Batch: 14959 Loss: 3168.154785\n",
      "Training Batch: 14960 Loss: 3485.901123\n",
      "Training Batch: 14961 Loss: 3823.151855\n",
      "Training Batch: 14962 Loss: 3463.510254\n",
      "Training Batch: 14963 Loss: 3202.931641\n",
      "Training Batch: 14964 Loss: 3216.429199\n",
      "Training Batch: 14965 Loss: 3109.581787\n",
      "Training Batch: 14966 Loss: 3019.032959\n",
      "Training Batch: 14967 Loss: 3113.361572\n",
      "Training Batch: 14968 Loss: 3153.183838\n",
      "Training Batch: 14969 Loss: 3157.744141\n",
      "Training Batch: 14970 Loss: 2956.505615\n",
      "Training Batch: 14971 Loss: 3009.172607\n",
      "Training Batch: 14972 Loss: 3068.975098\n",
      "Training Batch: 14973 Loss: 3108.335449\n",
      "Training Batch: 14974 Loss: 3191.668213\n",
      "Training Batch: 14975 Loss: 3078.082520\n",
      "Training Batch: 14976 Loss: 3107.840820\n",
      "Training Batch: 14977 Loss: 3158.768555\n",
      "Training Batch: 14978 Loss: 3111.684326\n",
      "Training Batch: 14979 Loss: 2969.340820\n",
      "Training Batch: 14980 Loss: 2986.467285\n",
      "Training Batch: 14981 Loss: 3047.269531\n",
      "Training Batch: 14982 Loss: 3121.533203\n",
      "Training Batch: 14983 Loss: 3116.526855\n",
      "Training Batch: 14984 Loss: 3157.568359\n",
      "Training Batch: 14985 Loss: 3420.265625\n",
      "Training Batch: 14986 Loss: 3074.412598\n",
      "Training Batch: 14987 Loss: 3032.060547\n",
      "Training Batch: 14988 Loss: 3110.246582\n",
      "Training Batch: 14989 Loss: 3722.644775\n",
      "Training Batch: 14990 Loss: 3030.585449\n",
      "Training Batch: 14991 Loss: 3083.406250\n",
      "Training Batch: 14992 Loss: 3068.671875\n",
      "Training Batch: 14993 Loss: 3137.845947\n",
      "Training Batch: 14994 Loss: 3168.174805\n",
      "Training Batch: 14995 Loss: 3137.590820\n",
      "Training Batch: 14996 Loss: 3172.589844\n",
      "Training Batch: 14997 Loss: 2984.588867\n",
      "Training Batch: 14998 Loss: 3023.191406\n",
      "Training Batch: 14999 Loss: 3057.139648\n",
      "Training Batch: 15000 Loss: 3135.739014\n",
      "Training Batch: 15001 Loss: 3267.197266\n",
      "Training Batch: 15002 Loss: 3258.797363\n",
      "Training Batch: 15003 Loss: 3012.820801\n",
      "Training Batch: 15004 Loss: 3171.162598\n",
      "Training Batch: 15005 Loss: 3086.651855\n",
      "Training Batch: 15006 Loss: 3226.408936\n",
      "Training Batch: 15007 Loss: 3074.696777\n",
      "Training Batch: 15008 Loss: 3150.900879\n",
      "Training Batch: 15009 Loss: 3095.114502\n",
      "Training Batch: 15010 Loss: 3165.659180\n",
      "Training Batch: 15011 Loss: 3088.387207\n",
      "Training Batch: 15012 Loss: 3262.044189\n",
      "Training Batch: 15013 Loss: 3242.974121\n",
      "Training Batch: 15014 Loss: 3153.745117\n",
      "Training Batch: 15015 Loss: 3178.388184\n",
      "Training Batch: 15016 Loss: 3187.317871\n",
      "Training Batch: 15017 Loss: 2994.054688\n",
      "Training Batch: 15018 Loss: 3196.710449\n",
      "Training Batch: 15019 Loss: 3198.674805\n",
      "Training Batch: 15020 Loss: 3162.471680\n",
      "Training Batch: 15021 Loss: 3049.514648\n",
      "Training Batch: 15022 Loss: 3191.023438\n",
      "Training Batch: 15023 Loss: 3132.157227\n",
      "Training Batch: 15024 Loss: 3021.691895\n",
      "Training Batch: 15025 Loss: 3059.093262\n",
      "Training Batch: 15026 Loss: 3039.364746\n",
      "Training Batch: 15027 Loss: 3027.581787\n",
      "Training Batch: 15028 Loss: 3145.468750\n",
      "Training Batch: 15029 Loss: 3437.429688\n",
      "Training Batch: 15030 Loss: 3068.785645\n",
      "Training Batch: 15031 Loss: 3106.935303\n",
      "Training Batch: 15032 Loss: 3055.869385\n",
      "Training Batch: 15033 Loss: 3247.072266\n",
      "Training Batch: 15034 Loss: 2992.421875\n",
      "Training Batch: 15035 Loss: 3113.795410\n",
      "Training Batch: 15036 Loss: 3149.233154\n",
      "Training Batch: 15037 Loss: 3227.780273\n",
      "Training Batch: 15038 Loss: 3073.532227\n",
      "Training Batch: 15039 Loss: 3036.569336\n",
      "Training Batch: 15040 Loss: 3120.447998\n",
      "Training Batch: 15041 Loss: 3049.677734\n",
      "Training Batch: 15042 Loss: 3107.139648\n",
      "Training Batch: 15043 Loss: 3071.566162\n",
      "Training Batch: 15044 Loss: 3065.827393\n",
      "Training Batch: 15045 Loss: 3154.805176\n",
      "Training Batch: 15046 Loss: 3069.927246\n",
      "Training Batch: 15047 Loss: 3393.733887\n",
      "Training Batch: 15048 Loss: 3500.196777\n",
      "Training Batch: 15049 Loss: 3342.100342\n",
      "Training Batch: 15050 Loss: 3167.835449\n",
      "Training Batch: 15051 Loss: 3261.468262\n",
      "Training Batch: 15052 Loss: 2980.570312\n",
      "Training Batch: 15053 Loss: 2988.814453\n",
      "Training Batch: 15054 Loss: 3090.712402\n",
      "Training Batch: 15055 Loss: 3157.758789\n",
      "Training Batch: 15056 Loss: 3165.448730\n",
      "Training Batch: 15057 Loss: 3106.840088\n",
      "Training Batch: 15058 Loss: 3060.970703\n",
      "Training Batch: 15059 Loss: 3041.466797\n",
      "Training Batch: 15060 Loss: 3095.867188\n",
      "Training Batch: 15061 Loss: 3121.367676\n",
      "Training Batch: 15062 Loss: 3293.946777\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Batch: 15063 Loss: 3104.358887\n",
      "Training Batch: 15064 Loss: 3028.250977\n",
      "Training Batch: 15065 Loss: 3006.931396\n",
      "Training Batch: 15066 Loss: 3099.511475\n",
      "Training Batch: 15067 Loss: 3205.789795\n",
      "Training Batch: 15068 Loss: 3462.785156\n",
      "Training Batch: 15069 Loss: 3092.447510\n",
      "Training Batch: 15070 Loss: 3235.543457\n",
      "Training Batch: 15071 Loss: 2965.911133\n",
      "Training Batch: 15072 Loss: 3318.127930\n",
      "Training Batch: 15073 Loss: 3195.103027\n",
      "Training Batch: 15074 Loss: 3073.426758\n",
      "Training Batch: 15075 Loss: 3075.861328\n",
      "Training Batch: 15076 Loss: 3116.752197\n",
      "Training Batch: 15077 Loss: 2963.698730\n",
      "Training Batch: 15078 Loss: 3084.058594\n",
      "Training Batch: 15079 Loss: 3035.182129\n",
      "Training Batch: 15080 Loss: 3136.125000\n",
      "Training Batch: 15081 Loss: 3338.438477\n",
      "Training Batch: 15082 Loss: 3238.213379\n",
      "Training Batch: 15083 Loss: 3200.044922\n",
      "Training Batch: 15084 Loss: 3114.463867\n",
      "Training Batch: 15085 Loss: 3151.895508\n",
      "Training Batch: 15086 Loss: 3043.861816\n",
      "Training Batch: 15087 Loss: 3066.453613\n",
      "Training Batch: 15088 Loss: 3475.860352\n",
      "Training Batch: 15089 Loss: 3154.394531\n",
      "Training Batch: 15090 Loss: 3080.060547\n",
      "Training Batch: 15091 Loss: 3018.900391\n",
      "Training Batch: 15092 Loss: 3048.810547\n",
      "Training Batch: 15093 Loss: 2995.599609\n",
      "Training Batch: 15094 Loss: 2993.531982\n",
      "Training Batch: 15095 Loss: 2952.670410\n",
      "Training Batch: 15096 Loss: 3111.454346\n",
      "Training Batch: 15097 Loss: 3009.092285\n",
      "Training Batch: 15098 Loss: 3152.197266\n",
      "Training Batch: 15099 Loss: 3043.007324\n",
      "Training Batch: 15100 Loss: 3165.832031\n",
      "Training Batch: 15101 Loss: 3269.305664\n",
      "Training Batch: 15102 Loss: 3133.713379\n",
      "Training Batch: 15103 Loss: 3229.602051\n",
      "Training Batch: 15104 Loss: 3082.118652\n",
      "Training Batch: 15105 Loss: 3145.884033\n",
      "Training Batch: 15106 Loss: 3142.765381\n",
      "Training Batch: 15107 Loss: 3081.075439\n",
      "Training Batch: 15108 Loss: 3056.912109\n",
      "Training Batch: 15109 Loss: 3097.742676\n",
      "Training Batch: 15110 Loss: 3217.121094\n",
      "Training Batch: 15111 Loss: 3015.136230\n",
      "Training Batch: 15112 Loss: 3131.761230\n",
      "Training Batch: 15113 Loss: 3112.484375\n",
      "Training Batch: 15114 Loss: 3123.900391\n",
      "Training Batch: 15115 Loss: 2978.891113\n",
      "Training Batch: 15116 Loss: 2989.378906\n",
      "Training Batch: 15117 Loss: 3016.375732\n",
      "Training Batch: 15118 Loss: 3120.169434\n",
      "Training Batch: 15119 Loss: 3041.766113\n",
      "Training Batch: 15120 Loss: 3091.232910\n",
      "Training Batch: 15121 Loss: 3116.258301\n",
      "Training Batch: 15122 Loss: 3161.947754\n",
      "Training Batch: 15123 Loss: 2993.358887\n",
      "Training Batch: 15124 Loss: 3022.150879\n",
      "Training Batch: 15125 Loss: 3250.403809\n",
      "Training Batch: 15126 Loss: 3229.451172\n",
      "Training Batch: 15127 Loss: 2999.962402\n",
      "Training Batch: 15128 Loss: 3066.270020\n",
      "Training Batch: 15129 Loss: 2972.611328\n",
      "Training Batch: 15130 Loss: 2965.181152\n",
      "Training Batch: 15131 Loss: 3023.692871\n",
      "Training Batch: 15132 Loss: 3352.692627\n",
      "Training Batch: 15133 Loss: 3115.819336\n",
      "Training Batch: 15134 Loss: 3054.006836\n",
      "Training Batch: 15135 Loss: 2964.491455\n",
      "Training Batch: 15136 Loss: 3004.874023\n",
      "Training Batch: 15137 Loss: 3269.112793\n",
      "Training Batch: 15138 Loss: 3105.328125\n",
      "Training Batch: 15139 Loss: 3207.760254\n",
      "Training Batch: 15140 Loss: 3068.218750\n",
      "Training Batch: 15141 Loss: 3240.880371\n",
      "Training Batch: 15142 Loss: 3072.824219\n",
      "Training Batch: 15143 Loss: 3381.410889\n",
      "Training Batch: 15144 Loss: 3050.794922\n",
      "Training Batch: 15145 Loss: 3161.256348\n",
      "Training Batch: 15146 Loss: 3051.332764\n",
      "Training Batch: 15147 Loss: 3049.941895\n",
      "Training Batch: 15148 Loss: 3138.963867\n",
      "Training Batch: 15149 Loss: 3050.960449\n",
      "Training Batch: 15150 Loss: 3224.593262\n",
      "Training Batch: 15151 Loss: 3075.862305\n",
      "Training Batch: 15152 Loss: 3124.496338\n",
      "Training Batch: 15153 Loss: 3070.024414\n",
      "Training Batch: 15154 Loss: 3036.060791\n",
      "Training Batch: 15155 Loss: 3169.783691\n",
      "Training Batch: 15156 Loss: 3063.867432\n",
      "Training Batch: 15157 Loss: 3052.550293\n",
      "Training Batch: 15158 Loss: 3073.848145\n",
      "Training Batch: 15159 Loss: 2998.211426\n",
      "Training Batch: 15160 Loss: 3346.802490\n",
      "Training Batch: 15161 Loss: 3263.613770\n",
      "Training Batch: 15162 Loss: 3262.793945\n",
      "Training Batch: 15163 Loss: 3117.042236\n",
      "Training Batch: 15164 Loss: 3426.589600\n",
      "Training Batch: 15165 Loss: 3027.709229\n",
      "Training Batch: 15166 Loss: 3036.353760\n",
      "Training Batch: 15167 Loss: 3111.718750\n",
      "Training Batch: 15168 Loss: 3166.275146\n",
      "Training Batch: 15169 Loss: 3052.272461\n",
      "Training Batch: 15170 Loss: 3127.184082\n",
      "Training Batch: 15171 Loss: 2984.930664\n",
      "Training Batch: 15172 Loss: 2987.813965\n",
      "Training Batch: 15173 Loss: 2981.614746\n",
      "Training Batch: 15174 Loss: 3194.111816\n",
      "Training Batch: 15175 Loss: 3059.662598\n",
      "Training Batch: 15176 Loss: 3096.000488\n",
      "Training Batch: 15177 Loss: 3289.773926\n",
      "Training Batch: 15178 Loss: 3104.879883\n",
      "Training Batch: 15179 Loss: 3073.743652\n",
      "Training Batch: 15180 Loss: 2989.833740\n",
      "Training Batch: 15181 Loss: 3161.920654\n",
      "Training Batch: 15182 Loss: 3082.049805\n",
      "Training Batch: 15183 Loss: 3263.664551\n",
      "Training Batch: 15184 Loss: 3170.001221\n",
      "Training Batch: 15185 Loss: 3153.794922\n",
      "Training Batch: 15186 Loss: 3188.091064\n",
      "Training Batch: 15187 Loss: 3195.925781\n",
      "Training Batch: 15188 Loss: 3233.345215\n",
      "Training Batch: 15189 Loss: 3121.095215\n",
      "Training Batch: 15190 Loss: 3144.611816\n",
      "Training Batch: 15191 Loss: 3075.293457\n",
      "Training Batch: 15192 Loss: 3126.191406\n",
      "Training Batch: 15193 Loss: 3156.180176\n",
      "Training Batch: 15194 Loss: 3086.798340\n",
      "Training Batch: 15195 Loss: 2977.053711\n",
      "Training Batch: 15196 Loss: 3073.902344\n",
      "Training Batch: 15197 Loss: 3129.502441\n",
      "Training Batch: 15198 Loss: 3303.701904\n",
      "Training Batch: 15199 Loss: 3061.444824\n",
      "Training Batch: 15200 Loss: 3124.848633\n",
      "Training Batch: 15201 Loss: 3159.683105\n",
      "Training Batch: 15202 Loss: 3177.261230\n",
      "Training Batch: 15203 Loss: 3048.237305\n",
      "Training Batch: 15204 Loss: 3185.846436\n",
      "Training Batch: 15205 Loss: 3197.756104\n",
      "Training Batch: 15206 Loss: 3293.187012\n",
      "Training Batch: 15207 Loss: 3369.190430\n",
      "Training Batch: 15208 Loss: 3254.384766\n",
      "Training Batch: 15209 Loss: 3091.821777\n",
      "Training Batch: 15210 Loss: 3036.075684\n",
      "Training Batch: 15211 Loss: 3096.655762\n",
      "Training Batch: 15212 Loss: 3176.883545\n",
      "Training Batch: 15213 Loss: 3418.448975\n",
      "Training Batch: 15214 Loss: 3037.685059\n",
      "Training Batch: 15215 Loss: 3313.216553\n",
      "Training Batch: 15216 Loss: 3199.428711\n",
      "Training Batch: 15217 Loss: 3013.765137\n",
      "Training Batch: 15218 Loss: 3176.348145\n",
      "Training Batch: 15219 Loss: 3289.977539\n",
      "Training Batch: 15220 Loss: 3127.029297\n",
      "Training Batch: 15221 Loss: 3274.894043\n",
      "Training Batch: 15222 Loss: 3244.023438\n",
      "Training Batch: 15223 Loss: 3251.485596\n",
      "Training Batch: 15224 Loss: 3300.374512\n",
      "Training Batch: 15225 Loss: 3080.950195\n",
      "Training Batch: 15226 Loss: 3323.367676\n",
      "Training Batch: 15227 Loss: 2960.683105\n",
      "Training Batch: 15228 Loss: 2993.896973\n",
      "Training Batch: 15229 Loss: 3058.439453\n",
      "Training Batch: 15230 Loss: 3366.940430\n",
      "Training Batch: 15231 Loss: 3122.600098\n",
      "Training Batch: 15232 Loss: 3064.557617\n",
      "Training Batch: 15233 Loss: 3132.630859\n",
      "Training Batch: 15234 Loss: 3003.928467\n",
      "Training Batch: 15235 Loss: 3041.786621\n",
      "Training Batch: 15236 Loss: 2959.672607\n",
      "Training Batch: 15237 Loss: 3191.059570\n",
      "Training Batch: 15238 Loss: 3067.250488\n",
      "Training Batch: 15239 Loss: 3127.333984\n",
      "Training Batch: 15240 Loss: 3175.170410\n",
      "Training Batch: 15241 Loss: 3100.277344\n",
      "Training Batch: 15242 Loss: 3237.043945\n",
      "Training Batch: 15243 Loss: 3146.993408\n",
      "Training Batch: 15244 Loss: 3174.378906\n",
      "Training Batch: 15245 Loss: 2993.544922\n",
      "Training Batch: 15246 Loss: 3074.872070\n",
      "Training Batch: 15247 Loss: 2993.331543\n",
      "Training Batch: 15248 Loss: 3130.638184\n",
      "Training Batch: 15249 Loss: 3077.385742\n",
      "Training Batch: 15250 Loss: 3032.552246\n",
      "Training Batch: 15251 Loss: 3113.212402\n",
      "Training Batch: 15252 Loss: 2948.846680\n",
      "Training Batch: 15253 Loss: 2999.566650\n",
      "Training Batch: 15254 Loss: 3099.526123\n",
      "Training Batch: 15255 Loss: 3073.120361\n",
      "Training Batch: 15256 Loss: 3075.452148\n",
      "Training Batch: 15257 Loss: 3178.068115\n",
      "Training Batch: 15258 Loss: 3169.065918\n",
      "Training Batch: 15259 Loss: 2999.643555\n",
      "Training Batch: 15260 Loss: 3178.483398\n",
      "Training Batch: 15261 Loss: 3194.224121\n",
      "Training Batch: 15262 Loss: 3118.051270\n",
      "Training Batch: 15263 Loss: 3106.869629\n",
      "Training Batch: 15264 Loss: 3034.548096\n",
      "Training Batch: 15265 Loss: 3417.145020\n",
      "Training Batch: 15266 Loss: 3114.042969\n",
      "Training Batch: 15267 Loss: 2975.855713\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Batch: 15268 Loss: 3096.874023\n",
      "Training Batch: 15269 Loss: 3000.006836\n",
      "Training Batch: 15270 Loss: 3060.197754\n",
      "Training Batch: 15271 Loss: 2990.460205\n",
      "Training Batch: 15272 Loss: 3030.949707\n",
      "Training Batch: 15273 Loss: 3039.249756\n",
      "Training Batch: 15274 Loss: 2983.301270\n",
      "Training Batch: 15275 Loss: 3012.990234\n",
      "Training Batch: 15276 Loss: 2996.087402\n",
      "Training Batch: 15277 Loss: 3127.265137\n",
      "Training Batch: 15278 Loss: 3010.217773\n",
      "Training Batch: 15279 Loss: 2970.551758\n",
      "Training Batch: 15280 Loss: 2983.426270\n",
      "Training Batch: 15281 Loss: 3036.207031\n",
      "Training Batch: 15282 Loss: 3085.203613\n",
      "Training Batch: 15283 Loss: 3104.535400\n",
      "Training Batch: 15284 Loss: 2977.619629\n",
      "Training Batch: 15285 Loss: 3187.453125\n",
      "Training Batch: 15286 Loss: 3086.015625\n",
      "Training Batch: 15287 Loss: 3174.726562\n",
      "Training Batch: 15288 Loss: 3115.365234\n",
      "Training Batch: 15289 Loss: 2998.595703\n",
      "Training Batch: 15290 Loss: 3142.685059\n",
      "Training Batch: 15291 Loss: 3051.748779\n",
      "Training Batch: 15292 Loss: 3063.707520\n",
      "Training Batch: 15293 Loss: 3000.150879\n",
      "Training Batch: 15294 Loss: 3091.443359\n",
      "Training Batch: 15295 Loss: 3113.538086\n",
      "Training Batch: 15296 Loss: 3114.949463\n",
      "Training Batch: 15297 Loss: 3041.309570\n",
      "Training Batch: 15298 Loss: 3131.068359\n",
      "Training Batch: 15299 Loss: 3147.302734\n",
      "Training Batch: 15300 Loss: 3022.238037\n",
      "Training Batch: 15301 Loss: 3023.513428\n",
      "Training Batch: 15302 Loss: 3242.684082\n",
      "Training Batch: 15303 Loss: 3175.530273\n",
      "Training Batch: 15304 Loss: 3122.596680\n",
      "Training Batch: 15305 Loss: 3212.887695\n",
      "Training Batch: 15306 Loss: 3191.711914\n",
      "Training Batch: 15307 Loss: 3072.408203\n",
      "Training Batch: 15308 Loss: 3073.249268\n",
      "Training Batch: 15309 Loss: 3140.060547\n",
      "Training Batch: 15310 Loss: 3097.172852\n",
      "Training Batch: 15311 Loss: 2975.571289\n",
      "Training Batch: 15312 Loss: 2950.264648\n",
      "Training Batch: 15313 Loss: 3155.664307\n",
      "Training Batch: 15314 Loss: 3128.107178\n",
      "Training Batch: 15315 Loss: 3102.391602\n",
      "Training Batch: 15316 Loss: 3089.670898\n",
      "Training Batch: 15317 Loss: 3112.114014\n",
      "Training Batch: 15318 Loss: 3280.294922\n",
      "Training Batch: 15319 Loss: 3006.042480\n",
      "Training Batch: 15320 Loss: 3102.702637\n",
      "Training Batch: 15321 Loss: 3115.138916\n",
      "Training Batch: 15322 Loss: 2951.168701\n",
      "Training Batch: 15323 Loss: 3065.270508\n",
      "Training Batch: 15324 Loss: 3072.567139\n",
      "Training Batch: 15325 Loss: 3113.330566\n",
      "Training Batch: 15326 Loss: 3206.008789\n",
      "Training Batch: 15327 Loss: 3222.986816\n",
      "Training Batch: 15328 Loss: 3188.152344\n",
      "Training Batch: 15329 Loss: 3051.402832\n",
      "Training Batch: 15330 Loss: 3135.586426\n",
      "Training Batch: 15331 Loss: 3083.040039\n",
      "Training Batch: 15332 Loss: 3171.808594\n",
      "Training Batch: 15333 Loss: 3121.039551\n",
      "Training Batch: 15334 Loss: 3150.364746\n",
      "Training Batch: 15335 Loss: 3056.366943\n",
      "Training Batch: 15336 Loss: 3060.982910\n",
      "Training Batch: 15337 Loss: 2990.217773\n",
      "Training Batch: 15338 Loss: 3135.598145\n",
      "Training Batch: 15339 Loss: 3107.550781\n",
      "Training Batch: 15340 Loss: 3323.158691\n",
      "Training Batch: 15341 Loss: 3094.579834\n",
      "Training Batch: 15342 Loss: 2990.591797\n",
      "Training Batch: 15343 Loss: 3201.285156\n",
      "Training Batch: 15344 Loss: 3049.139160\n",
      "Training Batch: 15345 Loss: 3102.104492\n",
      "Training Batch: 15346 Loss: 3132.464355\n",
      "Training Batch: 15347 Loss: 3041.248779\n",
      "Training Batch: 15348 Loss: 3055.725586\n",
      "Training Batch: 15349 Loss: 3048.768066\n",
      "Training Batch: 15350 Loss: 3164.533691\n",
      "Training Batch: 15351 Loss: 3121.378418\n",
      "Training Batch: 15352 Loss: 3091.667480\n",
      "Training Batch: 15353 Loss: 3423.924561\n",
      "Training Batch: 15354 Loss: 3125.722900\n",
      "Training Batch: 15355 Loss: 3242.354980\n",
      "Training Batch: 15356 Loss: 3182.758789\n",
      "Training Batch: 15357 Loss: 3102.803711\n",
      "Training Batch: 15358 Loss: 3141.335693\n",
      "Training Batch: 15359 Loss: 3460.163330\n",
      "Training Batch: 15360 Loss: 3213.902832\n",
      "Training Batch: 15361 Loss: 3126.192383\n",
      "Training Batch: 15362 Loss: 3282.369385\n",
      "Training Batch: 15363 Loss: 3293.270020\n",
      "Training Batch: 15364 Loss: 3148.387939\n",
      "Training Batch: 15365 Loss: 3133.336426\n",
      "Training Batch: 15366 Loss: 3361.122070\n",
      "Training Batch: 15367 Loss: 3095.177979\n",
      "Training Batch: 15368 Loss: 3259.376221\n",
      "Training Batch: 15369 Loss: 3122.569336\n",
      "Training Batch: 15370 Loss: 3159.125977\n",
      "Training Batch: 15371 Loss: 3282.645020\n",
      "Training Batch: 15372 Loss: 3414.988037\n",
      "Training Batch: 15373 Loss: 3036.527832\n",
      "Training Batch: 15374 Loss: 3151.218994\n",
      "Training Batch: 15375 Loss: 3063.606689\n",
      "Training Batch: 15376 Loss: 3281.192383\n",
      "Training Batch: 15377 Loss: 3142.023682\n",
      "Training Batch: 15378 Loss: 3237.472168\n",
      "Training Batch: 15379 Loss: 3157.253906\n",
      "Training Batch: 15380 Loss: 3106.988281\n",
      "Training Batch: 15381 Loss: 3076.612793\n",
      "Training Batch: 15382 Loss: 3084.376465\n",
      "Training Batch: 15383 Loss: 3043.701904\n",
      "Training Batch: 15384 Loss: 3085.409180\n",
      "Training Batch: 15385 Loss: 3287.549805\n",
      "Training Batch: 15386 Loss: 3272.276367\n",
      "Training Batch: 15387 Loss: 3057.629883\n",
      "Training Batch: 15388 Loss: 3351.484375\n",
      "Training Batch: 15389 Loss: 3090.013184\n",
      "Training Batch: 15390 Loss: 3218.567139\n",
      "Training Batch: 15391 Loss: 3065.032715\n",
      "Training Batch: 15392 Loss: 3095.340820\n",
      "Training Batch: 15393 Loss: 3230.321289\n",
      "Training Batch: 15394 Loss: 3083.858398\n",
      "Training Batch: 15395 Loss: 3146.291748\n",
      "Training Batch: 15396 Loss: 3259.506104\n",
      "Training Batch: 15397 Loss: 3052.182617\n",
      "Training Batch: 15398 Loss: 3095.520996\n",
      "Training Batch: 15399 Loss: 3127.935547\n",
      "Training Batch: 15400 Loss: 3241.089844\n",
      "Training Batch: 15401 Loss: 3103.458740\n",
      "Training Batch: 15402 Loss: 3805.852539\n",
      "Training Batch: 15403 Loss: 3183.619141\n",
      "Training Batch: 15404 Loss: 3125.714600\n",
      "Training Batch: 15405 Loss: 3100.085938\n",
      "Training Batch: 15406 Loss: 2989.698242\n",
      "Training Batch: 15407 Loss: 3116.627930\n",
      "Training Batch: 15408 Loss: 3035.801270\n",
      "Training Batch: 15409 Loss: 3170.387695\n",
      "Training Batch: 15410 Loss: 3276.848877\n",
      "Training Batch: 15411 Loss: 3183.778320\n",
      "Training Batch: 15412 Loss: 3205.969482\n",
      "Training Batch: 15413 Loss: 2996.787598\n",
      "Training Batch: 15414 Loss: 3145.229980\n",
      "Training Batch: 15415 Loss: 3049.801025\n",
      "Training Batch: 15416 Loss: 2961.138184\n",
      "Training Batch: 15417 Loss: 3102.822754\n",
      "Training Batch: 15418 Loss: 3083.764648\n",
      "Training Batch: 15419 Loss: 3231.616211\n",
      "Training Batch: 15420 Loss: 3188.542725\n",
      "Training Batch: 15421 Loss: 3076.251221\n",
      "Training Batch: 15422 Loss: 3102.005371\n",
      "Training Batch: 15423 Loss: 3267.212158\n",
      "Training Batch: 15424 Loss: 3072.047363\n",
      "Training Batch: 15425 Loss: 2979.534668\n",
      "Training Batch: 15426 Loss: 3538.232910\n",
      "Training Batch: 15427 Loss: 3051.388672\n",
      "Training Batch: 15428 Loss: 3186.263184\n",
      "Training Batch: 15429 Loss: 3494.901367\n",
      "Training Batch: 15430 Loss: 3062.931152\n",
      "Training Batch: 15431 Loss: 3083.975586\n",
      "Training Batch: 15432 Loss: 3072.218262\n",
      "Training Batch: 15433 Loss: 3177.566895\n",
      "Training Batch: 15434 Loss: 3013.450684\n",
      "Training Batch: 15435 Loss: 3072.023438\n",
      "Training Batch: 15436 Loss: 3082.500977\n",
      "Training Batch: 15437 Loss: 3089.364014\n",
      "Training Batch: 15438 Loss: 3037.857910\n",
      "Training Batch: 15439 Loss: 3055.790771\n",
      "Training Batch: 15440 Loss: 3020.451172\n",
      "Training Batch: 15441 Loss: 2953.711914\n",
      "Training Batch: 15442 Loss: 3256.158203\n",
      "Training Batch: 15443 Loss: 3201.632324\n",
      "Training Batch: 15444 Loss: 3124.156250\n",
      "Training Batch: 15445 Loss: 3423.459961\n",
      "Training Batch: 15446 Loss: 3153.526367\n",
      "Training Batch: 15447 Loss: 3112.409668\n",
      "Training Batch: 15448 Loss: 3140.038086\n",
      "Training Batch: 15449 Loss: 3067.409180\n",
      "Training Batch: 15450 Loss: 3067.573730\n",
      "Training Batch: 15451 Loss: 3173.352051\n",
      "Training Batch: 15452 Loss: 3011.114258\n",
      "Training Batch: 15453 Loss: 3069.109375\n",
      "Training Batch: 15454 Loss: 3338.210693\n",
      "Training Batch: 15455 Loss: 3077.708008\n",
      "Training Batch: 15456 Loss: 3026.021973\n",
      "Training Batch: 15457 Loss: 3020.152100\n",
      "Training Batch: 15458 Loss: 3157.520996\n",
      "Training Batch: 15459 Loss: 3346.003662\n",
      "Training Batch: 15460 Loss: 3044.167236\n",
      "Training Batch: 15461 Loss: 3022.264893\n",
      "Training Batch: 15462 Loss: 3185.252441\n",
      "Training Batch: 15463 Loss: 3059.468018\n",
      "Training Batch: 15464 Loss: 3089.560547\n",
      "Training Batch: 15465 Loss: 2994.939941\n",
      "Training Batch: 15466 Loss: 3211.132324\n",
      "Training Batch: 15467 Loss: 3018.300781\n",
      "Training Batch: 15468 Loss: 2940.109619\n",
      "Training Batch: 15469 Loss: 3137.999512\n",
      "Training Batch: 15470 Loss: 3037.540527\n",
      "Training Batch: 15471 Loss: 3120.430176\n",
      "Training Batch: 15472 Loss: 3063.928711\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Batch: 15473 Loss: 3127.584961\n",
      "Training Batch: 15474 Loss: 3314.858398\n",
      "Training Batch: 15475 Loss: 3040.956787\n",
      "Training Batch: 15476 Loss: 3242.058105\n",
      "Training Batch: 15477 Loss: 3324.506348\n",
      "Training Batch: 15478 Loss: 3029.055176\n",
      "Training Batch: 15479 Loss: 3452.766113\n",
      "Training Batch: 15480 Loss: 3223.469238\n",
      "Training Batch: 15481 Loss: 3077.848145\n",
      "Training Batch: 15482 Loss: 3207.138672\n",
      "Training Batch: 15483 Loss: 3062.661621\n",
      "Training Batch: 15484 Loss: 3053.583496\n",
      "Training Batch: 15485 Loss: 3269.046875\n",
      "Training Batch: 15486 Loss: 3024.992432\n",
      "Training Batch: 15487 Loss: 3149.445312\n",
      "Training Batch: 15488 Loss: 3104.537598\n",
      "Training Batch: 15489 Loss: 3171.281738\n",
      "Training Batch: 15490 Loss: 3045.037109\n",
      "Training Batch: 15491 Loss: 3115.127197\n",
      "Training Batch: 15492 Loss: 3209.715820\n",
      "Training Batch: 15493 Loss: 3050.234375\n",
      "Training Batch: 15494 Loss: 3087.370605\n",
      "Training Batch: 15495 Loss: 3068.874023\n",
      "Training Batch: 15496 Loss: 3193.733398\n",
      "Training Batch: 15497 Loss: 3086.251221\n",
      "Training Batch: 15498 Loss: 3174.085938\n",
      "Training Batch: 15499 Loss: 3289.565430\n",
      "Training Batch: 15500 Loss: 3166.023926\n",
      "Training Batch: 15501 Loss: 3001.603760\n",
      "Training Batch: 15502 Loss: 3188.532715\n",
      "Training Batch: 15503 Loss: 3166.728027\n",
      "Training Batch: 15504 Loss: 3087.734863\n",
      "Training Batch: 15505 Loss: 3024.676270\n",
      "Training Batch: 15506 Loss: 3386.306152\n",
      "Training Batch: 15507 Loss: 3177.349609\n",
      "Training Batch: 15508 Loss: 3075.933105\n",
      "Training Batch: 15509 Loss: 3333.912842\n",
      "Training Batch: 15510 Loss: 3185.040527\n",
      "Training Batch: 15511 Loss: 3111.790527\n",
      "Training Batch: 15512 Loss: 3082.231934\n",
      "Training Batch: 15513 Loss: 3194.337402\n",
      "Training Batch: 15514 Loss: 3117.353516\n",
      "Training Batch: 15515 Loss: 3629.439697\n",
      "Training Batch: 15516 Loss: 3543.086182\n",
      "Training Batch: 15517 Loss: 3150.491211\n",
      "Training Batch: 15518 Loss: 3320.812012\n",
      "Training Batch: 15519 Loss: 3286.065186\n",
      "Training Batch: 15520 Loss: 3121.101562\n",
      "Training Batch: 15521 Loss: 3172.966309\n",
      "Training Batch: 15522 Loss: 3070.452393\n",
      "Training Batch: 15523 Loss: 3433.853516\n",
      "Training Batch: 15524 Loss: 3171.525146\n",
      "Training Batch: 15525 Loss: 3190.075684\n",
      "Training Batch: 15526 Loss: 3073.690918\n",
      "Training Batch: 15527 Loss: 3407.257812\n",
      "Training Batch: 15528 Loss: 3302.804688\n",
      "Training Batch: 15529 Loss: 3070.964844\n",
      "Training Batch: 15530 Loss: 3074.181641\n",
      "Training Batch: 15531 Loss: 3109.423340\n",
      "Training Batch: 15532 Loss: 3036.046631\n",
      "Training Batch: 15533 Loss: 3353.019531\n",
      "Training Batch: 15534 Loss: 3081.484375\n",
      "Training Batch: 15535 Loss: 2958.571777\n",
      "Training Batch: 15536 Loss: 3000.711914\n",
      "Training Batch: 15537 Loss: 3114.397705\n",
      "Training Batch: 15538 Loss: 3255.164062\n",
      "Training Batch: 15539 Loss: 3015.904297\n",
      "Training Batch: 15540 Loss: 3259.106689\n",
      "Training Batch: 15541 Loss: 3398.168457\n",
      "Training Batch: 15542 Loss: 3126.228760\n",
      "Training Batch: 15543 Loss: 2985.892822\n",
      "Training Batch: 15544 Loss: 3367.575684\n",
      "Training Batch: 15545 Loss: 3632.707764\n",
      "Training Batch: 15546 Loss: 3214.334473\n",
      "Training Batch: 15547 Loss: 3162.848633\n",
      "Training Batch: 15548 Loss: 3024.829590\n",
      "Training Batch: 15549 Loss: 3097.970215\n",
      "Training Batch: 15550 Loss: 3225.078613\n",
      "Training Batch: 15551 Loss: 3224.111328\n",
      "Training Batch: 15552 Loss: 3041.796387\n",
      "Training Batch: 15553 Loss: 3202.746338\n",
      "Training Batch: 15554 Loss: 3030.457520\n",
      "Training Batch: 15555 Loss: 3116.124756\n",
      "Training Batch: 15556 Loss: 3171.514160\n",
      "Training Batch: 15557 Loss: 3051.714844\n",
      "Training Batch: 15558 Loss: 3001.734375\n",
      "Training Batch: 15559 Loss: 3238.510254\n",
      "Training Batch: 15560 Loss: 3211.407227\n",
      "Training Batch: 15561 Loss: 3244.957275\n",
      "Training Batch: 15562 Loss: 3038.117920\n",
      "Training Batch: 15563 Loss: 3135.537109\n",
      "Training Batch: 15564 Loss: 3030.252930\n",
      "Training Batch: 15565 Loss: 3150.215576\n",
      "Training Batch: 15566 Loss: 3173.645996\n",
      "Training Batch: 15567 Loss: 3114.789062\n",
      "Training Batch: 15568 Loss: 3039.921631\n",
      "Training Batch: 15569 Loss: 3063.531494\n",
      "Training Batch: 15570 Loss: 3245.781738\n",
      "Training Batch: 15571 Loss: 3173.360840\n",
      "Training Batch: 15572 Loss: 3215.649414\n",
      "Training Batch: 15573 Loss: 3099.077637\n",
      "Training Batch: 15574 Loss: 3106.572510\n",
      "Training Batch: 15575 Loss: 2997.771729\n",
      "Training Batch: 15576 Loss: 2908.250488\n",
      "Training Batch: 15577 Loss: 3168.728760\n",
      "Training Batch: 15578 Loss: 3072.900879\n",
      "Training Batch: 15579 Loss: 3117.124023\n",
      "Training Batch: 15580 Loss: 2988.696777\n",
      "Training Batch: 15581 Loss: 3043.430664\n",
      "Training Batch: 15582 Loss: 3334.368408\n",
      "Training Batch: 15583 Loss: 3773.286621\n",
      "Training Batch: 15584 Loss: 3075.264893\n",
      "Training Batch: 15585 Loss: 3011.522461\n",
      "Training Batch: 15586 Loss: 3206.710205\n",
      "Training Batch: 15587 Loss: 3075.877441\n",
      "Training Batch: 15588 Loss: 3109.802734\n",
      "Training Batch: 15589 Loss: 3043.241211\n",
      "Training Batch: 15590 Loss: 3157.232910\n",
      "Training Batch: 15591 Loss: 3171.507324\n",
      "Training Batch: 15592 Loss: 3060.623535\n",
      "Training Batch: 15593 Loss: 3132.902344\n",
      "Training Batch: 15594 Loss: 3160.881836\n",
      "Training Batch: 15595 Loss: 3105.681152\n",
      "Training Batch: 15596 Loss: 3188.966309\n",
      "Training Batch: 15597 Loss: 3198.177002\n",
      "Training Batch: 15598 Loss: 3146.108398\n",
      "Training Batch: 15599 Loss: 3190.080078\n",
      "Training Batch: 15600 Loss: 3125.752197\n",
      "Training Batch: 15601 Loss: 3324.133789\n",
      "Training Batch: 15602 Loss: 3166.767090\n",
      "Training Batch: 15603 Loss: 3151.547852\n",
      "Training Batch: 15604 Loss: 3144.280273\n",
      "Training Batch: 15605 Loss: 3108.015381\n",
      "Training Batch: 15606 Loss: 3038.230469\n",
      "Training Batch: 15607 Loss: 3059.940430\n",
      "Training Batch: 15608 Loss: 3012.208984\n",
      "Training Batch: 15609 Loss: 3063.466553\n",
      "Training Batch: 15610 Loss: 3033.668945\n",
      "Training Batch: 15611 Loss: 3044.412598\n",
      "Training Batch: 15612 Loss: 3110.627930\n",
      "Training Batch: 15613 Loss: 3340.874512\n",
      "Training Batch: 15614 Loss: 3051.947754\n",
      "Training Batch: 15615 Loss: 3038.628662\n",
      "Training Batch: 15616 Loss: 3066.186768\n",
      "Training Batch: 15617 Loss: 3050.147461\n",
      "Training Batch: 15618 Loss: 2972.080322\n",
      "Training Batch: 15619 Loss: 2996.468262\n",
      "Training Batch: 15620 Loss: 3158.672119\n",
      "Training Batch: 15621 Loss: 3120.606445\n",
      "Training Batch: 15622 Loss: 3120.368408\n",
      "Training Batch: 15623 Loss: 3198.033203\n",
      "Training Batch: 15624 Loss: 3025.611816\n",
      "Training Batch: 15625 Loss: 3061.218262\n",
      "Training Batch: 15626 Loss: 3130.249512\n",
      "Training Batch: 15627 Loss: 3107.575928\n",
      "Training Batch: 15628 Loss: 3035.902344\n",
      "Training Batch: 15629 Loss: 3056.926270\n",
      "Training Batch: 15630 Loss: 3125.458984\n",
      "Training Batch: 15631 Loss: 3192.169922\n",
      "Training Batch: 15632 Loss: 3156.599854\n",
      "Training Batch: 15633 Loss: 3131.878906\n",
      "Training Batch: 15634 Loss: 3035.866211\n",
      "Training Batch: 15635 Loss: 3204.606445\n",
      "Training Batch: 15636 Loss: 3191.840332\n",
      "Training Batch: 15637 Loss: 3152.656250\n",
      "Training Batch: 15638 Loss: 3110.788574\n",
      "Training Batch: 15639 Loss: 3272.746582\n",
      "Training Batch: 15640 Loss: 3227.984863\n",
      "Training Batch: 15641 Loss: 3486.464844\n",
      "Training Batch: 15642 Loss: 3374.960449\n",
      "Training Batch: 15643 Loss: 3085.483398\n",
      "Training Batch: 15644 Loss: 3049.447754\n",
      "Training Batch: 15645 Loss: 3117.799805\n",
      "Training Batch: 15646 Loss: 3424.923340\n",
      "Training Batch: 15647 Loss: 3281.639160\n",
      "Training Batch: 15648 Loss: 3141.913330\n",
      "Training Batch: 15649 Loss: 3261.054443\n",
      "Training Batch: 15650 Loss: 3456.989258\n",
      "Training Batch: 15651 Loss: 3133.559082\n",
      "Training Batch: 15652 Loss: 3044.740234\n",
      "Training Batch: 15653 Loss: 3021.025879\n",
      "Training Batch: 15654 Loss: 3065.639160\n",
      "Training Batch: 15655 Loss: 3093.347168\n",
      "Training Batch: 15656 Loss: 3159.089844\n",
      "Training Batch: 15657 Loss: 3092.666016\n",
      "Training Batch: 15658 Loss: 3031.118408\n",
      "Training Batch: 15659 Loss: 3012.264893\n",
      "Training Batch: 15660 Loss: 3196.901855\n",
      "Training Batch: 15661 Loss: 3229.253418\n",
      "Training Batch: 15662 Loss: 3247.252441\n",
      "Training Batch: 15663 Loss: 3114.855469\n",
      "Training Batch: 15664 Loss: 3250.099609\n",
      "Training Batch: 15665 Loss: 3217.948730\n",
      "Training Batch: 15666 Loss: 3145.252930\n",
      "Training Batch: 15667 Loss: 3169.034180\n",
      "Training Batch: 15668 Loss: 3546.116211\n",
      "Training Batch: 15669 Loss: 3112.315430\n",
      "Training Batch: 15670 Loss: 3143.557129\n",
      "Training Batch: 15671 Loss: 3138.232422\n",
      "Training Batch: 15672 Loss: 3245.681641\n",
      "Training Batch: 15673 Loss: 3201.973145\n",
      "Training Batch: 15674 Loss: 3021.012695\n",
      "Training Batch: 15675 Loss: 3119.950684\n",
      "Training Batch: 15676 Loss: 3106.988281\n",
      "Training Batch: 15677 Loss: 3097.685059\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Batch: 15678 Loss: 3016.158936\n",
      "Training Batch: 15679 Loss: 3224.086182\n",
      "Training Batch: 15680 Loss: 3251.370605\n",
      "Training Batch: 15681 Loss: 3153.125977\n",
      "Training Batch: 15682 Loss: 3000.242188\n",
      "Training Batch: 15683 Loss: 3095.958984\n",
      "Training Batch: 15684 Loss: 3101.586670\n",
      "Training Batch: 15685 Loss: 3037.726318\n",
      "Training Batch: 15686 Loss: 3054.538330\n",
      "Training Batch: 15687 Loss: 3093.809326\n",
      "Training Batch: 15688 Loss: 3200.509277\n",
      "Training Batch: 15689 Loss: 3237.145996\n",
      "Training Batch: 15690 Loss: 3218.378418\n",
      "Training Batch: 15691 Loss: 3134.700439\n",
      "Training Batch: 15692 Loss: 2963.586670\n",
      "Training Batch: 15693 Loss: 2988.787109\n",
      "Training Batch: 15694 Loss: 3056.094727\n",
      "Training Batch: 15695 Loss: 3033.860840\n",
      "Training Batch: 15696 Loss: 3268.370605\n",
      "Training Batch: 15697 Loss: 3073.898193\n",
      "Training Batch: 15698 Loss: 3144.786621\n",
      "Training Batch: 15699 Loss: 3154.973633\n",
      "Training Batch: 15700 Loss: 3204.159912\n",
      "Training Batch: 15701 Loss: 3008.265381\n",
      "Training Batch: 15702 Loss: 3142.200928\n",
      "Training Batch: 15703 Loss: 3110.718262\n",
      "Training Batch: 15704 Loss: 3048.003906\n",
      "Training Batch: 15705 Loss: 3036.265869\n",
      "Training Batch: 15706 Loss: 3129.264648\n",
      "Training Batch: 15707 Loss: 3123.887695\n",
      "Training Batch: 15708 Loss: 3306.097168\n",
      "Training Batch: 15709 Loss: 3162.973633\n",
      "Training Batch: 15710 Loss: 3086.697510\n",
      "Training Batch: 15711 Loss: 3138.331055\n",
      "Training Batch: 15712 Loss: 3372.061523\n",
      "Training Batch: 15713 Loss: 3271.519531\n",
      "Training Batch: 15714 Loss: 3068.919922\n",
      "Training Batch: 15715 Loss: 3111.232178\n",
      "Training Batch: 15716 Loss: 3103.578369\n",
      "Training Batch: 15717 Loss: 3144.756348\n",
      "Training Batch: 15718 Loss: 3046.455566\n",
      "Training Batch: 15719 Loss: 3225.779785\n",
      "Training Batch: 15720 Loss: 3110.152344\n",
      "Training Batch: 15721 Loss: 3132.736572\n",
      "Training Batch: 15722 Loss: 3131.769531\n",
      "Training Batch: 15723 Loss: 2994.673340\n",
      "Training Batch: 15724 Loss: 3345.395996\n",
      "Training Batch: 15725 Loss: 3102.822266\n",
      "Training Batch: 15726 Loss: 3116.835449\n",
      "Training Batch: 15727 Loss: 3277.937988\n",
      "Training Batch: 15728 Loss: 3053.734131\n",
      "Training Batch: 15729 Loss: 3195.250488\n",
      "Training Batch: 15730 Loss: 3054.067139\n",
      "Training Batch: 15731 Loss: 3201.260498\n",
      "Training Batch: 15732 Loss: 3148.690918\n",
      "Training Batch: 15733 Loss: 3234.695312\n",
      "Training Batch: 15734 Loss: 3134.835205\n",
      "Training Batch: 15735 Loss: 3171.204102\n",
      "Training Batch: 15736 Loss: 3080.255859\n",
      "Training Batch: 15737 Loss: 3222.975586\n",
      "Training Batch: 15738 Loss: 3147.471924\n",
      "Training Batch: 15739 Loss: 3106.181641\n",
      "Training Batch: 15740 Loss: 3135.805420\n",
      "Training Batch: 15741 Loss: 3077.166504\n",
      "Training Batch: 15742 Loss: 3360.772461\n",
      "Training Batch: 15743 Loss: 3324.424072\n",
      "Training Batch: 15744 Loss: 3152.097900\n",
      "Training Batch: 15745 Loss: 3041.702881\n",
      "Training Batch: 15746 Loss: 3076.885742\n",
      "Training Batch: 15747 Loss: 3001.615967\n",
      "Training Batch: 15748 Loss: 3098.186523\n",
      "Training Batch: 15749 Loss: 3087.455322\n",
      "Training Batch: 15750 Loss: 3152.763184\n",
      "Training Batch: 15751 Loss: 3079.544922\n",
      "Training Batch: 15752 Loss: 3064.775146\n",
      "Training Batch: 15753 Loss: 2994.108643\n",
      "Training Batch: 15754 Loss: 3108.122559\n",
      "Training Batch: 15755 Loss: 2998.190186\n",
      "Training Batch: 15756 Loss: 2969.552734\n",
      "Training Batch: 15757 Loss: 3136.532715\n",
      "Training Batch: 15758 Loss: 3023.760010\n",
      "Training Batch: 15759 Loss: 3141.006348\n",
      "Training Batch: 15760 Loss: 3376.638672\n",
      "Training Batch: 15761 Loss: 2980.335938\n",
      "Training Batch: 15762 Loss: 3463.455078\n",
      "Training Batch: 15763 Loss: 3055.373535\n",
      "Training Batch: 15764 Loss: 3016.306152\n",
      "Training Batch: 15765 Loss: 3058.045166\n",
      "Training Batch: 15766 Loss: 3023.207764\n",
      "Training Batch: 15767 Loss: 3286.713379\n",
      "Training Batch: 15768 Loss: 2992.483887\n",
      "Training Batch: 15769 Loss: 3094.188965\n",
      "Training Batch: 15770 Loss: 3143.706055\n",
      "Training Batch: 15771 Loss: 3064.805664\n",
      "Training Batch: 15772 Loss: 3060.609375\n",
      "Training Batch: 15773 Loss: 3054.389160\n",
      "Training Batch: 15774 Loss: 2961.183594\n",
      "Training Batch: 15775 Loss: 3151.570801\n",
      "Training Batch: 15776 Loss: 3030.855469\n",
      "Training Batch: 15777 Loss: 3048.747070\n",
      "Training Batch: 15778 Loss: 3262.526855\n",
      "Training Batch: 15779 Loss: 3318.720703\n",
      "Training Batch: 15780 Loss: 3026.676758\n",
      "Training Batch: 15781 Loss: 3055.333008\n",
      "Training Batch: 15782 Loss: 3060.845459\n",
      "Training Batch: 15783 Loss: 3166.254395\n",
      "Training Batch: 15784 Loss: 3158.852051\n",
      "Training Batch: 15785 Loss: 3055.937256\n",
      "Training Batch: 15786 Loss: 3092.267578\n",
      "Training Batch: 15787 Loss: 2931.474854\n",
      "Training Batch: 15788 Loss: 3151.988281\n",
      "Training Batch: 15789 Loss: 2999.763428\n",
      "Training Batch: 15790 Loss: 2980.298340\n",
      "Training Batch: 15791 Loss: 3204.395508\n",
      "Training Batch: 15792 Loss: 3094.093506\n",
      "Training Batch: 15793 Loss: 3084.084717\n",
      "Training Batch: 15794 Loss: 3145.155762\n",
      "Training Batch: 15795 Loss: 3190.797852\n",
      "Training Batch: 15796 Loss: 3013.833984\n",
      "Training Batch: 15797 Loss: 3196.956543\n",
      "Training Batch: 15798 Loss: 3024.741699\n",
      "Training Batch: 15799 Loss: 3207.063721\n",
      "Training Batch: 15800 Loss: 3315.764893\n",
      "Training Batch: 15801 Loss: 3672.327148\n",
      "Training Batch: 15802 Loss: 3204.676758\n",
      "Training Batch: 15803 Loss: 3267.879395\n",
      "Training Batch: 15804 Loss: 3108.710938\n",
      "Training Batch: 15805 Loss: 3081.191406\n",
      "Training Batch: 15806 Loss: 3056.747559\n",
      "Training Batch: 15807 Loss: 3156.497070\n",
      "Training Batch: 15808 Loss: 3001.376465\n",
      "Training Batch: 15809 Loss: 3077.756592\n",
      "Training Batch: 15810 Loss: 3077.267090\n",
      "Training Batch: 15811 Loss: 3066.787109\n",
      "Training Batch: 15812 Loss: 3267.132812\n",
      "Training Batch: 15813 Loss: 3251.712646\n",
      "Training Batch: 15814 Loss: 3036.902832\n",
      "Training Batch: 15815 Loss: 2993.807129\n",
      "Training Batch: 15816 Loss: 3057.122559\n",
      "Training Batch: 15817 Loss: 3068.848389\n",
      "Training Batch: 15818 Loss: 3173.555176\n",
      "Training Batch: 15819 Loss: 3058.002930\n",
      "Training Batch: 15820 Loss: 3074.054199\n",
      "Training Batch: 15821 Loss: 3243.534180\n",
      "Training Batch: 15822 Loss: 3040.793945\n",
      "Training Batch: 15823 Loss: 3332.924072\n",
      "Training Batch: 15824 Loss: 3283.269043\n",
      "Training Batch: 15825 Loss: 3053.006592\n",
      "Training Batch: 15826 Loss: 3171.044922\n",
      "Training Batch: 15827 Loss: 3111.340820\n",
      "Training Batch: 15828 Loss: 3220.302979\n",
      "Training Batch: 15829 Loss: 3033.975098\n",
      "Training Batch: 15830 Loss: 3072.033447\n",
      "Training Batch: 15831 Loss: 3097.074219\n",
      "Training Batch: 15832 Loss: 3003.417480\n",
      "Training Batch: 15833 Loss: 3271.378418\n",
      "Training Batch: 15834 Loss: 3160.322754\n",
      "Training Batch: 15835 Loss: 3002.867676\n",
      "Training Batch: 15836 Loss: 3068.454590\n",
      "Training Batch: 15837 Loss: 3069.198730\n",
      "Training Batch: 15838 Loss: 2980.448975\n",
      "Training Batch: 15839 Loss: 3078.354004\n",
      "Training Batch: 15840 Loss: 3283.509277\n",
      "Training Batch: 15841 Loss: 3133.889160\n",
      "Training Batch: 15842 Loss: 3044.390625\n",
      "Training Batch: 15843 Loss: 3083.597656\n",
      "Training Batch: 15844 Loss: 3189.432129\n",
      "Training Batch: 15845 Loss: 3176.524414\n",
      "Training Batch: 15846 Loss: 2997.488281\n",
      "Training Batch: 15847 Loss: 3162.032715\n",
      "Training Batch: 15848 Loss: 3234.557373\n",
      "Training Batch: 15849 Loss: 3156.256592\n",
      "Training Batch: 15850 Loss: 3326.972656\n",
      "Training Batch: 15851 Loss: 3061.548828\n",
      "Training Batch: 15852 Loss: 3527.905273\n",
      "Training Batch: 15853 Loss: 3229.552246\n",
      "Training Batch: 15854 Loss: 3179.321289\n",
      "Training Batch: 15855 Loss: 3201.606445\n",
      "Training Batch: 15856 Loss: 3021.934570\n",
      "Training Batch: 15857 Loss: 3137.303711\n",
      "Training Batch: 15858 Loss: 3067.185059\n",
      "Training Batch: 15859 Loss: 3056.881592\n",
      "Training Batch: 15860 Loss: 3056.228516\n",
      "Training Batch: 15861 Loss: 3273.907227\n",
      "Training Batch: 15862 Loss: 3121.677002\n",
      "Training Batch: 15863 Loss: 3053.432129\n",
      "Training Batch: 15864 Loss: 2995.205078\n",
      "Training Batch: 15865 Loss: 3076.635742\n",
      "Training Batch: 15866 Loss: 3143.509033\n",
      "Training Batch: 15867 Loss: 3245.239258\n",
      "Training Batch: 15868 Loss: 3119.131836\n",
      "Training Batch: 15869 Loss: 3177.538574\n",
      "Training Batch: 15870 Loss: 3124.085938\n",
      "Training Batch: 15871 Loss: 3000.721191\n",
      "Training Batch: 15872 Loss: 3121.442871\n",
      "Training Batch: 15873 Loss: 2986.918457\n",
      "Training Batch: 15874 Loss: 3072.429688\n",
      "Training Batch: 15875 Loss: 3176.291504\n",
      "Training Batch: 15876 Loss: 3039.076660\n",
      "Training Batch: 15877 Loss: 3104.477051\n",
      "Training Batch: 15878 Loss: 3095.368164\n",
      "Training Batch: 15879 Loss: 3097.085449\n",
      "Training Batch: 15880 Loss: 3149.965820\n",
      "Training Batch: 15881 Loss: 2998.574707\n",
      "Training Batch: 15882 Loss: 3053.816895\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Batch: 15883 Loss: 3171.296875\n",
      "Training Batch: 15884 Loss: 3120.394287\n",
      "Training Batch: 15885 Loss: 3150.369873\n",
      "Training Batch: 15886 Loss: 3139.203369\n",
      "Training Batch: 15887 Loss: 3064.526367\n",
      "Training Batch: 15888 Loss: 3107.567627\n",
      "Training Batch: 15889 Loss: 3061.314941\n",
      "Training Batch: 15890 Loss: 3025.087891\n",
      "Training Batch: 15891 Loss: 3003.260254\n",
      "Training Batch: 15892 Loss: 3114.804688\n",
      "Training Batch: 15893 Loss: 3197.607178\n",
      "Training Batch: 15894 Loss: 3037.027832\n",
      "Training Batch: 15895 Loss: 3074.730469\n",
      "Training Batch: 15896 Loss: 3217.604980\n",
      "Training Batch: 15897 Loss: 3118.895752\n",
      "Training Batch: 15898 Loss: 3105.888672\n",
      "Training Batch: 15899 Loss: 3265.261719\n",
      "Training Batch: 15900 Loss: 3034.399902\n",
      "Training Batch: 15901 Loss: 3115.349121\n",
      "Training Batch: 15902 Loss: 3287.855957\n",
      "Training Batch: 15903 Loss: 3070.236816\n",
      "Training Batch: 15904 Loss: 3115.677246\n",
      "Training Batch: 15905 Loss: 3199.744141\n",
      "Training Batch: 15906 Loss: 3299.062500\n",
      "Training Batch: 15907 Loss: 3120.776855\n",
      "Training Batch: 15908 Loss: 3098.857178\n",
      "Training Batch: 15909 Loss: 3198.458496\n",
      "Training Batch: 15910 Loss: 3056.615723\n",
      "Training Batch: 15911 Loss: 3110.185791\n",
      "Training Batch: 15912 Loss: 3011.375977\n",
      "Training Batch: 15913 Loss: 3091.139404\n",
      "Training Batch: 15914 Loss: 3148.471191\n",
      "Training Batch: 15915 Loss: 3208.718506\n",
      "Training Batch: 15916 Loss: 3029.194336\n",
      "Training Batch: 15917 Loss: 3110.203613\n",
      "Training Batch: 15918 Loss: 3115.565430\n",
      "Training Batch: 15919 Loss: 3080.655762\n",
      "Training Batch: 15920 Loss: 3020.983398\n",
      "Training Batch: 15921 Loss: 3430.788086\n",
      "Training Batch: 15922 Loss: 3044.767822\n",
      "Training Batch: 15923 Loss: 3212.162842\n",
      "Training Batch: 15924 Loss: 3180.819824\n",
      "Training Batch: 15925 Loss: 3248.389404\n",
      "Training Batch: 15926 Loss: 3131.697754\n",
      "Training Batch: 15927 Loss: 2986.264648\n",
      "Training Batch: 15928 Loss: 3164.079590\n",
      "Training Batch: 15929 Loss: 3077.545410\n",
      "Training Batch: 15930 Loss: 3152.704590\n",
      "Training Batch: 15931 Loss: 3125.848633\n",
      "Training Batch: 15932 Loss: 3117.204102\n",
      "Training Batch: 15933 Loss: 3148.610352\n",
      "Training Batch: 15934 Loss: 3197.971680\n",
      "Training Batch: 15935 Loss: 3066.302734\n",
      "Training Batch: 15936 Loss: 3112.840088\n",
      "Training Batch: 15937 Loss: 3193.918701\n",
      "Training Batch: 15938 Loss: 3000.607422\n",
      "Training Batch: 15939 Loss: 3070.528320\n",
      "Training Batch: 15940 Loss: 3148.697266\n",
      "Training Batch: 15941 Loss: 3097.493164\n",
      "Training Batch: 15942 Loss: 3268.890137\n",
      "Training Batch: 15943 Loss: 3158.687500\n",
      "Training Batch: 15944 Loss: 3097.722900\n",
      "Training Batch: 15945 Loss: 2970.451660\n",
      "Training Batch: 15946 Loss: 3135.728516\n",
      "Training Batch: 15947 Loss: 3130.996094\n",
      "Training Batch: 15948 Loss: 3099.013916\n",
      "Training Batch: 15949 Loss: 3276.881348\n",
      "Training Batch: 15950 Loss: 3109.892822\n",
      "Training Batch: 15951 Loss: 3220.994141\n",
      "Training Batch: 15952 Loss: 3069.352539\n",
      "Training Batch: 15953 Loss: 3006.625488\n",
      "Training Batch: 15954 Loss: 3173.495850\n",
      "Training Batch: 15955 Loss: 3204.610352\n",
      "Training Batch: 15956 Loss: 3153.557373\n",
      "Training Batch: 15957 Loss: 3280.688232\n",
      "Training Batch: 15958 Loss: 3085.370605\n",
      "Training Batch: 15959 Loss: 2996.091553\n",
      "Training Batch: 15960 Loss: 3086.142090\n",
      "Training Batch: 15961 Loss: 3194.861572\n",
      "Training Batch: 15962 Loss: 3296.122559\n",
      "Training Batch: 15963 Loss: 3232.411133\n",
      "Training Batch: 15964 Loss: 3077.617188\n",
      "Training Batch: 15965 Loss: 3061.270020\n",
      "Training Batch: 15966 Loss: 3142.723145\n",
      "Training Batch: 15967 Loss: 3016.888184\n",
      "Training Batch: 15968 Loss: 3141.394043\n",
      "Training Batch: 15969 Loss: 3028.907959\n",
      "Training Batch: 15970 Loss: 3036.927246\n",
      "Training Batch: 15971 Loss: 3018.426270\n",
      "Training Batch: 15972 Loss: 3109.909668\n",
      "Training Batch: 15973 Loss: 3188.456543\n",
      "Training Batch: 15974 Loss: 2986.399658\n",
      "Training Batch: 15975 Loss: 3009.086914\n",
      "Training Batch: 15976 Loss: 3283.872559\n",
      "Training Batch: 15977 Loss: 3166.480469\n",
      "Training Batch: 15978 Loss: 3145.561523\n",
      "Training Batch: 15979 Loss: 3182.222168\n",
      "Training Batch: 15980 Loss: 3098.689453\n",
      "Training Batch: 15981 Loss: 3169.077881\n",
      "Training Batch: 15982 Loss: 3079.901855\n",
      "Training Batch: 15983 Loss: 3088.111084\n",
      "Training Batch: 15984 Loss: 3220.393555\n",
      "Training Batch: 15985 Loss: 3143.062988\n",
      "Training Batch: 15986 Loss: 3019.274414\n",
      "Training Batch: 15987 Loss: 2963.708984\n",
      "Training Batch: 15988 Loss: 3123.948975\n",
      "Training Batch: 15989 Loss: 3180.958008\n",
      "Training Batch: 15990 Loss: 2925.483154\n",
      "Training Batch: 15991 Loss: 3216.884277\n",
      "Training Batch: 15992 Loss: 3081.810303\n",
      "Training Batch: 15993 Loss: 3069.492188\n",
      "Training Batch: 15994 Loss: 3069.798828\n",
      "Training Batch: 15995 Loss: 3034.664307\n",
      "Training Batch: 15996 Loss: 3007.247559\n",
      "Training Batch: 15997 Loss: 3192.017578\n",
      "Training Batch: 15998 Loss: 3121.305176\n",
      "Training Batch: 15999 Loss: 2978.472168\n",
      "Training Batch: 16000 Loss: 3029.406006\n",
      "Training Batch: 16001 Loss: 3091.214355\n",
      "Training Batch: 16002 Loss: 3042.453125\n",
      "Training Batch: 16003 Loss: 3223.602051\n",
      "Training Batch: 16004 Loss: 3224.040527\n",
      "Training Batch: 16005 Loss: 3095.431396\n",
      "Training Batch: 16006 Loss: 3124.374756\n",
      "Training Batch: 16007 Loss: 3084.207031\n",
      "Training Batch: 16008 Loss: 3129.513916\n",
      "Training Batch: 16009 Loss: 2992.479980\n",
      "Training Batch: 16010 Loss: 3340.142578\n",
      "Training Batch: 16011 Loss: 3016.068359\n",
      "Training Batch: 16012 Loss: 3109.382324\n",
      "Training Batch: 16013 Loss: 3036.497559\n",
      "Training Batch: 16014 Loss: 3373.834229\n",
      "Training Batch: 16015 Loss: 3296.991211\n",
      "Training Batch: 16016 Loss: 3187.214355\n",
      "Training Batch: 16017 Loss: 2987.392578\n",
      "Training Batch: 16018 Loss: 3022.369629\n",
      "Training Batch: 16019 Loss: 3133.915527\n",
      "Training Batch: 16020 Loss: 3468.915283\n",
      "Training Batch: 16021 Loss: 3210.159668\n",
      "Training Batch: 16022 Loss: 3127.174316\n",
      "Training Batch: 16023 Loss: 3315.859131\n",
      "Training Batch: 16024 Loss: 3139.579102\n",
      "Training Batch: 16025 Loss: 3182.604980\n",
      "Training Batch: 16026 Loss: 3287.431152\n",
      "Training Batch: 16027 Loss: 3302.292480\n",
      "Training Batch: 16028 Loss: 3168.744629\n",
      "Training Batch: 16029 Loss: 3130.584229\n",
      "Training Batch: 16030 Loss: 3273.713135\n",
      "Training Batch: 16031 Loss: 3143.089600\n",
      "Training Batch: 16032 Loss: 3012.490723\n",
      "Training Batch: 16033 Loss: 3162.163574\n",
      "Training Batch: 16034 Loss: 3316.628662\n",
      "Training Batch: 16035 Loss: 3096.810303\n",
      "Training Batch: 16036 Loss: 3036.444824\n",
      "Training Batch: 16037 Loss: 3123.524658\n",
      "Training Batch: 16038 Loss: 3106.084473\n",
      "Training Batch: 16039 Loss: 3264.682617\n",
      "Training Batch: 16040 Loss: 3242.204834\n",
      "Training Batch: 16041 Loss: 3077.289795\n",
      "Training Batch: 16042 Loss: 3211.198975\n",
      "Training Batch: 16043 Loss: 3222.644531\n",
      "Training Batch: 16044 Loss: 3122.565430\n",
      "Training Batch: 16045 Loss: 3149.921875\n",
      "Training Batch: 16046 Loss: 3122.439697\n",
      "Training Batch: 16047 Loss: 3138.231445\n",
      "Training Batch: 16048 Loss: 3067.482178\n",
      "Training Batch: 16049 Loss: 3001.880371\n",
      "Training Batch: 16050 Loss: 3184.750000\n",
      "Training Batch: 16051 Loss: 3059.196289\n",
      "Training Batch: 16052 Loss: 3079.621094\n",
      "Training Batch: 16053 Loss: 3125.594238\n",
      "Training Batch: 16054 Loss: 3175.319336\n",
      "Training Batch: 16055 Loss: 3177.533203\n",
      "Training Batch: 16056 Loss: 3419.821289\n",
      "Training Batch: 16057 Loss: 3064.024170\n",
      "Training Batch: 16058 Loss: 3181.584961\n",
      "Training Batch: 16059 Loss: 3166.626221\n",
      "Training Batch: 16060 Loss: 3111.452393\n",
      "Training Batch: 16061 Loss: 3018.822266\n",
      "Training Batch: 16062 Loss: 3033.139160\n",
      "Training Batch: 16063 Loss: 3006.022461\n",
      "Training Batch: 16064 Loss: 3014.296143\n",
      "Training Batch: 16065 Loss: 3158.153320\n",
      "Training Batch: 16066 Loss: 2964.570312\n",
      "Training Batch: 16067 Loss: 3067.169434\n",
      "Training Batch: 16068 Loss: 3008.952148\n",
      "Training Batch: 16069 Loss: 3141.543457\n",
      "Training Batch: 16070 Loss: 3112.901855\n",
      "Training Batch: 16071 Loss: 3066.590820\n",
      "Training Batch: 16072 Loss: 3075.935547\n",
      "Training Batch: 16073 Loss: 2963.505127\n",
      "Training Batch: 16074 Loss: 2969.063721\n",
      "Training Batch: 16075 Loss: 3147.269043\n",
      "Training Batch: 16076 Loss: 3122.419434\n",
      "Training Batch: 16077 Loss: 3060.895996\n",
      "Training Batch: 16078 Loss: 3017.671875\n",
      "Training Batch: 16079 Loss: 3098.686279\n",
      "Training Batch: 16080 Loss: 3074.305664\n",
      "Training Batch: 16081 Loss: 2997.776855\n",
      "Training Batch: 16082 Loss: 3049.371094\n",
      "Training Batch: 16083 Loss: 2966.260742\n",
      "Training Batch: 16084 Loss: 3256.764160\n",
      "Training Batch: 16085 Loss: 3091.562256\n",
      "Training Batch: 16086 Loss: 3288.861328\n",
      "Training Batch: 16087 Loss: 3450.231445\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Batch: 16088 Loss: 3181.356934\n",
      "Training Batch: 16089 Loss: 3111.472412\n",
      "Training Batch: 16090 Loss: 2983.629883\n",
      "Training Batch: 16091 Loss: 3033.933105\n",
      "Training Batch: 16092 Loss: 3125.891846\n",
      "Training Batch: 16093 Loss: 3022.099121\n",
      "Training Batch: 16094 Loss: 3033.614258\n",
      "Training Batch: 16095 Loss: 3166.742920\n",
      "Training Batch: 16096 Loss: 3207.295898\n",
      "Training Batch: 16097 Loss: 3024.855469\n",
      "Training Batch: 16098 Loss: 3096.155273\n",
      "Training Batch: 16099 Loss: 3028.893311\n",
      "Training Batch: 16100 Loss: 2963.455078\n",
      "Training Batch: 16101 Loss: 3105.464844\n",
      "Training Batch: 16102 Loss: 3050.000244\n",
      "Training Batch: 16103 Loss: 2990.888672\n",
      "Training Batch: 16104 Loss: 2982.050781\n",
      "Training Batch: 16105 Loss: 3154.581543\n",
      "Training Batch: 16106 Loss: 3115.918945\n",
      "Training Batch: 16107 Loss: 3024.957520\n",
      "Training Batch: 16108 Loss: 3114.098145\n",
      "Training Batch: 16109 Loss: 3035.245605\n",
      "Training Batch: 16110 Loss: 3298.531738\n",
      "Training Batch: 16111 Loss: 3121.628418\n",
      "Training Batch: 16112 Loss: 3003.352783\n",
      "Training Batch: 16113 Loss: 3317.329346\n",
      "Training Batch: 16114 Loss: 3005.907715\n",
      "Training Batch: 16115 Loss: 3014.852051\n",
      "Training Batch: 16116 Loss: 3097.793945\n",
      "Training Batch: 16117 Loss: 3346.529297\n",
      "Training Batch: 16118 Loss: 3159.122559\n",
      "Training Batch: 16119 Loss: 3471.327637\n",
      "Training Batch: 16120 Loss: 3271.172852\n",
      "Training Batch: 16121 Loss: 3166.184570\n",
      "Training Batch: 16122 Loss: 3235.278809\n",
      "Training Batch: 16123 Loss: 3122.942383\n",
      "Training Batch: 16124 Loss: 3100.887695\n",
      "Training Batch: 16125 Loss: 3179.823730\n",
      "Training Batch: 16126 Loss: 3116.089844\n",
      "Training Batch: 16127 Loss: 3124.167480\n",
      "Training Batch: 16128 Loss: 3037.759766\n",
      "Training Batch: 16129 Loss: 3179.466797\n",
      "Training Batch: 16130 Loss: 3031.763184\n",
      "Training Batch: 16131 Loss: 3051.628418\n",
      "Training Batch: 16132 Loss: 3101.101562\n",
      "Training Batch: 16133 Loss: 3468.631348\n",
      "Training Batch: 16134 Loss: 3313.272461\n",
      "Training Batch: 16135 Loss: 3167.348633\n",
      "Training Batch: 16136 Loss: 3309.111572\n",
      "Training Batch: 16137 Loss: 3116.992188\n",
      "Training Batch: 16138 Loss: 2996.586182\n",
      "Training Batch: 16139 Loss: 3187.964355\n",
      "Training Batch: 16140 Loss: 3082.174805\n",
      "Training Batch: 16141 Loss: 3300.961426\n",
      "Training Batch: 16142 Loss: 3158.102051\n",
      "Training Batch: 16143 Loss: 3185.586182\n",
      "Training Batch: 16144 Loss: 3330.808594\n",
      "Training Batch: 16145 Loss: 3355.298828\n",
      "Training Batch: 16146 Loss: 3091.788574\n",
      "Training Batch: 16147 Loss: 3101.919678\n",
      "Training Batch: 16148 Loss: 3043.190918\n",
      "Training Batch: 16149 Loss: 3041.708008\n",
      "Training Batch: 16150 Loss: 3167.508301\n",
      "Training Batch: 16151 Loss: 3396.225586\n",
      "Training Batch: 16152 Loss: 3352.588623\n",
      "Training Batch: 16153 Loss: 3142.828125\n",
      "Training Batch: 16154 Loss: 3010.200684\n",
      "Training Batch: 16155 Loss: 3027.102539\n",
      "Training Batch: 16156 Loss: 3095.592285\n",
      "Training Batch: 16157 Loss: 3134.762207\n",
      "Training Batch: 16158 Loss: 3197.583984\n",
      "Training Batch: 16159 Loss: 3152.187012\n",
      "Training Batch: 16160 Loss: 3278.917480\n",
      "Training Batch: 16161 Loss: 3258.598633\n",
      "Training Batch: 16162 Loss: 3212.655273\n",
      "Training Batch: 16163 Loss: 3274.759766\n",
      "Training Batch: 16164 Loss: 3360.057129\n",
      "Training Batch: 16165 Loss: 3275.489258\n",
      "Training Batch: 16166 Loss: 3115.668701\n",
      "Training Batch: 16167 Loss: 3013.994873\n",
      "Training Batch: 16168 Loss: 3176.530518\n",
      "Training Batch: 16169 Loss: 3281.382324\n",
      "Training Batch: 16170 Loss: 3136.783203\n",
      "Training Batch: 16171 Loss: 3163.419678\n",
      "Training Batch: 16172 Loss: 3218.713867\n",
      "Training Batch: 16173 Loss: 3087.904297\n",
      "Training Batch: 16174 Loss: 3080.450195\n",
      "Training Batch: 16175 Loss: 3082.470703\n",
      "Training Batch: 16176 Loss: 3000.648682\n",
      "Training Batch: 16177 Loss: 3052.226807\n",
      "Training Batch: 16178 Loss: 3148.196533\n",
      "Training Batch: 16179 Loss: 3055.121582\n",
      "Training Batch: 16180 Loss: 3050.836670\n",
      "Training Batch: 16181 Loss: 3127.725098\n",
      "Training Batch: 16182 Loss: 3042.127441\n",
      "Training Batch: 16183 Loss: 3096.295410\n",
      "Training Batch: 16184 Loss: 3092.201172\n",
      "Training Batch: 16185 Loss: 3064.908447\n",
      "Training Batch: 16186 Loss: 3090.734619\n",
      "Training Batch: 16187 Loss: 3030.767334\n",
      "Training Batch: 16188 Loss: 3071.853027\n",
      "Training Batch: 16189 Loss: 3084.427002\n",
      "Training Batch: 16190 Loss: 3081.791504\n",
      "Training Batch: 16191 Loss: 3042.087891\n",
      "Training Batch: 16192 Loss: 3063.130371\n",
      "Training Batch: 16193 Loss: 3011.929443\n",
      "Training Batch: 16194 Loss: 3138.788574\n",
      "Training Batch: 16195 Loss: 3182.119141\n",
      "Training Batch: 16196 Loss: 3046.187012\n",
      "Training Batch: 16197 Loss: 3088.490234\n",
      "Training Batch: 16198 Loss: 3137.044922\n",
      "Training Batch: 16199 Loss: 3055.919434\n",
      "Training Batch: 16200 Loss: 2976.166504\n",
      "Training Batch: 16201 Loss: 3099.287598\n",
      "Training Batch: 16202 Loss: 3091.729492\n",
      "Training Batch: 16203 Loss: 3122.659668\n",
      "Training Batch: 16204 Loss: 2991.436035\n",
      "Training Batch: 16205 Loss: 3015.049805\n",
      "Training Batch: 16206 Loss: 2933.004395\n",
      "Training Batch: 16207 Loss: 3021.811279\n",
      "Training Batch: 16208 Loss: 3134.093750\n",
      "Training Batch: 16209 Loss: 3078.541992\n",
      "Training Batch: 16210 Loss: 3028.435547\n",
      "Training Batch: 16211 Loss: 3203.791992\n",
      "Training Batch: 16212 Loss: 3227.341797\n",
      "Training Batch: 16213 Loss: 3039.610107\n",
      "Training Batch: 16214 Loss: 3136.413574\n",
      "Training Batch: 16215 Loss: 3028.723633\n",
      "Training Batch: 16216 Loss: 3075.122070\n",
      "Training Batch: 16217 Loss: 3201.856445\n",
      "Training Batch: 16218 Loss: 3092.890137\n",
      "Training Batch: 16219 Loss: 3181.117188\n",
      "Training Batch: 16220 Loss: 3089.058350\n",
      "Training Batch: 16221 Loss: 3243.632324\n",
      "Training Batch: 16222 Loss: 3295.016113\n",
      "Training Batch: 16223 Loss: 3027.738281\n",
      "Training Batch: 16224 Loss: 2922.173828\n",
      "Training Batch: 16225 Loss: 2961.847412\n",
      "Training Batch: 16226 Loss: 3279.090088\n",
      "Training Batch: 16227 Loss: 3054.557617\n",
      "Training Batch: 16228 Loss: 2945.112793\n",
      "Training Batch: 16229 Loss: 2929.325195\n",
      "Training Batch: 16230 Loss: 3092.671143\n",
      "Training Batch: 16231 Loss: 3001.778809\n",
      "Training Batch: 16232 Loss: 3279.176270\n",
      "Training Batch: 16233 Loss: 3180.282227\n",
      "Training Batch: 16234 Loss: 3173.144775\n",
      "Training Batch: 16235 Loss: 3108.782959\n",
      "Training Batch: 16236 Loss: 3050.495605\n",
      "Training Batch: 16237 Loss: 3044.085938\n",
      "Training Batch: 16238 Loss: 3084.823730\n",
      "Training Batch: 16239 Loss: 3158.329590\n",
      "Training Batch: 16240 Loss: 3014.584961\n",
      "Training Batch: 16241 Loss: 3106.895508\n",
      "Training Batch: 16242 Loss: 3216.053223\n",
      "Training Batch: 16243 Loss: 3084.688965\n",
      "Training Batch: 16244 Loss: 3020.094727\n",
      "Training Batch: 16245 Loss: 2989.864258\n",
      "Training Batch: 16246 Loss: 3113.641602\n",
      "Training Batch: 16247 Loss: 3171.637695\n",
      "Training Batch: 16248 Loss: 3050.360840\n",
      "Training Batch: 16249 Loss: 3105.156738\n",
      "Training Batch: 16250 Loss: 3183.969482\n",
      "Training Batch: 16251 Loss: 2904.706299\n",
      "Training Batch: 16252 Loss: 3069.211426\n",
      "Training Batch: 16253 Loss: 3073.983643\n",
      "Training Batch: 16254 Loss: 3022.930176\n",
      "Training Batch: 16255 Loss: 2963.585693\n",
      "Training Batch: 16256 Loss: 2980.046875\n",
      "Training Batch: 16257 Loss: 3284.031738\n",
      "Training Batch: 16258 Loss: 3058.541992\n",
      "Training Batch: 16259 Loss: 3060.755371\n",
      "Training Batch: 16260 Loss: 3084.444580\n",
      "Training Batch: 16261 Loss: 2909.142090\n",
      "Training Batch: 16262 Loss: 3143.422852\n",
      "Training Batch: 16263 Loss: 3271.812500\n",
      "Training Batch: 16264 Loss: 3310.817871\n",
      "Training Batch: 16265 Loss: 3159.739014\n",
      "Training Batch: 16266 Loss: 3182.187988\n",
      "Training Batch: 16267 Loss: 3353.548340\n",
      "Training Batch: 16268 Loss: 2990.515625\n",
      "Training Batch: 16269 Loss: 2948.495605\n",
      "Training Batch: 16270 Loss: 3074.083984\n",
      "Training Batch: 16271 Loss: 2983.778809\n",
      "Training Batch: 16272 Loss: 2972.507324\n",
      "Training Batch: 16273 Loss: 3074.947266\n",
      "Training Batch: 16274 Loss: 3111.429199\n",
      "Training Batch: 16275 Loss: 3097.865234\n",
      "Training Batch: 16276 Loss: 3340.814453\n",
      "Training Batch: 16277 Loss: 2963.240723\n",
      "Training Batch: 16278 Loss: 3072.816650\n",
      "Training Batch: 16279 Loss: 2972.741943\n",
      "Training Batch: 16280 Loss: 3042.515381\n",
      "Training Batch: 16281 Loss: 3006.576172\n",
      "Training Batch: 16282 Loss: 3087.830566\n",
      "Training Batch: 16283 Loss: 3153.419189\n",
      "Training Batch: 16284 Loss: 3084.428223\n",
      "Training Batch: 16285 Loss: 3054.591064\n",
      "Training Batch: 16286 Loss: 3085.093262\n",
      "Training Batch: 16287 Loss: 3258.944336\n",
      "Training Batch: 16288 Loss: 3270.558594\n",
      "Training Batch: 16289 Loss: 3072.489746\n",
      "Training Batch: 16290 Loss: 3110.135986\n",
      "Training Batch: 16291 Loss: 3020.082764\n",
      "Training Batch: 16292 Loss: 3059.995361\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Batch: 16293 Loss: 3015.776611\n",
      "Training Batch: 16294 Loss: 3021.384277\n",
      "Training Batch: 16295 Loss: 3065.477051\n",
      "Training Batch: 16296 Loss: 3020.902588\n",
      "Training Batch: 16297 Loss: 3055.858643\n",
      "Training Batch: 16298 Loss: 3057.282959\n",
      "Training Batch: 16299 Loss: 3092.091064\n",
      "Training Batch: 16300 Loss: 3104.771484\n",
      "Training Batch: 16301 Loss: 3176.169434\n",
      "Training Batch: 16302 Loss: 3189.071045\n",
      "Training Batch: 16303 Loss: 3093.926758\n",
      "Training Batch: 16304 Loss: 3247.963623\n",
      "Training Batch: 16305 Loss: 3001.349609\n",
      "Training Batch: 16306 Loss: 3028.749512\n",
      "Training Batch: 16307 Loss: 2980.921875\n",
      "Training Batch: 16308 Loss: 3020.156250\n",
      "Training Batch: 16309 Loss: 3048.625244\n",
      "Training Batch: 16310 Loss: 3078.755371\n",
      "Training Batch: 16311 Loss: 3660.086426\n",
      "Training Batch: 16312 Loss: 3063.148926\n",
      "Training Batch: 16313 Loss: 3075.904297\n",
      "Training Batch: 16314 Loss: 3218.170410\n",
      "Training Batch: 16315 Loss: 3405.539551\n",
      "Training Batch: 16316 Loss: 3113.918945\n",
      "Training Batch: 16317 Loss: 3054.873047\n",
      "Training Batch: 16318 Loss: 3073.716309\n",
      "Training Batch: 16319 Loss: 2954.783691\n",
      "Training Batch: 16320 Loss: 3009.744141\n",
      "Training Batch: 16321 Loss: 3013.698975\n",
      "Training Batch: 16322 Loss: 3158.551270\n",
      "Training Batch: 16323 Loss: 3095.811035\n",
      "Training Batch: 16324 Loss: 3115.242432\n",
      "Training Batch: 16325 Loss: 3198.873291\n",
      "Training Batch: 16326 Loss: 3140.722656\n",
      "Training Batch: 16327 Loss: 3039.996094\n",
      "Training Batch: 16328 Loss: 3293.252930\n",
      "Training Batch: 16329 Loss: 3042.326172\n",
      "Training Batch: 16330 Loss: 3134.992188\n",
      "Training Batch: 16331 Loss: 2986.067871\n",
      "Training Batch: 16332 Loss: 3189.856445\n",
      "Training Batch: 16333 Loss: 3140.612793\n",
      "Training Batch: 16334 Loss: 3211.312988\n",
      "Training Batch: 16335 Loss: 3100.025879\n",
      "Training Batch: 16336 Loss: 3065.109863\n",
      "Training Batch: 16337 Loss: 3032.063477\n",
      "Training Batch: 16338 Loss: 3126.618652\n",
      "Training Batch: 16339 Loss: 3012.122314\n",
      "Training Batch: 16340 Loss: 3113.872070\n",
      "Training Batch: 16341 Loss: 3053.461914\n",
      "Training Batch: 16342 Loss: 2977.599854\n",
      "Training Batch: 16343 Loss: 3190.649658\n",
      "Training Batch: 16344 Loss: 2971.976074\n",
      "Training Batch: 16345 Loss: 3126.255859\n",
      "Training Batch: 16346 Loss: 2969.645996\n",
      "Training Batch: 16347 Loss: 3064.186035\n",
      "Training Batch: 16348 Loss: 3049.856201\n",
      "Training Batch: 16349 Loss: 3034.586914\n",
      "Training Batch: 16350 Loss: 3047.697510\n",
      "Training Batch: 16351 Loss: 3062.654053\n",
      "Training Batch: 16352 Loss: 3005.551270\n",
      "Training Batch: 16353 Loss: 3002.080566\n",
      "Training Batch: 16354 Loss: 3023.475586\n",
      "Training Batch: 16355 Loss: 3002.578125\n",
      "Training Batch: 16356 Loss: 3128.094238\n",
      "Training Batch: 16357 Loss: 2999.011230\n",
      "Training Batch: 16358 Loss: 2969.429443\n",
      "Training Batch: 16359 Loss: 3012.645508\n",
      "Training Batch: 16360 Loss: 3064.927734\n",
      "Training Batch: 16361 Loss: 3092.841797\n",
      "Training Batch: 16362 Loss: 3094.320312\n",
      "Training Batch: 16363 Loss: 3106.916504\n",
      "Training Batch: 16364 Loss: 3125.679443\n",
      "Training Batch: 16365 Loss: 3108.792969\n",
      "Training Batch: 16366 Loss: 3040.656494\n",
      "Training Batch: 16367 Loss: 3134.803955\n",
      "Training Batch: 16368 Loss: 3014.720947\n",
      "Training Batch: 16369 Loss: 3146.493164\n",
      "Training Batch: 16370 Loss: 3387.133545\n",
      "Training Batch: 16371 Loss: 2954.632324\n",
      "Training Batch: 16372 Loss: 3167.961914\n",
      "Training Batch: 16373 Loss: 3102.919922\n",
      "Training Batch: 16374 Loss: 3131.294434\n",
      "Training Batch: 16375 Loss: 3234.338379\n",
      "Training Batch: 16376 Loss: 3245.555664\n",
      "Training Batch: 16377 Loss: 3039.280273\n",
      "Training Batch: 16378 Loss: 3356.425293\n",
      "Training Batch: 16379 Loss: 3057.104980\n",
      "Training Batch: 16380 Loss: 2959.991699\n",
      "Training Batch: 16381 Loss: 3033.651855\n",
      "Training Batch: 16382 Loss: 2988.172363\n",
      "Training Batch: 16383 Loss: 3025.250488\n",
      "Training Batch: 16384 Loss: 3157.562012\n",
      "Training Batch: 16385 Loss: 3066.256836\n",
      "Training Batch: 16386 Loss: 3037.258301\n",
      "Training Batch: 16387 Loss: 3107.730713\n",
      "Training Batch: 16388 Loss: 3067.739746\n",
      "Training Batch: 16389 Loss: 3057.715332\n",
      "Training Batch: 16390 Loss: 3007.424561\n",
      "Training Batch: 16391 Loss: 3155.654297\n",
      "Training Batch: 16392 Loss: 3183.456299\n",
      "Training Batch: 16393 Loss: 3234.972168\n",
      "Training Batch: 16394 Loss: 3101.015137\n",
      "Training Batch: 16395 Loss: 3065.011230\n",
      "Training Batch: 16396 Loss: 3126.377441\n",
      "Training Batch: 16397 Loss: 3233.466797\n",
      "Training Batch: 16398 Loss: 3098.001953\n",
      "Training Batch: 16399 Loss: 2992.813232\n",
      "Training Batch: 16400 Loss: 3227.634277\n",
      "Training Batch: 16401 Loss: 3183.682373\n",
      "Training Batch: 16402 Loss: 3127.523193\n",
      "Training Batch: 16403 Loss: 3065.966309\n",
      "Training Batch: 16404 Loss: 3056.932129\n",
      "Training Batch: 16405 Loss: 3192.142578\n",
      "Training Batch: 16406 Loss: 2980.399902\n",
      "Training Batch: 16407 Loss: 3122.060059\n",
      "Training Batch: 16408 Loss: 3201.158447\n",
      "Training Batch: 16409 Loss: 3097.778809\n",
      "Training Batch: 16410 Loss: 2966.707520\n",
      "Training Batch: 16411 Loss: 2986.014160\n",
      "Training Batch: 16412 Loss: 2983.113770\n",
      "Training Batch: 16413 Loss: 3072.818359\n",
      "Training Batch: 16414 Loss: 3112.825195\n",
      "Training Batch: 16415 Loss: 3121.605469\n",
      "Training Batch: 16416 Loss: 3052.294189\n",
      "Training Batch: 16417 Loss: 3007.517090\n",
      "Training Batch: 16418 Loss: 2967.877441\n",
      "Training Batch: 16419 Loss: 3209.922852\n",
      "Training Batch: 16420 Loss: 3132.935547\n",
      "Training Batch: 16421 Loss: 2993.073242\n",
      "Training Batch: 16422 Loss: 3244.499023\n",
      "Training Batch: 16423 Loss: 3071.893066\n",
      "Training Batch: 16424 Loss: 3071.854492\n",
      "Training Batch: 16425 Loss: 2960.038818\n",
      "Training Batch: 16426 Loss: 3003.951416\n",
      "Training Batch: 16427 Loss: 3028.977539\n",
      "Training Batch: 16428 Loss: 3181.352539\n",
      "Training Batch: 16429 Loss: 3032.291260\n",
      "Training Batch: 16430 Loss: 3131.156738\n",
      "Training Batch: 16431 Loss: 3090.561035\n",
      "Training Batch: 16432 Loss: 3153.299805\n",
      "Training Batch: 16433 Loss: 3102.914551\n",
      "Training Batch: 16434 Loss: 3161.871582\n",
      "Training Batch: 16435 Loss: 3107.657959\n",
      "Training Batch: 16436 Loss: 3196.946777\n",
      "Training Batch: 16437 Loss: 2965.561523\n",
      "Training Batch: 16438 Loss: 3063.435059\n",
      "Training Batch: 16439 Loss: 2996.359619\n",
      "Training Batch: 16440 Loss: 2997.197266\n",
      "Training Batch: 16441 Loss: 2987.850830\n",
      "Training Batch: 16442 Loss: 3067.138428\n",
      "Training Batch: 16443 Loss: 3076.520508\n",
      "Training Batch: 16444 Loss: 3192.892822\n",
      "Training Batch: 16445 Loss: 3054.463379\n",
      "Training Batch: 16446 Loss: 3245.046143\n",
      "Training Batch: 16447 Loss: 3113.796387\n",
      "Training Batch: 16448 Loss: 3060.084229\n",
      "Training Batch: 16449 Loss: 3309.468750\n",
      "Training Batch: 16450 Loss: 2979.384766\n",
      "Training Batch: 16451 Loss: 3076.556641\n",
      "Training Batch: 16452 Loss: 3247.615967\n",
      "Training Batch: 16453 Loss: 3091.893066\n",
      "Training Batch: 16454 Loss: 3309.603271\n",
      "Training Batch: 16455 Loss: 3051.909424\n",
      "Training Batch: 16456 Loss: 3233.883789\n",
      "Training Batch: 16457 Loss: 3199.951172\n",
      "Training Batch: 16458 Loss: 3073.463867\n",
      "Training Batch: 16459 Loss: 3250.739014\n",
      "Training Batch: 16460 Loss: 3167.914307\n",
      "Training Batch: 16461 Loss: 2980.125000\n",
      "Training Batch: 16462 Loss: 3062.268066\n",
      "Training Batch: 16463 Loss: 3106.186035\n",
      "Training Batch: 16464 Loss: 3485.463623\n",
      "Training Batch: 16465 Loss: 3123.974609\n",
      "Training Batch: 16466 Loss: 3090.089844\n",
      "Training Batch: 16467 Loss: 2965.350098\n",
      "Training Batch: 16468 Loss: 3106.562988\n",
      "Training Batch: 16469 Loss: 3132.851074\n",
      "Training Batch: 16470 Loss: 3026.367920\n",
      "Training Batch: 16471 Loss: 3015.937744\n",
      "Training Batch: 16472 Loss: 3160.543945\n",
      "Training Batch: 16473 Loss: 3102.649902\n",
      "Training Batch: 16474 Loss: 3016.740234\n",
      "Training Batch: 16475 Loss: 3176.875488\n",
      "Training Batch: 16476 Loss: 3004.875000\n",
      "Training Batch: 16477 Loss: 2999.092773\n",
      "Training Batch: 16478 Loss: 3174.110352\n",
      "Training Batch: 16479 Loss: 3099.528809\n",
      "Training Batch: 16480 Loss: 3083.737549\n",
      "Training Batch: 16481 Loss: 3125.126465\n",
      "Training Batch: 16482 Loss: 3082.382568\n",
      "Training Batch: 16483 Loss: 3079.587891\n",
      "Training Batch: 16484 Loss: 3023.839355\n",
      "Training Batch: 16485 Loss: 3011.514893\n",
      "Training Batch: 16486 Loss: 3043.763916\n",
      "Training Batch: 16487 Loss: 2963.330811\n",
      "Training Batch: 16488 Loss: 3116.646973\n",
      "Training Batch: 16489 Loss: 3037.452637\n",
      "Training Batch: 16490 Loss: 3097.110596\n",
      "Training Batch: 16491 Loss: 3131.222900\n",
      "Training Batch: 16492 Loss: 2985.003662\n",
      "Training Batch: 16493 Loss: 3171.778809\n",
      "Training Batch: 16494 Loss: 3051.724365\n",
      "Training Batch: 16495 Loss: 3082.374512\n",
      "Training Batch: 16496 Loss: 3082.704834\n",
      "Training Batch: 16497 Loss: 2970.419434\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Batch: 16498 Loss: 3107.757324\n",
      "Training Batch: 16499 Loss: 3170.091309\n",
      "Training Batch: 16500 Loss: 3226.533447\n",
      "Training Batch: 16501 Loss: 3038.825928\n",
      "Training Batch: 16502 Loss: 3107.187744\n",
      "Training Batch: 16503 Loss: 3079.928223\n",
      "Training Batch: 16504 Loss: 3047.414551\n",
      "Training Batch: 16505 Loss: 2924.078613\n",
      "Training Batch: 16506 Loss: 3213.713867\n",
      "Training Batch: 16507 Loss: 3080.934082\n",
      "Training Batch: 16508 Loss: 2999.437988\n",
      "Training Batch: 16509 Loss: 3138.350098\n",
      "Training Batch: 16510 Loss: 3111.538330\n",
      "Training Batch: 16511 Loss: 3115.197021\n",
      "Training Batch: 16512 Loss: 3062.772461\n",
      "Training Batch: 16513 Loss: 2940.189941\n",
      "Training Batch: 16514 Loss: 3115.215576\n",
      "Training Batch: 16515 Loss: 3104.797363\n",
      "Training Batch: 16516 Loss: 3128.992432\n",
      "Training Batch: 16517 Loss: 2995.915039\n",
      "Training Batch: 16518 Loss: 3020.597656\n",
      "Training Batch: 16519 Loss: 3051.069336\n",
      "Training Batch: 16520 Loss: 3252.370850\n",
      "Training Batch: 16521 Loss: 3047.092041\n",
      "Training Batch: 16522 Loss: 3214.428955\n",
      "Training Batch: 16523 Loss: 3124.430420\n",
      "Training Batch: 16524 Loss: 3061.937500\n",
      "Training Batch: 16525 Loss: 3014.713623\n",
      "Training Batch: 16526 Loss: 3135.884277\n",
      "Training Batch: 16527 Loss: 3016.646973\n",
      "Training Batch: 16528 Loss: 2979.604980\n",
      "Training Batch: 16529 Loss: 3016.380859\n",
      "Training Batch: 16530 Loss: 2968.728027\n",
      "Training Batch: 16531 Loss: 2975.207520\n",
      "Training Batch: 16532 Loss: 3041.601074\n",
      "Training Batch: 16533 Loss: 2966.946289\n",
      "Training Batch: 16534 Loss: 3329.440918\n",
      "Training Batch: 16535 Loss: 3124.452637\n",
      "Training Batch: 16536 Loss: 2964.630371\n",
      "Training Batch: 16537 Loss: 3004.965576\n",
      "Training Batch: 16538 Loss: 2961.743652\n",
      "Training Batch: 16539 Loss: 2942.006348\n",
      "Training Batch: 16540 Loss: 3091.637207\n",
      "Training Batch: 16541 Loss: 3041.273438\n",
      "Training Batch: 16542 Loss: 2998.922363\n",
      "Training Batch: 16543 Loss: 3052.031738\n",
      "Training Batch: 16544 Loss: 3063.855225\n",
      "Training Batch: 16545 Loss: 3034.838135\n",
      "Training Batch: 16546 Loss: 3125.356934\n",
      "Training Batch: 16547 Loss: 2961.864258\n",
      "Training Batch: 16548 Loss: 3092.046143\n",
      "Training Batch: 16549 Loss: 3335.266602\n",
      "Training Batch: 16550 Loss: 3181.509766\n",
      "Training Batch: 16551 Loss: 3252.993896\n",
      "Training Batch: 16552 Loss: 3628.655762\n",
      "Training Batch: 16553 Loss: 3209.936279\n",
      "Training Batch: 16554 Loss: 3111.838379\n",
      "Training Batch: 16555 Loss: 3100.166016\n",
      "Training Batch: 16556 Loss: 3062.274902\n",
      "Training Batch: 16557 Loss: 3120.322510\n",
      "Training Batch: 16558 Loss: 3027.894531\n",
      "Training Batch: 16559 Loss: 3197.820312\n",
      "Training Batch: 16560 Loss: 3039.417725\n",
      "Training Batch: 16561 Loss: 3155.666260\n",
      "Training Batch: 16562 Loss: 3102.467529\n",
      "Training Batch: 16563 Loss: 2983.706787\n",
      "Training Batch: 16564 Loss: 3180.603516\n",
      "Training Batch: 16565 Loss: 3104.124756\n",
      "Training Batch: 16566 Loss: 3378.502441\n",
      "Training Batch: 16567 Loss: 3228.419678\n",
      "Training Batch: 16568 Loss: 3007.104004\n",
      "Training Batch: 16569 Loss: 3188.891113\n",
      "Training Batch: 16570 Loss: 2966.337402\n",
      "Training Batch: 16571 Loss: 3194.037109\n",
      "Training Batch: 16572 Loss: 3316.487793\n",
      "Training Batch: 16573 Loss: 3263.304199\n",
      "Training Batch: 16574 Loss: 3193.555420\n",
      "Training Batch: 16575 Loss: 3115.851318\n",
      "Training Batch: 16576 Loss: 3140.139160\n",
      "Training Batch: 16577 Loss: 3033.181885\n",
      "Training Batch: 16578 Loss: 3174.424316\n",
      "Training Batch: 16579 Loss: 3073.268555\n",
      "Training Batch: 16580 Loss: 3059.822754\n",
      "Training Batch: 16581 Loss: 3042.180176\n",
      "Training Batch: 16582 Loss: 3157.922363\n",
      "Training Batch: 16583 Loss: 3285.975830\n",
      "Training Batch: 16584 Loss: 3286.218262\n",
      "Training Batch: 16585 Loss: 3217.691895\n",
      "Training Batch: 16586 Loss: 3159.835449\n",
      "Training Batch: 16587 Loss: 3186.814453\n",
      "Training Batch: 16588 Loss: 3000.400391\n",
      "Training Batch: 16589 Loss: 3200.957520\n",
      "Training Batch: 16590 Loss: 3099.188965\n",
      "Training Batch: 16591 Loss: 2993.720947\n",
      "Training Batch: 16592 Loss: 3083.690186\n",
      "Training Batch: 16593 Loss: 2953.777344\n",
      "Training Batch: 16594 Loss: 3001.674805\n",
      "Training Batch: 16595 Loss: 3144.461426\n",
      "Training Batch: 16596 Loss: 3291.392822\n",
      "Training Batch: 16597 Loss: 3056.281738\n",
      "Training Batch: 16598 Loss: 3108.894043\n",
      "Training Batch: 16599 Loss: 3050.516602\n",
      "Training Batch: 16600 Loss: 3053.171875\n",
      "Training Batch: 16601 Loss: 3047.717773\n",
      "Training Batch: 16602 Loss: 3044.684082\n",
      "Training Batch: 16603 Loss: 3341.186035\n",
      "Training Batch: 16604 Loss: 3367.611328\n",
      "Training Batch: 16605 Loss: 3126.812012\n",
      "Training Batch: 16606 Loss: 3141.746094\n",
      "Training Batch: 16607 Loss: 3043.981934\n",
      "Training Batch: 16608 Loss: 3317.853027\n",
      "Training Batch: 16609 Loss: 3160.799316\n",
      "Training Batch: 16610 Loss: 3308.469727\n",
      "Training Batch: 16611 Loss: 3122.795410\n",
      "Training Batch: 16612 Loss: 3045.953857\n",
      "Training Batch: 16613 Loss: 3071.295898\n",
      "Training Batch: 16614 Loss: 3090.675781\n",
      "Training Batch: 16615 Loss: 2972.759521\n",
      "Training Batch: 16616 Loss: 3058.241211\n",
      "Training Batch: 16617 Loss: 3064.582031\n",
      "Training Batch: 16618 Loss: 3084.597656\n",
      "Training Batch: 16619 Loss: 3052.326660\n",
      "Training Batch: 16620 Loss: 2998.555176\n",
      "Training Batch: 16621 Loss: 3016.404785\n",
      "Training Batch: 16622 Loss: 3184.728027\n",
      "Training Batch: 16623 Loss: 3010.024658\n",
      "Training Batch: 16624 Loss: 3129.276611\n",
      "Training Batch: 16625 Loss: 3298.166992\n",
      "Training Batch: 16626 Loss: 3223.106445\n",
      "Training Batch: 16627 Loss: 3211.537598\n",
      "Training Batch: 16628 Loss: 3150.235352\n",
      "Training Batch: 16629 Loss: 3211.176025\n",
      "Training Batch: 16630 Loss: 3174.461914\n",
      "Training Batch: 16631 Loss: 3157.718750\n",
      "Training Batch: 16632 Loss: 3177.841797\n",
      "Training Batch: 16633 Loss: 3009.327148\n",
      "Training Batch: 16634 Loss: 3267.881348\n",
      "Training Batch: 16635 Loss: 3044.682129\n",
      "Training Batch: 16636 Loss: 3071.318359\n",
      "Training Batch: 16637 Loss: 3216.235352\n",
      "Training Batch: 16638 Loss: 3196.173340\n",
      "Training Batch: 16639 Loss: 3164.682617\n",
      "Training Batch: 16640 Loss: 3149.365234\n",
      "Training Batch: 16641 Loss: 3171.160400\n",
      "Training Batch: 16642 Loss: 3162.746582\n",
      "Training Batch: 16643 Loss: 3183.495361\n",
      "Training Batch: 16644 Loss: 3049.337158\n",
      "Training Batch: 16645 Loss: 3024.952637\n",
      "Training Batch: 16646 Loss: 3174.463867\n",
      "Training Batch: 16647 Loss: 3115.562012\n",
      "Training Batch: 16648 Loss: 3118.093994\n",
      "Training Batch: 16649 Loss: 2995.583984\n",
      "Training Batch: 16650 Loss: 3085.258789\n",
      "Training Batch: 16651 Loss: 3118.710938\n",
      "Training Batch: 16652 Loss: 3122.543701\n",
      "Training Batch: 16653 Loss: 3193.775391\n",
      "Training Batch: 16654 Loss: 3272.934570\n",
      "Training Batch: 16655 Loss: 3281.463867\n",
      "Training Batch: 16656 Loss: 3010.137695\n",
      "Training Batch: 16657 Loss: 3119.968506\n",
      "Training Batch: 16658 Loss: 2990.773438\n",
      "Training Batch: 16659 Loss: 3127.299316\n",
      "Training Batch: 16660 Loss: 3115.760742\n",
      "Training Batch: 16661 Loss: 3062.292236\n",
      "Training Batch: 16662 Loss: 3025.772217\n",
      "Training Batch: 16663 Loss: 2972.283203\n",
      "Training Batch: 16664 Loss: 3052.676758\n",
      "Training Batch: 16665 Loss: 3088.297363\n",
      "Training Batch: 16666 Loss: 3594.558105\n",
      "Training Batch: 16667 Loss: 3029.016113\n",
      "Training Batch: 16668 Loss: 3071.055176\n",
      "Training Batch: 16669 Loss: 3063.959473\n",
      "Training Batch: 16670 Loss: 3144.042480\n",
      "Training Batch: 16671 Loss: 3206.905518\n",
      "Training Batch: 16672 Loss: 3066.195801\n",
      "Training Batch: 16673 Loss: 3085.597656\n",
      "Training Batch: 16674 Loss: 3168.418945\n",
      "Training Batch: 16675 Loss: 3038.393799\n",
      "Training Batch: 16676 Loss: 3033.869141\n",
      "Training Batch: 16677 Loss: 3171.218750\n",
      "Training Batch: 16678 Loss: 3099.312500\n",
      "Training Batch: 16679 Loss: 3094.524902\n",
      "Training Batch: 16680 Loss: 3223.602783\n",
      "Training Batch: 16681 Loss: 2996.390137\n",
      "Training Batch: 16682 Loss: 3150.276123\n",
      "Training Batch: 16683 Loss: 3001.436768\n",
      "Training Batch: 16684 Loss: 3019.849609\n",
      "Training Batch: 16685 Loss: 3108.469238\n",
      "Training Batch: 16686 Loss: 2933.842773\n",
      "Training Batch: 16687 Loss: 2970.364014\n",
      "Training Batch: 16688 Loss: 3186.015625\n",
      "Training Batch: 16689 Loss: 3039.950195\n",
      "Training Batch: 16690 Loss: 3110.741699\n",
      "Training Batch: 16691 Loss: 2944.931641\n",
      "Training Batch: 16692 Loss: 3036.415527\n",
      "Training Batch: 16693 Loss: 3408.607666\n",
      "Training Batch: 16694 Loss: 3351.691406\n",
      "Training Batch: 16695 Loss: 3254.894775\n",
      "Training Batch: 16696 Loss: 3057.439941\n",
      "Training Batch: 16697 Loss: 3233.628906\n",
      "Training Batch: 16698 Loss: 3028.436035\n",
      "Training Batch: 16699 Loss: 3077.620117\n",
      "Training Batch: 16700 Loss: 3210.676758\n",
      "Training Batch: 16701 Loss: 3062.195801\n",
      "Training Batch: 16702 Loss: 3129.237793\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Batch: 16703 Loss: 3231.777344\n",
      "Training Batch: 16704 Loss: 3065.517090\n",
      "Training Batch: 16705 Loss: 3146.501221\n",
      "Training Batch: 16706 Loss: 3091.305664\n",
      "Training Batch: 16707 Loss: 3160.762695\n",
      "Training Batch: 16708 Loss: 3241.157715\n",
      "Training Batch: 16709 Loss: 3143.018066\n",
      "Training Batch: 16710 Loss: 3128.428223\n",
      "Training Batch: 16711 Loss: 3146.662109\n",
      "Training Batch: 16712 Loss: 3079.953857\n",
      "Training Batch: 16713 Loss: 2969.743408\n",
      "Training Batch: 16714 Loss: 3159.439453\n",
      "Training Batch: 16715 Loss: 3005.242676\n",
      "Training Batch: 16716 Loss: 3066.658936\n",
      "Training Batch: 16717 Loss: 3083.004883\n",
      "Training Batch: 16718 Loss: 3144.744629\n",
      "Training Batch: 16719 Loss: 3157.061035\n",
      "Training Batch: 16720 Loss: 3125.342285\n",
      "Training Batch: 16721 Loss: 3083.054688\n",
      "Training Batch: 16722 Loss: 3085.348145\n",
      "Training Batch: 16723 Loss: 3046.063965\n",
      "Training Batch: 16724 Loss: 3179.440918\n",
      "Training Batch: 16725 Loss: 2983.343750\n",
      "Training Batch: 16726 Loss: 3539.903320\n",
      "Training Batch: 16727 Loss: 3074.401855\n",
      "Training Batch: 16728 Loss: 3062.259277\n",
      "Training Batch: 16729 Loss: 3275.158203\n",
      "Training Batch: 16730 Loss: 3190.753906\n",
      "Training Batch: 16731 Loss: 3251.847168\n",
      "Training Batch: 16732 Loss: 3245.971191\n",
      "Training Batch: 16733 Loss: 3174.620117\n",
      "Training Batch: 16734 Loss: 2990.288086\n",
      "Training Batch: 16735 Loss: 3109.878418\n",
      "Training Batch: 16736 Loss: 2942.873535\n",
      "Training Batch: 16737 Loss: 3216.298340\n",
      "Training Batch: 16738 Loss: 3330.275879\n",
      "Training Batch: 16739 Loss: 3033.355469\n",
      "Training Batch: 16740 Loss: 3109.020508\n",
      "Training Batch: 16741 Loss: 3259.796387\n",
      "Training Batch: 16742 Loss: 3051.784180\n",
      "Training Batch: 16743 Loss: 2997.400391\n",
      "Training Batch: 16744 Loss: 3243.899902\n",
      "Training Batch: 16745 Loss: 3051.346191\n",
      "Training Batch: 16746 Loss: 2972.562012\n",
      "Training Batch: 16747 Loss: 3238.485840\n",
      "Training Batch: 16748 Loss: 3036.355713\n",
      "Training Batch: 16749 Loss: 3093.580566\n",
      "Training Batch: 16750 Loss: 2974.437988\n",
      "Training Batch: 16751 Loss: 3307.100586\n",
      "Training Batch: 16752 Loss: 2929.234375\n",
      "Training Batch: 16753 Loss: 3047.217773\n",
      "Training Batch: 16754 Loss: 2985.764648\n",
      "Training Batch: 16755 Loss: 3065.792480\n",
      "Training Batch: 16756 Loss: 3137.439209\n",
      "Training Batch: 16757 Loss: 3082.597168\n",
      "Training Batch: 16758 Loss: 3072.206543\n",
      "Training Batch: 16759 Loss: 3091.201172\n",
      "Training Batch: 16760 Loss: 3064.865234\n",
      "Training Batch: 16761 Loss: 3056.234619\n",
      "Training Batch: 16762 Loss: 3043.531250\n",
      "Training Batch: 16763 Loss: 2995.477295\n",
      "Training Batch: 16764 Loss: 3030.018555\n",
      "Training Batch: 16765 Loss: 3043.054688\n",
      "Training Batch: 16766 Loss: 3083.303223\n",
      "Training Batch: 16767 Loss: 3054.030518\n",
      "Training Batch: 16768 Loss: 3164.427002\n",
      "Training Batch: 16769 Loss: 3126.188721\n",
      "Training Batch: 16770 Loss: 3141.439453\n",
      "Training Batch: 16771 Loss: 3183.518066\n",
      "Training Batch: 16772 Loss: 2970.232910\n",
      "Training Batch: 16773 Loss: 3087.202637\n",
      "Training Batch: 16774 Loss: 2982.229980\n",
      "Training Batch: 16775 Loss: 2951.385010\n",
      "Training Batch: 16776 Loss: 3078.916504\n",
      "Training Batch: 16777 Loss: 3030.932129\n",
      "Training Batch: 16778 Loss: 3199.537598\n",
      "Training Batch: 16779 Loss: 3173.342773\n",
      "Training Batch: 16780 Loss: 3219.909668\n",
      "Training Batch: 16781 Loss: 3068.413086\n",
      "Training Batch: 16782 Loss: 3126.989258\n",
      "Training Batch: 16783 Loss: 3169.595947\n",
      "Training Batch: 16784 Loss: 3084.256104\n",
      "Training Batch: 16785 Loss: 3018.097412\n",
      "Training Batch: 16786 Loss: 3042.544678\n",
      "Training Batch: 16787 Loss: 3082.840088\n",
      "Training Batch: 16788 Loss: 3307.541260\n",
      "Training Batch: 16789 Loss: 3123.785645\n",
      "Training Batch: 16790 Loss: 3040.033691\n",
      "Training Batch: 16791 Loss: 2944.695557\n",
      "Training Batch: 16792 Loss: 3018.796387\n",
      "Training Batch: 16793 Loss: 3072.992920\n",
      "Training Batch: 16794 Loss: 3185.729980\n",
      "Training Batch: 16795 Loss: 3076.319336\n",
      "Training Batch: 16796 Loss: 2989.517578\n",
      "Training Batch: 16797 Loss: 3081.860840\n",
      "Training Batch: 16798 Loss: 2995.497803\n",
      "Training Batch: 16799 Loss: 3005.337402\n",
      "Training Batch: 16800 Loss: 3032.919189\n",
      "Training Batch: 16801 Loss: 3096.915039\n",
      "Training Batch: 16802 Loss: 3258.626465\n",
      "Training Batch: 16803 Loss: 3068.491211\n",
      "Training Batch: 16804 Loss: 3018.537354\n",
      "Training Batch: 16805 Loss: 3212.404297\n",
      "Training Batch: 16806 Loss: 3089.308105\n",
      "Training Batch: 16807 Loss: 3168.434082\n",
      "Training Batch: 16808 Loss: 3063.009277\n",
      "Training Batch: 16809 Loss: 2980.497559\n",
      "Training Batch: 16810 Loss: 3005.957031\n",
      "Training Batch: 16811 Loss: 3129.806152\n",
      "Training Batch: 16812 Loss: 3015.141602\n",
      "Training Batch: 16813 Loss: 3170.036621\n",
      "Training Batch: 16814 Loss: 3135.885742\n",
      "Training Batch: 16815 Loss: 3014.155273\n",
      "Training Batch: 16816 Loss: 3151.217285\n",
      "Training Batch: 16817 Loss: 3293.210693\n",
      "Training Batch: 16818 Loss: 2932.484375\n",
      "Training Batch: 16819 Loss: 3099.810547\n",
      "Training Batch: 16820 Loss: 3081.631348\n",
      "Training Batch: 16821 Loss: 3069.511230\n",
      "Training Batch: 16822 Loss: 3267.526855\n",
      "Training Batch: 16823 Loss: 3309.277100\n",
      "Training Batch: 16824 Loss: 3219.574707\n",
      "Training Batch: 16825 Loss: 3222.861816\n",
      "Training Batch: 16826 Loss: 3360.972168\n",
      "Training Batch: 16827 Loss: 3021.966064\n",
      "Training Batch: 16828 Loss: 3133.079102\n",
      "Training Batch: 16829 Loss: 3085.756836\n",
      "Training Batch: 16830 Loss: 3106.698242\n",
      "Training Batch: 16831 Loss: 3013.339844\n",
      "Training Batch: 16832 Loss: 3072.374512\n",
      "Training Batch: 16833 Loss: 3188.124268\n",
      "Training Batch: 16834 Loss: 3116.761719\n",
      "Training Batch: 16835 Loss: 3023.322266\n",
      "Training Batch: 16836 Loss: 3091.593506\n",
      "Training Batch: 16837 Loss: 3124.178711\n",
      "Training Batch: 16838 Loss: 2966.062744\n",
      "Training Batch: 16839 Loss: 3001.214844\n",
      "Training Batch: 16840 Loss: 2985.949707\n",
      "Training Batch: 16841 Loss: 3099.396973\n",
      "Training Batch: 16842 Loss: 3076.581055\n",
      "Training Batch: 16843 Loss: 2966.066406\n",
      "Training Batch: 16844 Loss: 3114.596191\n",
      "Training Batch: 16845 Loss: 3204.473877\n",
      "Training Batch: 16846 Loss: 3089.549805\n",
      "Training Batch: 16847 Loss: 3053.481934\n",
      "Training Batch: 16848 Loss: 3055.009033\n",
      "Training Batch: 16849 Loss: 3000.141602\n",
      "Training Batch: 16850 Loss: 3147.165771\n",
      "Training Batch: 16851 Loss: 3056.406738\n",
      "Training Batch: 16852 Loss: 3331.838623\n",
      "Training Batch: 16853 Loss: 3103.889160\n",
      "Training Batch: 16854 Loss: 3158.048096\n",
      "Training Batch: 16855 Loss: 3090.088867\n",
      "Training Batch: 16856 Loss: 3089.993896\n",
      "Training Batch: 16857 Loss: 3084.670898\n",
      "Training Batch: 16858 Loss: 3200.065430\n",
      "Training Batch: 16859 Loss: 3077.056641\n",
      "Training Batch: 16860 Loss: 3161.020508\n",
      "Training Batch: 16861 Loss: 3014.528320\n",
      "Training Batch: 16862 Loss: 2969.238281\n",
      "Training Batch: 16863 Loss: 3397.463623\n",
      "Training Batch: 16864 Loss: 3443.910645\n",
      "Training Batch: 16865 Loss: 3278.184326\n",
      "Training Batch: 16866 Loss: 3072.458008\n",
      "Training Batch: 16867 Loss: 3088.259033\n",
      "Training Batch: 16868 Loss: 3100.341797\n",
      "Training Batch: 16869 Loss: 3086.760742\n",
      "Training Batch: 16870 Loss: 3243.753906\n",
      "Training Batch: 16871 Loss: 3049.664307\n",
      "Training Batch: 16872 Loss: 3217.806152\n",
      "Training Batch: 16873 Loss: 3192.000977\n",
      "Training Batch: 16874 Loss: 3122.571777\n",
      "Training Batch: 16875 Loss: 3033.970215\n",
      "Training Batch: 16876 Loss: 2979.157227\n",
      "Training Batch: 16877 Loss: 3185.061035\n",
      "Training Batch: 16878 Loss: 3027.169922\n",
      "Training Batch: 16879 Loss: 3075.331055\n",
      "Training Batch: 16880 Loss: 2993.801758\n",
      "Training Batch: 16881 Loss: 3225.629883\n",
      "Training Batch: 16882 Loss: 3027.619629\n",
      "Training Batch: 16883 Loss: 3059.574707\n",
      "Training Batch: 16884 Loss: 3058.081299\n",
      "Training Batch: 16885 Loss: 3027.967773\n",
      "Training Batch: 16886 Loss: 3054.822754\n",
      "Training Batch: 16887 Loss: 3203.266113\n",
      "Training Batch: 16888 Loss: 3130.067871\n",
      "Training Batch: 16889 Loss: 3290.350586\n",
      "Training Batch: 16890 Loss: 3061.137695\n",
      "Training Batch: 16891 Loss: 3156.234619\n",
      "Training Batch: 16892 Loss: 3094.827148\n",
      "Training Batch: 16893 Loss: 3150.252197\n",
      "Training Batch: 16894 Loss: 3259.593262\n",
      "Training Batch: 16895 Loss: 3043.358887\n",
      "Training Batch: 16896 Loss: 3209.057373\n",
      "Training Batch: 16897 Loss: 3209.493896\n",
      "Training Batch: 16898 Loss: 3097.332031\n",
      "Training Batch: 16899 Loss: 2940.307129\n",
      "Training Batch: 16900 Loss: 3130.837402\n",
      "Training Batch: 16901 Loss: 3229.244141\n",
      "Training Batch: 16902 Loss: 3044.083740\n",
      "Training Batch: 16903 Loss: 3013.967285\n",
      "Training Batch: 16904 Loss: 3225.280273\n",
      "Training Batch: 16905 Loss: 3020.426758\n",
      "Training Batch: 16906 Loss: 3126.095703\n",
      "Training Batch: 16907 Loss: 3007.579102\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Batch: 16908 Loss: 3122.605469\n",
      "Training Batch: 16909 Loss: 3171.797607\n",
      "Training Batch: 16910 Loss: 3132.229248\n",
      "Training Batch: 16911 Loss: 2999.780518\n",
      "Training Batch: 16912 Loss: 3213.261230\n",
      "Training Batch: 16913 Loss: 3184.992676\n",
      "Training Batch: 16914 Loss: 3122.318359\n",
      "Training Batch: 16915 Loss: 3021.826172\n",
      "Training Batch: 16916 Loss: 3016.717773\n",
      "Training Batch: 16917 Loss: 3200.530762\n",
      "Training Batch: 16918 Loss: 3108.729492\n",
      "Training Batch: 16919 Loss: 3255.363770\n",
      "Training Batch: 16920 Loss: 3079.597168\n",
      "Training Batch: 16921 Loss: 3001.310547\n",
      "Training Batch: 16922 Loss: 3233.812500\n",
      "Training Batch: 16923 Loss: 3009.864746\n",
      "Training Batch: 16924 Loss: 3023.095215\n",
      "Training Batch: 16925 Loss: 3125.428711\n",
      "Training Batch: 16926 Loss: 3018.145020\n",
      "Training Batch: 16927 Loss: 3098.705811\n",
      "Training Batch: 16928 Loss: 3419.005371\n",
      "Training Batch: 16929 Loss: 3141.054688\n",
      "Training Batch: 16930 Loss: 3107.933838\n",
      "Training Batch: 16931 Loss: 3090.128906\n",
      "Training Batch: 16932 Loss: 3098.902832\n",
      "Training Batch: 16933 Loss: 2982.167480\n",
      "Training Batch: 16934 Loss: 2994.397949\n",
      "Training Batch: 16935 Loss: 2928.823730\n",
      "Training Batch: 16936 Loss: 3000.952637\n",
      "Training Batch: 16937 Loss: 3062.852051\n",
      "Training Batch: 16938 Loss: 3025.203125\n",
      "Training Batch: 16939 Loss: 3058.505859\n",
      "Training Batch: 16940 Loss: 3093.595215\n",
      "Training Batch: 16941 Loss: 3070.168701\n",
      "Training Batch: 16942 Loss: 3024.072754\n",
      "Training Batch: 16943 Loss: 3032.181641\n",
      "Training Batch: 16944 Loss: 3110.112793\n",
      "Training Batch: 16945 Loss: 3093.905273\n",
      "Training Batch: 16946 Loss: 3036.976074\n",
      "Training Batch: 16947 Loss: 3211.652832\n",
      "Training Batch: 16948 Loss: 3063.057617\n",
      "Training Batch: 16949 Loss: 3672.645996\n",
      "Training Batch: 16950 Loss: 3318.861084\n",
      "Training Batch: 16951 Loss: 3095.669434\n",
      "Training Batch: 16952 Loss: 3157.817383\n",
      "Training Batch: 16953 Loss: 3072.517578\n",
      "Training Batch: 16954 Loss: 3113.583496\n",
      "Training Batch: 16955 Loss: 3336.304199\n",
      "Training Batch: 16956 Loss: 3157.190918\n",
      "Training Batch: 16957 Loss: 2967.439453\n",
      "Training Batch: 16958 Loss: 2956.333740\n",
      "Training Batch: 16959 Loss: 3112.782227\n",
      "Training Batch: 16960 Loss: 3119.739502\n",
      "Training Batch: 16961 Loss: 3142.684570\n",
      "Training Batch: 16962 Loss: 3299.006348\n",
      "Training Batch: 16963 Loss: 3198.944336\n",
      "Training Batch: 16964 Loss: 3242.055664\n",
      "Training Batch: 16965 Loss: 3008.552246\n",
      "Training Batch: 16966 Loss: 3032.509521\n",
      "Training Batch: 16967 Loss: 3033.038330\n",
      "Training Batch: 16968 Loss: 3186.673340\n",
      "Training Batch: 16969 Loss: 3169.078613\n",
      "Training Batch: 16970 Loss: 3385.461670\n",
      "Training Batch: 16971 Loss: 3062.707520\n",
      "Training Batch: 16972 Loss: 2964.843750\n",
      "Training Batch: 16973 Loss: 3050.453613\n",
      "Training Batch: 16974 Loss: 3064.673828\n",
      "Training Batch: 16975 Loss: 3062.690918\n",
      "Training Batch: 16976 Loss: 3028.154297\n",
      "Training Batch: 16977 Loss: 3016.274902\n",
      "Training Batch: 16978 Loss: 3094.740234\n",
      "Training Batch: 16979 Loss: 3210.576172\n",
      "Training Batch: 16980 Loss: 3038.977051\n",
      "Training Batch: 16981 Loss: 3137.086914\n",
      "Training Batch: 16982 Loss: 3058.610352\n",
      "Training Batch: 16983 Loss: 3061.261719\n",
      "Training Batch: 16984 Loss: 3103.384766\n",
      "Training Batch: 16985 Loss: 3230.118652\n",
      "Training Batch: 16986 Loss: 3333.375488\n",
      "Training Batch: 16987 Loss: 2941.892822\n",
      "Training Batch: 16988 Loss: 3085.109375\n",
      "Training Batch: 16989 Loss: 3084.867676\n",
      "Training Batch: 16990 Loss: 3069.860840\n",
      "Training Batch: 16991 Loss: 3130.086182\n",
      "Training Batch: 16992 Loss: 3064.653809\n",
      "Training Batch: 16993 Loss: 3036.502197\n",
      "Training Batch: 16994 Loss: 3026.047363\n",
      "Training Batch: 16995 Loss: 3109.126953\n",
      "Training Batch: 16996 Loss: 3045.635254\n",
      "Training Batch: 16997 Loss: 3150.885498\n",
      "Training Batch: 16998 Loss: 3189.490234\n",
      "Training Batch: 16999 Loss: 3171.805420\n",
      "Training Batch: 17000 Loss: 3005.406250\n",
      "Training Batch: 17001 Loss: 3034.514160\n",
      "Training Batch: 17002 Loss: 3074.543213\n",
      "Training Batch: 17003 Loss: 3125.692139\n",
      "Training Batch: 17004 Loss: 2984.418457\n",
      "Training Batch: 17005 Loss: 3001.364014\n",
      "Training Batch: 17006 Loss: 3117.105713\n",
      "Training Batch: 17007 Loss: 3133.355469\n",
      "Training Batch: 17008 Loss: 3031.747070\n",
      "Training Batch: 17009 Loss: 3405.093262\n",
      "Training Batch: 17010 Loss: 3283.623535\n",
      "Training Batch: 17011 Loss: 3132.509521\n",
      "Training Batch: 17012 Loss: 3109.640137\n",
      "Training Batch: 17013 Loss: 2955.709961\n",
      "Training Batch: 17014 Loss: 3048.022217\n",
      "Training Batch: 17015 Loss: 3090.657715\n",
      "Training Batch: 17016 Loss: 2978.923340\n",
      "Training Batch: 17017 Loss: 3104.541260\n",
      "Training Batch: 17018 Loss: 2976.637207\n",
      "Training Batch: 17019 Loss: 3007.663086\n",
      "Training Batch: 17020 Loss: 3062.483887\n",
      "Training Batch: 17021 Loss: 2999.340820\n",
      "Training Batch: 17022 Loss: 3098.169434\n",
      "Training Batch: 17023 Loss: 3188.210938\n",
      "Training Batch: 17024 Loss: 3254.729980\n",
      "Training Batch: 17025 Loss: 3076.048340\n",
      "Training Batch: 17026 Loss: 3106.206299\n",
      "Training Batch: 17027 Loss: 3119.126221\n",
      "Training Batch: 17028 Loss: 3288.914795\n",
      "Training Batch: 17029 Loss: 3216.369629\n",
      "Training Batch: 17030 Loss: 3028.718994\n",
      "Training Batch: 17031 Loss: 2939.658691\n",
      "Training Batch: 17032 Loss: 2988.297119\n",
      "Training Batch: 17033 Loss: 3027.885010\n",
      "Training Batch: 17034 Loss: 2960.377930\n",
      "Training Batch: 17035 Loss: 2971.488037\n",
      "Training Batch: 17036 Loss: 3092.752930\n",
      "Training Batch: 17037 Loss: 3003.715332\n",
      "Training Batch: 17038 Loss: 2978.472900\n",
      "Training Batch: 17039 Loss: 3100.529541\n",
      "Training Batch: 17040 Loss: 2957.678955\n",
      "Training Batch: 17041 Loss: 3150.287109\n",
      "Training Batch: 17042 Loss: 3148.575195\n",
      "Training Batch: 17043 Loss: 3224.531494\n",
      "Training Batch: 17044 Loss: 2985.413574\n",
      "Training Batch: 17045 Loss: 3102.582031\n",
      "Training Batch: 17046 Loss: 3085.813232\n",
      "Training Batch: 17047 Loss: 3032.338379\n",
      "Training Batch: 17048 Loss: 3152.659912\n",
      "Training Batch: 17049 Loss: 3034.882324\n",
      "Training Batch: 17050 Loss: 3081.421631\n",
      "Training Batch: 17051 Loss: 3158.282227\n",
      "Training Batch: 17052 Loss: 3136.838379\n",
      "Training Batch: 17053 Loss: 3221.003906\n",
      "Training Batch: 17054 Loss: 2966.111816\n",
      "Training Batch: 17055 Loss: 2970.841309\n",
      "Training Batch: 17056 Loss: 2968.389160\n",
      "Training Batch: 17057 Loss: 3115.026367\n",
      "Training Batch: 17058 Loss: 2928.131592\n",
      "Training Batch: 17059 Loss: 2998.831055\n",
      "Training Batch: 17060 Loss: 3015.642090\n",
      "Training Batch: 17061 Loss: 3031.416016\n",
      "Training Batch: 17062 Loss: 3118.103516\n",
      "Training Batch: 17063 Loss: 2992.625488\n",
      "Training Batch: 17064 Loss: 3029.575928\n",
      "Training Batch: 17065 Loss: 3021.380859\n",
      "Training Batch: 17066 Loss: 3111.667969\n",
      "Training Batch: 17067 Loss: 3056.038574\n",
      "Training Batch: 17068 Loss: 3004.829102\n",
      "Training Batch: 17069 Loss: 2967.144043\n",
      "Training Batch: 17070 Loss: 3056.058594\n",
      "Training Batch: 17071 Loss: 3094.319336\n",
      "Training Batch: 17072 Loss: 3203.094971\n",
      "Training Batch: 17073 Loss: 2968.465576\n",
      "Training Batch: 17074 Loss: 3068.399902\n",
      "Training Batch: 17075 Loss: 3101.457520\n",
      "Training Batch: 17076 Loss: 3007.550781\n",
      "Training Batch: 17077 Loss: 3041.871094\n",
      "Training Batch: 17078 Loss: 3387.729492\n",
      "Training Batch: 17079 Loss: 3134.227539\n",
      "Training Batch: 17080 Loss: 3204.058838\n",
      "Training Batch: 17081 Loss: 3007.063965\n",
      "Training Batch: 17082 Loss: 2975.765381\n",
      "Training Batch: 17083 Loss: 3135.018555\n",
      "Training Batch: 17084 Loss: 3084.447021\n",
      "Training Batch: 17085 Loss: 3017.424805\n",
      "Training Batch: 17086 Loss: 3189.131836\n",
      "Training Batch: 17087 Loss: 3098.008301\n",
      "Training Batch: 17088 Loss: 3039.207031\n",
      "Training Batch: 17089 Loss: 3143.812012\n",
      "Training Batch: 17090 Loss: 3108.050537\n",
      "Training Batch: 17091 Loss: 3001.200195\n",
      "Training Batch: 17092 Loss: 3070.283203\n",
      "Training Batch: 17093 Loss: 3354.070068\n",
      "Training Batch: 17094 Loss: 3227.182129\n",
      "Training Batch: 17095 Loss: 3153.030273\n",
      "Training Batch: 17096 Loss: 3015.272705\n",
      "Training Batch: 17097 Loss: 3058.161865\n",
      "Training Batch: 17098 Loss: 3119.151855\n",
      "Training Batch: 17099 Loss: 3238.867432\n",
      "Training Batch: 17100 Loss: 3294.943359\n",
      "Training Batch: 17101 Loss: 3322.224609\n",
      "Training Batch: 17102 Loss: 3175.131348\n",
      "Training Batch: 17103 Loss: 3003.827393\n",
      "Training Batch: 17104 Loss: 3049.197021\n",
      "Training Batch: 17105 Loss: 3233.207520\n",
      "Training Batch: 17106 Loss: 3032.079834\n",
      "Training Batch: 17107 Loss: 3232.364258\n",
      "Training Batch: 17108 Loss: 3026.222656\n",
      "Training Batch: 17109 Loss: 3226.514893\n",
      "Training Batch: 17110 Loss: 3063.843018\n",
      "Training Batch: 17111 Loss: 2979.038574\n",
      "Training Batch: 17112 Loss: 3077.213135\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Batch: 17113 Loss: 3212.360596\n",
      "Training Batch: 17114 Loss: 2952.985840\n",
      "Training Batch: 17115 Loss: 4076.976562\n",
      "Training Batch: 17116 Loss: 3415.113037\n",
      "Training Batch: 17117 Loss: 3416.719238\n",
      "Training Batch: 17118 Loss: 3126.275879\n",
      "Training Batch: 17119 Loss: 3165.145508\n",
      "Training Batch: 17120 Loss: 3099.679199\n",
      "Training Batch: 17121 Loss: 3109.776611\n",
      "Training Batch: 17122 Loss: 3060.734863\n",
      "Training Batch: 17123 Loss: 3181.632324\n",
      "Training Batch: 17124 Loss: 3180.197754\n",
      "Training Batch: 17125 Loss: 3085.688965\n",
      "Training Batch: 17126 Loss: 3045.165039\n",
      "Training Batch: 17127 Loss: 3109.944092\n",
      "Training Batch: 17128 Loss: 3111.198242\n",
      "Training Batch: 17129 Loss: 3068.582275\n",
      "Training Batch: 17130 Loss: 3100.031250\n",
      "Training Batch: 17131 Loss: 3013.167480\n",
      "Training Batch: 17132 Loss: 3249.567383\n",
      "Training Batch: 17133 Loss: 3410.839844\n",
      "Training Batch: 17134 Loss: 2963.851074\n",
      "Training Batch: 17135 Loss: 3112.623535\n",
      "Training Batch: 17136 Loss: 3043.243164\n",
      "Training Batch: 17137 Loss: 3234.804443\n",
      "Training Batch: 17138 Loss: 3416.347168\n",
      "Training Batch: 17139 Loss: 3026.170410\n",
      "Training Batch: 17140 Loss: 3027.078613\n",
      "Training Batch: 17141 Loss: 2977.231445\n",
      "Training Batch: 17142 Loss: 3152.754150\n",
      "Training Batch: 17143 Loss: 3075.005371\n",
      "Training Batch: 17144 Loss: 3020.615723\n",
      "Training Batch: 17145 Loss: 3018.017090\n",
      "Training Batch: 17146 Loss: 3011.518066\n",
      "Training Batch: 17147 Loss: 3046.012207\n",
      "Training Batch: 17148 Loss: 2970.140137\n",
      "Training Batch: 17149 Loss: 3077.319336\n",
      "Training Batch: 17150 Loss: 2977.958008\n",
      "Training Batch: 17151 Loss: 3139.636230\n",
      "Training Batch: 17152 Loss: 3058.351318\n",
      "Training Batch: 17153 Loss: 2961.958008\n",
      "Training Batch: 17154 Loss: 3010.197266\n",
      "Training Batch: 17155 Loss: 2961.956299\n",
      "Training Batch: 17156 Loss: 3133.587646\n",
      "Training Batch: 17157 Loss: 3036.931152\n",
      "Training Batch: 17158 Loss: 2943.443848\n",
      "Training Batch: 17159 Loss: 3093.328613\n",
      "Training Batch: 17160 Loss: 2951.039795\n",
      "Training Batch: 17161 Loss: 3055.113770\n",
      "Training Batch: 17162 Loss: 3141.892578\n",
      "Training Batch: 17163 Loss: 3032.311035\n",
      "Training Batch: 17164 Loss: 2974.452881\n",
      "Training Batch: 17165 Loss: 2942.650635\n",
      "Training Batch: 17166 Loss: 3014.806641\n",
      "Training Batch: 17167 Loss: 3049.305176\n",
      "Training Batch: 17168 Loss: 3008.229980\n",
      "Training Batch: 17169 Loss: 3039.246338\n",
      "Training Batch: 17170 Loss: 3153.165039\n",
      "Training Batch: 17171 Loss: 3057.706299\n",
      "Training Batch: 17172 Loss: 2999.424072\n",
      "Training Batch: 17173 Loss: 3077.233887\n",
      "Training Batch: 17174 Loss: 3028.485352\n",
      "Training Batch: 17175 Loss: 3010.570557\n",
      "Training Batch: 17176 Loss: 3007.342285\n",
      "Training Batch: 17177 Loss: 3035.822754\n",
      "Training Batch: 17178 Loss: 3022.522949\n",
      "Training Batch: 17179 Loss: 3023.214844\n",
      "Training Batch: 17180 Loss: 2976.060547\n",
      "Training Batch: 17181 Loss: 3130.381592\n",
      "Training Batch: 17182 Loss: 3359.434570\n",
      "Training Batch: 17183 Loss: 3052.277100\n",
      "Training Batch: 17184 Loss: 3271.133301\n",
      "Training Batch: 17185 Loss: 3318.145996\n",
      "Training Batch: 17186 Loss: 2997.176025\n",
      "Training Batch: 17187 Loss: 3188.947266\n",
      "Training Batch: 17188 Loss: 3050.667480\n",
      "Training Batch: 17189 Loss: 3120.175293\n",
      "Training Batch: 17190 Loss: 3352.937012\n",
      "Training Batch: 17191 Loss: 3321.956055\n",
      "Training Batch: 17192 Loss: 3093.584473\n",
      "Training Batch: 17193 Loss: 3124.113281\n",
      "Training Batch: 17194 Loss: 3100.845703\n",
      "Training Batch: 17195 Loss: 3092.239014\n",
      "Training Batch: 17196 Loss: 3273.651367\n",
      "Training Batch: 17197 Loss: 3019.511230\n",
      "Training Batch: 17198 Loss: 3234.525391\n",
      "Training Batch: 17199 Loss: 3123.881836\n",
      "Training Batch: 17200 Loss: 3041.675781\n",
      "Training Batch: 17201 Loss: 3117.668457\n",
      "Training Batch: 17202 Loss: 3113.228516\n",
      "Training Batch: 17203 Loss: 3136.310059\n",
      "Training Batch: 17204 Loss: 3024.067871\n",
      "Training Batch: 17205 Loss: 3011.702148\n",
      "Training Batch: 17206 Loss: 3095.077637\n",
      "Training Batch: 17207 Loss: 3079.479248\n",
      "Training Batch: 17208 Loss: 3142.736816\n",
      "Training Batch: 17209 Loss: 2935.083984\n",
      "Training Batch: 17210 Loss: 3038.313721\n",
      "Training Batch: 17211 Loss: 3134.289551\n",
      "Training Batch: 17212 Loss: 2967.839355\n",
      "Training Batch: 17213 Loss: 3053.344971\n",
      "Training Batch: 17214 Loss: 3224.243896\n",
      "Training Batch: 17215 Loss: 3066.406250\n",
      "Training Batch: 17216 Loss: 3115.909668\n",
      "Training Batch: 17217 Loss: 3302.581543\n",
      "Training Batch: 17218 Loss: 3200.390625\n",
      "Training Batch: 17219 Loss: 3186.142090\n",
      "Training Batch: 17220 Loss: 3025.407959\n",
      "Training Batch: 17221 Loss: 2932.856934\n",
      "Training Batch: 17222 Loss: 3084.658691\n",
      "Training Batch: 17223 Loss: 2976.575439\n",
      "Training Batch: 17224 Loss: 3048.014160\n",
      "Training Batch: 17225 Loss: 2986.508789\n",
      "Training Batch: 17226 Loss: 2943.112549\n",
      "Training Batch: 17227 Loss: 2930.144531\n",
      "Training Batch: 17228 Loss: 2993.982178\n",
      "Training Batch: 17229 Loss: 3092.293457\n",
      "Training Batch: 17230 Loss: 3118.833008\n",
      "Training Batch: 17231 Loss: 3015.204102\n",
      "Training Batch: 17232 Loss: 2954.475342\n",
      "Training Batch: 17233 Loss: 2973.966797\n",
      "Training Batch: 17234 Loss: 3083.095703\n",
      "Training Batch: 17235 Loss: 3077.162598\n",
      "Training Batch: 17236 Loss: 3134.435547\n",
      "Training Batch: 17237 Loss: 3327.432861\n",
      "Training Batch: 17238 Loss: 3225.766113\n",
      "Training Batch: 17239 Loss: 2984.586426\n",
      "Training Batch: 17240 Loss: 3088.937988\n",
      "Training Batch: 17241 Loss: 2985.838867\n",
      "Training Batch: 17242 Loss: 3061.810547\n",
      "Training Batch: 17243 Loss: 2998.354004\n",
      "Training Batch: 17244 Loss: 3032.948242\n",
      "Training Batch: 17245 Loss: 3187.809082\n",
      "Training Batch: 17246 Loss: 3110.905273\n",
      "Training Batch: 17247 Loss: 3012.994629\n",
      "Training Batch: 17248 Loss: 3164.445801\n",
      "Training Batch: 17249 Loss: 3152.412598\n",
      "Training Batch: 17250 Loss: 3010.104004\n",
      "Training Batch: 17251 Loss: 3155.686523\n",
      "Training Batch: 17252 Loss: 3264.963135\n",
      "Training Batch: 17253 Loss: 3133.594727\n",
      "Training Batch: 17254 Loss: 3083.624023\n",
      "Training Batch: 17255 Loss: 3072.135010\n",
      "Training Batch: 17256 Loss: 3006.295898\n",
      "Training Batch: 17257 Loss: 3081.306641\n",
      "Training Batch: 17258 Loss: 2961.548340\n",
      "Training Batch: 17259 Loss: 3028.892578\n",
      "Training Batch: 17260 Loss: 2999.096436\n",
      "Training Batch: 17261 Loss: 3014.812988\n",
      "Training Batch: 17262 Loss: 3084.190430\n",
      "Training Batch: 17263 Loss: 3160.748047\n",
      "Training Batch: 17264 Loss: 3020.025391\n",
      "Training Batch: 17265 Loss: 3092.979004\n",
      "Training Batch: 17266 Loss: 3227.417480\n",
      "Training Batch: 17267 Loss: 3057.164307\n",
      "Training Batch: 17268 Loss: 3119.158691\n",
      "Training Batch: 17269 Loss: 3264.630371\n",
      "Training Batch: 17270 Loss: 3105.545898\n",
      "Training Batch: 17271 Loss: 3228.734863\n",
      "Training Batch: 17272 Loss: 3201.377930\n",
      "Training Batch: 17273 Loss: 3128.489746\n",
      "Training Batch: 17274 Loss: 3097.537598\n",
      "Training Batch: 17275 Loss: 3545.030518\n",
      "Training Batch: 17276 Loss: 3083.581543\n",
      "Training Batch: 17277 Loss: 3185.915527\n",
      "Training Batch: 17278 Loss: 2925.297852\n",
      "Training Batch: 17279 Loss: 3068.989258\n",
      "Training Batch: 17280 Loss: 2934.297363\n",
      "Training Batch: 17281 Loss: 3072.271973\n",
      "Training Batch: 17282 Loss: 3054.995605\n",
      "Training Batch: 17283 Loss: 2966.524414\n",
      "Training Batch: 17284 Loss: 2924.506836\n",
      "Training Batch: 17285 Loss: 3199.311035\n",
      "Training Batch: 17286 Loss: 3089.460449\n",
      "Training Batch: 17287 Loss: 3067.636963\n",
      "Training Batch: 17288 Loss: 3118.074707\n",
      "Training Batch: 17289 Loss: 3125.979492\n",
      "Training Batch: 17290 Loss: 3160.775391\n",
      "Training Batch: 17291 Loss: 2950.290527\n",
      "Training Batch: 17292 Loss: 2993.908691\n",
      "Training Batch: 17293 Loss: 3064.587891\n",
      "Training Batch: 17294 Loss: 2927.752441\n",
      "Training Batch: 17295 Loss: 3213.536377\n",
      "Training Batch: 17296 Loss: 2997.287109\n",
      "Training Batch: 17297 Loss: 2995.192871\n",
      "Training Batch: 17298 Loss: 3109.193848\n",
      "Training Batch: 17299 Loss: 3125.157227\n",
      "Training Batch: 17300 Loss: 3070.444092\n",
      "Training Batch: 17301 Loss: 3015.214844\n",
      "Training Batch: 17302 Loss: 3084.825684\n",
      "Training Batch: 17303 Loss: 2973.144043\n",
      "Training Batch: 17304 Loss: 3154.349609\n",
      "Training Batch: 17305 Loss: 3072.421143\n",
      "Training Batch: 17306 Loss: 3098.518066\n",
      "Training Batch: 17307 Loss: 3145.541260\n",
      "Training Batch: 17308 Loss: 3094.075195\n",
      "Training Batch: 17309 Loss: 3154.261230\n",
      "Training Batch: 17310 Loss: 3130.973145\n",
      "Training Batch: 17311 Loss: 2962.046387\n",
      "Training Batch: 17312 Loss: 2944.156738\n",
      "Training Batch: 17313 Loss: 3071.922852\n",
      "Training Batch: 17314 Loss: 3161.397949\n",
      "Training Batch: 17315 Loss: 3092.593750\n",
      "Training Batch: 17316 Loss: 3138.017090\n",
      "Training Batch: 17317 Loss: 3164.292480\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Batch: 17318 Loss: 2989.721680\n",
      "Training Batch: 17319 Loss: 2997.907227\n",
      "Training Batch: 17320 Loss: 2945.174805\n",
      "Training Batch: 17321 Loss: 3208.295410\n",
      "Training Batch: 17322 Loss: 3060.985107\n",
      "Training Batch: 17323 Loss: 2997.394043\n",
      "Training Batch: 17324 Loss: 3175.173584\n",
      "Training Batch: 17325 Loss: 3187.478760\n",
      "Training Batch: 17326 Loss: 2978.680908\n",
      "Training Batch: 17327 Loss: 3019.535400\n",
      "Training Batch: 17328 Loss: 3161.110352\n",
      "Training Batch: 17329 Loss: 3208.090820\n",
      "Training Batch: 17330 Loss: 3177.545410\n",
      "Training Batch: 17331 Loss: 3060.236328\n",
      "Training Batch: 17332 Loss: 2987.479004\n",
      "Training Batch: 17333 Loss: 3234.999268\n",
      "Training Batch: 17334 Loss: 3271.286377\n",
      "Training Batch: 17335 Loss: 3085.086182\n",
      "Training Batch: 17336 Loss: 3576.457520\n",
      "Training Batch: 17337 Loss: 3102.452881\n",
      "Training Batch: 17338 Loss: 3055.087646\n",
      "Training Batch: 17339 Loss: 3090.764648\n",
      "Training Batch: 17340 Loss: 3070.842773\n",
      "Training Batch: 17341 Loss: 3007.036621\n",
      "Training Batch: 17342 Loss: 2949.091553\n",
      "Training Batch: 17343 Loss: 2934.452148\n",
      "Training Batch: 17344 Loss: 3050.915283\n",
      "Training Batch: 17345 Loss: 3120.354980\n",
      "Training Batch: 17346 Loss: 3054.228516\n",
      "Training Batch: 17347 Loss: 3041.582520\n",
      "Training Batch: 17348 Loss: 3072.608887\n",
      "Training Batch: 17349 Loss: 3083.452637\n",
      "Training Batch: 17350 Loss: 3094.277344\n",
      "Training Batch: 17351 Loss: 2869.140137\n",
      "Training Batch: 17352 Loss: 3054.058350\n",
      "Training Batch: 17353 Loss: 3268.722168\n",
      "Training Batch: 17354 Loss: 3056.058594\n",
      "Training Batch: 17355 Loss: 2950.776367\n",
      "Training Batch: 17356 Loss: 3074.868164\n",
      "Training Batch: 17357 Loss: 3039.788818\n",
      "Training Batch: 17358 Loss: 3011.842773\n",
      "Training Batch: 17359 Loss: 3181.852051\n",
      "Training Batch: 17360 Loss: 3177.028809\n",
      "Training Batch: 17361 Loss: 3191.149658\n",
      "Training Batch: 17362 Loss: 2936.447998\n",
      "Training Batch: 17363 Loss: 3081.404541\n",
      "Training Batch: 17364 Loss: 3180.561523\n",
      "Training Batch: 17365 Loss: 3117.437012\n",
      "Training Batch: 17366 Loss: 3054.096680\n",
      "Training Batch: 17367 Loss: 3114.103760\n",
      "Training Batch: 17368 Loss: 3052.623047\n",
      "Training Batch: 17369 Loss: 3107.692139\n",
      "Training Batch: 17370 Loss: 3122.054443\n",
      "Training Batch: 17371 Loss: 2989.301758\n",
      "Training Batch: 17372 Loss: 3230.370605\n",
      "Training Batch: 17373 Loss: 3119.247559\n",
      "Training Batch: 17374 Loss: 3171.507324\n",
      "Training Batch: 17375 Loss: 3062.037842\n",
      "Training Batch: 17376 Loss: 3306.491211\n",
      "Training Batch: 17377 Loss: 3111.020508\n",
      "Training Batch: 17378 Loss: 3037.645508\n",
      "Training Batch: 17379 Loss: 3034.715332\n",
      "Training Batch: 17380 Loss: 3074.391113\n",
      "Training Batch: 17381 Loss: 3332.481934\n",
      "Training Batch: 17382 Loss: 3147.044922\n",
      "Training Batch: 17383 Loss: 3126.979492\n",
      "Training Batch: 17384 Loss: 3093.218506\n",
      "Training Batch: 17385 Loss: 3187.561279\n",
      "Training Batch: 17386 Loss: 3173.069336\n",
      "Training Batch: 17387 Loss: 3247.570557\n",
      "Training Batch: 17388 Loss: 2933.436523\n",
      "Training Batch: 17389 Loss: 3207.668457\n",
      "Training Batch: 17390 Loss: 3534.986328\n",
      "Training Batch: 17391 Loss: 3104.039551\n",
      "Training Batch: 17392 Loss: 3099.749512\n",
      "Training Batch: 17393 Loss: 3020.337891\n",
      "Training Batch: 17394 Loss: 3153.726807\n",
      "Training Batch: 17395 Loss: 3082.309570\n",
      "Training Batch: 17396 Loss: 3110.891357\n",
      "Training Batch: 17397 Loss: 3254.163086\n",
      "Training Batch: 17398 Loss: 3220.536133\n",
      "Training Batch: 17399 Loss: 2985.695801\n",
      "Training Batch: 17400 Loss: 3015.787109\n",
      "Training Batch: 17401 Loss: 3158.055664\n",
      "Training Batch: 17402 Loss: 3047.714844\n",
      "Training Batch: 17403 Loss: 3293.590332\n",
      "Training Batch: 17404 Loss: 3209.501709\n",
      "Training Batch: 17405 Loss: 3108.076660\n",
      "Training Batch: 17406 Loss: 3075.421875\n",
      "Training Batch: 17407 Loss: 3045.891113\n",
      "Training Batch: 17408 Loss: 3015.850342\n",
      "Training Batch: 17409 Loss: 3039.951904\n",
      "Training Batch: 17410 Loss: 3125.627930\n",
      "Training Batch: 17411 Loss: 3107.615723\n",
      "Training Batch: 17412 Loss: 2990.267578\n",
      "Training Batch: 17413 Loss: 3015.647461\n",
      "Training Batch: 17414 Loss: 2973.718262\n",
      "Training Batch: 17415 Loss: 3116.956055\n",
      "Training Batch: 17416 Loss: 3094.625000\n",
      "Training Batch: 17417 Loss: 3128.262695\n",
      "Training Batch: 17418 Loss: 3164.142090\n",
      "Training Batch: 17419 Loss: 3090.861084\n",
      "Training Batch: 17420 Loss: 3137.927734\n",
      "Training Batch: 17421 Loss: 3065.981934\n",
      "Training Batch: 17422 Loss: 3091.012451\n",
      "Training Batch: 17423 Loss: 3075.719482\n",
      "Training Batch: 17424 Loss: 2986.331543\n",
      "Training Batch: 17425 Loss: 3167.521240\n",
      "Training Batch: 17426 Loss: 3223.442383\n",
      "Training Batch: 17427 Loss: 2973.807617\n",
      "Training Batch: 17428 Loss: 3096.470703\n",
      "Training Batch: 17429 Loss: 3191.368164\n",
      "Training Batch: 17430 Loss: 3040.333984\n",
      "Training Batch: 17431 Loss: 3046.076660\n",
      "Training Batch: 17432 Loss: 3270.104492\n",
      "Training Batch: 17433 Loss: 3058.173828\n",
      "Training Batch: 17434 Loss: 3202.565430\n",
      "Training Batch: 17435 Loss: 2969.116699\n",
      "Training Batch: 17436 Loss: 3109.448730\n",
      "Training Batch: 17437 Loss: 2917.570557\n",
      "Training Batch: 17438 Loss: 3121.194092\n",
      "Training Batch: 17439 Loss: 3035.178711\n",
      "Training Batch: 17440 Loss: 2965.488525\n",
      "Training Batch: 17441 Loss: 3009.488770\n",
      "Training Batch: 17442 Loss: 2997.052490\n",
      "Training Batch: 17443 Loss: 3051.265137\n",
      "Training Batch: 17444 Loss: 3322.606934\n",
      "Training Batch: 17445 Loss: 2997.418457\n",
      "Training Batch: 17446 Loss: 3099.783447\n",
      "Training Batch: 17447 Loss: 3085.597656\n",
      "Training Batch: 17448 Loss: 3078.531982\n",
      "Training Batch: 17449 Loss: 3024.647705\n",
      "Training Batch: 17450 Loss: 3256.600586\n",
      "Training Batch: 17451 Loss: 3094.954590\n",
      "Training Batch: 17452 Loss: 3085.727051\n",
      "Training Batch: 17453 Loss: 3088.864746\n",
      "Training Batch: 17454 Loss: 3089.213379\n",
      "Training Batch: 17455 Loss: 3005.174561\n",
      "Training Batch: 17456 Loss: 3198.231445\n",
      "Training Batch: 17457 Loss: 3192.064941\n",
      "Training Batch: 17458 Loss: 3084.200439\n",
      "Training Batch: 17459 Loss: 3081.270508\n",
      "Training Batch: 17460 Loss: 3069.148438\n",
      "Training Batch: 17461 Loss: 3075.609375\n",
      "Training Batch: 17462 Loss: 2994.599609\n",
      "Training Batch: 17463 Loss: 3012.846191\n",
      "Training Batch: 17464 Loss: 3041.839355\n",
      "Training Batch: 17465 Loss: 3034.979492\n",
      "Training Batch: 17466 Loss: 3104.122559\n",
      "Training Batch: 17467 Loss: 2960.446777\n",
      "Training Batch: 17468 Loss: 3082.016602\n",
      "Training Batch: 17469 Loss: 3183.726074\n",
      "Training Batch: 17470 Loss: 3063.503906\n",
      "Training Batch: 17471 Loss: 3062.733398\n",
      "Training Batch: 17472 Loss: 3072.989746\n",
      "Training Batch: 17473 Loss: 3054.834229\n",
      "Training Batch: 17474 Loss: 2993.592285\n",
      "Training Batch: 17475 Loss: 3185.377930\n",
      "Training Batch: 17476 Loss: 3007.488525\n",
      "Training Batch: 17477 Loss: 3063.229004\n",
      "Training Batch: 17478 Loss: 3076.735596\n",
      "Training Batch: 17479 Loss: 3008.894043\n",
      "Training Batch: 17480 Loss: 3067.038330\n",
      "Training Batch: 17481 Loss: 3030.640625\n",
      "Training Batch: 17482 Loss: 3197.327148\n",
      "Training Batch: 17483 Loss: 3116.728516\n",
      "Training Batch: 17484 Loss: 3019.990234\n",
      "Training Batch: 17485 Loss: 3012.362061\n",
      "Training Batch: 17486 Loss: 3131.482178\n",
      "Training Batch: 17487 Loss: 3194.234863\n",
      "Training Batch: 17488 Loss: 3010.349365\n",
      "Training Batch: 17489 Loss: 3002.112061\n",
      "Training Batch: 17490 Loss: 2987.461426\n",
      "Training Batch: 17491 Loss: 3042.477051\n",
      "Training Batch: 17492 Loss: 3006.770996\n",
      "Training Batch: 17493 Loss: 3397.846680\n",
      "Training Batch: 17494 Loss: 3114.079590\n",
      "Training Batch: 17495 Loss: 3136.878906\n",
      "Training Batch: 17496 Loss: 3058.430420\n",
      "Training Batch: 17497 Loss: 3061.735840\n",
      "Training Batch: 17498 Loss: 3176.811768\n",
      "Training Batch: 17499 Loss: 3008.402832\n",
      "Training Batch: 17500 Loss: 2957.537598\n",
      "Training Batch: 17501 Loss: 3115.950684\n",
      "Training Batch: 17502 Loss: 2973.917969\n",
      "Training Batch: 17503 Loss: 3096.053223\n",
      "Training Batch: 17504 Loss: 3071.123291\n",
      "Training Batch: 17505 Loss: 3087.009277\n",
      "Training Batch: 17506 Loss: 3060.184570\n",
      "Training Batch: 17507 Loss: 3202.040039\n",
      "Training Batch: 17508 Loss: 3056.931152\n",
      "Training Batch: 17509 Loss: 3090.353027\n",
      "Training Batch: 17510 Loss: 3068.461914\n",
      "Training Batch: 17511 Loss: 3130.489990\n",
      "Training Batch: 17512 Loss: 2940.963379\n",
      "Training Batch: 17513 Loss: 3124.048584\n",
      "Training Batch: 17514 Loss: 3163.757812\n",
      "Training Batch: 17515 Loss: 3059.178467\n",
      "Training Batch: 17516 Loss: 3055.366211\n",
      "Training Batch: 17517 Loss: 3176.770508\n",
      "Training Batch: 17518 Loss: 3088.734863\n",
      "Training Batch: 17519 Loss: 3069.799316\n",
      "Training Batch: 17520 Loss: 3232.265625\n",
      "Training Batch: 17521 Loss: 2936.155273\n",
      "Training Batch: 17522 Loss: 2985.433838\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Batch: 17523 Loss: 2997.373535\n",
      "Training Batch: 17524 Loss: 3091.461914\n",
      "Training Batch: 17525 Loss: 3004.232666\n",
      "Training Batch: 17526 Loss: 3209.715820\n",
      "Training Batch: 17527 Loss: 3082.378174\n",
      "Training Batch: 17528 Loss: 3269.823975\n",
      "Training Batch: 17529 Loss: 3199.270508\n",
      "Training Batch: 17530 Loss: 3020.544922\n",
      "Training Batch: 17531 Loss: 3080.005859\n",
      "Training Batch: 17532 Loss: 3198.065918\n",
      "Training Batch: 17533 Loss: 3027.722900\n",
      "Training Batch: 17534 Loss: 3092.182617\n",
      "Training Batch: 17535 Loss: 3157.998535\n",
      "Training Batch: 17536 Loss: 3146.713867\n",
      "Training Batch: 17537 Loss: 3146.533936\n",
      "Training Batch: 17538 Loss: 3048.408936\n",
      "Training Batch: 17539 Loss: 3259.125488\n",
      "Training Batch: 17540 Loss: 3116.223633\n",
      "Training Batch: 17541 Loss: 3137.394043\n",
      "Training Batch: 17542 Loss: 3069.907227\n",
      "Training Batch: 17543 Loss: 2997.189941\n",
      "Training Batch: 17544 Loss: 3017.627441\n",
      "Training Batch: 17545 Loss: 2989.980957\n",
      "Training Batch: 17546 Loss: 3021.061035\n",
      "Training Batch: 17547 Loss: 2986.632812\n",
      "Training Batch: 17548 Loss: 3323.483643\n",
      "Training Batch: 17549 Loss: 3048.828857\n",
      "Training Batch: 17550 Loss: 3074.035156\n",
      "Training Batch: 17551 Loss: 3270.657715\n",
      "Training Batch: 17552 Loss: 3043.125732\n",
      "Training Batch: 17553 Loss: 3080.546875\n",
      "Training Batch: 17554 Loss: 3000.114258\n",
      "Training Batch: 17555 Loss: 2993.550293\n",
      "Training Batch: 17556 Loss: 3248.670410\n",
      "Training Batch: 17557 Loss: 3300.373535\n",
      "Training Batch: 17558 Loss: 3082.297852\n",
      "Training Batch: 17559 Loss: 3134.800781\n",
      "Training Batch: 17560 Loss: 3071.135742\n",
      "Training Batch: 17561 Loss: 3020.921875\n",
      "Training Batch: 17562 Loss: 2977.776855\n",
      "Training Batch: 17563 Loss: 3352.450195\n",
      "Training Batch: 17564 Loss: 2989.652832\n",
      "Training Batch: 17565 Loss: 3136.979004\n",
      "Training Batch: 17566 Loss: 3123.551025\n",
      "Training Batch: 17567 Loss: 3105.179932\n",
      "Training Batch: 17568 Loss: 3142.935547\n",
      "Training Batch: 17569 Loss: 3079.092041\n",
      "Training Batch: 17570 Loss: 3120.859375\n",
      "Training Batch: 17571 Loss: 3174.508301\n",
      "Training Batch: 17572 Loss: 3088.210205\n",
      "Training Batch: 17573 Loss: 3169.983154\n",
      "Training Batch: 17574 Loss: 3124.343262\n",
      "Training Batch: 17575 Loss: 3063.367920\n",
      "Training Batch: 17576 Loss: 2985.844727\n",
      "Training Batch: 17577 Loss: 3061.249756\n",
      "Training Batch: 17578 Loss: 3108.290527\n",
      "Training Batch: 17579 Loss: 3140.559570\n",
      "Training Batch: 17580 Loss: 3100.458496\n",
      "Training Batch: 17581 Loss: 3062.713867\n",
      "Training Batch: 17582 Loss: 3102.868164\n",
      "Training Batch: 17583 Loss: 2986.874023\n",
      "Training Batch: 17584 Loss: 3118.219727\n",
      "Training Batch: 17585 Loss: 3043.386719\n",
      "Training Batch: 17586 Loss: 2960.293701\n",
      "Training Batch: 17587 Loss: 2998.009766\n",
      "Training Batch: 17588 Loss: 3040.506348\n",
      "Training Batch: 17589 Loss: 3063.650879\n",
      "Training Batch: 17590 Loss: 2998.148193\n",
      "Training Batch: 17591 Loss: 3137.130859\n",
      "Training Batch: 17592 Loss: 3107.062500\n",
      "Training Batch: 17593 Loss: 3106.143066\n",
      "Training Batch: 17594 Loss: 3142.610840\n",
      "Training Batch: 17595 Loss: 3001.508789\n",
      "Training Batch: 17596 Loss: 3010.377930\n",
      "Training Batch: 17597 Loss: 3177.645020\n",
      "Training Batch: 17598 Loss: 3166.756836\n",
      "Training Batch: 17599 Loss: 3074.832520\n",
      "Training Batch: 17600 Loss: 3213.934570\n",
      "Training Batch: 17601 Loss: 3060.760254\n",
      "Training Batch: 17602 Loss: 3044.682861\n",
      "Training Batch: 17603 Loss: 3216.061035\n",
      "Training Batch: 17604 Loss: 3063.454590\n",
      "Training Batch: 17605 Loss: 3340.302246\n",
      "Training Batch: 17606 Loss: 3125.212891\n",
      "Training Batch: 17607 Loss: 2954.747559\n",
      "Training Batch: 17608 Loss: 3171.830811\n",
      "Training Batch: 17609 Loss: 3354.424316\n",
      "Training Batch: 17610 Loss: 3172.840576\n",
      "Training Batch: 17611 Loss: 3174.437500\n",
      "Training Batch: 17612 Loss: 3159.581299\n",
      "Training Batch: 17613 Loss: 2975.262939\n",
      "Training Batch: 17614 Loss: 3099.701660\n",
      "Training Batch: 17615 Loss: 3103.363281\n",
      "Training Batch: 17616 Loss: 3070.494385\n",
      "Training Batch: 17617 Loss: 3140.894043\n",
      "Training Batch: 17618 Loss: 3196.462402\n",
      "Training Batch: 17619 Loss: 3163.116211\n",
      "Training Batch: 17620 Loss: 3149.873535\n",
      "Training Batch: 17621 Loss: 3130.007324\n",
      "Training Batch: 17622 Loss: 3002.906250\n",
      "Training Batch: 17623 Loss: 3280.532471\n",
      "Training Batch: 17624 Loss: 3142.034180\n",
      "Training Batch: 17625 Loss: 2942.441406\n",
      "Training Batch: 17626 Loss: 3135.270508\n",
      "Training Batch: 17627 Loss: 2980.226318\n",
      "Training Batch: 17628 Loss: 3028.978516\n",
      "Training Batch: 17629 Loss: 2999.047852\n",
      "Training Batch: 17630 Loss: 2997.883789\n",
      "Training Batch: 17631 Loss: 3117.312988\n",
      "Training Batch: 17632 Loss: 3144.216797\n",
      "Training Batch: 17633 Loss: 3496.344727\n",
      "Training Batch: 17634 Loss: 3089.313477\n",
      "Training Batch: 17635 Loss: 3027.045898\n",
      "Training Batch: 17636 Loss: 3059.375488\n",
      "Training Batch: 17637 Loss: 3110.502930\n",
      "Training Batch: 17638 Loss: 3045.270996\n",
      "Training Batch: 17639 Loss: 3209.554199\n",
      "Training Batch: 17640 Loss: 2991.874756\n",
      "Training Batch: 17641 Loss: 3212.013184\n",
      "Training Batch: 17642 Loss: 3100.866699\n",
      "Training Batch: 17643 Loss: 3139.134766\n",
      "Training Batch: 17644 Loss: 3028.381836\n",
      "Training Batch: 17645 Loss: 3118.456543\n",
      "Training Batch: 17646 Loss: 3071.490234\n",
      "Training Batch: 17647 Loss: 3004.922852\n",
      "Training Batch: 17648 Loss: 3089.030762\n",
      "Training Batch: 17649 Loss: 3104.439453\n",
      "Training Batch: 17650 Loss: 2940.522461\n",
      "Training Batch: 17651 Loss: 2885.487793\n",
      "Training Batch: 17652 Loss: 2986.683350\n",
      "Training Batch: 17653 Loss: 3000.768066\n",
      "Training Batch: 17654 Loss: 2993.786865\n",
      "Training Batch: 17655 Loss: 3012.469971\n",
      "Training Batch: 17656 Loss: 3061.775879\n",
      "Training Batch: 17657 Loss: 3138.633545\n",
      "Training Batch: 17658 Loss: 3087.560059\n",
      "Training Batch: 17659 Loss: 3074.427246\n",
      "Training Batch: 17660 Loss: 2984.946533\n",
      "Training Batch: 17661 Loss: 2994.041748\n",
      "Training Batch: 17662 Loss: 3017.755371\n",
      "Training Batch: 17663 Loss: 3039.964355\n",
      "Training Batch: 17664 Loss: 3162.008789\n",
      "Training Batch: 17665 Loss: 3041.993652\n",
      "Training Batch: 17666 Loss: 3293.753906\n",
      "Training Batch: 17667 Loss: 3032.917236\n",
      "Training Batch: 17668 Loss: 2941.016602\n",
      "Training Batch: 17669 Loss: 2915.077881\n",
      "Training Batch: 17670 Loss: 2938.118164\n",
      "Training Batch: 17671 Loss: 3086.302490\n",
      "Training Batch: 17672 Loss: 3245.948730\n",
      "Training Batch: 17673 Loss: 3098.264648\n",
      "Training Batch: 17674 Loss: 3005.869141\n",
      "Training Batch: 17675 Loss: 3015.166992\n",
      "Training Batch: 17676 Loss: 2983.516602\n",
      "Training Batch: 17677 Loss: 3096.110840\n",
      "Training Batch: 17678 Loss: 3105.382324\n",
      "Training Batch: 17679 Loss: 2948.687500\n",
      "Training Batch: 17680 Loss: 3135.766602\n",
      "Training Batch: 17681 Loss: 3035.632324\n",
      "Training Batch: 17682 Loss: 2978.951660\n",
      "Training Batch: 17683 Loss: 3064.992188\n",
      "Training Batch: 17684 Loss: 3152.662598\n",
      "Training Batch: 17685 Loss: 3099.893555\n",
      "Training Batch: 17686 Loss: 3201.399414\n",
      "Training Batch: 17687 Loss: 3103.955811\n",
      "Training Batch: 17688 Loss: 3333.693848\n",
      "Training Batch: 17689 Loss: 3031.401367\n",
      "Training Batch: 17690 Loss: 2923.049805\n",
      "Training Batch: 17691 Loss: 3096.545654\n",
      "Training Batch: 17692 Loss: 3142.994141\n",
      "Training Batch: 17693 Loss: 3151.272461\n",
      "Training Batch: 17694 Loss: 3137.855469\n",
      "Training Batch: 17695 Loss: 3091.823242\n",
      "Training Batch: 17696 Loss: 3054.356445\n",
      "Training Batch: 17697 Loss: 3149.771973\n",
      "Training Batch: 17698 Loss: 3024.107910\n",
      "Training Batch: 17699 Loss: 3162.022949\n",
      "Training Batch: 17700 Loss: 3118.808105\n",
      "Training Batch: 17701 Loss: 3030.516357\n",
      "Training Batch: 17702 Loss: 3195.735840\n",
      "Training Batch: 17703 Loss: 3348.818115\n",
      "Training Batch: 17704 Loss: 3111.408936\n",
      "Training Batch: 17705 Loss: 3080.743652\n",
      "Training Batch: 17706 Loss: 3011.762207\n",
      "Training Batch: 17707 Loss: 3042.143799\n",
      "Training Batch: 17708 Loss: 3118.923828\n",
      "Training Batch: 17709 Loss: 3069.308105\n",
      "Training Batch: 17710 Loss: 3157.938232\n",
      "Training Batch: 17711 Loss: 3228.970703\n",
      "Training Batch: 17712 Loss: 3492.480957\n",
      "Training Batch: 17713 Loss: 3168.820801\n",
      "Training Batch: 17714 Loss: 3644.178223\n",
      "Training Batch: 17715 Loss: 3164.259521\n",
      "Training Batch: 17716 Loss: 3020.833496\n",
      "Training Batch: 17717 Loss: 3244.432129\n",
      "Training Batch: 17718 Loss: 3215.206055\n",
      "Training Batch: 17719 Loss: 3227.211670\n",
      "Training Batch: 17720 Loss: 2986.350342\n",
      "Training Batch: 17721 Loss: 3213.588135\n",
      "Training Batch: 17722 Loss: 3221.860840\n",
      "Training Batch: 17723 Loss: 3461.020752\n",
      "Training Batch: 17724 Loss: 3506.537598\n",
      "Training Batch: 17725 Loss: 3316.649414\n",
      "Training Batch: 17726 Loss: 3009.902832\n",
      "Training Batch: 17727 Loss: 2994.166992\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Batch: 17728 Loss: 3236.301025\n",
      "Training Batch: 17729 Loss: 3159.492432\n",
      "Training Batch: 17730 Loss: 3072.677979\n",
      "Training Batch: 17731 Loss: 3170.342041\n",
      "Training Batch: 17732 Loss: 3051.829102\n",
      "Training Batch: 17733 Loss: 3051.574951\n",
      "Training Batch: 17734 Loss: 3099.765869\n",
      "Training Batch: 17735 Loss: 3108.892578\n",
      "Training Batch: 17736 Loss: 3123.966553\n",
      "Training Batch: 17737 Loss: 3251.006836\n",
      "Training Batch: 17738 Loss: 2990.229980\n",
      "Training Batch: 17739 Loss: 3034.202148\n",
      "Training Batch: 17740 Loss: 3371.249512\n",
      "Training Batch: 17741 Loss: 3196.959961\n",
      "Training Batch: 17742 Loss: 3098.655762\n",
      "Training Batch: 17743 Loss: 3190.197754\n",
      "Training Batch: 17744 Loss: 3097.673584\n",
      "Training Batch: 17745 Loss: 3050.992676\n",
      "Training Batch: 17746 Loss: 3165.976318\n",
      "Training Batch: 17747 Loss: 3077.624756\n",
      "Training Batch: 17748 Loss: 3088.631836\n",
      "Training Batch: 17749 Loss: 3124.104736\n",
      "Training Batch: 17750 Loss: 3032.493652\n",
      "Training Batch: 17751 Loss: 3318.472656\n",
      "Training Batch: 17752 Loss: 2979.607910\n",
      "Training Batch: 17753 Loss: 3085.880371\n",
      "Training Batch: 17754 Loss: 2942.414307\n",
      "Training Batch: 17755 Loss: 3097.795166\n",
      "Training Batch: 17756 Loss: 3041.457275\n",
      "Training Batch: 17757 Loss: 3084.104980\n",
      "Training Batch: 17758 Loss: 3084.498535\n",
      "Training Batch: 17759 Loss: 3120.784912\n",
      "Training Batch: 17760 Loss: 3126.251953\n",
      "Training Batch: 17761 Loss: 3130.905762\n",
      "Training Batch: 17762 Loss: 3060.285645\n",
      "Training Batch: 17763 Loss: 3065.144043\n",
      "Training Batch: 17764 Loss: 3054.952881\n",
      "Training Batch: 17765 Loss: 3027.916016\n",
      "Training Batch: 17766 Loss: 3073.776367\n",
      "Training Batch: 17767 Loss: 2963.620850\n",
      "Training Batch: 17768 Loss: 3014.503174\n",
      "Training Batch: 17769 Loss: 3035.886719\n",
      "Training Batch: 17770 Loss: 3195.617676\n",
      "Training Batch: 17771 Loss: 3260.468018\n",
      "Training Batch: 17772 Loss: 3048.411133\n",
      "Training Batch: 17773 Loss: 3209.083008\n",
      "Training Batch: 17774 Loss: 3206.152832\n",
      "Training Batch: 17775 Loss: 3052.353027\n",
      "Training Batch: 17776 Loss: 3089.429443\n",
      "Training Batch: 17777 Loss: 3177.192871\n",
      "Training Batch: 17778 Loss: 3061.937500\n",
      "Training Batch: 17779 Loss: 3061.381348\n",
      "Training Batch: 17780 Loss: 3208.779785\n",
      "Training Batch: 17781 Loss: 3081.717773\n",
      "Training Batch: 17782 Loss: 3025.467773\n",
      "Training Batch: 17783 Loss: 3189.808838\n",
      "Training Batch: 17784 Loss: 3337.552246\n",
      "Training Batch: 17785 Loss: 3163.246582\n",
      "Training Batch: 17786 Loss: 3255.225098\n",
      "Training Batch: 17787 Loss: 3192.811768\n",
      "Training Batch: 17788 Loss: 2907.784912\n",
      "Training Batch: 17789 Loss: 3012.881348\n",
      "Training Batch: 17790 Loss: 3110.096680\n",
      "Training Batch: 17791 Loss: 3169.278809\n",
      "Training Batch: 17792 Loss: 3130.210938\n",
      "Training Batch: 17793 Loss: 3089.236572\n",
      "Training Batch: 17794 Loss: 3171.179199\n",
      "Training Batch: 17795 Loss: 3109.198730\n",
      "Training Batch: 17796 Loss: 3004.550293\n",
      "Training Batch: 17797 Loss: 3060.158203\n",
      "Training Batch: 17798 Loss: 3080.595215\n",
      "Training Batch: 17799 Loss: 2942.988281\n",
      "Training Batch: 17800 Loss: 3088.278320\n",
      "Training Batch: 17801 Loss: 3037.571533\n",
      "Training Batch: 17802 Loss: 3163.617188\n",
      "Training Batch: 17803 Loss: 2984.422363\n",
      "Training Batch: 17804 Loss: 2976.986328\n",
      "Training Batch: 17805 Loss: 3133.406738\n",
      "Training Batch: 17806 Loss: 3117.444336\n",
      "Training Batch: 17807 Loss: 3117.750244\n",
      "Training Batch: 17808 Loss: 3047.656250\n",
      "Training Batch: 17809 Loss: 3032.035889\n",
      "Training Batch: 17810 Loss: 3080.504883\n",
      "Training Batch: 17811 Loss: 3007.929688\n",
      "Training Batch: 17812 Loss: 2990.545166\n",
      "Training Batch: 17813 Loss: 3093.498535\n",
      "Training Batch: 17814 Loss: 3003.196045\n",
      "Training Batch: 17815 Loss: 3074.489746\n",
      "Training Batch: 17816 Loss: 3066.567871\n",
      "Training Batch: 17817 Loss: 3041.746826\n",
      "Training Batch: 17818 Loss: 3021.750977\n",
      "Training Batch: 17819 Loss: 3433.902832\n",
      "Training Batch: 17820 Loss: 3134.171387\n",
      "Training Batch: 17821 Loss: 3078.850586\n",
      "Training Batch: 17822 Loss: 2980.559082\n",
      "Training Batch: 17823 Loss: 3073.137207\n",
      "Training Batch: 17824 Loss: 3239.895264\n",
      "Training Batch: 17825 Loss: 3023.557129\n",
      "Training Batch: 17826 Loss: 2947.941406\n",
      "Training Batch: 17827 Loss: 3214.585938\n",
      "Training Batch: 17828 Loss: 3360.962891\n",
      "Training Batch: 17829 Loss: 3253.817871\n",
      "Training Batch: 17830 Loss: 3157.982666\n",
      "Training Batch: 17831 Loss: 2899.451660\n",
      "Training Batch: 17832 Loss: 2968.904785\n",
      "Training Batch: 17833 Loss: 3020.820068\n",
      "Training Batch: 17834 Loss: 2980.623047\n",
      "Training Batch: 17835 Loss: 3133.207275\n",
      "Training Batch: 17836 Loss: 3158.418457\n",
      "Training Batch: 17837 Loss: 3356.922852\n",
      "Training Batch: 17838 Loss: 3195.451172\n",
      "Training Batch: 17839 Loss: 3096.182617\n",
      "Training Batch: 17840 Loss: 3109.762207\n",
      "Training Batch: 17841 Loss: 3081.550781\n",
      "Training Batch: 17842 Loss: 3267.050293\n",
      "Training Batch: 17843 Loss: 3122.010742\n",
      "Training Batch: 17844 Loss: 3126.254395\n",
      "Training Batch: 17845 Loss: 3052.306396\n",
      "Training Batch: 17846 Loss: 3004.600098\n",
      "Training Batch: 17847 Loss: 3081.684082\n",
      "Training Batch: 17848 Loss: 3077.047363\n",
      "Training Batch: 17849 Loss: 2976.145020\n",
      "Training Batch: 17850 Loss: 2993.016602\n",
      "Training Batch: 17851 Loss: 2991.739746\n",
      "Training Batch: 17852 Loss: 3120.993652\n",
      "Training Batch: 17853 Loss: 2971.572754\n",
      "Training Batch: 17854 Loss: 3135.301758\n",
      "Training Batch: 17855 Loss: 3060.402832\n",
      "Training Batch: 17856 Loss: 3053.090820\n",
      "Training Batch: 17857 Loss: 3066.328125\n",
      "Training Batch: 17858 Loss: 3337.923096\n",
      "Training Batch: 17859 Loss: 3079.384766\n",
      "Training Batch: 17860 Loss: 3141.519531\n",
      "Training Batch: 17861 Loss: 3012.692871\n",
      "Training Batch: 17862 Loss: 3188.466797\n",
      "Training Batch: 17863 Loss: 3081.750977\n",
      "Training Batch: 17864 Loss: 3141.712402\n",
      "Training Batch: 17865 Loss: 3088.994141\n",
      "Training Batch: 17866 Loss: 3110.317627\n",
      "Training Batch: 17867 Loss: 2969.074707\n",
      "Training Batch: 17868 Loss: 3278.191895\n",
      "Training Batch: 17869 Loss: 3139.971680\n",
      "Training Batch: 17870 Loss: 3100.537109\n",
      "Training Batch: 17871 Loss: 3112.443848\n",
      "Training Batch: 17872 Loss: 3146.082520\n",
      "Training Batch: 17873 Loss: 3095.580566\n",
      "Training Batch: 17874 Loss: 3241.196777\n",
      "Training Batch: 17875 Loss: 2964.536377\n",
      "Training Batch: 17876 Loss: 3030.071289\n",
      "Training Batch: 17877 Loss: 3015.850830\n",
      "Training Batch: 17878 Loss: 3227.238770\n",
      "Training Batch: 17879 Loss: 3106.145508\n",
      "Training Batch: 17880 Loss: 3484.502930\n",
      "Training Batch: 17881 Loss: 3044.466064\n",
      "Training Batch: 17882 Loss: 3008.490234\n",
      "Training Batch: 17883 Loss: 3039.521484\n",
      "Training Batch: 17884 Loss: 2983.614014\n",
      "Training Batch: 17885 Loss: 3334.797852\n",
      "Training Batch: 17886 Loss: 3066.403320\n",
      "Training Batch: 17887 Loss: 3110.440430\n",
      "Training Batch: 17888 Loss: 3389.051270\n",
      "Training Batch: 17889 Loss: 3105.956055\n",
      "Training Batch: 17890 Loss: 3234.966553\n",
      "Training Batch: 17891 Loss: 3135.578613\n",
      "Training Batch: 17892 Loss: 3093.481445\n",
      "Training Batch: 17893 Loss: 3053.771484\n",
      "Training Batch: 17894 Loss: 2981.758301\n",
      "Training Batch: 17895 Loss: 3133.369873\n",
      "Training Batch: 17896 Loss: 3083.070801\n",
      "Training Batch: 17897 Loss: 3054.196289\n",
      "Training Batch: 17898 Loss: 3009.437500\n",
      "Training Batch: 17899 Loss: 3053.548828\n",
      "Training Batch: 17900 Loss: 3191.855713\n",
      "Training Batch: 17901 Loss: 3195.033691\n",
      "Training Batch: 17902 Loss: 3193.699951\n",
      "Training Batch: 17903 Loss: 3186.082520\n",
      "Training Batch: 17904 Loss: 2986.013428\n",
      "Training Batch: 17905 Loss: 3139.946289\n",
      "Training Batch: 17906 Loss: 3030.776367\n",
      "Training Batch: 17907 Loss: 3003.371338\n",
      "Training Batch: 17908 Loss: 3056.516357\n",
      "Training Batch: 17909 Loss: 3145.215332\n",
      "Training Batch: 17910 Loss: 3202.451660\n",
      "Training Batch: 17911 Loss: 3093.026123\n",
      "Training Batch: 17912 Loss: 3150.480957\n",
      "Training Batch: 17913 Loss: 3024.787109\n",
      "Training Batch: 17914 Loss: 3140.617188\n",
      "Training Batch: 17915 Loss: 3020.877930\n",
      "Training Batch: 17916 Loss: 3076.158203\n",
      "Training Batch: 17917 Loss: 2928.173828\n",
      "Training Batch: 17918 Loss: 3234.709961\n",
      "Training Batch: 17919 Loss: 3090.979004\n",
      "Training Batch: 17920 Loss: 3006.081055\n",
      "Training Batch: 17921 Loss: 3014.927734\n",
      "Training Batch: 17922 Loss: 3047.393066\n",
      "Training Batch: 17923 Loss: 2980.595459\n",
      "Training Batch: 17924 Loss: 3149.304932\n",
      "Training Batch: 17925 Loss: 3036.618164\n",
      "Training Batch: 17926 Loss: 2981.023438\n",
      "Training Batch: 17927 Loss: 3092.264160\n",
      "Training Batch: 17928 Loss: 3450.924805\n",
      "Training Batch: 17929 Loss: 3156.660400\n",
      "Training Batch: 17930 Loss: 3260.619629\n",
      "Training Batch: 17931 Loss: 3153.373535\n",
      "Training Batch: 17932 Loss: 3011.807617\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Batch: 17933 Loss: 3024.938477\n",
      "Training Batch: 17934 Loss: 3110.578125\n",
      "Training Batch: 17935 Loss: 3046.908203\n",
      "Training Batch: 17936 Loss: 2923.270508\n",
      "Training Batch: 17937 Loss: 3086.701904\n",
      "Training Batch: 17938 Loss: 3013.443115\n",
      "Training Batch: 17939 Loss: 3103.881836\n",
      "Training Batch: 17940 Loss: 3222.344238\n",
      "Training Batch: 17941 Loss: 3335.965576\n",
      "Training Batch: 17942 Loss: 3048.088379\n",
      "Training Batch: 17943 Loss: 3050.013672\n",
      "Training Batch: 17944 Loss: 2981.668457\n",
      "Training Batch: 17945 Loss: 3255.733398\n",
      "Training Batch: 17946 Loss: 3237.397461\n",
      "Training Batch: 17947 Loss: 2968.781738\n",
      "Training Batch: 17948 Loss: 3123.894287\n",
      "Training Batch: 17949 Loss: 3075.201416\n",
      "Training Batch: 17950 Loss: 3192.280273\n",
      "Training Batch: 17951 Loss: 3003.329102\n",
      "Training Batch: 17952 Loss: 3031.764160\n",
      "Training Batch: 17953 Loss: 3051.090820\n",
      "Training Batch: 17954 Loss: 3016.074707\n",
      "Training Batch: 17955 Loss: 3271.810059\n",
      "Training Batch: 17956 Loss: 3105.530273\n",
      "Training Batch: 17957 Loss: 3149.205078\n",
      "Training Batch: 17958 Loss: 2982.910156\n",
      "Training Batch: 17959 Loss: 3101.850586\n",
      "Training Batch: 17960 Loss: 2986.274414\n",
      "Training Batch: 17961 Loss: 3033.122559\n",
      "Training Batch: 17962 Loss: 3078.230225\n",
      "Training Batch: 17963 Loss: 3020.408936\n",
      "Training Batch: 17964 Loss: 2975.126465\n",
      "Training Batch: 17965 Loss: 3021.071289\n",
      "Training Batch: 17966 Loss: 3281.164307\n",
      "Training Batch: 17967 Loss: 2978.708984\n",
      "Training Batch: 17968 Loss: 3082.511230\n",
      "Training Batch: 17969 Loss: 3001.354492\n",
      "Training Batch: 17970 Loss: 3094.767090\n",
      "Training Batch: 17971 Loss: 3092.918213\n",
      "Training Batch: 17972 Loss: 3030.948975\n",
      "Training Batch: 17973 Loss: 3014.482666\n",
      "Training Batch: 17974 Loss: 2993.369141\n",
      "Training Batch: 17975 Loss: 2927.749512\n",
      "Training Batch: 17976 Loss: 2983.401611\n",
      "Training Batch: 17977 Loss: 2973.854492\n",
      "Training Batch: 17978 Loss: 3193.313477\n",
      "Training Batch: 17979 Loss: 2962.218750\n",
      "Training Batch: 17980 Loss: 3056.537842\n",
      "Training Batch: 17981 Loss: 3065.757324\n",
      "Training Batch: 17982 Loss: 2976.912598\n",
      "Training Batch: 17983 Loss: 2965.416260\n",
      "Training Batch: 17984 Loss: 2999.352051\n",
      "Training Batch: 17985 Loss: 3035.729248\n",
      "Training Batch: 17986 Loss: 2958.978516\n",
      "Training Batch: 17987 Loss: 2999.441895\n",
      "Training Batch: 17988 Loss: 3103.178711\n",
      "Training Batch: 17989 Loss: 3184.209961\n",
      "Training Batch: 17990 Loss: 3026.790039\n",
      "Training Batch: 17991 Loss: 3037.179199\n",
      "Training Batch: 17992 Loss: 3092.394043\n",
      "Training Batch: 17993 Loss: 3104.141602\n",
      "Training Batch: 17994 Loss: 3032.622070\n",
      "Training Batch: 17995 Loss: 3351.352051\n",
      "Training Batch: 17996 Loss: 3005.481934\n",
      "Training Batch: 17997 Loss: 3105.690918\n",
      "Training Batch: 17998 Loss: 3272.247559\n",
      "Training Batch: 17999 Loss: 2987.941650\n",
      "Training Batch: 18000 Loss: 3060.835938\n",
      "Training Batch: 18001 Loss: 3738.123535\n",
      "Training Batch: 18002 Loss: 3016.039551\n",
      "Training Batch: 18003 Loss: 3197.122559\n",
      "Training Batch: 18004 Loss: 3149.916992\n",
      "Training Batch: 18005 Loss: 3324.061279\n",
      "Training Batch: 18006 Loss: 3343.366699\n",
      "Training Batch: 18007 Loss: 3022.611328\n",
      "Training Batch: 18008 Loss: 3036.211914\n",
      "Training Batch: 18009 Loss: 2937.167236\n",
      "Training Batch: 18010 Loss: 3015.103027\n",
      "Training Batch: 18011 Loss: 2970.719482\n",
      "Training Batch: 18012 Loss: 2916.664062\n",
      "Training Batch: 18013 Loss: 3119.302246\n",
      "Training Batch: 18014 Loss: 2893.725586\n",
      "Training Batch: 18015 Loss: 3098.053467\n",
      "Training Batch: 18016 Loss: 3108.980957\n",
      "Training Batch: 18017 Loss: 3000.724121\n",
      "Training Batch: 18018 Loss: 2972.909180\n",
      "Training Batch: 18019 Loss: 2989.619141\n",
      "Training Batch: 18020 Loss: 3175.278320\n",
      "Training Batch: 18021 Loss: 3129.072510\n",
      "Training Batch: 18022 Loss: 3086.148438\n",
      "Training Batch: 18023 Loss: 3054.740723\n",
      "Training Batch: 18024 Loss: 3133.256348\n",
      "Training Batch: 18025 Loss: 3061.794434\n",
      "Training Batch: 18026 Loss: 3168.682617\n",
      "Training Batch: 18027 Loss: 3222.989746\n",
      "Training Batch: 18028 Loss: 3023.270508\n",
      "Training Batch: 18029 Loss: 3162.239746\n",
      "Training Batch: 18030 Loss: 3031.521484\n",
      "Training Batch: 18031 Loss: 2940.592529\n",
      "Training Batch: 18032 Loss: 3047.425049\n",
      "Training Batch: 18033 Loss: 3008.937744\n",
      "Training Batch: 18034 Loss: 3388.288086\n",
      "Training Batch: 18035 Loss: 2946.516846\n",
      "Training Batch: 18036 Loss: 2975.965820\n",
      "Training Batch: 18037 Loss: 3102.339844\n",
      "Training Batch: 18038 Loss: 3002.921875\n",
      "Training Batch: 18039 Loss: 3009.574219\n",
      "Training Batch: 18040 Loss: 2993.359619\n",
      "Training Batch: 18041 Loss: 3089.611816\n",
      "Training Batch: 18042 Loss: 3014.774414\n",
      "Training Batch: 18043 Loss: 3217.165283\n",
      "Training Batch: 18044 Loss: 2986.362793\n",
      "Training Batch: 18045 Loss: 3138.352051\n",
      "Training Batch: 18046 Loss: 2872.250977\n",
      "Training Batch: 18047 Loss: 3106.343994\n",
      "Training Batch: 18048 Loss: 3059.882812\n",
      "Training Batch: 18049 Loss: 3167.441162\n",
      "Training Batch: 18050 Loss: 3035.784668\n",
      "Training Batch: 18051 Loss: 3236.987305\n",
      "Training Batch: 18052 Loss: 2968.993408\n",
      "Training Batch: 18053 Loss: 3077.870850\n",
      "Training Batch: 18054 Loss: 2983.931641\n",
      "Training Batch: 18055 Loss: 3048.065430\n",
      "Training Batch: 18056 Loss: 2940.754395\n",
      "Training Batch: 18057 Loss: 3092.295654\n",
      "Training Batch: 18058 Loss: 3006.917725\n",
      "Training Batch: 18059 Loss: 3027.413574\n",
      "Training Batch: 18060 Loss: 3149.743896\n",
      "Training Batch: 18061 Loss: 2964.327881\n",
      "Training Batch: 18062 Loss: 3179.948486\n",
      "Training Batch: 18063 Loss: 3194.435547\n",
      "Training Batch: 18064 Loss: 3048.980469\n",
      "Training Batch: 18065 Loss: 3004.332520\n",
      "Training Batch: 18066 Loss: 3015.701172\n",
      "Training Batch: 18067 Loss: 2890.853271\n",
      "Training Batch: 18068 Loss: 3040.173096\n",
      "Training Batch: 18069 Loss: 3154.928223\n",
      "Training Batch: 18070 Loss: 3020.503418\n",
      "Training Batch: 18071 Loss: 2937.621094\n",
      "Training Batch: 18072 Loss: 3351.206055\n",
      "Training Batch: 18073 Loss: 3156.890869\n",
      "Training Batch: 18074 Loss: 3241.412109\n",
      "Training Batch: 18075 Loss: 3255.984131\n",
      "Training Batch: 18076 Loss: 3000.387451\n",
      "Training Batch: 18077 Loss: 3086.125488\n",
      "Training Batch: 18078 Loss: 2996.528320\n",
      "Training Batch: 18079 Loss: 3123.483887\n",
      "Training Batch: 18080 Loss: 2975.179688\n",
      "Training Batch: 18081 Loss: 3010.602539\n",
      "Training Batch: 18082 Loss: 3076.595703\n",
      "Training Batch: 18083 Loss: 3094.521240\n",
      "Training Batch: 18084 Loss: 3089.090820\n",
      "Training Batch: 18085 Loss: 3054.006592\n",
      "Training Batch: 18086 Loss: 2973.348633\n",
      "Training Batch: 18087 Loss: 3008.644043\n",
      "Training Batch: 18088 Loss: 3034.928223\n",
      "Training Batch: 18089 Loss: 3102.194824\n",
      "Training Batch: 18090 Loss: 3092.811035\n",
      "Training Batch: 18091 Loss: 3019.582520\n",
      "Training Batch: 18092 Loss: 3131.019775\n",
      "Training Batch: 18093 Loss: 3104.820801\n",
      "Training Batch: 18094 Loss: 3166.503418\n",
      "Training Batch: 18095 Loss: 3011.758545\n",
      "Training Batch: 18096 Loss: 2991.791992\n",
      "Training Batch: 18097 Loss: 3091.079590\n",
      "Training Batch: 18098 Loss: 3659.604980\n",
      "Training Batch: 18099 Loss: 3127.630127\n",
      "Training Batch: 18100 Loss: 3067.375977\n",
      "Training Batch: 18101 Loss: 3117.673828\n",
      "Training Batch: 18102 Loss: 3001.305664\n",
      "Training Batch: 18103 Loss: 3252.823242\n",
      "Training Batch: 18104 Loss: 3000.592041\n",
      "Training Batch: 18105 Loss: 3099.493164\n",
      "Training Batch: 18106 Loss: 3164.986816\n",
      "Training Batch: 18107 Loss: 3005.908203\n",
      "Training Batch: 18108 Loss: 3037.583008\n",
      "Training Batch: 18109 Loss: 3202.213867\n",
      "Training Batch: 18110 Loss: 2939.240234\n",
      "Training Batch: 18111 Loss: 3064.864746\n",
      "Training Batch: 18112 Loss: 3261.031982\n",
      "Training Batch: 18113 Loss: 3104.706055\n",
      "Training Batch: 18114 Loss: 2933.374268\n",
      "Training Batch: 18115 Loss: 2938.811035\n",
      "Training Batch: 18116 Loss: 3076.474609\n",
      "Training Batch: 18117 Loss: 3031.831543\n",
      "Training Batch: 18118 Loss: 3208.532227\n",
      "Training Batch: 18119 Loss: 2949.197998\n",
      "Training Batch: 18120 Loss: 3350.887207\n",
      "Training Batch: 18121 Loss: 3044.475586\n",
      "Training Batch: 18122 Loss: 3186.223145\n",
      "Training Batch: 18123 Loss: 3075.455566\n",
      "Training Batch: 18124 Loss: 3549.927979\n",
      "Training Batch: 18125 Loss: 3073.805420\n",
      "Training Batch: 18126 Loss: 3001.168457\n",
      "Training Batch: 18127 Loss: 3008.820312\n",
      "Training Batch: 18128 Loss: 3180.497559\n",
      "Training Batch: 18129 Loss: 3050.803223\n",
      "Training Batch: 18130 Loss: 3213.806641\n",
      "Training Batch: 18131 Loss: 3229.468262\n",
      "Training Batch: 18132 Loss: 3139.741699\n",
      "Training Batch: 18133 Loss: 3174.237061\n",
      "Training Batch: 18134 Loss: 3038.497559\n",
      "Training Batch: 18135 Loss: 3316.357422\n",
      "Training Batch: 18136 Loss: 3283.658691\n",
      "Training Batch: 18137 Loss: 3086.917480\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Batch: 18138 Loss: 3141.551270\n",
      "Training Batch: 18139 Loss: 2949.125000\n",
      "Training Batch: 18140 Loss: 3188.815430\n",
      "Training Batch: 18141 Loss: 3087.755615\n",
      "Training Batch: 18142 Loss: 3048.154785\n",
      "Training Batch: 18143 Loss: 3166.396729\n",
      "Training Batch: 18144 Loss: 3035.070312\n",
      "Training Batch: 18145 Loss: 3127.021973\n",
      "Training Batch: 18146 Loss: 3068.792480\n",
      "Training Batch: 18147 Loss: 3293.608887\n",
      "Training Batch: 18148 Loss: 2914.467285\n",
      "Training Batch: 18149 Loss: 2960.037109\n",
      "Training Batch: 18150 Loss: 3011.441406\n",
      "Training Batch: 18151 Loss: 2977.587891\n",
      "Training Batch: 18152 Loss: 3014.179688\n",
      "Training Batch: 18153 Loss: 2992.302002\n",
      "Training Batch: 18154 Loss: 3099.068604\n",
      "Training Batch: 18155 Loss: 2999.650146\n",
      "Training Batch: 18156 Loss: 3191.027588\n",
      "Training Batch: 18157 Loss: 3090.973633\n",
      "Training Batch: 18158 Loss: 3061.711426\n",
      "Training Batch: 18159 Loss: 3119.708008\n",
      "Training Batch: 18160 Loss: 3111.011230\n",
      "Training Batch: 18161 Loss: 3041.716797\n",
      "Training Batch: 18162 Loss: 2968.744141\n",
      "Training Batch: 18163 Loss: 3218.280273\n",
      "Training Batch: 18164 Loss: 3039.950928\n",
      "Training Batch: 18165 Loss: 2985.692383\n",
      "Training Batch: 18166 Loss: 3021.973633\n",
      "Training Batch: 18167 Loss: 3235.461914\n",
      "Training Batch: 18168 Loss: 3048.187988\n",
      "Training Batch: 18169 Loss: 3004.390137\n",
      "Training Batch: 18170 Loss: 3005.967773\n",
      "Training Batch: 18171 Loss: 3023.761230\n",
      "Training Batch: 18172 Loss: 3050.577637\n",
      "Training Batch: 18173 Loss: 2997.073730\n",
      "Training Batch: 18174 Loss: 3119.825195\n",
      "Training Batch: 18175 Loss: 3052.470703\n",
      "Training Batch: 18176 Loss: 2972.521973\n",
      "Training Batch: 18177 Loss: 3362.876465\n",
      "Training Batch: 18178 Loss: 3106.189941\n",
      "Training Batch: 18179 Loss: 3181.407471\n",
      "Training Batch: 18180 Loss: 2953.569824\n",
      "Training Batch: 18181 Loss: 3145.377930\n",
      "Training Batch: 18182 Loss: 3041.385498\n",
      "Training Batch: 18183 Loss: 3011.057861\n",
      "Training Batch: 18184 Loss: 3052.206055\n",
      "Training Batch: 18185 Loss: 2994.073730\n",
      "Training Batch: 18186 Loss: 3057.857422\n",
      "Training Batch: 18187 Loss: 2969.860840\n",
      "Training Batch: 18188 Loss: 2988.559814\n",
      "Training Batch: 18189 Loss: 2950.784180\n",
      "Training Batch: 18190 Loss: 2941.988770\n",
      "Training Batch: 18191 Loss: 3069.035645\n",
      "Training Batch: 18192 Loss: 3084.516113\n",
      "Training Batch: 18193 Loss: 3146.668457\n",
      "Training Batch: 18194 Loss: 3461.956299\n",
      "Training Batch: 18195 Loss: 3086.351562\n",
      "Training Batch: 18196 Loss: 3037.299805\n",
      "Training Batch: 18197 Loss: 3107.898438\n",
      "Training Batch: 18198 Loss: 3505.018066\n",
      "Training Batch: 18199 Loss: 3312.486328\n",
      "Training Batch: 18200 Loss: 2986.646484\n",
      "Training Batch: 18201 Loss: 2931.859375\n",
      "Training Batch: 18202 Loss: 3065.783203\n",
      "Training Batch: 18203 Loss: 2974.631348\n",
      "Training Batch: 18204 Loss: 3188.175293\n",
      "Training Batch: 18205 Loss: 3214.868652\n",
      "Training Batch: 18206 Loss: 3246.209961\n",
      "Training Batch: 18207 Loss: 3202.315430\n",
      "Training Batch: 18208 Loss: 3122.640625\n",
      "Training Batch: 18209 Loss: 3100.104736\n",
      "Training Batch: 18210 Loss: 3178.019775\n",
      "Training Batch: 18211 Loss: 3005.782227\n",
      "Training Batch: 18212 Loss: 2951.302002\n",
      "Training Batch: 18213 Loss: 3081.065186\n",
      "Training Batch: 18214 Loss: 3177.826904\n",
      "Training Batch: 18215 Loss: 3058.641846\n",
      "Training Batch: 18216 Loss: 3169.382812\n",
      "Training Batch: 18217 Loss: 3078.815430\n",
      "Training Batch: 18218 Loss: 3337.492188\n",
      "Training Batch: 18219 Loss: 3129.597168\n",
      "Training Batch: 18220 Loss: 3117.339600\n",
      "Training Batch: 18221 Loss: 3085.399658\n",
      "Training Batch: 18222 Loss: 3181.599365\n",
      "Training Batch: 18223 Loss: 3086.834473\n",
      "Training Batch: 18224 Loss: 3079.904297\n",
      "Training Batch: 18225 Loss: 2965.786377\n",
      "Training Batch: 18226 Loss: 3021.689941\n",
      "Training Batch: 18227 Loss: 3009.560303\n",
      "Training Batch: 18228 Loss: 3189.873047\n",
      "Training Batch: 18229 Loss: 2934.027832\n",
      "Training Batch: 18230 Loss: 2968.495605\n",
      "Training Batch: 18231 Loss: 3016.116699\n",
      "Training Batch: 18232 Loss: 3028.161133\n",
      "Training Batch: 18233 Loss: 3019.644775\n",
      "Training Batch: 18234 Loss: 3048.773438\n",
      "Training Batch: 18235 Loss: 3169.014160\n",
      "Training Batch: 18236 Loss: 3010.167725\n",
      "Training Batch: 18237 Loss: 2943.534668\n",
      "Training Batch: 18238 Loss: 3158.018066\n",
      "Training Batch: 18239 Loss: 2982.232422\n",
      "Training Batch: 18240 Loss: 3286.906250\n",
      "Training Batch: 18241 Loss: 3062.972168\n",
      "Training Batch: 18242 Loss: 3090.283936\n",
      "Training Batch: 18243 Loss: 3402.340576\n",
      "Training Batch: 18244 Loss: 3075.691406\n",
      "Training Batch: 18245 Loss: 3123.111816\n",
      "Training Batch: 18246 Loss: 3208.947754\n",
      "Training Batch: 18247 Loss: 3081.885010\n",
      "Training Batch: 18248 Loss: 3023.059570\n",
      "Training Batch: 18249 Loss: 2995.629883\n",
      "Training Batch: 18250 Loss: 3690.734863\n",
      "Training Batch: 18251 Loss: 3083.854980\n",
      "Training Batch: 18252 Loss: 3207.183350\n",
      "Training Batch: 18253 Loss: 3152.562256\n",
      "Training Batch: 18254 Loss: 3085.555176\n",
      "Training Batch: 18255 Loss: 3215.653320\n",
      "Training Batch: 18256 Loss: 3396.036621\n",
      "Training Batch: 18257 Loss: 3092.653809\n",
      "Training Batch: 18258 Loss: 3345.943359\n",
      "Training Batch: 18259 Loss: 2999.442383\n",
      "Training Batch: 18260 Loss: 2951.555664\n",
      "Training Batch: 18261 Loss: 3143.594727\n",
      "Training Batch: 18262 Loss: 2961.687744\n",
      "Training Batch: 18263 Loss: 3098.982178\n",
      "Training Batch: 18264 Loss: 2992.822266\n",
      "Training Batch: 18265 Loss: 2957.058594\n",
      "Training Batch: 18266 Loss: 3082.349854\n",
      "Training Batch: 18267 Loss: 3053.647461\n",
      "Training Batch: 18268 Loss: 3082.023193\n",
      "Training Batch: 18269 Loss: 2990.582520\n",
      "Training Batch: 18270 Loss: 2895.350830\n",
      "Training Batch: 18271 Loss: 2984.645996\n",
      "Training Batch: 18272 Loss: 3015.173584\n",
      "Training Batch: 18273 Loss: 3120.357910\n",
      "Training Batch: 18274 Loss: 2938.424316\n",
      "Training Batch: 18275 Loss: 3280.645996\n",
      "Training Batch: 18276 Loss: 3072.203125\n",
      "Training Batch: 18277 Loss: 3119.104980\n",
      "Training Batch: 18278 Loss: 3266.074707\n",
      "Training Batch: 18279 Loss: 3109.832031\n",
      "Training Batch: 18280 Loss: 3062.844727\n",
      "Training Batch: 18281 Loss: 3074.345459\n",
      "Training Batch: 18282 Loss: 2975.702637\n",
      "Training Batch: 18283 Loss: 3150.479980\n",
      "Training Batch: 18284 Loss: 3037.019287\n",
      "Training Batch: 18285 Loss: 3118.363037\n",
      "Training Batch: 18286 Loss: 3385.183594\n",
      "Training Batch: 18287 Loss: 3152.480957\n",
      "Training Batch: 18288 Loss: 3292.506836\n",
      "Training Batch: 18289 Loss: 2901.293701\n",
      "Training Batch: 18290 Loss: 2935.733887\n",
      "Training Batch: 18291 Loss: 3019.008301\n",
      "Training Batch: 18292 Loss: 3039.289062\n",
      "Training Batch: 18293 Loss: 3388.990234\n",
      "Training Batch: 18294 Loss: 2984.021973\n",
      "Training Batch: 18295 Loss: 3026.560303\n",
      "Training Batch: 18296 Loss: 2993.037109\n",
      "Training Batch: 18297 Loss: 3253.056885\n",
      "Training Batch: 18298 Loss: 2939.773682\n",
      "Training Batch: 18299 Loss: 3115.060303\n",
      "Training Batch: 18300 Loss: 2982.832031\n",
      "Training Batch: 18301 Loss: 2941.370361\n",
      "Training Batch: 18302 Loss: 2962.622559\n",
      "Training Batch: 18303 Loss: 2977.696289\n",
      "Training Batch: 18304 Loss: 2973.894043\n",
      "Training Batch: 18305 Loss: 3019.290771\n",
      "Training Batch: 18306 Loss: 2944.496582\n",
      "Training Batch: 18307 Loss: 3031.263916\n",
      "Training Batch: 18308 Loss: 2996.169189\n",
      "Training Batch: 18309 Loss: 2936.560059\n",
      "Training Batch: 18310 Loss: 2964.591309\n",
      "Training Batch: 18311 Loss: 3191.999023\n",
      "Training Batch: 18312 Loss: 2900.604004\n",
      "Training Batch: 18313 Loss: 3104.106689\n",
      "Training Batch: 18314 Loss: 3025.859131\n",
      "Training Batch: 18315 Loss: 3011.699219\n",
      "Training Batch: 18316 Loss: 3038.282471\n",
      "Training Batch: 18317 Loss: 3205.225342\n",
      "Training Batch: 18318 Loss: 3046.938965\n",
      "Training Batch: 18319 Loss: 3125.057129\n",
      "Training Batch: 18320 Loss: 2960.902588\n",
      "Training Batch: 18321 Loss: 3038.522461\n",
      "Training Batch: 18322 Loss: 3006.995605\n",
      "Training Batch: 18323 Loss: 3153.068359\n",
      "Training Batch: 18324 Loss: 3064.363281\n",
      "Training Batch: 18325 Loss: 2887.647217\n",
      "Training Batch: 18326 Loss: 3006.969482\n",
      "Training Batch: 18327 Loss: 3076.023438\n",
      "Training Batch: 18328 Loss: 3035.249023\n",
      "Training Batch: 18329 Loss: 2987.890137\n",
      "Training Batch: 18330 Loss: 2980.155762\n",
      "Training Batch: 18331 Loss: 3750.324219\n",
      "Training Batch: 18332 Loss: 2998.975586\n",
      "Training Batch: 18333 Loss: 3147.005615\n",
      "Training Batch: 18334 Loss: 2944.596191\n",
      "Training Batch: 18335 Loss: 3079.998047\n",
      "Training Batch: 18336 Loss: 3132.191895\n",
      "Training Batch: 18337 Loss: 2919.154297\n",
      "Training Batch: 18338 Loss: 3037.763184\n",
      "Training Batch: 18339 Loss: 3077.431152\n",
      "Training Batch: 18340 Loss: 2924.332764\n",
      "Training Batch: 18341 Loss: 3057.579102\n",
      "Training Batch: 18342 Loss: 3227.390869\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Batch: 18343 Loss: 3211.272461\n",
      "Training Batch: 18344 Loss: 3104.275391\n",
      "Training Batch: 18345 Loss: 3087.318359\n",
      "Training Batch: 18346 Loss: 3048.386719\n",
      "Training Batch: 18347 Loss: 3053.943359\n",
      "Training Batch: 18348 Loss: 3100.062744\n",
      "Training Batch: 18349 Loss: 3040.510742\n",
      "Training Batch: 18350 Loss: 3170.300781\n",
      "Training Batch: 18351 Loss: 2997.268311\n",
      "Training Batch: 18352 Loss: 3267.426025\n",
      "Training Batch: 18353 Loss: 3142.156738\n",
      "Training Batch: 18354 Loss: 3027.233398\n",
      "Training Batch: 18355 Loss: 3191.908691\n",
      "Training Batch: 18356 Loss: 3238.216309\n",
      "Training Batch: 18357 Loss: 2970.368164\n",
      "Training Batch: 18358 Loss: 3119.805664\n",
      "Training Batch: 18359 Loss: 3081.855469\n",
      "Training Batch: 18360 Loss: 3266.436768\n",
      "Training Batch: 18361 Loss: 3003.446777\n",
      "Training Batch: 18362 Loss: 3051.989258\n",
      "Training Batch: 18363 Loss: 3155.698730\n",
      "Training Batch: 18364 Loss: 3047.606445\n",
      "Training Batch: 18365 Loss: 3140.250488\n",
      "Training Batch: 18366 Loss: 3037.218262\n",
      "Training Batch: 18367 Loss: 2998.288818\n",
      "Training Batch: 18368 Loss: 3046.958496\n",
      "Training Batch: 18369 Loss: 3038.239502\n",
      "Training Batch: 18370 Loss: 3243.023193\n",
      "Training Batch: 18371 Loss: 3151.906250\n",
      "Training Batch: 18372 Loss: 3090.440430\n",
      "Training Batch: 18373 Loss: 3046.693359\n",
      "Training Batch: 18374 Loss: 3016.365479\n",
      "Training Batch: 18375 Loss: 3153.390137\n",
      "Training Batch: 18376 Loss: 3196.182129\n",
      "Training Batch: 18377 Loss: 3118.857422\n",
      "Training Batch: 18378 Loss: 3061.415283\n",
      "Training Batch: 18379 Loss: 3017.970215\n",
      "Training Batch: 18380 Loss: 3184.448975\n",
      "Training Batch: 18381 Loss: 3095.220459\n",
      "Training Batch: 18382 Loss: 3009.371582\n",
      "Training Batch: 18383 Loss: 3137.993408\n",
      "Training Batch: 18384 Loss: 3142.299316\n",
      "Training Batch: 18385 Loss: 3124.181396\n",
      "Training Batch: 18386 Loss: 3047.323730\n",
      "Training Batch: 18387 Loss: 3113.508545\n",
      "Training Batch: 18388 Loss: 3204.981445\n",
      "Training Batch: 18389 Loss: 3275.079346\n",
      "Training Batch: 18390 Loss: 3067.525879\n",
      "Training Batch: 18391 Loss: 3056.628906\n",
      "Training Batch: 18392 Loss: 3151.639648\n",
      "Training Batch: 18393 Loss: 3167.648926\n",
      "Training Batch: 18394 Loss: 3188.531250\n",
      "Training Batch: 18395 Loss: 3099.341797\n",
      "Training Batch: 18396 Loss: 3083.836426\n",
      "Training Batch: 18397 Loss: 3074.646484\n",
      "Training Batch: 18398 Loss: 3066.347656\n",
      "Training Batch: 18399 Loss: 3053.184570\n",
      "Training Batch: 18400 Loss: 3001.697021\n",
      "Training Batch: 18401 Loss: 2992.794434\n",
      "Training Batch: 18402 Loss: 3118.957520\n",
      "Training Batch: 18403 Loss: 2970.529297\n",
      "Training Batch: 18404 Loss: 3007.018311\n",
      "Training Batch: 18405 Loss: 3042.854492\n",
      "Training Batch: 18406 Loss: 3035.447021\n",
      "Training Batch: 18407 Loss: 2961.198242\n",
      "Training Batch: 18408 Loss: 3130.868164\n",
      "Training Batch: 18409 Loss: 3107.754883\n",
      "Training Batch: 18410 Loss: 3020.237305\n",
      "Training Batch: 18411 Loss: 3154.787109\n",
      "Training Batch: 18412 Loss: 3323.393555\n",
      "Training Batch: 18413 Loss: 2984.901367\n",
      "Training Batch: 18414 Loss: 3276.635498\n",
      "Training Batch: 18415 Loss: 3108.142090\n",
      "Training Batch: 18416 Loss: 3174.644287\n",
      "Training Batch: 18417 Loss: 3142.180176\n",
      "Training Batch: 18418 Loss: 3101.303955\n",
      "Training Batch: 18419 Loss: 2975.409668\n",
      "Training Batch: 18420 Loss: 3177.974609\n",
      "Training Batch: 18421 Loss: 3065.592529\n",
      "Training Batch: 18422 Loss: 3227.796143\n",
      "Training Batch: 18423 Loss: 3102.608887\n",
      "Training Batch: 18424 Loss: 3069.322266\n",
      "Training Batch: 18425 Loss: 2976.686279\n",
      "Training Batch: 18426 Loss: 3027.528564\n",
      "Training Batch: 18427 Loss: 3064.003418\n",
      "Training Batch: 18428 Loss: 3085.359375\n",
      "Training Batch: 18429 Loss: 3031.281738\n",
      "Training Batch: 18430 Loss: 3099.112305\n",
      "Training Batch: 18431 Loss: 3036.395508\n",
      "Training Batch: 18432 Loss: 3044.273926\n",
      "Training Batch: 18433 Loss: 3015.411133\n",
      "Training Batch: 18434 Loss: 3022.176758\n",
      "Training Batch: 18435 Loss: 3082.957520\n",
      "Training Batch: 18436 Loss: 3084.150391\n",
      "Training Batch: 18437 Loss: 3158.343750\n",
      "Training Batch: 18438 Loss: 3106.012939\n",
      "Training Batch: 18439 Loss: 3045.280029\n",
      "Training Batch: 18440 Loss: 3126.799805\n",
      "Training Batch: 18441 Loss: 3101.501221\n",
      "Training Batch: 18442 Loss: 2996.308350\n",
      "Training Batch: 18443 Loss: 3048.663086\n",
      "Training Batch: 18444 Loss: 3218.008789\n",
      "Training Batch: 18445 Loss: 3271.469727\n",
      "Training Batch: 18446 Loss: 3145.980957\n",
      "Training Batch: 18447 Loss: 3301.678223\n",
      "Training Batch: 18448 Loss: 3012.578125\n",
      "Training Batch: 18449 Loss: 3168.718262\n",
      "Training Batch: 18450 Loss: 3095.326172\n",
      "Training Batch: 18451 Loss: 3049.468262\n",
      "Training Batch: 18452 Loss: 3079.251465\n",
      "Training Batch: 18453 Loss: 3071.762451\n",
      "Training Batch: 18454 Loss: 3160.743408\n",
      "Training Batch: 18455 Loss: 3019.009277\n",
      "Training Batch: 18456 Loss: 2965.575684\n",
      "Training Batch: 18457 Loss: 3062.550293\n",
      "Training Batch: 18458 Loss: 3096.177490\n",
      "Training Batch: 18459 Loss: 3004.319824\n",
      "Training Batch: 18460 Loss: 3200.244629\n",
      "Training Batch: 18461 Loss: 3254.101074\n",
      "Training Batch: 18462 Loss: 3218.764160\n",
      "Training Batch: 18463 Loss: 3023.237305\n",
      "Training Batch: 18464 Loss: 3013.171143\n",
      "Training Batch: 18465 Loss: 3128.593262\n",
      "Training Batch: 18466 Loss: 3036.912109\n",
      "Training Batch: 18467 Loss: 3267.987305\n",
      "Training Batch: 18468 Loss: 3013.941406\n",
      "Training Batch: 18469 Loss: 2985.658203\n",
      "Training Batch: 18470 Loss: 3021.730957\n",
      "Training Batch: 18471 Loss: 3033.482910\n",
      "Training Batch: 18472 Loss: 2960.270996\n",
      "Training Batch: 18473 Loss: 2988.756348\n",
      "Training Batch: 18474 Loss: 3355.519531\n",
      "Training Batch: 18475 Loss: 3229.422119\n",
      "Training Batch: 18476 Loss: 3028.529297\n",
      "Training Batch: 18477 Loss: 3401.754639\n",
      "Training Batch: 18478 Loss: 3534.829590\n",
      "Training Batch: 18479 Loss: 3202.211914\n",
      "Training Batch: 18480 Loss: 3367.754883\n",
      "Training Batch: 18481 Loss: 3010.758057\n",
      "Training Batch: 18482 Loss: 3104.223389\n",
      "Training Batch: 18483 Loss: 3067.363281\n",
      "Training Batch: 18484 Loss: 2989.536133\n",
      "Training Batch: 18485 Loss: 3168.430664\n",
      "Training Batch: 18486 Loss: 2955.789062\n",
      "Training Batch: 18487 Loss: 3052.480469\n",
      "Training Batch: 18488 Loss: 3091.873291\n",
      "Training Batch: 18489 Loss: 3215.506348\n",
      "Training Batch: 18490 Loss: 3114.033936\n",
      "Training Batch: 18491 Loss: 3204.629883\n",
      "Training Batch: 18492 Loss: 2960.733887\n",
      "Training Batch: 18493 Loss: 2995.648438\n",
      "Training Batch: 18494 Loss: 3149.940674\n",
      "Training Batch: 18495 Loss: 2990.378906\n",
      "Training Batch: 18496 Loss: 3030.491699\n",
      "Training Batch: 18497 Loss: 3248.728516\n",
      "Training Batch: 18498 Loss: 3514.744629\n",
      "Training Batch: 18499 Loss: 3264.387695\n",
      "Training Batch: 18500 Loss: 3096.979248\n",
      "Training Batch: 18501 Loss: 3312.920410\n",
      "Training Batch: 18502 Loss: 3016.471191\n",
      "Training Batch: 18503 Loss: 3076.592773\n",
      "Training Batch: 18504 Loss: 3125.935791\n",
      "Training Batch: 18505 Loss: 3042.922852\n",
      "Training Batch: 18506 Loss: 3115.348145\n",
      "Training Batch: 18507 Loss: 3042.672852\n",
      "Training Batch: 18508 Loss: 2944.967285\n",
      "Training Batch: 18509 Loss: 3289.469971\n",
      "Training Batch: 18510 Loss: 3201.644043\n",
      "Training Batch: 18511 Loss: 3064.472656\n",
      "Training Batch: 18512 Loss: 2985.680908\n",
      "Training Batch: 18513 Loss: 3054.056885\n",
      "Training Batch: 18514 Loss: 3084.009277\n",
      "Training Batch: 18515 Loss: 3167.974121\n",
      "Training Batch: 18516 Loss: 3060.423340\n",
      "Training Batch: 18517 Loss: 3133.544189\n",
      "Training Batch: 18518 Loss: 2996.291016\n",
      "Training Batch: 18519 Loss: 2918.650879\n",
      "Training Batch: 18520 Loss: 3039.421387\n",
      "Training Batch: 18521 Loss: 3089.583252\n",
      "Training Batch: 18522 Loss: 3100.814453\n",
      "Training Batch: 18523 Loss: 3022.431885\n",
      "Training Batch: 18524 Loss: 3091.928955\n",
      "Training Batch: 18525 Loss: 3089.048828\n",
      "Training Batch: 18526 Loss: 3248.057617\n",
      "Training Batch: 18527 Loss: 3157.571777\n",
      "Training Batch: 18528 Loss: 3221.227783\n",
      "Training Batch: 18529 Loss: 3129.052002\n",
      "Training Batch: 18530 Loss: 3123.144531\n",
      "Training Batch: 18531 Loss: 2995.479492\n",
      "Training Batch: 18532 Loss: 3006.967285\n",
      "Training Batch: 18533 Loss: 3115.811523\n",
      "Training Batch: 18534 Loss: 3142.956055\n",
      "Training Batch: 18535 Loss: 3093.906738\n",
      "Training Batch: 18536 Loss: 3154.975098\n",
      "Training Batch: 18537 Loss: 2977.818848\n",
      "Training Batch: 18538 Loss: 3082.666016\n",
      "Training Batch: 18539 Loss: 3119.939453\n",
      "Training Batch: 18540 Loss: 3036.481445\n",
      "Training Batch: 18541 Loss: 3132.769531\n",
      "Training Batch: 18542 Loss: 3007.427246\n",
      "Training Batch: 18543 Loss: 3095.393799\n",
      "Training Batch: 18544 Loss: 3120.758301\n",
      "Training Batch: 18545 Loss: 3097.546875\n",
      "Training Batch: 18546 Loss: 3076.351562\n",
      "Training Batch: 18547 Loss: 3171.624268\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Batch: 18548 Loss: 2981.073242\n",
      "Training Batch: 18549 Loss: 3359.503906\n",
      "Training Batch: 18550 Loss: 3097.210938\n",
      "Training Batch: 18551 Loss: 3097.256836\n",
      "Training Batch: 18552 Loss: 3033.262939\n",
      "Training Batch: 18553 Loss: 3099.946777\n",
      "Training Batch: 18554 Loss: 3190.204102\n",
      "Training Batch: 18555 Loss: 3131.865723\n",
      "Training Batch: 18556 Loss: 3080.137695\n",
      "Training Batch: 18557 Loss: 3081.812988\n",
      "Training Batch: 18558 Loss: 3082.971191\n",
      "Training Batch: 18559 Loss: 3067.347412\n",
      "Training Batch: 18560 Loss: 2992.972656\n",
      "Training Batch: 18561 Loss: 2966.131104\n",
      "Training Batch: 18562 Loss: 3010.384033\n",
      "Training Batch: 18563 Loss: 3101.401611\n",
      "Training Batch: 18564 Loss: 3014.944824\n",
      "Training Batch: 18565 Loss: 2995.407471\n",
      "Training Batch: 18566 Loss: 2990.693359\n",
      "Training Batch: 18567 Loss: 3048.209961\n",
      "Training Batch: 18568 Loss: 3065.900391\n",
      "Training Batch: 18569 Loss: 3248.484375\n",
      "Training Batch: 18570 Loss: 3073.456055\n",
      "Training Batch: 18571 Loss: 3074.740723\n",
      "Training Batch: 18572 Loss: 2932.081299\n",
      "Training Batch: 18573 Loss: 3101.356934\n",
      "Training Batch: 18574 Loss: 3081.348633\n",
      "Training Batch: 18575 Loss: 3044.598633\n",
      "Training Batch: 18576 Loss: 3055.241699\n",
      "Training Batch: 18577 Loss: 3073.129395\n",
      "Training Batch: 18578 Loss: 3044.449951\n",
      "Training Batch: 18579 Loss: 3007.772461\n",
      "Training Batch: 18580 Loss: 3106.716309\n",
      "Training Batch: 18581 Loss: 3146.050293\n",
      "Training Batch: 18582 Loss: 2992.883301\n",
      "Training Batch: 18583 Loss: 3102.925781\n",
      "Training Batch: 18584 Loss: 3022.492676\n",
      "Training Batch: 18585 Loss: 3093.710205\n",
      "Training Batch: 18586 Loss: 3053.941406\n",
      "Training Batch: 18587 Loss: 3208.954102\n",
      "Training Batch: 18588 Loss: 3117.747314\n",
      "Training Batch: 18589 Loss: 2982.693359\n",
      "Training Batch: 18590 Loss: 3164.240234\n",
      "Training Batch: 18591 Loss: 3007.474854\n",
      "Training Batch: 18592 Loss: 2920.654785\n",
      "Training Batch: 18593 Loss: 3059.184326\n",
      "Training Batch: 18594 Loss: 2921.989746\n",
      "Training Batch: 18595 Loss: 3075.899414\n",
      "Training Batch: 18596 Loss: 3178.092773\n",
      "Training Batch: 18597 Loss: 2997.996582\n",
      "Training Batch: 18598 Loss: 3109.793945\n",
      "Training Batch: 18599 Loss: 3068.778809\n",
      "Training Batch: 18600 Loss: 3124.667969\n",
      "Training Batch: 18601 Loss: 3051.821777\n",
      "Training Batch: 18602 Loss: 3068.258545\n",
      "Training Batch: 18603 Loss: 3406.045410\n",
      "Training Batch: 18604 Loss: 3064.519531\n",
      "Training Batch: 18605 Loss: 3030.591309\n",
      "Training Batch: 18606 Loss: 2988.205811\n",
      "Training Batch: 18607 Loss: 3220.541016\n",
      "Training Batch: 18608 Loss: 3174.485596\n",
      "Training Batch: 18609 Loss: 3138.591309\n",
      "Training Batch: 18610 Loss: 3019.545898\n",
      "Training Batch: 18611 Loss: 2973.958496\n",
      "Training Batch: 18612 Loss: 3032.915527\n",
      "Training Batch: 18613 Loss: 2940.750977\n",
      "Training Batch: 18614 Loss: 3123.851562\n",
      "Training Batch: 18615 Loss: 3152.649170\n",
      "Training Batch: 18616 Loss: 3251.556641\n",
      "Training Batch: 18617 Loss: 3009.929688\n",
      "Training Batch: 18618 Loss: 3034.895508\n",
      "Training Batch: 18619 Loss: 3146.908447\n",
      "Training Batch: 18620 Loss: 2993.235352\n",
      "Training Batch: 18621 Loss: 2999.729004\n",
      "Training Batch: 18622 Loss: 3072.144043\n",
      "Training Batch: 18623 Loss: 3046.703613\n",
      "Training Batch: 18624 Loss: 3001.355225\n",
      "Training Batch: 18625 Loss: 3223.765137\n",
      "Training Batch: 18626 Loss: 3114.205322\n",
      "Training Batch: 18627 Loss: 3068.943359\n",
      "Training Batch: 18628 Loss: 3083.495605\n",
      "Training Batch: 18629 Loss: 3069.412109\n",
      "Training Batch: 18630 Loss: 3010.310547\n",
      "Training Batch: 18631 Loss: 3099.586426\n",
      "Training Batch: 18632 Loss: 2974.576660\n",
      "Training Batch: 18633 Loss: 3088.128174\n",
      "Training Batch: 18634 Loss: 2957.103027\n",
      "Training Batch: 18635 Loss: 2970.840332\n",
      "Training Batch: 18636 Loss: 3103.656250\n",
      "Training Batch: 18637 Loss: 2932.554688\n",
      "Training Batch: 18638 Loss: 3039.595703\n",
      "Training Batch: 18639 Loss: 2971.228027\n",
      "Training Batch: 18640 Loss: 3122.999268\n",
      "Training Batch: 18641 Loss: 3400.460449\n",
      "Training Batch: 18642 Loss: 3273.641602\n",
      "Training Batch: 18643 Loss: 3149.915527\n",
      "Training Batch: 18644 Loss: 3192.230957\n",
      "Training Batch: 18645 Loss: 2981.157471\n",
      "Training Batch: 18646 Loss: 3071.458984\n",
      "Training Batch: 18647 Loss: 3076.490723\n",
      "Training Batch: 18648 Loss: 3264.788086\n",
      "Training Batch: 18649 Loss: 3064.818359\n",
      "Training Batch: 18650 Loss: 3039.652344\n",
      "Training Batch: 18651 Loss: 3084.432617\n",
      "Training Batch: 18652 Loss: 2981.370605\n",
      "Training Batch: 18653 Loss: 3021.082031\n",
      "Training Batch: 18654 Loss: 2915.052734\n",
      "Training Batch: 18655 Loss: 3026.755371\n",
      "Training Batch: 18656 Loss: 3045.424316\n",
      "Training Batch: 18657 Loss: 3144.573242\n",
      "Training Batch: 18658 Loss: 3227.035645\n",
      "Training Batch: 18659 Loss: 3002.539551\n",
      "Training Batch: 18660 Loss: 3150.877930\n",
      "Training Batch: 18661 Loss: 3108.638672\n",
      "Training Batch: 18662 Loss: 3005.578125\n",
      "Training Batch: 18663 Loss: 3084.904785\n",
      "Training Batch: 18664 Loss: 3037.116211\n",
      "Training Batch: 18665 Loss: 2967.173340\n",
      "Training Batch: 18666 Loss: 3162.906738\n",
      "Training Batch: 18667 Loss: 3200.351562\n",
      "Training Batch: 18668 Loss: 3107.010742\n",
      "Training Batch: 18669 Loss: 3089.105469\n",
      "Training Batch: 18670 Loss: 3247.757812\n",
      "Training Batch: 18671 Loss: 2899.159912\n",
      "Training Batch: 18672 Loss: 2985.036621\n",
      "Training Batch: 18673 Loss: 2975.673340\n",
      "Training Batch: 18674 Loss: 2974.051758\n",
      "Training Batch: 18675 Loss: 3152.605957\n",
      "Training Batch: 18676 Loss: 3015.305908\n",
      "Training Batch: 18677 Loss: 3193.947266\n",
      "Training Batch: 18678 Loss: 3464.413574\n",
      "Training Batch: 18679 Loss: 3286.959229\n",
      "Training Batch: 18680 Loss: 3077.689941\n",
      "Training Batch: 18681 Loss: 3028.113037\n",
      "Training Batch: 18682 Loss: 3150.549805\n",
      "Training Batch: 18683 Loss: 3012.627686\n",
      "Training Batch: 18684 Loss: 2997.076172\n",
      "Training Batch: 18685 Loss: 3085.623779\n",
      "Training Batch: 18686 Loss: 3052.995605\n",
      "Training Batch: 18687 Loss: 3070.558350\n",
      "Training Batch: 18688 Loss: 3062.070557\n",
      "Training Batch: 18689 Loss: 2987.178223\n",
      "Training Batch: 18690 Loss: 3164.629883\n",
      "Training Batch: 18691 Loss: 2913.234619\n",
      "Training Batch: 18692 Loss: 3060.112793\n",
      "Training Batch: 18693 Loss: 3159.139648\n",
      "Training Batch: 18694 Loss: 3009.980957\n",
      "Training Batch: 18695 Loss: 3054.474609\n",
      "Training Batch: 18696 Loss: 3068.169922\n",
      "Training Batch: 18697 Loss: 2989.909180\n",
      "Training Batch: 18698 Loss: 3035.998291\n",
      "Training Batch: 18699 Loss: 3269.959717\n",
      "Training Batch: 18700 Loss: 2967.450195\n",
      "Training Batch: 18701 Loss: 3021.011719\n",
      "Training Batch: 18702 Loss: 3176.863770\n",
      "Training Batch: 18703 Loss: 3326.312500\n",
      "Training Batch: 18704 Loss: 3155.018311\n",
      "Training Batch: 18705 Loss: 3151.996826\n",
      "Training Batch: 18706 Loss: 2983.555176\n",
      "Training Batch: 18707 Loss: 3146.202148\n",
      "Training Batch: 18708 Loss: 3080.408691\n",
      "Training Batch: 18709 Loss: 3112.139893\n",
      "Training Batch: 18710 Loss: 2973.755859\n",
      "Training Batch: 18711 Loss: 3019.038574\n",
      "Training Batch: 18712 Loss: 3091.359375\n",
      "Training Batch: 18713 Loss: 3080.877686\n",
      "Training Batch: 18714 Loss: 3082.564453\n",
      "Training Batch: 18715 Loss: 2940.280762\n",
      "Training Batch: 18716 Loss: 3028.852539\n",
      "Training Batch: 18717 Loss: 2978.675537\n",
      "Training Batch: 18718 Loss: 3136.892334\n",
      "Training Batch: 18719 Loss: 3234.614258\n",
      "Training Batch: 18720 Loss: 3071.398926\n",
      "Training Batch: 18721 Loss: 3039.235840\n",
      "Training Batch: 18722 Loss: 3085.636475\n",
      "Training Batch: 18723 Loss: 3152.267578\n",
      "Training Batch: 18724 Loss: 3093.145508\n",
      "Training Batch: 18725 Loss: 2979.913574\n",
      "Training Batch: 18726 Loss: 3069.700439\n",
      "Training Batch: 18727 Loss: 3106.280762\n",
      "Training Batch: 18728 Loss: 3323.711914\n",
      "Training Batch: 18729 Loss: 3118.033203\n",
      "Training Batch: 18730 Loss: 3146.675537\n",
      "Training Batch: 18731 Loss: 3280.903809\n",
      "Training Batch: 18732 Loss: 3068.110352\n",
      "Training Batch: 18733 Loss: 3321.650391\n",
      "Training Batch: 18734 Loss: 3102.383789\n",
      "Training Batch: 18735 Loss: 3091.537598\n",
      "Training Batch: 18736 Loss: 3063.526367\n",
      "Training Batch: 18737 Loss: 3077.928223\n",
      "Training Batch: 18738 Loss: 3046.745850\n",
      "Training Batch: 18739 Loss: 3071.692383\n",
      "Training Batch: 18740 Loss: 3053.301270\n",
      "Training Batch: 18741 Loss: 3022.884277\n",
      "Training Batch: 18742 Loss: 3011.431152\n",
      "Training Batch: 18743 Loss: 2996.942383\n",
      "Training Batch: 18744 Loss: 3371.173340\n",
      "Training Batch: 18745 Loss: 3378.464600\n",
      "Training Batch: 18746 Loss: 3128.244141\n",
      "Training Batch: 18747 Loss: 3148.763672\n",
      "Training Batch: 18748 Loss: 2970.646973\n",
      "Training Batch: 18749 Loss: 3128.923828\n",
      "Training Batch: 18750 Loss: 3109.751953\n",
      "Training Batch: 18751 Loss: 3038.164062\n",
      "Training Batch: 18752 Loss: 3041.155273\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Batch: 18753 Loss: 3331.117676\n",
      "Training Batch: 18754 Loss: 3027.138672\n",
      "Training Batch: 18755 Loss: 2918.097656\n",
      "Training Batch: 18756 Loss: 3034.731934\n",
      "Training Batch: 18757 Loss: 2981.499023\n",
      "Training Batch: 18758 Loss: 3024.723633\n",
      "Training Batch: 18759 Loss: 3093.242432\n",
      "Training Batch: 18760 Loss: 3267.855713\n",
      "Training Batch: 18761 Loss: 3070.196777\n",
      "Training Batch: 18762 Loss: 3111.614258\n",
      "Training Batch: 18763 Loss: 3010.648438\n",
      "Training Batch: 18764 Loss: 3136.845215\n",
      "Training Batch: 18765 Loss: 3123.025879\n",
      "Training Batch: 18766 Loss: 3275.666260\n",
      "Training Batch: 18767 Loss: 3119.649902\n",
      "Training Batch: 18768 Loss: 2996.650391\n",
      "Training Batch: 18769 Loss: 3113.262695\n",
      "Training Batch: 18770 Loss: 3015.199219\n",
      "Training Batch: 18771 Loss: 3003.888184\n",
      "Training Batch: 18772 Loss: 2930.695557\n",
      "Training Batch: 18773 Loss: 3058.540039\n",
      "Training Batch: 18774 Loss: 3039.051758\n",
      "Training Batch: 18775 Loss: 2968.627930\n",
      "Training Batch: 18776 Loss: 2919.956055\n",
      "Training Batch: 18777 Loss: 2982.959473\n",
      "Training Batch: 18778 Loss: 3012.328125\n",
      "Training Batch: 18779 Loss: 2959.547363\n",
      "Training Batch: 18780 Loss: 3031.603027\n",
      "Training Batch: 18781 Loss: 2971.331055\n",
      "Training Batch: 18782 Loss: 2919.229492\n",
      "Training Batch: 18783 Loss: 2992.144531\n",
      "Training Batch: 18784 Loss: 3022.071777\n",
      "Training Batch: 18785 Loss: 2951.138672\n",
      "Training Batch: 18786 Loss: 3144.041260\n",
      "Training Batch: 18787 Loss: 3027.852051\n",
      "Training Batch: 18788 Loss: 3204.650146\n",
      "Training Batch: 18789 Loss: 3075.758301\n",
      "Training Batch: 18790 Loss: 3011.085449\n",
      "Training Batch: 18791 Loss: 3064.900879\n",
      "Training Batch: 18792 Loss: 3012.114746\n",
      "Training Batch: 18793 Loss: 2983.916016\n",
      "Training Batch: 18794 Loss: 2937.394043\n",
      "Training Batch: 18795 Loss: 3000.430176\n",
      "Training Batch: 18796 Loss: 2984.271973\n",
      "Training Batch: 18797 Loss: 2973.666504\n",
      "Training Batch: 18798 Loss: 2999.595215\n",
      "Training Batch: 18799 Loss: 2961.125000\n",
      "Training Batch: 18800 Loss: 3180.497803\n",
      "Training Batch: 18801 Loss: 2974.843750\n",
      "Training Batch: 18802 Loss: 2926.448486\n",
      "Training Batch: 18803 Loss: 3015.797852\n",
      "Training Batch: 18804 Loss: 3187.847168\n",
      "Training Batch: 18805 Loss: 2913.957275\n",
      "Training Batch: 18806 Loss: 2923.753906\n",
      "Training Batch: 18807 Loss: 3022.038330\n",
      "Training Batch: 18808 Loss: 2955.892334\n",
      "Training Batch: 18809 Loss: 2969.199463\n",
      "Training Batch: 18810 Loss: 2938.882812\n",
      "Training Batch: 18811 Loss: 3094.096191\n",
      "Training Batch: 18812 Loss: 2977.295410\n",
      "Training Batch: 18813 Loss: 3161.268066\n",
      "Training Batch: 18814 Loss: 3018.583008\n",
      "Training Batch: 18815 Loss: 3270.985352\n",
      "Training Batch: 18816 Loss: 3175.792969\n",
      "Training Batch: 18817 Loss: 3008.252441\n",
      "Training Batch: 18818 Loss: 3019.849365\n",
      "Training Batch: 18819 Loss: 3003.413818\n",
      "Training Batch: 18820 Loss: 3200.452148\n",
      "Training Batch: 18821 Loss: 3062.091553\n",
      "Training Batch: 18822 Loss: 3090.966797\n",
      "Training Batch: 18823 Loss: 3317.390137\n",
      "Training Batch: 18824 Loss: 3181.135254\n",
      "Training Batch: 18825 Loss: 2974.967041\n",
      "Training Batch: 18826 Loss: 3086.115479\n",
      "Training Batch: 18827 Loss: 2990.265137\n",
      "Training Batch: 18828 Loss: 3046.463867\n",
      "Training Batch: 18829 Loss: 3114.652588\n",
      "Training Batch: 18830 Loss: 3238.088379\n",
      "Training Batch: 18831 Loss: 3157.041992\n",
      "Training Batch: 18832 Loss: 2990.726074\n",
      "Training Batch: 18833 Loss: 2976.179199\n",
      "Training Batch: 18834 Loss: 3027.125977\n",
      "Training Batch: 18835 Loss: 2966.723389\n",
      "Training Batch: 18836 Loss: 2872.837158\n",
      "Training Batch: 18837 Loss: 3066.447510\n",
      "Training Batch: 18838 Loss: 3110.581543\n",
      "Training Batch: 18839 Loss: 2971.697266\n",
      "Training Batch: 18840 Loss: 2954.650391\n",
      "Training Batch: 18841 Loss: 3066.872070\n",
      "Training Batch: 18842 Loss: 3043.754883\n",
      "Training Batch: 18843 Loss: 2957.206543\n",
      "Training Batch: 18844 Loss: 2908.015869\n",
      "Training Batch: 18845 Loss: 3036.753418\n",
      "Training Batch: 18846 Loss: 2959.997803\n",
      "Training Batch: 18847 Loss: 2970.000977\n",
      "Training Batch: 18848 Loss: 3086.593750\n",
      "Training Batch: 18849 Loss: 3016.368896\n",
      "Training Batch: 18850 Loss: 3063.562988\n",
      "Training Batch: 18851 Loss: 3044.952148\n",
      "Training Batch: 18852 Loss: 3036.093262\n",
      "Training Batch: 18853 Loss: 3139.564941\n",
      "Training Batch: 18854 Loss: 2960.814209\n",
      "Training Batch: 18855 Loss: 3123.039062\n",
      "Training Batch: 18856 Loss: 3154.252441\n",
      "Training Batch: 18857 Loss: 3083.545898\n",
      "Training Batch: 18858 Loss: 3109.395020\n",
      "Training Batch: 18859 Loss: 3150.084229\n",
      "Training Batch: 18860 Loss: 3025.143555\n",
      "Training Batch: 18861 Loss: 3025.992188\n",
      "Training Batch: 18862 Loss: 3144.838867\n",
      "Training Batch: 18863 Loss: 2972.335449\n",
      "Training Batch: 18864 Loss: 2968.830078\n",
      "Training Batch: 18865 Loss: 3078.965576\n",
      "Training Batch: 18866 Loss: 3031.698730\n",
      "Training Batch: 18867 Loss: 3244.822998\n",
      "Training Batch: 18868 Loss: 2927.640137\n",
      "Training Batch: 18869 Loss: 3096.292480\n",
      "Training Batch: 18870 Loss: 2934.862061\n",
      "Training Batch: 18871 Loss: 2963.618164\n",
      "Training Batch: 18872 Loss: 3060.590332\n",
      "Training Batch: 18873 Loss: 3052.815430\n",
      "Training Batch: 18874 Loss: 3340.676758\n",
      "Training Batch: 18875 Loss: 2984.149902\n",
      "Training Batch: 18876 Loss: 3090.846680\n",
      "Training Batch: 18877 Loss: 2975.226562\n",
      "Training Batch: 18878 Loss: 2999.866699\n",
      "Training Batch: 18879 Loss: 3203.232422\n",
      "Training Batch: 18880 Loss: 2994.085693\n",
      "Training Batch: 18881 Loss: 3195.616211\n",
      "Training Batch: 18882 Loss: 3319.040527\n",
      "Training Batch: 18883 Loss: 2933.738037\n",
      "Training Batch: 18884 Loss: 3185.473633\n",
      "Training Batch: 18885 Loss: 2995.212158\n",
      "Training Batch: 18886 Loss: 2998.787598\n",
      "Training Batch: 18887 Loss: 3061.112305\n",
      "Training Batch: 18888 Loss: 2986.396729\n",
      "Training Batch: 18889 Loss: 3043.717041\n",
      "Training Batch: 18890 Loss: 3015.828613\n",
      "Training Batch: 18891 Loss: 3156.276855\n",
      "Training Batch: 18892 Loss: 3005.478271\n",
      "Training Batch: 18893 Loss: 2939.896484\n",
      "Training Batch: 18894 Loss: 3094.255371\n",
      "Training Batch: 18895 Loss: 3036.321777\n",
      "Training Batch: 18896 Loss: 3060.300781\n",
      "Training Batch: 18897 Loss: 2954.561035\n",
      "Training Batch: 18898 Loss: 3289.538818\n",
      "Training Batch: 18899 Loss: 3115.773682\n",
      "Training Batch: 18900 Loss: 3042.148926\n",
      "Training Batch: 18901 Loss: 2973.981934\n",
      "Training Batch: 18902 Loss: 3077.478516\n",
      "Training Batch: 18903 Loss: 3065.113281\n",
      "Training Batch: 18904 Loss: 3047.897949\n",
      "Training Batch: 18905 Loss: 3127.366699\n",
      "Training Batch: 18906 Loss: 3055.915039\n",
      "Training Batch: 18907 Loss: 3055.033936\n",
      "Training Batch: 18908 Loss: 3141.886230\n",
      "Training Batch: 18909 Loss: 3119.560303\n",
      "Training Batch: 18910 Loss: 3108.763672\n",
      "Training Batch: 18911 Loss: 3005.586914\n",
      "Training Batch: 18912 Loss: 3107.674561\n",
      "Training Batch: 18913 Loss: 3216.642090\n",
      "Training Batch: 18914 Loss: 3222.093018\n",
      "Training Batch: 18915 Loss: 3113.191650\n",
      "Training Batch: 18916 Loss: 3119.452637\n",
      "Training Batch: 18917 Loss: 2960.098145\n",
      "Training Batch: 18918 Loss: 3075.121582\n",
      "Training Batch: 18919 Loss: 3003.799805\n",
      "Training Batch: 18920 Loss: 3032.336426\n",
      "Training Batch: 18921 Loss: 3182.495850\n",
      "Training Batch: 18922 Loss: 3077.077148\n",
      "Training Batch: 18923 Loss: 3022.025391\n",
      "Training Batch: 18924 Loss: 3291.213867\n",
      "Training Batch: 18925 Loss: 3428.137695\n",
      "Training Batch: 18926 Loss: 3083.256836\n",
      "Training Batch: 18927 Loss: 3050.998047\n",
      "Training Batch: 18928 Loss: 2970.268311\n",
      "Training Batch: 18929 Loss: 3101.436523\n",
      "Training Batch: 18930 Loss: 3155.185059\n",
      "Training Batch: 18931 Loss: 2931.021240\n",
      "Training Batch: 18932 Loss: 3137.327637\n",
      "Training Batch: 18933 Loss: 3135.084473\n",
      "Training Batch: 18934 Loss: 3095.554688\n",
      "Training Batch: 18935 Loss: 2975.584961\n",
      "Training Batch: 18936 Loss: 3020.164062\n",
      "Training Batch: 18937 Loss: 2961.557617\n",
      "Training Batch: 18938 Loss: 2951.393799\n",
      "Training Batch: 18939 Loss: 2961.321045\n",
      "Training Batch: 18940 Loss: 2925.948730\n",
      "Training Batch: 18941 Loss: 3220.436035\n",
      "Training Batch: 18942 Loss: 3134.219727\n",
      "Training Batch: 18943 Loss: 3007.533691\n",
      "Training Batch: 18944 Loss: 3095.823242\n",
      "Training Batch: 18945 Loss: 3061.876709\n",
      "Training Batch: 18946 Loss: 3050.950195\n",
      "Training Batch: 18947 Loss: 3022.609863\n",
      "Training Batch: 18948 Loss: 3002.745850\n",
      "Training Batch: 18949 Loss: 2897.676758\n",
      "Training Batch: 18950 Loss: 3015.379639\n",
      "Training Batch: 18951 Loss: 2927.305176\n",
      "Training Batch: 18952 Loss: 3045.163574\n",
      "Training Batch: 18953 Loss: 3141.208496\n",
      "Training Batch: 18954 Loss: 3016.704590\n",
      "Training Batch: 18955 Loss: 3196.787109\n",
      "Training Batch: 18956 Loss: 3067.883545\n",
      "Training Batch: 18957 Loss: 3194.121582\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Batch: 18958 Loss: 2913.702148\n",
      "Training Batch: 18959 Loss: 2879.585938\n",
      "Training Batch: 18960 Loss: 2990.124512\n",
      "Training Batch: 18961 Loss: 2988.948242\n",
      "Training Batch: 18962 Loss: 3193.072266\n",
      "Training Batch: 18963 Loss: 3049.812012\n",
      "Training Batch: 18964 Loss: 3250.287842\n",
      "Training Batch: 18965 Loss: 3084.526855\n",
      "Training Batch: 18966 Loss: 3005.628662\n",
      "Training Batch: 18967 Loss: 2971.413086\n",
      "Training Batch: 18968 Loss: 2947.725098\n",
      "Training Batch: 18969 Loss: 3057.898438\n",
      "Training Batch: 18970 Loss: 2953.402344\n",
      "Training Batch: 18971 Loss: 3014.931641\n",
      "Training Batch: 18972 Loss: 3294.560791\n",
      "Training Batch: 18973 Loss: 3021.561035\n",
      "Training Batch: 18974 Loss: 3096.962891\n",
      "Training Batch: 18975 Loss: 3045.001465\n",
      "Training Batch: 18976 Loss: 3042.979736\n",
      "Training Batch: 18977 Loss: 3110.360840\n",
      "Training Batch: 18978 Loss: 3098.775391\n",
      "Training Batch: 18979 Loss: 3080.293945\n",
      "Training Batch: 18980 Loss: 3097.718750\n",
      "Training Batch: 18981 Loss: 3195.326172\n",
      "Training Batch: 18982 Loss: 3283.442871\n",
      "Training Batch: 18983 Loss: 3018.933838\n",
      "Training Batch: 18984 Loss: 3019.027344\n",
      "Training Batch: 18985 Loss: 3055.862305\n",
      "Training Batch: 18986 Loss: 3021.321045\n",
      "Training Batch: 18987 Loss: 3043.557129\n",
      "Training Batch: 18988 Loss: 3049.042480\n",
      "Training Batch: 18989 Loss: 2985.095703\n",
      "Training Batch: 18990 Loss: 3121.010254\n",
      "Training Batch: 18991 Loss: 2965.945801\n",
      "Training Batch: 18992 Loss: 3264.522217\n",
      "Training Batch: 18993 Loss: 3481.693848\n",
      "Training Batch: 18994 Loss: 3086.278320\n",
      "Training Batch: 18995 Loss: 3284.927246\n",
      "Training Batch: 18996 Loss: 3288.784668\n",
      "Training Batch: 18997 Loss: 3100.678711\n",
      "Training Batch: 18998 Loss: 3145.362793\n",
      "Training Batch: 18999 Loss: 3711.965332\n",
      "Training Batch: 19000 Loss: 3090.012451\n",
      "Training Batch: 19001 Loss: 2981.803711\n",
      "Training Batch: 19002 Loss: 2960.835938\n",
      "Training Batch: 19003 Loss: 3014.392822\n",
      "Training Batch: 19004 Loss: 3008.551270\n",
      "Training Batch: 19005 Loss: 3186.113770\n",
      "Training Batch: 19006 Loss: 2980.119873\n",
      "Training Batch: 19007 Loss: 3112.862793\n",
      "Training Batch: 19008 Loss: 3004.182617\n",
      "Training Batch: 19009 Loss: 3232.119629\n",
      "Training Batch: 19010 Loss: 3268.320801\n",
      "Training Batch: 19011 Loss: 3090.835205\n",
      "Training Batch: 19012 Loss: 3016.720215\n",
      "Training Batch: 19013 Loss: 3184.452637\n",
      "Training Batch: 19014 Loss: 3111.377930\n",
      "Training Batch: 19015 Loss: 3007.935059\n",
      "Training Batch: 19016 Loss: 3017.914551\n",
      "Training Batch: 19017 Loss: 2958.170898\n",
      "Training Batch: 19018 Loss: 3097.628174\n",
      "Training Batch: 19019 Loss: 3140.363770\n",
      "Training Batch: 19020 Loss: 3151.648926\n",
      "Training Batch: 19021 Loss: 3146.132812\n",
      "Training Batch: 19022 Loss: 2966.609863\n",
      "Training Batch: 19023 Loss: 2929.209229\n",
      "Training Batch: 19024 Loss: 3029.462402\n",
      "Training Batch: 19025 Loss: 2982.815430\n",
      "Training Batch: 19026 Loss: 3007.246338\n",
      "Training Batch: 19027 Loss: 2930.810547\n",
      "Training Batch: 19028 Loss: 3023.261719\n",
      "Training Batch: 19029 Loss: 3111.180176\n",
      "Training Batch: 19030 Loss: 3092.541016\n",
      "Training Batch: 19031 Loss: 3076.867432\n",
      "Training Batch: 19032 Loss: 2961.392578\n",
      "Training Batch: 19033 Loss: 3118.104004\n",
      "Training Batch: 19034 Loss: 3007.174805\n",
      "Training Batch: 19035 Loss: 2987.054688\n",
      "Training Batch: 19036 Loss: 3068.600342\n",
      "Training Batch: 19037 Loss: 3040.602051\n",
      "Training Batch: 19038 Loss: 3214.388184\n",
      "Training Batch: 19039 Loss: 3061.618164\n",
      "Training Batch: 19040 Loss: 3143.373047\n",
      "Training Batch: 19041 Loss: 3133.831787\n",
      "Training Batch: 19042 Loss: 3081.386230\n",
      "Training Batch: 19043 Loss: 3271.051270\n",
      "Training Batch: 19044 Loss: 3130.932373\n",
      "Training Batch: 19045 Loss: 3250.257080\n",
      "Training Batch: 19046 Loss: 3226.236572\n",
      "Training Batch: 19047 Loss: 3137.775879\n",
      "Training Batch: 19048 Loss: 3296.118652\n",
      "Training Batch: 19049 Loss: 3036.107422\n",
      "Training Batch: 19050 Loss: 3053.426270\n",
      "Training Batch: 19051 Loss: 2960.209473\n",
      "Training Batch: 19052 Loss: 3213.374023\n",
      "Training Batch: 19053 Loss: 3039.782959\n",
      "Training Batch: 19054 Loss: 3047.721680\n",
      "Training Batch: 19055 Loss: 2917.114746\n",
      "Training Batch: 19056 Loss: 3111.600098\n",
      "Training Batch: 19057 Loss: 3075.215088\n",
      "Training Batch: 19058 Loss: 2978.048340\n",
      "Training Batch: 19059 Loss: 3040.821533\n",
      "Training Batch: 19060 Loss: 3100.550537\n",
      "Training Batch: 19061 Loss: 2978.956543\n",
      "Training Batch: 19062 Loss: 3095.592773\n",
      "Training Batch: 19063 Loss: 3426.309814\n",
      "Training Batch: 19064 Loss: 3139.312744\n",
      "Training Batch: 19065 Loss: 3069.457520\n",
      "Training Batch: 19066 Loss: 3133.953613\n",
      "Training Batch: 19067 Loss: 3088.079590\n",
      "Training Batch: 19068 Loss: 3097.788574\n",
      "Training Batch: 19069 Loss: 3045.860107\n",
      "Training Batch: 19070 Loss: 2949.332520\n",
      "Training Batch: 19071 Loss: 3208.191162\n",
      "Training Batch: 19072 Loss: 3073.216064\n",
      "Training Batch: 19073 Loss: 3014.269043\n",
      "Training Batch: 19074 Loss: 3077.473145\n",
      "Training Batch: 19075 Loss: 3119.270264\n",
      "Training Batch: 19076 Loss: 3097.669922\n",
      "Training Batch: 19077 Loss: 3118.421387\n",
      "Training Batch: 19078 Loss: 3054.918945\n",
      "Training Batch: 19079 Loss: 3017.049072\n",
      "Training Batch: 19080 Loss: 2889.544678\n",
      "Training Batch: 19081 Loss: 3057.854736\n",
      "Training Batch: 19082 Loss: 2921.884766\n",
      "Training Batch: 19083 Loss: 2869.621582\n",
      "Training Batch: 19084 Loss: 2949.883301\n",
      "Training Batch: 19085 Loss: 2992.164551\n",
      "Training Batch: 19086 Loss: 3018.210938\n",
      "Training Batch: 19087 Loss: 3214.604980\n",
      "Training Batch: 19088 Loss: 3028.515625\n",
      "Training Batch: 19089 Loss: 3296.725098\n",
      "Training Batch: 19090 Loss: 3146.960938\n",
      "Training Batch: 19091 Loss: 3031.334961\n",
      "Training Batch: 19092 Loss: 3089.781250\n",
      "Training Batch: 19093 Loss: 3198.185791\n",
      "Training Batch: 19094 Loss: 3099.333496\n",
      "Training Batch: 19095 Loss: 3096.289307\n",
      "Training Batch: 19096 Loss: 3071.420898\n",
      "Training Batch: 19097 Loss: 3004.604492\n",
      "Training Batch: 19098 Loss: 2945.349365\n",
      "Training Batch: 19099 Loss: 3087.671143\n",
      "Training Batch: 19100 Loss: 3012.666748\n",
      "Training Batch: 19101 Loss: 3007.065186\n",
      "Training Batch: 19102 Loss: 2997.090820\n",
      "Training Batch: 19103 Loss: 2975.287598\n",
      "Training Batch: 19104 Loss: 2941.187988\n",
      "Training Batch: 19105 Loss: 3000.685059\n",
      "Training Batch: 19106 Loss: 3064.018066\n",
      "Training Batch: 19107 Loss: 3132.563965\n",
      "Training Batch: 19108 Loss: 3078.840820\n",
      "Training Batch: 19109 Loss: 3168.221436\n",
      "Training Batch: 19110 Loss: 2950.848633\n",
      "Training Batch: 19111 Loss: 3066.946533\n",
      "Training Batch: 19112 Loss: 3062.049561\n",
      "Training Batch: 19113 Loss: 3075.930664\n",
      "Training Batch: 19114 Loss: 3001.476318\n",
      "Training Batch: 19115 Loss: 3072.455566\n",
      "Training Batch: 19116 Loss: 2996.709961\n",
      "Training Batch: 19117 Loss: 2981.726074\n",
      "Training Batch: 19118 Loss: 3134.791260\n",
      "Training Batch: 19119 Loss: 3050.133789\n",
      "Training Batch: 19120 Loss: 2997.480713\n",
      "Training Batch: 19121 Loss: 3013.245605\n",
      "Training Batch: 19122 Loss: 3008.055664\n",
      "Training Batch: 19123 Loss: 3038.022461\n",
      "Training Batch: 19124 Loss: 3230.653809\n",
      "Training Batch: 19125 Loss: 3230.336426\n",
      "Training Batch: 19126 Loss: 3147.263672\n",
      "Training Batch: 19127 Loss: 3316.079590\n",
      "Training Batch: 19128 Loss: 3226.303223\n",
      "Training Batch: 19129 Loss: 3045.123535\n",
      "Training Batch: 19130 Loss: 3393.819336\n",
      "Training Batch: 19131 Loss: 3111.583496\n",
      "Training Batch: 19132 Loss: 2987.378418\n",
      "Training Batch: 19133 Loss: 3086.992676\n",
      "Training Batch: 19134 Loss: 3016.128418\n",
      "Training Batch: 19135 Loss: 3034.660645\n",
      "Training Batch: 19136 Loss: 3070.714844\n",
      "Training Batch: 19137 Loss: 2970.658203\n",
      "Training Batch: 19138 Loss: 3175.510010\n",
      "Training Batch: 19139 Loss: 3097.026855\n",
      "Training Batch: 19140 Loss: 3042.506836\n",
      "Training Batch: 19141 Loss: 3124.410156\n",
      "Training Batch: 19142 Loss: 3139.441895\n",
      "Training Batch: 19143 Loss: 3099.706787\n",
      "Training Batch: 19144 Loss: 3044.082520\n",
      "Training Batch: 19145 Loss: 3252.888428\n",
      "Training Batch: 19146 Loss: 3092.360352\n",
      "Training Batch: 19147 Loss: 2958.429199\n",
      "Training Batch: 19148 Loss: 3143.548828\n",
      "Training Batch: 19149 Loss: 2965.045898\n",
      "Training Batch: 19150 Loss: 3117.627441\n",
      "Training Batch: 19151 Loss: 3073.851562\n",
      "Training Batch: 19152 Loss: 3146.647949\n",
      "Training Batch: 19153 Loss: 3105.441895\n",
      "Training Batch: 19154 Loss: 3060.856445\n",
      "Training Batch: 19155 Loss: 3016.261719\n",
      "Training Batch: 19156 Loss: 3026.736816\n",
      "Training Batch: 19157 Loss: 2977.874756\n",
      "Training Batch: 19158 Loss: 3022.125977\n",
      "Training Batch: 19159 Loss: 3046.453857\n",
      "Training Batch: 19160 Loss: 2933.530029\n",
      "Training Batch: 19161 Loss: 3166.137207\n",
      "Training Batch: 19162 Loss: 3006.443848\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Batch: 19163 Loss: 3263.810059\n",
      "Training Batch: 19164 Loss: 2985.096924\n",
      "Training Batch: 19165 Loss: 3085.143555\n",
      "Training Batch: 19166 Loss: 2907.951172\n",
      "Training Batch: 19167 Loss: 3252.555664\n",
      "Training Batch: 19168 Loss: 3025.066895\n",
      "Training Batch: 19169 Loss: 2995.979492\n",
      "Training Batch: 19170 Loss: 3042.808105\n",
      "Training Batch: 19171 Loss: 3164.700928\n",
      "Training Batch: 19172 Loss: 3079.544678\n",
      "Training Batch: 19173 Loss: 3197.114258\n",
      "Training Batch: 19174 Loss: 3324.416016\n",
      "Training Batch: 19175 Loss: 3045.965820\n",
      "Training Batch: 19176 Loss: 2980.984863\n",
      "Training Batch: 19177 Loss: 3078.157959\n",
      "Training Batch: 19178 Loss: 3115.598633\n",
      "Training Batch: 19179 Loss: 3043.796875\n",
      "Training Batch: 19180 Loss: 3009.987305\n",
      "Training Batch: 19181 Loss: 2834.009521\n",
      "Training Batch: 19182 Loss: 3127.904785\n",
      "Training Batch: 19183 Loss: 3063.224121\n",
      "Training Batch: 19184 Loss: 3211.070068\n",
      "Training Batch: 19185 Loss: 3121.853027\n",
      "Training Batch: 19186 Loss: 3231.111816\n",
      "Training Batch: 19187 Loss: 3262.327881\n",
      "Training Batch: 19188 Loss: 3160.912598\n",
      "Training Batch: 19189 Loss: 2972.812012\n",
      "Training Batch: 19190 Loss: 3043.968262\n",
      "Training Batch: 19191 Loss: 2997.940674\n",
      "Training Batch: 19192 Loss: 2955.341309\n",
      "Training Batch: 19193 Loss: 3152.589355\n",
      "Training Batch: 19194 Loss: 3224.279297\n",
      "Training Batch: 19195 Loss: 3044.919922\n",
      "Training Batch: 19196 Loss: 3288.761719\n",
      "Training Batch: 19197 Loss: 3048.027832\n",
      "Training Batch: 19198 Loss: 2922.047607\n",
      "Training Batch: 19199 Loss: 3177.098633\n",
      "Training Batch: 19200 Loss: 3048.418213\n",
      "Training Batch: 19201 Loss: 2995.893066\n",
      "Training Batch: 19202 Loss: 3162.805176\n",
      "Training Batch: 19203 Loss: 3076.490234\n",
      "Training Batch: 19204 Loss: 2911.010742\n",
      "Training Batch: 19205 Loss: 3051.150879\n",
      "Training Batch: 19206 Loss: 3026.634766\n",
      "Training Batch: 19207 Loss: 3107.947021\n",
      "Training Batch: 19208 Loss: 3021.455811\n",
      "Training Batch: 19209 Loss: 3016.226074\n",
      "Training Batch: 19210 Loss: 3003.551270\n",
      "Training Batch: 19211 Loss: 3054.249512\n",
      "Training Batch: 19212 Loss: 3023.270508\n",
      "Training Batch: 19213 Loss: 3093.315430\n",
      "Training Batch: 19214 Loss: 3098.420410\n",
      "Training Batch: 19215 Loss: 3071.714111\n",
      "Training Batch: 19216 Loss: 3011.403809\n",
      "Training Batch: 19217 Loss: 3048.209961\n",
      "Training Batch: 19218 Loss: 3246.317871\n",
      "Training Batch: 19219 Loss: 3117.257324\n",
      "Training Batch: 19220 Loss: 3187.260254\n",
      "Training Batch: 19221 Loss: 3133.859131\n",
      "Training Batch: 19222 Loss: 3069.492188\n",
      "Training Batch: 19223 Loss: 3195.009033\n",
      "Training Batch: 19224 Loss: 3013.758789\n",
      "Training Batch: 19225 Loss: 3089.507080\n",
      "Training Batch: 19226 Loss: 3048.779785\n",
      "Training Batch: 19227 Loss: 3017.315430\n",
      "Training Batch: 19228 Loss: 3174.802002\n",
      "Training Batch: 19229 Loss: 2984.849121\n",
      "Training Batch: 19230 Loss: 3263.055176\n",
      "Training Batch: 19231 Loss: 3056.350586\n",
      "Training Batch: 19232 Loss: 3157.015869\n",
      "Training Batch: 19233 Loss: 3086.046387\n",
      "Training Batch: 19234 Loss: 3146.000732\n",
      "Training Batch: 19235 Loss: 3047.741699\n",
      "Training Batch: 19236 Loss: 3038.740723\n",
      "Training Batch: 19237 Loss: 3079.793457\n",
      "Training Batch: 19238 Loss: 3001.739014\n",
      "Training Batch: 19239 Loss: 2928.079102\n",
      "Training Batch: 19240 Loss: 3018.433594\n",
      "Training Batch: 19241 Loss: 3124.761719\n",
      "Training Batch: 19242 Loss: 2997.135986\n",
      "Training Batch: 19243 Loss: 3061.498047\n",
      "Training Batch: 19244 Loss: 3060.531250\n",
      "Training Batch: 19245 Loss: 3024.537598\n",
      "Training Batch: 19246 Loss: 2938.779297\n",
      "Training Batch: 19247 Loss: 3039.803711\n",
      "Training Batch: 19248 Loss: 2966.546387\n",
      "Training Batch: 19249 Loss: 3195.832031\n",
      "Training Batch: 19250 Loss: 3076.977051\n",
      "Training Batch: 19251 Loss: 2949.608887\n",
      "Training Batch: 19252 Loss: 3005.469727\n",
      "Training Batch: 19253 Loss: 3048.528809\n",
      "Training Batch: 19254 Loss: 2969.252686\n",
      "Training Batch: 19255 Loss: 3115.308350\n",
      "Training Batch: 19256 Loss: 3140.925293\n",
      "Training Batch: 19257 Loss: 3114.405518\n",
      "Training Batch: 19258 Loss: 3002.446777\n",
      "Training Batch: 19259 Loss: 3087.230957\n",
      "Training Batch: 19260 Loss: 3040.616699\n",
      "Training Batch: 19261 Loss: 2979.073242\n",
      "Training Batch: 19262 Loss: 3054.248535\n",
      "Training Batch: 19263 Loss: 3166.069824\n",
      "Training Batch: 19264 Loss: 2998.853516\n",
      "Training Batch: 19265 Loss: 3135.265381\n",
      "Training Batch: 19266 Loss: 3456.945801\n",
      "Training Batch: 19267 Loss: 3157.370605\n",
      "Training Batch: 19268 Loss: 2973.677734\n",
      "Training Batch: 19269 Loss: 3014.121582\n",
      "Training Batch: 19270 Loss: 3316.217285\n",
      "Training Batch: 19271 Loss: 3065.522461\n",
      "Training Batch: 19272 Loss: 3581.075439\n",
      "Training Batch: 19273 Loss: 3082.464355\n",
      "Training Batch: 19274 Loss: 3148.476562\n",
      "Training Batch: 19275 Loss: 3117.588379\n",
      "Training Batch: 19276 Loss: 3123.171875\n",
      "Training Batch: 19277 Loss: 2981.962646\n",
      "Training Batch: 19278 Loss: 3552.913086\n",
      "Training Batch: 19279 Loss: 3091.027588\n",
      "Training Batch: 19280 Loss: 3099.843750\n",
      "Training Batch: 19281 Loss: 2942.250977\n",
      "Training Batch: 19282 Loss: 3001.698975\n",
      "Training Batch: 19283 Loss: 3019.635742\n",
      "Training Batch: 19284 Loss: 3139.675293\n",
      "Training Batch: 19285 Loss: 3107.845703\n",
      "Training Batch: 19286 Loss: 3142.392578\n",
      "Training Batch: 19287 Loss: 3032.663086\n",
      "Training Batch: 19288 Loss: 3115.313721\n",
      "Training Batch: 19289 Loss: 3037.002686\n",
      "Training Batch: 19290 Loss: 3093.536621\n",
      "Training Batch: 19291 Loss: 3180.188232\n",
      "Training Batch: 19292 Loss: 3057.576660\n",
      "Training Batch: 19293 Loss: 3044.358887\n",
      "Training Batch: 19294 Loss: 2937.314941\n",
      "Training Batch: 19295 Loss: 2978.489746\n",
      "Training Batch: 19296 Loss: 3043.175781\n",
      "Training Batch: 19297 Loss: 2931.546387\n",
      "Training Batch: 19298 Loss: 3127.229980\n",
      "Training Batch: 19299 Loss: 3059.960449\n",
      "Training Batch: 19300 Loss: 2952.337402\n",
      "Training Batch: 19301 Loss: 3096.550781\n",
      "Training Batch: 19302 Loss: 3125.322266\n",
      "Training Batch: 19303 Loss: 3013.251953\n",
      "Training Batch: 19304 Loss: 2988.300781\n",
      "Training Batch: 19305 Loss: 2963.291016\n",
      "Training Batch: 19306 Loss: 2928.656494\n",
      "Training Batch: 19307 Loss: 3027.774414\n",
      "Training Batch: 19308 Loss: 2982.957520\n",
      "Training Batch: 19309 Loss: 2989.376221\n",
      "Training Batch: 19310 Loss: 2954.599854\n",
      "Training Batch: 19311 Loss: 3111.657227\n",
      "Training Batch: 19312 Loss: 3083.179199\n",
      "Training Batch: 19313 Loss: 3106.271973\n",
      "Training Batch: 19314 Loss: 3033.794922\n",
      "Training Batch: 19315 Loss: 3139.411621\n",
      "Training Batch: 19316 Loss: 3173.016602\n",
      "Training Batch: 19317 Loss: 3005.051758\n",
      "Training Batch: 19318 Loss: 3093.750000\n",
      "Training Batch: 19319 Loss: 3204.491943\n",
      "Training Batch: 19320 Loss: 2935.023438\n",
      "Training Batch: 19321 Loss: 2988.865723\n",
      "Training Batch: 19322 Loss: 3006.507812\n",
      "Training Batch: 19323 Loss: 3023.121094\n",
      "Training Batch: 19324 Loss: 3053.106445\n",
      "Training Batch: 19325 Loss: 2999.549072\n",
      "Training Batch: 19326 Loss: 3097.325684\n",
      "Training Batch: 19327 Loss: 3001.941406\n",
      "Training Batch: 19328 Loss: 3160.946045\n",
      "Training Batch: 19329 Loss: 3030.487305\n",
      "Training Batch: 19330 Loss: 3139.300781\n",
      "Training Batch: 19331 Loss: 3195.490723\n",
      "Training Batch: 19332 Loss: 3028.963623\n",
      "Training Batch: 19333 Loss: 3039.262451\n",
      "Training Batch: 19334 Loss: 3042.619629\n",
      "Training Batch: 19335 Loss: 3092.116943\n",
      "Training Batch: 19336 Loss: 3005.258789\n",
      "Training Batch: 19337 Loss: 3106.984375\n",
      "Training Batch: 19338 Loss: 3178.414551\n",
      "Training Batch: 19339 Loss: 3530.446777\n",
      "Training Batch: 19340 Loss: 3157.907227\n",
      "Training Batch: 19341 Loss: 3162.544434\n",
      "Training Batch: 19342 Loss: 3122.742432\n",
      "Training Batch: 19343 Loss: 2965.772461\n",
      "Training Batch: 19344 Loss: 3084.657227\n",
      "Training Batch: 19345 Loss: 3027.734131\n",
      "Training Batch: 19346 Loss: 3043.980469\n",
      "Training Batch: 19347 Loss: 3088.837402\n",
      "Training Batch: 19348 Loss: 3064.276367\n",
      "Training Batch: 19349 Loss: 3153.044434\n",
      "Training Batch: 19350 Loss: 2904.564941\n",
      "Training Batch: 19351 Loss: 3007.653320\n",
      "Training Batch: 19352 Loss: 2961.414062\n",
      "Training Batch: 19353 Loss: 3185.208008\n",
      "Training Batch: 19354 Loss: 3012.915283\n",
      "Training Batch: 19355 Loss: 3023.022461\n",
      "Training Batch: 19356 Loss: 2974.537109\n",
      "Training Batch: 19357 Loss: 3174.754395\n",
      "Training Batch: 19358 Loss: 2999.083496\n",
      "Training Batch: 19359 Loss: 3097.991699\n",
      "Training Batch: 19360 Loss: 3013.749512\n",
      "Training Batch: 19361 Loss: 2952.405762\n",
      "Training Batch: 19362 Loss: 3071.888672\n",
      "Training Batch: 19363 Loss: 3056.222168\n",
      "Training Batch: 19364 Loss: 3092.533691\n",
      "Training Batch: 19365 Loss: 3119.357178\n",
      "Training Batch: 19366 Loss: 2973.870117\n",
      "Training Batch: 19367 Loss: 3036.301758\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Batch: 19368 Loss: 2945.922852\n",
      "Training Batch: 19369 Loss: 2904.420898\n",
      "Training Batch: 19370 Loss: 3038.274414\n",
      "Training Batch: 19371 Loss: 3024.906250\n",
      "Training Batch: 19372 Loss: 2973.622803\n",
      "Training Batch: 19373 Loss: 2979.352783\n",
      "Training Batch: 19374 Loss: 3061.192871\n",
      "Training Batch: 19375 Loss: 2985.527832\n",
      "Training Batch: 19376 Loss: 2931.412598\n",
      "Training Batch: 19377 Loss: 2911.264648\n",
      "Training Batch: 19378 Loss: 3012.884766\n",
      "Training Batch: 19379 Loss: 3188.708008\n",
      "Training Batch: 19380 Loss: 3078.488525\n",
      "Training Batch: 19381 Loss: 2923.886963\n",
      "Training Batch: 19382 Loss: 3293.795410\n",
      "Training Batch: 19383 Loss: 3570.827148\n",
      "Training Batch: 19384 Loss: 3000.749512\n",
      "Training Batch: 19385 Loss: 3024.650391\n",
      "Training Batch: 19386 Loss: 2939.723145\n",
      "Training Batch: 19387 Loss: 3032.486328\n",
      "Training Batch: 19388 Loss: 3089.467773\n",
      "Training Batch: 19389 Loss: 2982.202637\n",
      "Training Batch: 19390 Loss: 2942.052246\n",
      "Training Batch: 19391 Loss: 3207.904297\n",
      "Training Batch: 19392 Loss: 3462.182617\n",
      "Training Batch: 19393 Loss: 3104.275635\n",
      "Training Batch: 19394 Loss: 3026.881104\n",
      "Training Batch: 19395 Loss: 3021.760986\n",
      "Training Batch: 19396 Loss: 2995.400879\n",
      "Training Batch: 19397 Loss: 3017.046387\n",
      "Training Batch: 19398 Loss: 3052.294922\n",
      "Training Batch: 19399 Loss: 3037.134277\n",
      "Training Batch: 19400 Loss: 3042.618652\n",
      "Training Batch: 19401 Loss: 3075.748047\n",
      "Training Batch: 19402 Loss: 3145.281738\n",
      "Training Batch: 19403 Loss: 3043.666016\n",
      "Training Batch: 19404 Loss: 2939.101074\n",
      "Training Batch: 19405 Loss: 2984.816406\n",
      "Training Batch: 19406 Loss: 3097.502930\n",
      "Training Batch: 19407 Loss: 2905.915283\n",
      "Training Batch: 19408 Loss: 3054.821777\n",
      "Training Batch: 19409 Loss: 2983.127441\n",
      "Training Batch: 19410 Loss: 2967.004883\n",
      "Training Batch: 19411 Loss: 2970.440918\n",
      "Training Batch: 19412 Loss: 2998.669434\n",
      "Training Batch: 19413 Loss: 3067.424316\n",
      "Training Batch: 19414 Loss: 2918.838867\n",
      "Training Batch: 19415 Loss: 2991.513672\n",
      "Training Batch: 19416 Loss: 3034.259521\n",
      "Training Batch: 19417 Loss: 3031.671387\n",
      "Training Batch: 19418 Loss: 3167.325684\n",
      "Training Batch: 19419 Loss: 3075.248535\n",
      "Training Batch: 19420 Loss: 3057.373535\n",
      "Training Batch: 19421 Loss: 3079.331543\n",
      "Training Batch: 19422 Loss: 3039.729248\n",
      "Training Batch: 19423 Loss: 3122.854492\n",
      "Training Batch: 19424 Loss: 3099.312988\n",
      "Training Batch: 19425 Loss: 2992.716309\n",
      "Training Batch: 19426 Loss: 3125.211670\n",
      "Training Batch: 19427 Loss: 3014.734619\n",
      "Training Batch: 19428 Loss: 3066.074707\n",
      "Training Batch: 19429 Loss: 3127.085938\n",
      "Training Batch: 19430 Loss: 3065.615234\n",
      "Training Batch: 19431 Loss: 2914.512695\n",
      "Training Batch: 19432 Loss: 3262.386719\n",
      "Training Batch: 19433 Loss: 3163.705566\n",
      "Training Batch: 19434 Loss: 3047.106934\n",
      "Training Batch: 19435 Loss: 3093.498047\n",
      "Training Batch: 19436 Loss: 2994.751465\n",
      "Training Batch: 19437 Loss: 2972.909180\n",
      "Training Batch: 19438 Loss: 3083.101074\n",
      "Training Batch: 19439 Loss: 3045.855225\n",
      "Training Batch: 19440 Loss: 3087.659668\n",
      "Training Batch: 19441 Loss: 2943.853516\n",
      "Training Batch: 19442 Loss: 3304.311523\n",
      "Training Batch: 19443 Loss: 3099.658936\n",
      "Training Batch: 19444 Loss: 3062.980713\n",
      "Training Batch: 19445 Loss: 3121.663818\n",
      "Training Batch: 19446 Loss: 2964.717529\n",
      "Training Batch: 19447 Loss: 2967.643066\n",
      "Training Batch: 19448 Loss: 3159.011963\n",
      "Training Batch: 19449 Loss: 3096.526855\n",
      "Training Batch: 19450 Loss: 3045.319336\n",
      "Training Batch: 19451 Loss: 3137.370117\n",
      "Training Batch: 19452 Loss: 3217.359375\n",
      "Training Batch: 19453 Loss: 3115.636230\n",
      "Training Batch: 19454 Loss: 3032.264648\n",
      "Training Batch: 19455 Loss: 3060.695801\n",
      "Training Batch: 19456 Loss: 2979.014160\n",
      "Training Batch: 19457 Loss: 3000.334717\n",
      "Training Batch: 19458 Loss: 3053.724121\n",
      "Training Batch: 19459 Loss: 3050.265137\n",
      "Training Batch: 19460 Loss: 2871.351562\n",
      "Training Batch: 19461 Loss: 3045.236816\n",
      "Training Batch: 19462 Loss: 3300.176270\n",
      "Training Batch: 19463 Loss: 2970.046143\n",
      "Training Batch: 19464 Loss: 2925.270996\n",
      "Training Batch: 19465 Loss: 3049.101562\n",
      "Training Batch: 19466 Loss: 2917.871582\n",
      "Training Batch: 19467 Loss: 3057.632324\n",
      "Training Batch: 19468 Loss: 3047.679688\n",
      "Training Batch: 19469 Loss: 3213.222900\n",
      "Training Batch: 19470 Loss: 2947.361328\n",
      "Training Batch: 19471 Loss: 3058.301758\n",
      "Training Batch: 19472 Loss: 3046.000244\n",
      "Training Batch: 19473 Loss: 2940.592041\n",
      "Training Batch: 19474 Loss: 3178.795410\n",
      "Training Batch: 19475 Loss: 3218.350830\n",
      "Training Batch: 19476 Loss: 3031.514160\n",
      "Training Batch: 19477 Loss: 3041.843750\n",
      "Training Batch: 19478 Loss: 3101.897461\n",
      "Training Batch: 19479 Loss: 3217.799805\n",
      "Training Batch: 19480 Loss: 3124.648926\n",
      "Training Batch: 19481 Loss: 2971.639648\n",
      "Training Batch: 19482 Loss: 3054.814453\n",
      "Training Batch: 19483 Loss: 3058.969727\n",
      "Training Batch: 19484 Loss: 3106.521973\n",
      "Training Batch: 19485 Loss: 3053.549805\n",
      "Training Batch: 19486 Loss: 2990.752686\n",
      "Training Batch: 19487 Loss: 3159.651855\n",
      "Training Batch: 19488 Loss: 3129.235107\n",
      "Training Batch: 19489 Loss: 3233.579590\n",
      "Training Batch: 19490 Loss: 3123.417969\n",
      "Training Batch: 19491 Loss: 3098.371826\n",
      "Training Batch: 19492 Loss: 3068.412598\n",
      "Training Batch: 19493 Loss: 3053.825439\n",
      "Training Batch: 19494 Loss: 3041.437256\n",
      "Training Batch: 19495 Loss: 2928.160645\n",
      "Training Batch: 19496 Loss: 3028.289307\n",
      "Training Batch: 19497 Loss: 3007.198975\n",
      "Training Batch: 19498 Loss: 3112.686523\n",
      "Training Batch: 19499 Loss: 3018.809082\n",
      "Training Batch: 19500 Loss: 3051.058594\n",
      "Training Batch: 19501 Loss: 3560.888672\n",
      "Training Batch: 19502 Loss: 2950.129395\n",
      "Training Batch: 19503 Loss: 2979.083008\n",
      "Training Batch: 19504 Loss: 3010.613525\n",
      "Training Batch: 19505 Loss: 3026.095703\n",
      "Training Batch: 19506 Loss: 3041.577148\n",
      "Training Batch: 19507 Loss: 3336.535645\n",
      "Training Batch: 19508 Loss: 2939.640625\n",
      "Training Batch: 19509 Loss: 2917.965332\n",
      "Training Batch: 19510 Loss: 2923.785156\n",
      "Training Batch: 19511 Loss: 2957.938477\n",
      "Training Batch: 19512 Loss: 2922.185547\n",
      "Training Batch: 19513 Loss: 3193.247803\n",
      "Training Batch: 19514 Loss: 3212.493652\n",
      "Training Batch: 19515 Loss: 2962.963867\n",
      "Training Batch: 19516 Loss: 3007.512695\n",
      "Training Batch: 19517 Loss: 3056.511719\n",
      "Training Batch: 19518 Loss: 3042.203613\n",
      "Training Batch: 19519 Loss: 3052.471191\n",
      "Training Batch: 19520 Loss: 3027.556641\n",
      "Training Batch: 19521 Loss: 2973.069824\n",
      "Training Batch: 19522 Loss: 3005.784668\n",
      "Training Batch: 19523 Loss: 3093.259766\n",
      "Training Batch: 19524 Loss: 3060.976807\n",
      "Training Batch: 19525 Loss: 3357.622070\n",
      "Training Batch: 19526 Loss: 3078.441162\n",
      "Training Batch: 19527 Loss: 3124.498047\n",
      "Training Batch: 19528 Loss: 3013.268066\n",
      "Training Batch: 19529 Loss: 2931.879395\n",
      "Training Batch: 19530 Loss: 3235.112793\n",
      "Training Batch: 19531 Loss: 3188.079590\n",
      "Training Batch: 19532 Loss: 3157.514160\n",
      "Training Batch: 19533 Loss: 3141.647461\n",
      "Training Batch: 19534 Loss: 3184.698730\n",
      "Training Batch: 19535 Loss: 3044.856445\n",
      "Training Batch: 19536 Loss: 2951.917725\n",
      "Training Batch: 19537 Loss: 2963.902832\n",
      "Training Batch: 19538 Loss: 3028.303711\n",
      "Training Batch: 19539 Loss: 2951.006104\n",
      "Training Batch: 19540 Loss: 3103.332520\n",
      "Training Batch: 19541 Loss: 2998.261719\n",
      "Training Batch: 19542 Loss: 3150.609375\n",
      "Training Batch: 19543 Loss: 3407.565918\n",
      "Training Batch: 19544 Loss: 2954.556152\n",
      "Training Batch: 19545 Loss: 3272.784424\n",
      "Training Batch: 19546 Loss: 3203.012939\n",
      "Training Batch: 19547 Loss: 3255.626221\n",
      "Training Batch: 19548 Loss: 3125.441650\n",
      "Training Batch: 19549 Loss: 3042.021484\n",
      "Training Batch: 19550 Loss: 3092.997559\n",
      "Training Batch: 19551 Loss: 3014.161621\n",
      "Training Batch: 19552 Loss: 2982.160889\n",
      "Training Batch: 19553 Loss: 2981.136719\n",
      "Training Batch: 19554 Loss: 3186.399414\n",
      "Training Batch: 19555 Loss: 3037.762695\n",
      "Training Batch: 19556 Loss: 3034.565430\n",
      "Training Batch: 19557 Loss: 3089.725342\n",
      "Training Batch: 19558 Loss: 3018.285156\n",
      "Training Batch: 19559 Loss: 3149.228027\n",
      "Training Batch: 19560 Loss: 2982.996826\n",
      "Training Batch: 19561 Loss: 3033.924316\n",
      "Training Batch: 19562 Loss: 3006.285156\n",
      "Training Batch: 19563 Loss: 2990.696289\n",
      "Training Batch: 19564 Loss: 3482.617188\n",
      "Training Batch: 19565 Loss: 3175.667480\n",
      "Training Batch: 19566 Loss: 3157.037598\n",
      "Training Batch: 19567 Loss: 3006.189941\n",
      "Training Batch: 19568 Loss: 3101.066895\n",
      "Training Batch: 19569 Loss: 3123.285156\n",
      "Training Batch: 19570 Loss: 2939.183594\n",
      "Training Batch: 19571 Loss: 3135.214600\n",
      "Training Batch: 19572 Loss: 3162.972168\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Batch: 19573 Loss: 3168.477783\n",
      "Training Batch: 19574 Loss: 3370.356445\n",
      "Training Batch: 19575 Loss: 3218.922363\n",
      "Training Batch: 19576 Loss: 3019.217773\n",
      "Training Batch: 19577 Loss: 3123.629883\n",
      "Training Batch: 19578 Loss: 3069.675537\n",
      "Training Batch: 19579 Loss: 3039.749268\n",
      "Training Batch: 19580 Loss: 3038.659668\n",
      "Training Batch: 19581 Loss: 3074.777100\n",
      "Training Batch: 19582 Loss: 3016.237793\n",
      "Training Batch: 19583 Loss: 3108.328613\n",
      "Training Batch: 19584 Loss: 2963.407227\n",
      "Training Batch: 19585 Loss: 3160.166016\n",
      "Training Batch: 19586 Loss: 3006.594727\n",
      "Training Batch: 19587 Loss: 3056.291748\n",
      "Training Batch: 19588 Loss: 2954.562012\n",
      "Training Batch: 19589 Loss: 3185.275391\n",
      "Training Batch: 19590 Loss: 3091.248535\n",
      "Training Batch: 19591 Loss: 2976.517578\n",
      "Training Batch: 19592 Loss: 3170.781250\n",
      "Training Batch: 19593 Loss: 3023.279297\n",
      "Training Batch: 19594 Loss: 3038.197021\n",
      "Training Batch: 19595 Loss: 2958.718262\n",
      "Training Batch: 19596 Loss: 3025.520508\n",
      "Training Batch: 19597 Loss: 2986.291992\n",
      "Training Batch: 19598 Loss: 3026.513672\n",
      "Training Batch: 19599 Loss: 2994.725586\n",
      "Training Batch: 19600 Loss: 3084.805664\n",
      "Training Batch: 19601 Loss: 3012.375244\n",
      "Training Batch: 19602 Loss: 3112.870605\n",
      "Training Batch: 19603 Loss: 3136.576172\n",
      "Training Batch: 19604 Loss: 3246.695068\n",
      "Training Batch: 19605 Loss: 3017.678711\n",
      "Training Batch: 19606 Loss: 2934.455566\n",
      "Training Batch: 19607 Loss: 3019.704834\n",
      "Training Batch: 19608 Loss: 3321.445312\n",
      "Training Batch: 19609 Loss: 2902.910400\n",
      "Training Batch: 19610 Loss: 2996.991211\n",
      "Training Batch: 19611 Loss: 3027.503174\n",
      "Training Batch: 19612 Loss: 2911.927246\n",
      "Training Batch: 19613 Loss: 2846.831787\n",
      "Training Batch: 19614 Loss: 3038.145264\n",
      "Training Batch: 19615 Loss: 2979.814697\n",
      "Training Batch: 19616 Loss: 3073.024658\n",
      "Training Batch: 19617 Loss: 3027.380371\n",
      "Training Batch: 19618 Loss: 2859.459473\n",
      "Training Batch: 19619 Loss: 3014.047119\n",
      "Training Batch: 19620 Loss: 2900.803467\n",
      "Training Batch: 19621 Loss: 3258.884277\n",
      "Training Batch: 19622 Loss: 3175.188477\n",
      "Training Batch: 19623 Loss: 2986.929199\n",
      "Training Batch: 19624 Loss: 3038.726562\n",
      "Training Batch: 19625 Loss: 3127.765137\n",
      "Training Batch: 19626 Loss: 3158.146973\n",
      "Training Batch: 19627 Loss: 3082.702637\n",
      "Training Batch: 19628 Loss: 3093.654053\n",
      "Training Batch: 19629 Loss: 2991.242920\n",
      "Training Batch: 19630 Loss: 3125.220703\n",
      "Training Batch: 19631 Loss: 3090.000977\n",
      "Training Batch: 19632 Loss: 3144.879150\n",
      "Training Batch: 19633 Loss: 3191.219238\n",
      "Training Batch: 19634 Loss: 3009.148926\n",
      "Training Batch: 19635 Loss: 2927.380859\n",
      "Training Batch: 19636 Loss: 2982.142822\n",
      "Training Batch: 19637 Loss: 3079.235840\n",
      "Training Batch: 19638 Loss: 3058.569580\n",
      "Training Batch: 19639 Loss: 2989.575195\n",
      "Training Batch: 19640 Loss: 2925.542969\n",
      "Training Batch: 19641 Loss: 2977.852295\n",
      "Training Batch: 19642 Loss: 2999.907227\n",
      "Training Batch: 19643 Loss: 2944.807129\n",
      "Training Batch: 19644 Loss: 3081.815918\n",
      "Training Batch: 19645 Loss: 3129.115234\n",
      "Training Batch: 19646 Loss: 3137.804199\n",
      "Training Batch: 19647 Loss: 3136.272217\n",
      "Training Batch: 19648 Loss: 3216.757080\n",
      "Training Batch: 19649 Loss: 3041.489746\n",
      "Training Batch: 19650 Loss: 2937.675293\n",
      "Training Batch: 19651 Loss: 3246.843262\n",
      "Training Batch: 19652 Loss: 3038.826660\n",
      "Training Batch: 19653 Loss: 2984.195312\n",
      "Training Batch: 19654 Loss: 3024.919189\n",
      "Training Batch: 19655 Loss: 2922.222656\n",
      "Training Batch: 19656 Loss: 3103.920410\n",
      "Training Batch: 19657 Loss: 2955.022461\n",
      "Training Batch: 19658 Loss: 2952.765137\n",
      "Training Batch: 19659 Loss: 2915.473633\n",
      "Training Batch: 19660 Loss: 3064.847656\n",
      "Training Batch: 19661 Loss: 3021.938477\n",
      "Training Batch: 19662 Loss: 3176.417480\n",
      "Training Batch: 19663 Loss: 3018.444336\n",
      "Training Batch: 19664 Loss: 3042.274902\n",
      "Training Batch: 19665 Loss: 2967.701660\n",
      "Training Batch: 19666 Loss: 2929.609619\n",
      "Training Batch: 19667 Loss: 3135.340576\n",
      "Training Batch: 19668 Loss: 3002.822754\n",
      "Training Batch: 19669 Loss: 2859.738281\n",
      "Training Batch: 19670 Loss: 3116.446777\n",
      "Training Batch: 19671 Loss: 3000.103760\n",
      "Training Batch: 19672 Loss: 3125.601074\n",
      "Training Batch: 19673 Loss: 2947.994141\n",
      "Training Batch: 19674 Loss: 3004.127686\n",
      "Training Batch: 19675 Loss: 2930.834961\n",
      "Training Batch: 19676 Loss: 2974.170898\n",
      "Training Batch: 19677 Loss: 3168.364502\n",
      "Training Batch: 19678 Loss: 2979.532959\n",
      "Training Batch: 19679 Loss: 2968.960938\n",
      "Training Batch: 19680 Loss: 2986.050293\n",
      "Training Batch: 19681 Loss: 3040.827637\n",
      "Training Batch: 19682 Loss: 3109.587158\n",
      "Training Batch: 19683 Loss: 3015.342285\n",
      "Training Batch: 19684 Loss: 3060.068359\n",
      "Training Batch: 19685 Loss: 3124.546387\n",
      "Training Batch: 19686 Loss: 3025.548096\n",
      "Training Batch: 19687 Loss: 3275.142090\n",
      "Training Batch: 19688 Loss: 3276.052490\n",
      "Training Batch: 19689 Loss: 3117.220215\n",
      "Training Batch: 19690 Loss: 3099.392334\n",
      "Training Batch: 19691 Loss: 3117.279297\n",
      "Training Batch: 19692 Loss: 3054.891113\n",
      "Training Batch: 19693 Loss: 3050.807861\n",
      "Training Batch: 19694 Loss: 2963.460938\n",
      "Training Batch: 19695 Loss: 2976.993164\n",
      "Training Batch: 19696 Loss: 3232.416992\n",
      "Training Batch: 19697 Loss: 3191.110352\n",
      "Training Batch: 19698 Loss: 3069.483887\n",
      "Training Batch: 19699 Loss: 3037.203613\n",
      "Training Batch: 19700 Loss: 3109.503174\n",
      "Training Batch: 19701 Loss: 2937.062012\n",
      "Training Batch: 19702 Loss: 3003.793213\n",
      "Training Batch: 19703 Loss: 3235.212402\n",
      "Training Batch: 19704 Loss: 2981.068115\n",
      "Training Batch: 19705 Loss: 3085.996582\n",
      "Training Batch: 19706 Loss: 3018.062500\n",
      "Training Batch: 19707 Loss: 3065.105469\n",
      "Training Batch: 19708 Loss: 2994.812012\n",
      "Training Batch: 19709 Loss: 3138.272949\n",
      "Training Batch: 19710 Loss: 3060.976562\n",
      "Training Batch: 19711 Loss: 2878.457031\n",
      "Training Batch: 19712 Loss: 2889.136719\n",
      "Training Batch: 19713 Loss: 2964.027344\n",
      "Training Batch: 19714 Loss: 3075.809570\n",
      "Training Batch: 19715 Loss: 3022.534180\n",
      "Training Batch: 19716 Loss: 3081.430176\n",
      "Training Batch: 19717 Loss: 3022.418701\n",
      "Training Batch: 19718 Loss: 3050.246094\n",
      "Training Batch: 19719 Loss: 2976.541016\n",
      "Training Batch: 19720 Loss: 2960.650879\n",
      "Training Batch: 19721 Loss: 2936.545410\n",
      "Training Batch: 19722 Loss: 2936.368652\n",
      "Training Batch: 19723 Loss: 2953.688477\n",
      "Training Batch: 19724 Loss: 3054.599609\n",
      "Training Batch: 19725 Loss: 3101.688477\n",
      "Training Batch: 19726 Loss: 3000.162109\n",
      "Training Batch: 19727 Loss: 2940.353516\n",
      "Training Batch: 19728 Loss: 3113.780273\n",
      "Training Batch: 19729 Loss: 3188.392578\n",
      "Training Batch: 19730 Loss: 3232.001953\n",
      "Training Batch: 19731 Loss: 3359.468750\n",
      "Training Batch: 19732 Loss: 3002.087891\n",
      "Training Batch: 19733 Loss: 3010.808594\n",
      "Training Batch: 19734 Loss: 3048.276855\n",
      "Training Batch: 19735 Loss: 3032.397461\n",
      "Training Batch: 19736 Loss: 3084.928223\n",
      "Training Batch: 19737 Loss: 2931.250732\n",
      "Training Batch: 19738 Loss: 3015.736816\n",
      "Training Batch: 19739 Loss: 2919.229736\n",
      "Training Batch: 19740 Loss: 3167.544434\n",
      "Training Batch: 19741 Loss: 3003.630371\n",
      "Training Batch: 19742 Loss: 2878.652832\n",
      "Training Batch: 19743 Loss: 2991.066895\n",
      "Training Batch: 19744 Loss: 3146.531494\n",
      "Training Batch: 19745 Loss: 3103.934570\n",
      "Training Batch: 19746 Loss: 2981.291016\n",
      "Training Batch: 19747 Loss: 3084.410156\n",
      "Training Batch: 19748 Loss: 2939.722168\n",
      "Training Batch: 19749 Loss: 3032.618164\n",
      "Training Batch: 19750 Loss: 2968.092773\n",
      "Training Batch: 19751 Loss: 3226.280029\n",
      "Training Batch: 19752 Loss: 3260.492676\n",
      "Training Batch: 19753 Loss: 3063.509277\n",
      "Training Batch: 19754 Loss: 3079.203613\n",
      "Training Batch: 19755 Loss: 3104.693604\n",
      "Training Batch: 19756 Loss: 3166.329102\n",
      "Training Batch: 19757 Loss: 2988.506348\n",
      "Training Batch: 19758 Loss: 3071.714844\n",
      "Training Batch: 19759 Loss: 2998.329102\n",
      "Training Batch: 19760 Loss: 2961.759766\n",
      "Training Batch: 19761 Loss: 3051.687012\n",
      "Training Batch: 19762 Loss: 3024.447266\n",
      "Training Batch: 19763 Loss: 3231.626465\n",
      "Training Batch: 19764 Loss: 3022.034180\n",
      "Training Batch: 19765 Loss: 3086.008545\n",
      "Training Batch: 19766 Loss: 3031.686768\n",
      "Training Batch: 19767 Loss: 3045.097656\n",
      "Training Batch: 19768 Loss: 3229.058350\n",
      "Training Batch: 19769 Loss: 2928.567871\n",
      "Training Batch: 19770 Loss: 3024.173340\n",
      "Training Batch: 19771 Loss: 2980.552734\n",
      "Training Batch: 19772 Loss: 3061.090820\n",
      "Training Batch: 19773 Loss: 2992.194824\n",
      "Training Batch: 19774 Loss: 2943.428711\n",
      "Training Batch: 19775 Loss: 3095.195312\n",
      "Training Batch: 19776 Loss: 3006.350586\n",
      "Training Batch: 19777 Loss: 2995.300781\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Batch: 19778 Loss: 3097.894043\n",
      "Training Batch: 19779 Loss: 2919.309570\n",
      "Training Batch: 19780 Loss: 2968.429932\n",
      "Training Batch: 19781 Loss: 2997.363037\n",
      "Training Batch: 19782 Loss: 3030.764160\n",
      "Training Batch: 19783 Loss: 2998.129883\n",
      "Training Batch: 19784 Loss: 3047.246094\n",
      "Training Batch: 19785 Loss: 2937.526855\n",
      "Training Batch: 19786 Loss: 3150.457031\n",
      "Training Batch: 19787 Loss: 3128.015137\n",
      "Training Batch: 19788 Loss: 3207.178223\n",
      "Training Batch: 19789 Loss: 3170.361816\n",
      "Training Batch: 19790 Loss: 3062.353760\n",
      "Training Batch: 19791 Loss: 2968.218506\n",
      "Training Batch: 19792 Loss: 3146.367676\n",
      "Training Batch: 19793 Loss: 3058.244141\n",
      "Training Batch: 19794 Loss: 2947.488037\n",
      "Training Batch: 19795 Loss: 3129.954590\n",
      "Training Batch: 19796 Loss: 3036.703613\n",
      "Training Batch: 19797 Loss: 2927.328613\n",
      "Training Batch: 19798 Loss: 2961.227539\n",
      "Training Batch: 19799 Loss: 3145.143555\n",
      "Training Batch: 19800 Loss: 2967.235596\n",
      "Training Batch: 19801 Loss: 3077.722900\n",
      "Training Batch: 19802 Loss: 3077.027344\n",
      "Training Batch: 19803 Loss: 2979.660645\n",
      "Training Batch: 19804 Loss: 3215.297852\n",
      "Training Batch: 19805 Loss: 3207.258301\n",
      "Training Batch: 19806 Loss: 2995.268799\n",
      "Training Batch: 19807 Loss: 3240.099609\n",
      "Training Batch: 19808 Loss: 3081.840820\n",
      "Training Batch: 19809 Loss: 3056.526367\n",
      "Training Batch: 19810 Loss: 2932.295410\n",
      "Training Batch: 19811 Loss: 3008.769043\n",
      "Training Batch: 19812 Loss: 3006.031982\n",
      "Training Batch: 19813 Loss: 3084.001465\n",
      "Training Batch: 19814 Loss: 2964.134521\n",
      "Training Batch: 19815 Loss: 3178.877930\n",
      "Training Batch: 19816 Loss: 2989.882080\n",
      "Training Batch: 19817 Loss: 3065.810059\n",
      "Training Batch: 19818 Loss: 2985.775391\n",
      "Training Batch: 19819 Loss: 3004.167969\n",
      "Training Batch: 19820 Loss: 2984.854248\n",
      "Training Batch: 19821 Loss: 3024.734375\n",
      "Training Batch: 19822 Loss: 2979.947754\n",
      "Training Batch: 19823 Loss: 3180.061035\n",
      "Training Batch: 19824 Loss: 3028.978516\n",
      "Training Batch: 19825 Loss: 3001.703857\n",
      "Training Batch: 19826 Loss: 3040.186523\n",
      "Training Batch: 19827 Loss: 2975.947754\n",
      "Training Batch: 19828 Loss: 3044.041992\n",
      "Training Batch: 19829 Loss: 2991.482910\n",
      "Training Batch: 19830 Loss: 3105.962402\n",
      "Training Batch: 19831 Loss: 3146.251709\n",
      "Training Batch: 19832 Loss: 2976.018066\n",
      "Training Batch: 19833 Loss: 3116.511475\n",
      "Training Batch: 19834 Loss: 2977.762695\n",
      "Training Batch: 19835 Loss: 3009.622559\n",
      "Training Batch: 19836 Loss: 3090.048828\n",
      "Training Batch: 19837 Loss: 2954.910156\n",
      "Training Batch: 19838 Loss: 2959.211426\n",
      "Training Batch: 19839 Loss: 3028.418213\n",
      "Training Batch: 19840 Loss: 3098.382324\n",
      "Training Batch: 19841 Loss: 3159.849121\n",
      "Training Batch: 19842 Loss: 2963.013184\n",
      "Training Batch: 19843 Loss: 3182.097412\n",
      "Training Batch: 19844 Loss: 2946.438965\n",
      "Training Batch: 19845 Loss: 2905.908203\n",
      "Training Batch: 19846 Loss: 3128.174316\n",
      "Training Batch: 19847 Loss: 3076.509766\n",
      "Training Batch: 19848 Loss: 3043.116699\n",
      "Training Batch: 19849 Loss: 3043.154297\n",
      "Training Batch: 19850 Loss: 3001.384033\n",
      "Training Batch: 19851 Loss: 2940.184326\n",
      "Training Batch: 19852 Loss: 2978.182861\n",
      "Training Batch: 19853 Loss: 2972.764160\n",
      "Training Batch: 19854 Loss: 3103.505859\n",
      "Training Batch: 19855 Loss: 2980.722656\n",
      "Training Batch: 19856 Loss: 3061.950684\n",
      "Training Batch: 19857 Loss: 3062.962646\n",
      "Training Batch: 19858 Loss: 3103.345215\n",
      "Training Batch: 19859 Loss: 2992.547852\n",
      "Training Batch: 19860 Loss: 3123.843018\n",
      "Training Batch: 19861 Loss: 3077.968262\n",
      "Training Batch: 19862 Loss: 3050.323242\n",
      "Training Batch: 19863 Loss: 3076.785156\n",
      "Training Batch: 19864 Loss: 2977.959717\n",
      "Training Batch: 19865 Loss: 2998.626953\n",
      "Training Batch: 19866 Loss: 3238.882324\n",
      "Training Batch: 19867 Loss: 3053.617188\n",
      "Training Batch: 19868 Loss: 3151.683105\n",
      "Training Batch: 19869 Loss: 3026.750732\n",
      "Training Batch: 19870 Loss: 3031.415527\n",
      "Training Batch: 19871 Loss: 3047.489990\n",
      "Training Batch: 19872 Loss: 3246.013916\n",
      "Training Batch: 19873 Loss: 2973.145508\n",
      "Training Batch: 19874 Loss: 3102.042969\n",
      "Training Batch: 19875 Loss: 3019.330566\n",
      "Training Batch: 19876 Loss: 2988.733398\n",
      "Training Batch: 19877 Loss: 2942.232910\n",
      "Training Batch: 19878 Loss: 3071.331543\n",
      "Training Batch: 19879 Loss: 2964.153564\n",
      "Training Batch: 19880 Loss: 3010.612305\n",
      "Training Batch: 19881 Loss: 2936.460205\n",
      "Training Batch: 19882 Loss: 3070.128906\n",
      "Training Batch: 19883 Loss: 2939.848145\n",
      "Training Batch: 19884 Loss: 2928.607178\n",
      "Training Batch: 19885 Loss: 2956.746094\n",
      "Training Batch: 19886 Loss: 2961.082764\n",
      "Training Batch: 19887 Loss: 3006.431152\n",
      "Training Batch: 19888 Loss: 3176.333984\n",
      "Training Batch: 19889 Loss: 2908.200684\n",
      "Training Batch: 19890 Loss: 3133.370605\n",
      "Training Batch: 19891 Loss: 3003.332520\n",
      "Training Batch: 19892 Loss: 3089.983643\n",
      "Training Batch: 19893 Loss: 2940.758301\n",
      "Training Batch: 19894 Loss: 2975.545410\n",
      "Training Batch: 19895 Loss: 3034.714355\n",
      "Training Batch: 19896 Loss: 2963.006836\n",
      "Training Batch: 19897 Loss: 3065.207520\n",
      "Training Batch: 19898 Loss: 3052.302734\n",
      "Training Batch: 19899 Loss: 3109.322754\n",
      "Training Batch: 19900 Loss: 3119.185059\n",
      "Training Batch: 19901 Loss: 3029.995117\n",
      "Training Batch: 19902 Loss: 3031.246094\n",
      "Training Batch: 19903 Loss: 3172.187500\n",
      "Training Batch: 19904 Loss: 2941.252197\n",
      "Training Batch: 19905 Loss: 3093.280273\n",
      "Training Batch: 19906 Loss: 2988.289307\n",
      "Training Batch: 19907 Loss: 2977.164062\n",
      "Training Batch: 19908 Loss: 3217.986328\n",
      "Training Batch: 19909 Loss: 3664.098145\n",
      "Training Batch: 19910 Loss: 3099.534180\n",
      "Training Batch: 19911 Loss: 3247.440430\n",
      "Training Batch: 19912 Loss: 3258.048584\n",
      "Training Batch: 19913 Loss: 3116.075684\n",
      "Training Batch: 19914 Loss: 3232.938965\n",
      "Training Batch: 19915 Loss: 3289.750488\n",
      "Training Batch: 19916 Loss: 2977.732666\n",
      "Training Batch: 19917 Loss: 3264.180908\n",
      "Training Batch: 19918 Loss: 3010.071777\n",
      "Training Batch: 19919 Loss: 2951.382080\n",
      "Training Batch: 19920 Loss: 3257.339844\n",
      "Training Batch: 19921 Loss: 3144.487793\n",
      "Training Batch: 19922 Loss: 3355.816406\n",
      "Training Batch: 19923 Loss: 3112.037109\n",
      "Training Batch: 19924 Loss: 3004.765625\n",
      "Training Batch: 19925 Loss: 3118.526367\n",
      "Training Batch: 19926 Loss: 3107.110840\n",
      "Training Batch: 19927 Loss: 3117.168945\n",
      "Training Batch: 19928 Loss: 3093.211914\n",
      "Training Batch: 19929 Loss: 3102.032959\n",
      "Training Batch: 19930 Loss: 3028.199707\n",
      "Training Batch: 19931 Loss: 3147.052734\n",
      "Training Batch: 19932 Loss: 3285.498535\n",
      "Training Batch: 19933 Loss: 3189.597656\n",
      "Training Batch: 19934 Loss: 3006.829590\n",
      "Training Batch: 19935 Loss: 3020.587158\n",
      "Training Batch: 19936 Loss: 2992.755859\n",
      "Training Batch: 19937 Loss: 3130.740479\n",
      "Training Batch: 19938 Loss: 3014.106445\n",
      "Training Batch: 19939 Loss: 2985.003418\n",
      "Training Batch: 19940 Loss: 3204.512207\n",
      "Training Batch: 19941 Loss: 3061.774414\n",
      "Training Batch: 19942 Loss: 3035.811523\n",
      "Training Batch: 19943 Loss: 3191.109863\n",
      "Training Batch: 19944 Loss: 3035.768555\n",
      "Training Batch: 19945 Loss: 3065.234863\n",
      "Training Batch: 19946 Loss: 3180.322754\n",
      "Training Batch: 19947 Loss: 3190.808594\n",
      "Training Batch: 19948 Loss: 3161.664307\n",
      "Training Batch: 19949 Loss: 3189.519287\n",
      "Training Batch: 19950 Loss: 3179.043457\n",
      "Training Batch: 19951 Loss: 2998.361816\n",
      "Training Batch: 19952 Loss: 3188.442871\n",
      "Training Batch: 19953 Loss: 3006.074219\n",
      "Training Batch: 19954 Loss: 3141.683594\n",
      "Training Batch: 19955 Loss: 3210.928955\n",
      "Training Batch: 19956 Loss: 2928.680664\n",
      "Training Batch: 19957 Loss: 2976.385254\n",
      "Training Batch: 19958 Loss: 3020.491211\n",
      "Training Batch: 19959 Loss: 3060.167725\n",
      "Training Batch: 19960 Loss: 3069.250977\n",
      "Training Batch: 19961 Loss: 3071.174316\n",
      "Training Batch: 19962 Loss: 3057.309570\n",
      "Training Batch: 19963 Loss: 3024.507568\n",
      "Training Batch: 19964 Loss: 2941.891113\n",
      "Training Batch: 19965 Loss: 3063.825195\n",
      "Training Batch: 19966 Loss: 3005.067871\n",
      "Training Batch: 19967 Loss: 3108.155029\n",
      "Training Batch: 19968 Loss: 3293.464111\n",
      "Training Batch: 19969 Loss: 3152.329102\n",
      "Training Batch: 19970 Loss: 3030.832764\n",
      "Training Batch: 19971 Loss: 2968.205078\n",
      "Training Batch: 19972 Loss: 3056.093750\n",
      "Training Batch: 19973 Loss: 3158.078857\n",
      "Training Batch: 19974 Loss: 3150.997070\n",
      "Training Batch: 19975 Loss: 2998.578125\n",
      "Training Batch: 19976 Loss: 3016.583984\n",
      "Training Batch: 19977 Loss: 3205.296631\n",
      "Training Batch: 19978 Loss: 3060.327637\n",
      "Training Batch: 19979 Loss: 3017.641113\n",
      "Training Batch: 19980 Loss: 3185.634766\n",
      "Training Batch: 19981 Loss: 3270.754639\n",
      "Training Batch: 19982 Loss: 3084.273438\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Batch: 19983 Loss: 3112.737305\n",
      "Training Batch: 19984 Loss: 3061.824219\n",
      "Training Batch: 19985 Loss: 3103.078613\n",
      "Training Batch: 19986 Loss: 2936.615723\n",
      "Training Batch: 19987 Loss: 3152.825928\n",
      "Training Batch: 19988 Loss: 3045.738281\n",
      "Training Batch: 19989 Loss: 2951.237305\n",
      "Training Batch: 19990 Loss: 2987.757812\n",
      "Training Batch: 19991 Loss: 3111.164307\n",
      "Training Batch: 19992 Loss: 3085.825684\n",
      "Training Batch: 19993 Loss: 2974.908203\n",
      "Training Batch: 19994 Loss: 3106.500977\n",
      "Training Batch: 19995 Loss: 3047.771729\n",
      "Training Batch: 19996 Loss: 2947.548828\n",
      "Training Batch: 19997 Loss: 3075.679443\n",
      "Training Batch: 19998 Loss: 3260.703369\n",
      "Training Batch: 19999 Loss: 3273.434570\n",
      "Training Batch: 20000 Loss: 3077.841064\n",
      "<class 'numpy.ndarray'>\n",
      "Validation Batch: 1 Loss: 2975.851562\n",
      "<class 'numpy.ndarray'>\n",
      "Validation Batch: 2 Loss: 3034.404785\n",
      "<class 'numpy.ndarray'>\n",
      "Validation Batch: 3 Loss: 2979.757324\n",
      "<class 'numpy.ndarray'>\n",
      "Validation Batch: 4 Loss: 3202.358154\n",
      "<class 'numpy.ndarray'>\n",
      "Validation Batch: 5 Loss: 3057.050781\n",
      "<class 'numpy.ndarray'>\n",
      "Validation Batch: 6 Loss: 3148.753418\n",
      "<class 'numpy.ndarray'>\n",
      "Validation Batch: 7 Loss: 3052.737793\n",
      "<class 'numpy.ndarray'>\n",
      "Validation Batch: 8 Loss: 3096.330566\n",
      "<class 'numpy.ndarray'>\n",
      "Validation Batch: 9 Loss: 3072.012695\n",
      "<class 'numpy.ndarray'>\n",
      "Validation Batch: 10 Loss: 3075.692383\n",
      "<class 'numpy.ndarray'>\n",
      "Validation Batch: 11 Loss: 3061.280273\n",
      "<class 'numpy.ndarray'>\n",
      "Validation Batch: 12 Loss: 3046.549316\n",
      "<class 'numpy.ndarray'>\n",
      "Validation Batch: 13 Loss: 3009.109863\n",
      "<class 'numpy.ndarray'>\n",
      "Validation Batch: 14 Loss: 3214.282715\n",
      "<class 'numpy.ndarray'>\n",
      "Validation Batch: 15 Loss: 2974.154541\n",
      "<class 'numpy.ndarray'>\n",
      "Validation Batch: 16 Loss: 2938.849121\n",
      "<class 'numpy.ndarray'>\n",
      "Validation Batch: 17 Loss: 3019.821289\n",
      "<class 'numpy.ndarray'>\n",
      "Validation Batch: 18 Loss: 3475.189941\n",
      "<class 'numpy.ndarray'>\n",
      "Validation Batch: 19 Loss: 2992.214844\n",
      "<class 'numpy.ndarray'>\n",
      "Validation Batch: 20 Loss: 3207.038086\n",
      "<class 'numpy.ndarray'>\n",
      "Validation Batch: 21 Loss: 3005.390137\n",
      "<class 'numpy.ndarray'>\n",
      "Validation Batch: 22 Loss: 3019.600098\n",
      "<class 'numpy.ndarray'>\n",
      "Validation Batch: 23 Loss: 3055.379395\n",
      "<class 'numpy.ndarray'>\n",
      "Validation Batch: 24 Loss: 3030.900635\n",
      "<class 'numpy.ndarray'>\n",
      "Validation Batch: 25 Loss: 2976.597168\n",
      "<class 'numpy.ndarray'>\n",
      "Validation Batch: 26 Loss: 3041.970703\n",
      "<class 'numpy.ndarray'>\n",
      "Validation Batch: 27 Loss: 3173.664062\n",
      "<class 'numpy.ndarray'>\n",
      "Validation Batch: 28 Loss: 2926.376465\n",
      "<class 'numpy.ndarray'>\n",
      "Validation Batch: 29 Loss: 3024.093994\n",
      "<class 'numpy.ndarray'>\n",
      "Validation Batch: 30 Loss: 3002.834717\n",
      "<class 'numpy.ndarray'>\n",
      "Validation Batch: 31 Loss: 3242.000732\n",
      "<class 'numpy.ndarray'>\n",
      "Validation Batch: 32 Loss: 3019.928955\n",
      "<class 'numpy.ndarray'>\n",
      "Validation Batch: 33 Loss: 3041.328613\n",
      "<class 'numpy.ndarray'>\n",
      "Validation Batch: 34 Loss: 3056.728516\n",
      "<class 'numpy.ndarray'>\n",
      "Validation Batch: 35 Loss: 2964.174316\n",
      "<class 'numpy.ndarray'>\n",
      "Validation Batch: 36 Loss: 3064.226807\n",
      "<class 'numpy.ndarray'>\n",
      "Validation Batch: 37 Loss: 2985.713379\n",
      "<class 'numpy.ndarray'>\n",
      "Validation Batch: 38 Loss: 2998.378906\n",
      "<class 'numpy.ndarray'>\n",
      "Validation Batch: 39 Loss: 3026.623047\n",
      "<class 'numpy.ndarray'>\n",
      "Validation Batch: 40 Loss: 3344.960449\n",
      "<class 'numpy.ndarray'>\n",
      "Validation Batch: 41 Loss: 3111.263916\n",
      "<class 'numpy.ndarray'>\n",
      "Validation Batch: 42 Loss: 2970.323975\n",
      "<class 'numpy.ndarray'>\n",
      "Validation Batch: 43 Loss: 3028.926270\n",
      "<class 'numpy.ndarray'>\n",
      "Validation Batch: 44 Loss: 2929.371582\n",
      "<class 'numpy.ndarray'>\n",
      "Validation Batch: 45 Loss: 3047.763184\n",
      "<class 'numpy.ndarray'>\n",
      "Validation Batch: 46 Loss: 3049.721191\n",
      "<class 'numpy.ndarray'>\n",
      "Validation Batch: 47 Loss: 3269.986572\n",
      "<class 'numpy.ndarray'>\n",
      "Validation Batch: 48 Loss: 3075.144531\n",
      "<class 'numpy.ndarray'>\n",
      "Validation Batch: 49 Loss: 2962.951172\n",
      "<class 'numpy.ndarray'>\n",
      "Validation Batch: 50 Loss: 3030.839600\n",
      "<class 'numpy.ndarray'>\n",
      "Validation Batch: 51 Loss: 3178.869141\n",
      "<class 'numpy.ndarray'>\n",
      "Validation Batch: 52 Loss: 3008.832520\n",
      "<class 'numpy.ndarray'>\n",
      "Validation Batch: 53 Loss: 3056.726074\n",
      "<class 'numpy.ndarray'>\n",
      "Validation Batch: 54 Loss: 3034.260742\n",
      "<class 'numpy.ndarray'>\n",
      "Validation Batch: 55 Loss: 3055.914551\n",
      "<class 'numpy.ndarray'>\n",
      "Validation Batch: 56 Loss: 3078.875000\n",
      "<class 'numpy.ndarray'>\n",
      "Validation Batch: 57 Loss: 3077.897461\n",
      "<class 'numpy.ndarray'>\n",
      "Validation Batch: 58 Loss: 3031.103027\n",
      "<class 'numpy.ndarray'>\n",
      "Validation Batch: 59 Loss: 3016.096191\n",
      "<class 'numpy.ndarray'>\n",
      "Validation Batch: 60 Loss: 3009.203613\n",
      "<class 'numpy.ndarray'>\n",
      "Validation Batch: 61 Loss: 2918.072266\n",
      "<class 'numpy.ndarray'>\n",
      "Validation Batch: 62 Loss: 2894.721191\n",
      "<class 'numpy.ndarray'>\n",
      "Validation Batch: 63 Loss: 3131.400146\n",
      "<class 'numpy.ndarray'>\n",
      "Validation Batch: 64 Loss: 3327.798584\n",
      "<class 'numpy.ndarray'>\n",
      "Validation Batch: 65 Loss: 2949.008301\n",
      "<class 'numpy.ndarray'>\n",
      "Validation Batch: 66 Loss: 3104.868164\n",
      "<class 'numpy.ndarray'>\n",
      "Validation Batch: 67 Loss: 2956.955322\n",
      "<class 'numpy.ndarray'>\n",
      "Validation Batch: 68 Loss: 2921.522461\n",
      "<class 'numpy.ndarray'>\n",
      "Validation Batch: 69 Loss: 3201.843750\n",
      "<class 'numpy.ndarray'>\n",
      "Validation Batch: 70 Loss: 3091.941406\n",
      "<class 'numpy.ndarray'>\n",
      "Validation Batch: 71 Loss: 3202.512695\n",
      "<class 'numpy.ndarray'>\n",
      "Validation Batch: 72 Loss: 3034.532471\n",
      "<class 'numpy.ndarray'>\n",
      "Validation Batch: 73 Loss: 2984.153809\n",
      "<class 'numpy.ndarray'>\n",
      "Validation Batch: 74 Loss: 3022.089844\n",
      "<class 'numpy.ndarray'>\n",
      "Validation Batch: 75 Loss: 3136.831543\n",
      "<class 'numpy.ndarray'>\n",
      "Validation Batch: 76 Loss: 3074.779785\n",
      "<class 'numpy.ndarray'>\n",
      "Validation Batch: 77 Loss: 3043.328613\n",
      "<class 'numpy.ndarray'>\n",
      "Validation Batch: 78 Loss: 2986.055420\n",
      "<class 'numpy.ndarray'>\n",
      "Validation Batch: 79 Loss: 3016.500488\n",
      "<class 'numpy.ndarray'>\n",
      "Validation Batch: 80 Loss: 3058.987305\n",
      "<class 'numpy.ndarray'>\n",
      "Validation Batch: 81 Loss: 3057.681641\n",
      "<class 'numpy.ndarray'>\n",
      "Validation Batch: 82 Loss: 2986.223633\n",
      "<class 'numpy.ndarray'>\n",
      "Validation Batch: 83 Loss: 3136.726074\n",
      "<class 'numpy.ndarray'>\n",
      "Validation Batch: 84 Loss: 3111.520508\n",
      "<class 'numpy.ndarray'>\n",
      "Validation Batch: 85 Loss: 3211.628418\n",
      "<class 'numpy.ndarray'>\n",
      "Validation Batch: 86 Loss: 3006.178223\n",
      "<class 'numpy.ndarray'>\n",
      "Validation Batch: 87 Loss: 3250.474121\n",
      "<class 'numpy.ndarray'>\n",
      "Validation Batch: 88 Loss: 2992.922607\n",
      "<class 'numpy.ndarray'>\n",
      "Validation Batch: 89 Loss: 2999.521973\n",
      "<class 'numpy.ndarray'>\n",
      "Validation Batch: 90 Loss: 3000.369385\n",
      "<class 'numpy.ndarray'>\n",
      "Validation Batch: 91 Loss: 3003.826172\n",
      "<class 'numpy.ndarray'>\n",
      "Validation Batch: 92 Loss: 2989.869141\n",
      "<class 'numpy.ndarray'>\n",
      "Validation Batch: 93 Loss: 2966.813477\n",
      "<class 'numpy.ndarray'>\n",
      "Validation Batch: 94 Loss: 2921.476562\n",
      "<class 'numpy.ndarray'>\n",
      "Validation Batch: 95 Loss: 3208.426758\n",
      "<class 'numpy.ndarray'>\n",
      "Validation Batch: 96 Loss: 3093.900635\n",
      "<class 'numpy.ndarray'>\n",
      "Validation Batch: 97 Loss: 2986.323242\n",
      "<class 'numpy.ndarray'>\n",
      "Validation Batch: 98 Loss: 3013.599121\n",
      "<class 'numpy.ndarray'>\n",
      "Validation Batch: 99 Loss: 3440.777588\n",
      "<class 'numpy.ndarray'>\n",
      "Validation Batch: 100 Loss: 2979.529297\n",
      "<class 'numpy.ndarray'>\n",
      "Validation Batch: 101 Loss: 3014.568604\n",
      "<class 'numpy.ndarray'>\n",
      "Validation Batch: 102 Loss: 3205.882324\n",
      "<class 'numpy.ndarray'>\n",
      "Validation Batch: 103 Loss: 3205.898926\n",
      "<class 'numpy.ndarray'>\n",
      "Validation Batch: 104 Loss: 3175.591309\n",
      "<class 'numpy.ndarray'>\n",
      "Validation Batch: 105 Loss: 3063.250488\n",
      "<class 'numpy.ndarray'>\n",
      "Validation Batch: 106 Loss: 2988.335938\n",
      "<class 'numpy.ndarray'>\n",
      "Validation Batch: 107 Loss: 2999.715820\n",
      "<class 'numpy.ndarray'>\n",
      "Validation Batch: 108 Loss: 3017.094238\n",
      "<class 'numpy.ndarray'>\n",
      "Validation Batch: 109 Loss: 2977.406006\n",
      "<class 'numpy.ndarray'>\n",
      "Validation Batch: 110 Loss: 3321.623047\n",
      "<class 'numpy.ndarray'>\n",
      "Validation Batch: 111 Loss: 3055.103516\n",
      "<class 'numpy.ndarray'>\n",
      "Validation Batch: 112 Loss: 3158.368896\n",
      "<class 'numpy.ndarray'>\n",
      "Validation Batch: 113 Loss: 3177.593994\n",
      "<class 'numpy.ndarray'>\n",
      "Validation Batch: 114 Loss: 2996.096436\n",
      "<class 'numpy.ndarray'>\n",
      "Validation Batch: 115 Loss: 3199.789307\n",
      "<class 'numpy.ndarray'>\n",
      "Validation Batch: 116 Loss: 3300.433594\n",
      "<class 'numpy.ndarray'>\n",
      "Validation Batch: 117 Loss: 2929.410645\n",
      "<class 'numpy.ndarray'>\n",
      "Validation Batch: 118 Loss: 3099.257324\n",
      "<class 'numpy.ndarray'>\n",
      "Validation Batch: 119 Loss: 3012.744141\n",
      "<class 'numpy.ndarray'>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Batch: 120 Loss: 3073.511719\n",
      "<class 'numpy.ndarray'>\n",
      "Validation Batch: 121 Loss: 2986.963379\n",
      "<class 'numpy.ndarray'>\n",
      "Validation Batch: 122 Loss: 2988.987549\n",
      "<class 'numpy.ndarray'>\n",
      "Validation Batch: 123 Loss: 3071.815918\n",
      "<class 'numpy.ndarray'>\n",
      "Validation Batch: 124 Loss: 3211.139648\n",
      "<class 'numpy.ndarray'>\n",
      "Validation Batch: 125 Loss: 3132.654297\n",
      "<class 'numpy.ndarray'>\n",
      "Validation Batch: 126 Loss: 3092.617188\n",
      "<class 'numpy.ndarray'>\n",
      "Validation Batch: 127 Loss: 3078.454834\n",
      "<class 'numpy.ndarray'>\n",
      "Validation Batch: 128 Loss: 3061.311523\n",
      "<class 'numpy.ndarray'>\n",
      "Validation Batch: 129 Loss: 3145.677734\n",
      "<class 'numpy.ndarray'>\n",
      "Validation Batch: 130 Loss: 3065.910645\n",
      "<class 'numpy.ndarray'>\n",
      "Validation Batch: 131 Loss: 3055.240234\n",
      "<class 'numpy.ndarray'>\n",
      "Validation Batch: 132 Loss: 2930.890625\n",
      "<class 'numpy.ndarray'>\n",
      "Validation Batch: 133 Loss: 3020.069336\n",
      "<class 'numpy.ndarray'>\n",
      "Validation Batch: 134 Loss: 2947.734375\n",
      "<class 'numpy.ndarray'>\n",
      "Validation Batch: 135 Loss: 3004.217773\n",
      "<class 'numpy.ndarray'>\n",
      "Validation Batch: 136 Loss: 3019.801758\n",
      "<class 'numpy.ndarray'>\n",
      "Validation Batch: 137 Loss: 3234.162109\n",
      "<class 'numpy.ndarray'>\n",
      "Validation Batch: 138 Loss: 3020.720703\n",
      "<class 'numpy.ndarray'>\n",
      "Validation Batch: 139 Loss: 2969.884277\n",
      "<class 'numpy.ndarray'>\n",
      "Validation Batch: 140 Loss: 2976.199951\n",
      "<class 'numpy.ndarray'>\n",
      "Validation Batch: 141 Loss: 2986.700684\n",
      "<class 'numpy.ndarray'>\n",
      "Validation Batch: 142 Loss: 3135.995605\n",
      "<class 'numpy.ndarray'>\n",
      "Validation Batch: 143 Loss: 3157.767090\n",
      "<class 'numpy.ndarray'>\n",
      "Validation Batch: 144 Loss: 3019.011719\n",
      "<class 'numpy.ndarray'>\n",
      "Validation Batch: 145 Loss: 2976.105713\n",
      "<class 'numpy.ndarray'>\n",
      "Validation Batch: 146 Loss: 2996.708496\n",
      "<class 'numpy.ndarray'>\n",
      "Validation Batch: 147 Loss: 3062.658936\n",
      "<class 'numpy.ndarray'>\n",
      "Validation Batch: 148 Loss: 2955.915039\n",
      "<class 'numpy.ndarray'>\n",
      "Validation Batch: 149 Loss: 3003.569824\n",
      "<class 'numpy.ndarray'>\n",
      "Validation Batch: 150 Loss: 3045.321533\n",
      "<class 'numpy.ndarray'>\n",
      "Validation Batch: 151 Loss: 2962.295410\n",
      "<class 'numpy.ndarray'>\n",
      "Validation Batch: 152 Loss: 2996.372070\n",
      "<class 'numpy.ndarray'>\n",
      "Validation Batch: 153 Loss: 2997.990723\n",
      "<class 'numpy.ndarray'>\n",
      "Validation Batch: 154 Loss: 2970.504883\n",
      "<class 'numpy.ndarray'>\n",
      "Validation Batch: 155 Loss: 3159.177246\n",
      "<class 'numpy.ndarray'>\n",
      "Validation Batch: 156 Loss: 3173.977051\n",
      "<class 'numpy.ndarray'>\n",
      "Validation Batch: 157 Loss: 3275.426270\n",
      "<class 'numpy.ndarray'>\n",
      "Validation Batch: 158 Loss: 2986.644043\n",
      "<class 'numpy.ndarray'>\n",
      "Validation Batch: 159 Loss: 3234.077393\n",
      "<class 'numpy.ndarray'>\n",
      "Validation Batch: 160 Loss: 3006.233643\n",
      "<class 'numpy.ndarray'>\n",
      "Validation Batch: 161 Loss: 3014.178223\n",
      "<class 'numpy.ndarray'>\n",
      "Validation Batch: 162 Loss: 2975.696289\n",
      "<class 'numpy.ndarray'>\n",
      "Validation Batch: 163 Loss: 2944.799316\n",
      "<class 'numpy.ndarray'>\n",
      "Validation Batch: 164 Loss: 3142.979004\n",
      "<class 'numpy.ndarray'>\n",
      "Validation Batch: 165 Loss: 3114.923828\n",
      "<class 'numpy.ndarray'>\n",
      "Validation Batch: 166 Loss: 2985.845947\n",
      "<class 'numpy.ndarray'>\n",
      "Validation Batch: 167 Loss: 3409.853027\n",
      "<class 'numpy.ndarray'>\n",
      "Validation Batch: 168 Loss: 3071.047363\n",
      "<class 'numpy.ndarray'>\n",
      "Validation Batch: 169 Loss: 3075.035156\n",
      "<class 'numpy.ndarray'>\n",
      "Validation Batch: 170 Loss: 3070.276367\n",
      "<class 'numpy.ndarray'>\n",
      "Validation Batch: 171 Loss: 3067.341797\n",
      "<class 'numpy.ndarray'>\n",
      "Validation Batch: 172 Loss: 3003.139893\n",
      "<class 'numpy.ndarray'>\n",
      "Validation Batch: 173 Loss: 3138.519043\n",
      "<class 'numpy.ndarray'>\n",
      "Validation Batch: 174 Loss: 2942.421387\n",
      "<class 'numpy.ndarray'>\n",
      "Validation Batch: 175 Loss: 3027.488037\n",
      "<class 'numpy.ndarray'>\n",
      "Validation Batch: 176 Loss: 3191.103760\n",
      "<class 'numpy.ndarray'>\n",
      "Validation Batch: 177 Loss: 2974.974609\n",
      "<class 'numpy.ndarray'>\n",
      "Validation Batch: 178 Loss: 3070.922852\n",
      "<class 'numpy.ndarray'>\n",
      "Validation Batch: 179 Loss: 3032.380859\n",
      "<class 'numpy.ndarray'>\n",
      "Validation Batch: 180 Loss: 3010.372559\n",
      "<class 'numpy.ndarray'>\n",
      "Validation Batch: 181 Loss: 3088.595947\n",
      "<class 'numpy.ndarray'>\n",
      "Validation Batch: 182 Loss: 3161.594482\n",
      "<class 'numpy.ndarray'>\n",
      "Validation Batch: 183 Loss: 3060.901367\n",
      "<class 'numpy.ndarray'>\n",
      "Validation Batch: 184 Loss: 2960.834473\n",
      "<class 'numpy.ndarray'>\n",
      "Validation Batch: 185 Loss: 2999.861816\n",
      "<class 'numpy.ndarray'>\n",
      "Validation Batch: 186 Loss: 2924.491699\n",
      "<class 'numpy.ndarray'>\n",
      "Validation Batch: 187 Loss: 3136.218262\n",
      "<class 'numpy.ndarray'>\n",
      "Validation Batch: 188 Loss: 3071.648926\n",
      "<class 'numpy.ndarray'>\n",
      "Validation Batch: 189 Loss: 3085.558105\n",
      "<class 'numpy.ndarray'>\n",
      "Validation Batch: 190 Loss: 2941.024414\n",
      "<class 'numpy.ndarray'>\n",
      "Validation Batch: 191 Loss: 3113.612549\n",
      "<class 'numpy.ndarray'>\n",
      "Validation Batch: 192 Loss: 3070.553223\n",
      "<class 'numpy.ndarray'>\n",
      "Validation Batch: 193 Loss: 3072.356689\n",
      "<class 'numpy.ndarray'>\n",
      "Validation Batch: 194 Loss: 3164.879395\n",
      "<class 'numpy.ndarray'>\n",
      "Validation Batch: 195 Loss: 3032.963623\n",
      "<class 'numpy.ndarray'>\n",
      "Validation Batch: 196 Loss: 3053.961182\n",
      "<class 'numpy.ndarray'>\n",
      "Validation Batch: 197 Loss: 2973.242676\n",
      "<class 'numpy.ndarray'>\n",
      "Validation Batch: 198 Loss: 3014.376709\n",
      "<class 'numpy.ndarray'>\n",
      "Validation Batch: 199 Loss: 2988.627930\n",
      "<class 'numpy.ndarray'>\n",
      "Validation Batch: 200 Loss: 3052.854004\n",
      "<class 'numpy.ndarray'>\n",
      "Validation Batch: 201 Loss: 2982.647949\n",
      "<class 'numpy.ndarray'>\n",
      "Validation Batch: 202 Loss: 3041.262451\n",
      "<class 'numpy.ndarray'>\n",
      "Validation Batch: 203 Loss: 3091.052734\n",
      "<class 'numpy.ndarray'>\n",
      "Validation Batch: 204 Loss: 3297.488770\n",
      "<class 'numpy.ndarray'>\n",
      "Validation Batch: 205 Loss: 3072.359863\n",
      "<class 'numpy.ndarray'>\n",
      "Validation Batch: 206 Loss: 3222.795898\n",
      "<class 'numpy.ndarray'>\n",
      "Validation Batch: 207 Loss: 3081.742188\n",
      "<class 'numpy.ndarray'>\n",
      "Validation Batch: 208 Loss: 3083.022461\n",
      "<class 'numpy.ndarray'>\n",
      "Validation Batch: 209 Loss: 3029.009766\n",
      "<class 'numpy.ndarray'>\n",
      "Validation Batch: 210 Loss: 3085.527832\n",
      "<class 'numpy.ndarray'>\n",
      "Validation Batch: 211 Loss: 2975.537109\n",
      "<class 'numpy.ndarray'>\n",
      "Validation Batch: 212 Loss: 3046.935303\n",
      "<class 'numpy.ndarray'>\n",
      "Validation Batch: 213 Loss: 2933.921631\n",
      "<class 'numpy.ndarray'>\n",
      "Validation Batch: 214 Loss: 3002.684570\n",
      "<class 'numpy.ndarray'>\n",
      "Validation Batch: 215 Loss: 3012.621582\n",
      "<class 'numpy.ndarray'>\n",
      "Validation Batch: 216 Loss: 2997.604980\n",
      "<class 'numpy.ndarray'>\n",
      "Validation Batch: 217 Loss: 3038.043945\n",
      "<class 'numpy.ndarray'>\n",
      "Validation Batch: 218 Loss: 2921.563965\n",
      "<class 'numpy.ndarray'>\n",
      "Validation Batch: 219 Loss: 3100.715576\n",
      "<class 'numpy.ndarray'>\n",
      "Validation Batch: 220 Loss: 3000.771729\n",
      "<class 'numpy.ndarray'>\n",
      "Validation Batch: 221 Loss: 3057.827148\n",
      "<class 'numpy.ndarray'>\n",
      "Validation Batch: 222 Loss: 3064.848877\n",
      "<class 'numpy.ndarray'>\n",
      "Validation Batch: 223 Loss: 3042.984375\n",
      "<class 'numpy.ndarray'>\n",
      "Validation Batch: 224 Loss: 3049.719727\n",
      "<class 'numpy.ndarray'>\n",
      "Validation Batch: 225 Loss: 2999.992676\n",
      "<class 'numpy.ndarray'>\n",
      "Validation Batch: 226 Loss: 2992.774414\n",
      "<class 'numpy.ndarray'>\n",
      "Validation Batch: 227 Loss: 3039.193848\n",
      "<class 'numpy.ndarray'>\n",
      "Validation Batch: 228 Loss: 3032.674561\n",
      "<class 'numpy.ndarray'>\n",
      "Validation Batch: 229 Loss: 3179.513672\n",
      "<class 'numpy.ndarray'>\n",
      "Validation Batch: 230 Loss: 3057.446045\n",
      "<class 'numpy.ndarray'>\n",
      "Validation Batch: 231 Loss: 3015.265381\n",
      "<class 'numpy.ndarray'>\n",
      "Validation Batch: 232 Loss: 3111.253906\n",
      "<class 'numpy.ndarray'>\n",
      "Validation Batch: 233 Loss: 3064.146973\n",
      "<class 'numpy.ndarray'>\n",
      "Validation Batch: 234 Loss: 3014.297363\n",
      "<class 'numpy.ndarray'>\n",
      "Validation Batch: 235 Loss: 3171.790039\n",
      "<class 'numpy.ndarray'>\n",
      "Validation Batch: 236 Loss: 2979.622314\n",
      "<class 'numpy.ndarray'>\n",
      "Validation Batch: 237 Loss: 3016.375977\n",
      "<class 'numpy.ndarray'>\n",
      "Validation Batch: 238 Loss: 3029.680420\n",
      "<class 'numpy.ndarray'>\n",
      "Validation Batch: 239 Loss: 3035.297363\n",
      "<class 'numpy.ndarray'>\n",
      "Validation Batch: 240 Loss: 3085.044922\n",
      "<class 'numpy.ndarray'>\n",
      "Validation Batch: 241 Loss: 3035.913086\n",
      "<class 'numpy.ndarray'>\n",
      "Validation Batch: 242 Loss: 3193.346680\n",
      "<class 'numpy.ndarray'>\n",
      "Validation Batch: 243 Loss: 2970.261719\n",
      "<class 'numpy.ndarray'>\n",
      "Validation Batch: 244 Loss: 3080.842285\n",
      "<class 'numpy.ndarray'>\n",
      "Validation Batch: 245 Loss: 3020.939941\n",
      "<class 'numpy.ndarray'>\n",
      "Validation Batch: 246 Loss: 2979.396729\n",
      "<class 'numpy.ndarray'>\n",
      "Validation Batch: 247 Loss: 2979.390869\n",
      "<class 'numpy.ndarray'>\n",
      "Validation Batch: 248 Loss: 3220.599121\n",
      "<class 'numpy.ndarray'>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Batch: 249 Loss: 3030.142090\n",
      "<class 'numpy.ndarray'>\n",
      "Validation Batch: 250 Loss: 3041.422119\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(2):\n",
    "    transformation_net.train()\n",
    "    for batch, _ in enumerate(range(0, MAX_TRAIN, BATCH_SIZE)):\n",
    "        # The content batch is the same as the train batch, except train batch has noise added to it\n",
    "        train_batch = load_training_batch(batch, BATCH_SIZE, 'train')\n",
    "        content_batch = np.copy(train_batch)\n",
    "\n",
    "        # Add noise to the training batch\n",
    "        train_batch = add_noise(train_batch)\n",
    "\n",
    "        # Convert the batches to tensors\n",
    "        train_batch = torch.from_numpy(train_batch).float()\n",
    "        content_batch = torch.from_numpy(content_batch).float()\n",
    "\n",
    "        # Zero the gradients\n",
    "        opt.zero_grad()\n",
    "\n",
    "        # Forward propagate\n",
    "        gen_images = transformation_net(train_batch)\n",
    "\n",
    "        # Compute loss\n",
    "        loss = total_cost(gen_images, [content_batch, STYLE_IMAGE_TENSOR])\n",
    "\n",
    "        # Backprop\n",
    "        loss.backward()\n",
    "\n",
    "        # Clip the gradient to minimize chance of exploding gradients\n",
    "        torch.nn.utils.clip_grad_norm_(transformation_net.parameters(), 1.0)\n",
    "\n",
    "        # Apply gradients\n",
    "        opt.step()\n",
    "\n",
    "        print(\"Training Batch: {}\".format(batch + 1), \"Loss: {:f}\".format(loss))\n",
    "    \n",
    "    transformation_net.eval()\n",
    "    for batch, _ in enumerate(range(MAX_TRAIN, MAX_VAL, BATCH_SIZE)):\n",
    "        # The content batch is the same as the train batch, except train batch has noise added to it\n",
    "        val_batch = load_training_batch(batch, BATCH_SIZE, 'val')\n",
    "        content_batch = np.copy(val_batch)\n",
    "        \n",
    "        # Add noise to the training batch\n",
    "        val_batch = add_noise(val_batch)\n",
    "        \n",
    "        # Convert the batches to tensors\n",
    "        val_batch = torch.from_numpy(val_batch).float()\n",
    "        content_batch = torch.from_numpy(content_batch).float()\n",
    "        \n",
    "        # Forward propagate\n",
    "        gen_images = transformation_net(val_batch)\n",
    "\n",
    "        # Compute loss\n",
    "        loss = total_cost(gen_images, [content_batch, STYLE_IMAGE_TENSOR])\n",
    "        \n",
    "        print(\"Validation Batch: {}\".format(batch + 1), \"Loss: {:f}\".format(loss))\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(transformation_net.state_dict(), 'african_style.pt')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
