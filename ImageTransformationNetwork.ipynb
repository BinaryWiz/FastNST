{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision.models import vgg16\n",
    "from torchvision import transforms\n",
    "import numpy as np\n",
    "from collections import namedtuple"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Image Transformation Network\n",
    "This is based on Justin C. Johnson's paper, [Perceptual Losses for Real-Time Style Transfer and Super-Resolution](https://cs.stanford.edu/people/jcjohns/papers/eccv16/JohnsonECCV16.pdf).\n",
    "Here, we are going to make the proposed network for generating the end image (as in the convolutional neural network the will learn weights in order to efficiently calculate how to create an image based on a style)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Architecture\n",
    "![Architecture](./images/transformation-network-table.PNG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        \n",
    "        # For the transformation network, the authors only used 3x3 convolutions\n",
    "        self.conv = nn.Conv2d(in_channels = self.in_channels,\n",
    "                               out_channels = self.out_channels,\n",
    "                               kernel_size = 3)\n",
    "        self.batch_norm = nn.InstanceNorm2d(self.out_channels, affine=True)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # First convolution\n",
    "        orig_x = x.clone()\n",
    "        x = self.conv(x)\n",
    "        x = self.batch_norm(x)\n",
    "        x = self.relu(x)\n",
    "        \n",
    "        # Second convolution\n",
    "        x = self.conv(x)\n",
    "        x = self.batch_norm(x)\n",
    "        \n",
    "        # Now add the original to the new one (and use center cropping)\n",
    "        # Calulate the different between the size of each feature (in terms \n",
    "        # of height/width) to get the center of the original feature\n",
    "        height_diff = orig_x.size()[2] - x.size()[2]\n",
    "        width_diff = orig_x.size()[3] - x.size()[3]\n",
    "        \n",
    "        # Add the original to the new (complete the residual block)\n",
    "        x = x + orig_x[:, :,\n",
    "                                 height_diff//2:(orig_x.size()[2] - height_diff//2), \n",
    "                                 width_diff//2:(orig_x.size()[3] - width_diff//2)]\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageTransformationNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ImageTransformationNetwork, self).__init__()\n",
    "        # Use reflection padding to keep the end shape\n",
    "        self.ref_pad = nn.ReflectionPad2d(40)\n",
    "        \n",
    "        # Initial convolutions\n",
    "        self.conv1 = nn.Conv2d(in_channels = 3,\n",
    "                               out_channels = 32,\n",
    "                               kernel_size = 9,\n",
    "                               padding = 6,\n",
    "                               padding_mode = 'reflect')\n",
    "        \n",
    "        self.conv2 = nn.Conv2d(in_channels = 32,\n",
    "                               out_channels = 64,\n",
    "                               kernel_size = 3,\n",
    "                               stride = 2)\n",
    "        \n",
    "        self.conv3 = nn.Conv2d(in_channels = 64,\n",
    "                               out_channels = 128,\n",
    "                               kernel_size = 3,\n",
    "                               stride = 2)\n",
    "        \n",
    "        # Residual Blocks\n",
    "        self.resblock1 = ResidualBlock(in_channels = 128,\n",
    "                                       out_channels = 128)\n",
    "        \n",
    "        self.resblock2 = ResidualBlock(in_channels = 128,\n",
    "                                       out_channels = 128)\n",
    "        \n",
    "        self.resblock3 = ResidualBlock(in_channels = 128,\n",
    "                                       out_channels = 128)\n",
    "        \n",
    "        self.resblock4 = ResidualBlock(in_channels = 128,\n",
    "                                       out_channels = 128)\n",
    "        \n",
    "        self.resblock5 = ResidualBlock(in_channels = 128,\n",
    "                                       out_channels = 128)\n",
    "        \n",
    "        # Transpose convoltutions\n",
    "        self.trans_conv1 = nn.ConvTranspose2d(in_channels=128,\n",
    "                                             out_channels=64,\n",
    "                                             kernel_size=2,\n",
    "                                             stride=2)\n",
    "        \n",
    "        self.trans_conv2 = nn.ConvTranspose2d(in_channels=64,\n",
    "                                              out_channels=32,\n",
    "                                              kernel_size=2,\n",
    "                                              stride=2)\n",
    "        \n",
    "        # End with one last convolution\n",
    "        self.conv4 = nn.Conv2d(in_channels = 32,\n",
    "                               out_channels = 3,\n",
    "                               kernel_size = 9,\n",
    "                               padding = 4,\n",
    "                               padding_mode = 'reflect')\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Apply reflection padding\n",
    "        x = self.ref_pad(x)\n",
    "        \n",
    "        # Apply the initial convolutions\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.conv3(x)\n",
    "        \n",
    "        # Apply the residual blocks\n",
    "        x = self.resblock1(x)\n",
    "        x = self.resblock2(x)\n",
    "        x = self.resblock3(x)\n",
    "        x = self.resblock4(x)\n",
    "        x = self.resblock5(x)        \n",
    "        \n",
    "        #  Apply the transpose convolutions\n",
    "        x = self.trans_conv1(x)\n",
    "        x = self.trans_conv2(x)\n",
    "        \n",
    "        # Apply the final convolution\n",
    "        x = self.conv4(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 128, 80, 80])\n"
     ]
    }
   ],
   "source": [
    "# Test to confirm the residual network works\n",
    "resblock = ResidualBlock(128, 128)\n",
    "test = torch.randn(2, 128, 84, 84)\n",
    "out = resblock(test)\n",
    "print(out.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3, 256, 256])\n"
     ]
    }
   ],
   "source": [
    "# Test to confirm the transormational network works\n",
    "transformation_net = ImageTransformationNetwork()\n",
    "test = torch.randn(2, 3, 256, 256)\n",
    "out = transformation_net(test)\n",
    "print(out.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cost Functions\n",
    "Now we must implement the different cost functions\n",
    "\n",
    "Style Relu Indices: 3, 8, 15, 22  \n",
    "Content Relu Index: 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [],
   "source": [
    "def squared_norm(matrix):\n",
    "    \"\"\"\n",
    "    Computes the squared normalization given a matrix\n",
    "    \"\"\"\n",
    "    batches, channels, height, width = matrix.size()\n",
    "    return (1/(channels * height * width)) * torch.square(matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gram(matrix):\n",
    "    \"\"\"\n",
    "    Computes the gram matrix\n",
    "    \"\"\"\n",
    "    batches, channels, height, width = matrix.size()\n",
    "    return (1/(channels * height * width)) * (torch.matmul(matrix.view(batches, channels, -1),\n",
    "            torch.transpose(matrix.view(batches, channels, -1), 1, 2)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_fro_norm(matrix):\n",
    "    \"\"\"\n",
    "    Computes the frobenius norm\n",
    "    \"\"\"\n",
    "    batches, height, width = matrix.size()\n",
    "    return torch.norm(matrix.view(batches, 1, -1), 'fro', dim=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VGG19(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(VGG19, self).__init__()\n",
    "        features = list(vgg19(pretrained=True).features)\n",
    "        self.features = nn.ModuleList(features).eval()\n",
    "        \n",
    "    def forward(self, x, style):\n",
    "        results = []\n",
    "        for i, model in enumerate(self.features):\n",
    "            x = model(x)\n",
    "            if style:\n",
    "                if i in {3, 8, 15, 22}:\n",
    "                    results.append(x)\n",
    "            \n",
    "            else:\n",
    "                if i == 15:\n",
    "                    results.append(x)\n",
    "        \n",
    "        return results\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Tensor' object has no attribute 'features'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-10-b23be2fa6532>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mtest\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m224\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m244\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mvgg16\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpretrained\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m: 'Tensor' object has no attribute 'features'"
     ]
    }
   ],
   "source": [
    "test = torch.randn(4, 3, 224, 244)\n",
    "vgg16(pretrained = True)(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = VGG19()\n",
    "len(net(test, False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
