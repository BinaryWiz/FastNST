{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "\n",
    "from src.image_transformations import add_noise, normalize_batch\n",
    "from src.common import Common\n",
    "from src.plotting_images import plot_losses\n",
    "import scipy.ndimage\n",
    "import scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_DIM = (1200, 1600)\n",
    "LOW_IMG_DIM = (400, 534)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Content image\n",
    "input_img = np.asarray(Image.open('data/mya_img.jpg').resize(IMG_DIM))\n",
    "\n",
    "# Style image\n",
    "style_img = np.asarray(Image.open('data/anime.jpg').resize(IMG_DIM))\n",
    "\n",
    "print(input_img.shape)\n",
    "print(style_img.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(input_img)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(style_img)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using coarse-to-fine image stylization\n",
    "low_input_img = np.asarray(Image.fromarray(input_img).resize(LOW_IMG_DIM))\n",
    "low_style_img = np.asarray(Image.fromarray(style_img).resize(LOW_IMG_DIM))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(low_input_img)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(low_style_img)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mse = torch.nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HookModule(nn.Module):\n",
    "    def __init__(self, model):\n",
    "        super(HookModule, self).__init__()\n",
    "        self.model = model\n",
    "        self.style_hooks = {}\n",
    "        self.content_hooks = {}\n",
    "        self.style_layers = ['vgg_19_conv1_conv1_1_Conv2D', 'vgg_19_conv2_conv2_1_Conv2D', 'vgg_19_conv3_conv3_1_Conv2D', 'vgg_19_conv4_conv4_1_Conv2D', 'vgg_19_conv5_conv5_1_Conv2D']\n",
    "        self.content_layers = ['vgg_19_conv4_conv4_3_Conv2D']\n",
    "        self.content = []\n",
    "        self.style = []\n",
    "    \n",
    "    def forward(self, x):\n",
    "        self.reinit_hooks()\n",
    "        self.content = []\n",
    "        self.style = []\n",
    "        self.model(x)\n",
    "        with torch.no_grad():\n",
    "            for k, v in self.style_hooks.items():\n",
    "                self.style_hooks[k].remove()\n",
    "            for k, v in self.content_hooks.items():\n",
    "                self.content_hooks[k].remove()\n",
    "        \n",
    "        return self.content, self.style\n",
    "    \n",
    "    \n",
    "    def reinit_hooks(self):\n",
    "        for name, module in self.model.named_modules():\n",
    "            if name in self.style_layers:\n",
    "                self.style_hooks[name] = module.register_forward_hook(self.style_hook)\n",
    "            \n",
    "            if name in self.content_layers:\n",
    "                self.content_hooks[name] = module.register_forward_hook(self.content_hook)\n",
    "    \n",
    "    def style_hook(self, module, input, output):\n",
    "        self.style.append(output)\n",
    "        \n",
    "    def content_hook(self, module, input, output):\n",
    "        self.content.append(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def denormalize_batch(batch):\n",
    "    vgg_means = [103.939, 116.779, 123.68]\n",
    "    ret = torch.zeros(*batch.size())\n",
    "    ret[:, 0, :, :] = batch[:, 0, :, :] + vgg_means[0]\n",
    "    ret[:, 1, :, :] = batch[:, 1, :, :] + vgg_means[1]\n",
    "    ret[:, 2, :, :] = batch[:, 2, :, :] + vgg_means[2]\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_img(ten):\n",
    "    img = ten.detach().numpy()\n",
    "    img = img[0].transpose(1, 2, 0).astype('uint8')\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.imshow(img)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(img):\n",
    "    img = np.expand_dims(img.transpose(2, 0, 1), axis=0)\n",
    "    img = torch.from_numpy(np.copy(img)).float()\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Common.forward_vgg.features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gram(matrix):\n",
    "    '''\n",
    "    Computes the gram matrix\n",
    "    '''\n",
    "    batches, channels, height, width = matrix.size()\n",
    "    matrix = matrix.view(channels, height * width)\n",
    "    return (1 / (channels * height * width)) * torch.mm(matrix, matrix.t())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def content_cost(input, target):\n",
    "    # First normalize both the input and target (preprocess for VGG16)\n",
    "    input_norm = normalize_batch(input)\n",
    "    target_norm = normalize_batch(target)\n",
    "\n",
    "    input_layers = Common.forward_vgg(input_norm, [26])\n",
    "    target_layers = Common.forward_vgg(target_norm, [26])\n",
    "    \n",
    "    accumulated_loss = 0\n",
    "    for layer in range(len(input_layers)):\n",
    "        batch, channels, height, width = input_layers[layer].size()\n",
    "        accumulated_loss = accumulated_loss + mse(input_layers[layer].view(channels, -1),\n",
    "                                                   target_layers[layer].view(channels, -1))\n",
    "    \n",
    "    return accumulated_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def style_cost(input, target):\n",
    "    # First normalize both the input and target (preprocess for VGG16)\n",
    "    input_norm = normalize_batch(input)\n",
    "    target_norm = normalize_batch(target)\n",
    "\n",
    "    input_layers = Common.forward_vgg(input_norm, [3, 8, 17, 26, 35])\n",
    "    target_layers = Common.forward_vgg(target_norm, [3, 8, 17, 26, 35])\n",
    "    \n",
    "    # layer weights\n",
    "    #layer_weights = [1.5, 1.5, 0.55, 0.33, 0.22, 0.11]\n",
    "    layer_weights = [0.2, 0.2, 0.2, 0.5, 0.5]\n",
    "    # The accumulated losses for the style\n",
    "    accumulated_loss = 0\n",
    "    \n",
    "    for layer in range(len(input_layers)):\n",
    "        batch, channels, height, width = input_layers[layer].size()\n",
    "        accumulated_loss = accumulated_loss + layer_weights[layer] * mse(compute_gram(input_layers[layer]),\n",
    "                                                                         compute_gram(target_layers[layer]))\n",
    "    \n",
    "    return accumulated_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def total_variation_cost(input):\n",
    "    tvloss = (\n",
    "        torch.sum(torch.abs(input[:, :, :, :-1] - input[:, :, :, 1:])) + \n",
    "        torch.sum(torch.abs(input[:, :, :-1, :] - input[:, :, 1:, :]))\n",
    "    )\n",
    "    return tvloss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def total_cost(input, targets):    \n",
    "    # Extract content and style images\n",
    "    content, style = targets\n",
    "    \n",
    "    REG_CONTENT = 1.0e5\n",
    "    REG_STYLE = 4e12\n",
    "    REG_TV = 3e-5\n",
    "    \n",
    "    # Get the content, style and tv variation losses\n",
    "    closs = content_cost(input, content) * REG_CONTENT\n",
    "    sloss = style_cost(input, style) * REG_STYLE\n",
    "    tvloss = total_variation_cost(input) * REG_TV\n",
    "        \n",
    "    # Add it to the running list of losses\n",
    "    Common.content_losses.append(closs)\n",
    "    Common.style_losses.append(sloss)\n",
    "    Common.tv_losses.append(tvloss)\n",
    "    \n",
    "    print('****************************')\n",
    "    print('Content Loss: {}'.format(closs.item()))\n",
    "    print('Style Loss: {}'.format(sloss.item()))\n",
    "    print('Total Variation Loss: {}'.format(tvloss.item()))\n",
    "    \n",
    "    return closs + sloss + tvloss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def upsample(img):\n",
    "    init_img = Image.fromarray(img[0].transpose(1, 2, 0).astype('uint8'))\n",
    "    init_img = np.asarray(init_img.resize(IMG_DIM, resample=0))\n",
    "    return preprocess(init_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_img(img, file_name):\n",
    "    img = img[0].transpose(1, 2, 0)\n",
    "    img = img.astype('uint8')\n",
    "    img = Image.fromarray(img)\n",
    "    img.save('generated_images/' + file_name + '.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PrecomputedStyle(torch.nn.Module):\n",
    "    def __init__(self, style):\n",
    "        super(PrecomputedStyle, self).__init__()\n",
    "        style = normalize_batch(style)\n",
    "        self.vgg = Common.forward_vgg(style, [3, 8, 15, 22])\n",
    "        self.precomputed = []\n",
    "        for x in self.vgg:\n",
    "            self.precomputed.append(compute_gram(x))\n",
    "            \n",
    "    def forward(self):\n",
    "        ret = []\n",
    "        for x in self.precomputed:\n",
    "            ret.append(torch.clone(x))\n",
    "        return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape the images\n",
    "low_style_img_ten = preprocess(low_style_img)\n",
    "low_input_img_ten = preprocess(low_input_img)\n",
    "low_content_img_ten = preprocess(low_input_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape the images\n",
    "style_img_ten = preprocess(style_img)\n",
    "input_img_ten = preprocess(input_img)\n",
    "content_img_ten = preprocess(input_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure the sizes are right\n",
    "print(style_img_ten.size())\n",
    "print(content_img_ten.size())\n",
    "print(input_img_ten.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#low_input_img_ten = torch.ones(3, 534, 400).mul(130).unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure the sizes are right\n",
    "print(low_style_img_ten.size())\n",
    "print(low_content_img_ten.size())\n",
    "print(low_input_img_ten.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "low_input_img_ten.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(init_img, content_img, style_img, opt):\n",
    "    for epoch in range(5):\n",
    "        for batch in range(100):\n",
    "            # Skip what we've already done\n",
    "            if epoch == 0 and batch < 0:\n",
    "                continue\n",
    "\n",
    "            # Zero the gradients\n",
    "            opt.zero_grad()\n",
    "\n",
    "            # Compute loss\n",
    "            loss = total_cost(init_img, [content_img, style_img])\n",
    "\n",
    "            # Backprop\n",
    "            loss.backward()\n",
    "\n",
    "            # Apply gradients\n",
    "            opt.step()\n",
    "\n",
    "            # Make sure the values are not more than 255 or less than 0\n",
    "            init_img.data.clamp_(0, 255)\n",
    "\n",
    "            # Every 20 batches, show the loss graphs and the image so far\n",
    "            if (batch % 20 == 19):\n",
    "                #plot_losses()\n",
    "                plot_img(init_img)\n",
    "                plt.show()\n",
    "\n",
    "            print(\"Epoch: {} Training Batch: {}\".format(epoch + 1, batch + 1), \"Loss: {:f}\".format(loss))\n",
    "            print('****************************')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "opt = optim.Adam([low_input_img_ten], lr=2.0)\n",
    "train(low_input_img_ten, low_content_img_ten, low_style_img_ten, opt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_img(low_input_img_ten.detach().numpy(), 'mya_anime_low_res')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_img = upsample(low_input_img_ten.detach().numpy()).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_img.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "opt = optim.Adam([init_img], lr=2.0)\n",
    "train(init_img, content_img_ten, style_img_ten, opt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_img(init_img.detach().numpy(), 'mya_anime_high_res')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plot_img(low_style_img_ten)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
